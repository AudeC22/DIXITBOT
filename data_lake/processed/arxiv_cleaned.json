{
  "source": "arXiv",
  "query": "multimodal transformer",
  "count": 5,
  "papers": [
    {
      "title": "BenchSeg: A Large-Scale Dataset and Benchmark for Multi-View Food Video Segmentation",
      "abstract": "Food image segmentation is a critical task for dietary analysis, enabling accurate estimation of food volume and nutrients. However, current methods suffer from limited multi-view data and poor generalization to new viewpoints. We introduce BenchSeg, a novel multi-view food video segmentation dataset and benchmark. BenchSeg aggregates 55 dish scenes (from Nutrition5k, Vegetables & Fruits, MetaFood3D, and FoodKit) with 25,284 meticulously annotated frames, capturing each dish under free 360° camera motion. We evaluate a diverse set of 20 state-of-the-art segmentation models (e.g., SAM-based, transformer , CNN, and large multimodal ) on the existing FoodSeg103 dataset and evaluate them (alone and combined with video-memory modules) on BenchSeg. Quantitative and qualitative results demonstrate that while standard image segmenters degrade sharply under novel viewpoints, memory-augmented methods maintain temporal consistency across frames. Our best model based on a combination of SeTR-MLA+XMem2 outperforms prior work (e.g., improving over FoodMem by ~2.63% mAP), offering new insights into food segmentation and tracking for dietary analysis. We release BenchSeg to foster future research.",
      "authors": [
        "Ahmad AlMughrabi",
        "Guillermo Rivo",
        "Carlos Jiménez-Farfán",
        "Umair Haroon",
        "Farid Al-Areqi",
        "Hyunjun Jung",
        "Benjamin Busam",
        "Ricardo Marques",
        "Petia Radeva"
      ],
      "submitted_date": "12 January, 2026",
      "source_url": "https://arxiv.org/abs/2601.07581"
    },
    {
      "title": "Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions",
      "abstract": "Vision-language models are increasingly employed as multimodal conversational agents (MCAs) for diverse conversational tasks. Recently, reinforcement learning (RL) has been widely explored for adapting MCAs to various human-AI interaction scenarios. Despite showing great enhancement in generalization performance, fine-tuning MCAs via RL still faces challenges in handling the extremely large text token space. To address this, we learn a compact latent action space for RL fine-tuning instead. Specifically, we adopt the learning from observation mechanism to construct the codebook for the latent action space, where future observations are leveraged to estimate current latent actions that could further be used to reconstruct future observations. However, the scarcity of paired image-text data hinders learning a codebook with sufficient coverage. Thus, we leverage both paired image-text data and text-only data to construct the latent action space, using a cross-modal projector for transforming text embeddings into image-text embeddings. We initialize the cross-modal projector on paired image-text data, and further train it on massive text-only data with a novel cycle consistency loss to",
      "authors": [
        "Yongqi Li",
        "Hao Lang",
        "Tieyun Qian",
        "Yongbin Li"
      ],
      "submitted_date": "12 January, 2026",
      "source_url": "https://arxiv.org/abs/2601.07516"
    },
    {
      "title": "Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges",
      "abstract": "This paper presents a comprehensive survey of sentiment analysis methods for movie reviews, a benchmark task that has played a central role in advancing natural language processing. We review the evolution of techniques from early lexicon-based and classical machine learning approaches to modern deep learning architectures and large language models, covering widely used datasets such as IMDb, Rotten Tomatoes, and SST-2, and models ranging from Naive Bayes and support vector machines to LSTM networks, BERT, and attention-based transformers . Beyond summarizing prior work, this survey differentiates itself by offering a comparative, challenge-driven analysis of how these modeling paradigms address domain-specific issues such as sarcasm, negation, contextual ambiguity, and domain shift, which remain open problems in existing literature. Unlike earlier reviews that focus primarily on text-only pipelines, we also synthesize recent advances in multimodal sentiment analysis that integrate textual, audio, and visual cues from movie trailers and clips. In addition, we examine emerging concerns related to interpretability, fairness, and robustness that are often underexplored in prior survey",
      "authors": [
        "Agnivo Gosai",
        "Shuvodeep De",
        "Karun Thankachan"
      ],
      "submitted_date": "12 January, 2026",
      "source_url": "https://arxiv.org/abs/2601.07235"
    },
    {
      "title": "MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation",
      "abstract": "Most existing 3D referring expression segmentation (3DRES) methods rely on dense, high-quality point clouds, while real-world agents such as robots and mobile phones operate with only a few sparse RGB views and strict latency constraints. We introduce Multi-view 3D Referring Expression Segmentation (MV-3DRES), where the model must recover scene structure and segment the referred object directly from sparse multi-view images. Traditional two-stage pipelines, which first reconstruct a point cloud and then perform segmentation, often yield low-quality geometry, produce coarse or degraded target regions, and run slowly. We propose the Multimodal Visual Geometry Grounded Transformer (MVGGT), an efficient end-to-end framework that integrates language information into sparse-view geometric reasoning through a dual-branch design. Training in this setting exposes a critical optimization barrier, termed Foreground Gradient Dilution (FGD), where sparse 3D signals lead to weak supervision. To resolve this, we introduce Per-view No-target Suppression Optimization (PVSO), which provides stronger and more balanced gradients across views, enabling stable and efficient learning. To support consiste",
      "authors": [
        "Changli Wu",
        "Haodong Wang",
        "Jiayi Ji",
        "Yutian Yao",
        "Chunsai Du",
        "Jihua Kang",
        "Yanwei Fu",
        "Liujuan Cao"
      ],
      "submitted_date": "11 January, 2026",
      "source_url": "https://arxiv.org/abs/2601.06874"
    },
    {
      "title": "MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data",
      "abstract": "Vision-Language Models (VLMs) can generate convincing clinical narratives, yet frequently struggle to visually ground their statements. We posit this limitation arises from the scarcity of high-quality, large-scale clinical referring-localization pairs. To address this, we introduce MedGround, an automated pipeline that transforms segmentation resources into high-quality medical referring grounding data. Leveraging expert masks as spatial anchors, MedGround precisely derives localization targets, extracts shape and spatial cues, and guides VLMs to synthesize natural, clinically grounded queries that reflect morphology and location. To ensure data rigor, a multi-stage verification system integrates strict formatting checks, geometry- and medical-prior rules, and image-based visual judging to filter out ambiguous or visually unsupported samples. Finally, we present MedGround-35K, a novel multimodal medical dataset. Extensive experiments demonstrate that VLMs trained with MedGround-35K consistently achieve improved referring grounding performance, enhance multi-object semantic disambiguation, and exhibit strong generalization to unseen grounding settings. This work highlights MedGroun",
      "authors": [
        "Mengmeng Zhang",
        "Xiaoping Wu",
        "Hao Luo",
        "Fan Wang",
        "Yisheng Lv"
      ],
      "submitted_date": "11 January, 2026",
      "source_url": "https://arxiv.org/abs/2601.06847"
    }
  ]
}