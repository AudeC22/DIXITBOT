<!-- ===== SEARCH URL: https://arxiv.org/search/cs?query=multimodal%20transformer&searchtype=all&abstracts=show&size=50&start=0 | HTTP 200 ===== -->

<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/1.0.0a5/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;50 of 2,812 results for all: <span class="mathjax">multimodal transformer</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/cs"  aria-role="search">
    
      Searching in archive <strong>cs</strong>. <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=0">Search in all archives.</a>
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="multimodal transformer">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option selected value="all">All fields</option><option value="title">Title</option><option value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=multimodal+transformer&amp;terms-0-field=all&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option selected value="all">All fields</option><option value="title">Title</option><option value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="multimodal transformer">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                     
          
          <li>
            <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=50"
              class="pagination-link "
              aria-label="Page 2"
              aria-current="page">2
            </a>
          </li>
          
          <li>
            <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=100"
              class="pagination-link "
              aria-label="Page 3"
              aria-current="page">3
            </a>
          </li>
          
          <li>
            <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=150"
              class="pagination-link "
              aria-label="Page 4"
              aria-current="page">4
            </a>
          </li>
          
          <li>
            <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=200"
              class="pagination-link "
              aria-label="Page 5"
              aria-current="page">5
            </a>
          </li>
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.08457">arXiv:2601.08457</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.08457">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An Under-Explored Application for Explainable <span class="search-hit mathjax">Multimodal</span> Misogyny Detection in code-mixed Hindi-English
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Yadav%2C+S">Sargam Yadav</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kaushik%2C+A">Abhishek Kaushik</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Daid%2C+K+M">Kevin Mc Daid</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.08457v1-abstract-short" style="display: inline;">
        &hellip;we present a multi-modal and explainable web application for detecting misogyny in text and memes in code-mixed Hindi and English. The system leverages state-of-the-art <span class="search-hit mathjax">transformer</span>-based models that support multilingual and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.08457v1-abstract-full').style.display = 'inline'; document.getElementById('2601.08457v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.08457v1-abstract-full" style="display: none;">
        Digital platforms have an ever-expanding user base, and act as a hub for communication, business, and connectivity. However, this has also allowed for the spread of hate speech and misogyny. Artificial intelligence models have emerged as an effective solution for countering online hate speech but are under explored for low resource and code-mixed languages and suffer from a lack of interpretability. Explainable Artificial Intelligence (XAI) can enhance transparency in the decisions of deep learning models, which is crucial for a sensitive domain such as hate speech detection. In this paper, we present a multi-modal and explainable web application for detecting misogyny in text and memes in code-mixed Hindi and English. The system leverages state-of-the-art <span class="search-hit mathjax">transformer</span>-based models that support multilingual and <span class="search-hit mathjax">multimodal</span> settings. For text-based misogyny identification, the system utilizes XLM-RoBERTa (XLM-R) and multilingual Bidirectional Encoder Representations from <span class="search-hit mathjax">Transformers</span> (mBERT) on a dataset of approximately 4,193 comments. For <span class="search-hit mathjax">multimodal</span> misogyny identification from memes, the system utilizes mBERT + EfficientNet, and mBERT + ResNET trained on a dataset of approximately 4,218 memes. It also provides feature importance scores using explainability techniques including Shapley Additive Values (SHAP) and Local Interpretable Model Agnostic Explanations (LIME). The application aims to serve as a tool for both researchers and content moderators, to promote further research in the field, combat gender based digital violence, and ensure a safe digital space. The system has been evaluated using human evaluators who provided their responses on Chatbot Usability Questionnaire (CUQ) and User Experience Questionnaire (UEQ) to determine overall usability.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.08457v1-abstract-full').style.display = 'none'; document.getElementById('2601.08457v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.08179">arXiv:2601.08179</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.08179">pdf</a>, <a href="https://arxiv.org/ps/2601.08179">ps</a>, <a href="https://arxiv.org/format/2601.08179">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/TMM.2025.3565929">10.1109/TMM.2025.3565929 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Instruction-Driven 3D Facial Expression Generation and Transition
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Vo%2C+A+H">Anh H. Vo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kim%2C+T">Tae-Seok Kim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jin%2C+H">Hulin Jin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Choi%2C+S">Soo-Mi Choi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kim%2C+Y">Yong-Guk Kim</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.08179v1-abstract-short" style="display: inline;">
        &hellip;expressions. This study presents a new framework for instruction-driven facial expression generation that produces a 3D face and, starting from an image of the face, <span class="search-hit mathjax">transforms</span> the facial expression from one designated facial expression to another. The Instruction-driven Facial Expression Decomposer (IFED) module is introduced to facilitate&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.08179v1-abstract-full').style.display = 'inline'; document.getElementById('2601.08179v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.08179v1-abstract-full" style="display: none;">
        A 3D avatar typically has one of six cardinal facial expressions. To simulate realistic emotional variation, we should be able to render a facial transition between two arbitrary expressions. This study presents a new framework for instruction-driven facial expression generation that produces a 3D face and, starting from an image of the face, <span class="search-hit mathjax">transforms</span> the facial expression from one designated facial expression to another. The Instruction-driven Facial Expression Decomposer (IFED) module is introduced to facilitate <span class="search-hit mathjax">multimodal</span> data learning and capture the correlation between textual descriptions and facial expression features. Subsequently, we propose the Instruction to Facial Expression Transition (I2FET) method, which leverages IFED and a vertex reconstruction loss function to refine the semantic comprehension of latent vectors, thus generating a facial expression sequence according to the given instruction. Lastly, we present the Facial Expression Transition model to generate smooth transitions between facial expressions. Extensive evaluation suggests that the proposed model outperforms state-of-the-art methods on the CK+ and CelebV-HQ datasets. The results show that our framework can generate facial expression trajectories according to text instruction. Considering that text prompts allow us to make diverse descriptions of human emotional states, the repertoire of facial expressions and the transitions between them can be expanded greatly. We expect our framework to find various practical applications More information about our project can be found at https://vohoanganh.github.io/tg3dfet/
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.08179v1-abstract-full').style.display = 'none'; document.getElementById('2601.08179v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        IEEE Transactions on Multimedia, 2025
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.08151">arXiv:2601.08151</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.08151">pdf</a>, <a href="https://arxiv.org/ps/2601.08151">ps</a>, <a href="https://arxiv.org/format/2601.08151">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Song%2C+S">Shezheng Song</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+S">Shasha Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+J">Jie Yu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.08151v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Multimodal</span> Large Language Models (MLLMs) have achieved remarkable progress in vision-language understanding, yet how they internally integrate visual and textual information remains poorly understood. To bridge this gap, we perform a systematic layer-wise masking analysis across multiple architectures, revealing how visual-text fusion evolves within MLLMs. T&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.08151v1-abstract-full').style.display = 'inline'; document.getElementById('2601.08151v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.08151v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Multimodal</span> Large Language Models (MLLMs) have achieved remarkable progress in vision-language understanding, yet how they internally integrate visual and textual information remains poorly understood. To bridge this gap, we perform a systematic layer-wise masking analysis across multiple architectures, revealing how visual-text fusion evolves within MLLMs. The results show that fusion emerges at several specific layers rather than being uniformly distributed across the network, and certain models exhibit a late-stage &#34;review&#34; phenomenon where visual signals are reactivated before output generation. Besides, we further analyze layer-wise attention evolution and observe persistent high-attention noise on irrelevant regions, along with gradually increasing attention on text-aligned areas. Guided by these insights, we introduce a training-free contrastive attention framework that models the <span class="search-hit mathjax">transformation</span> between early fusion and final layers to highlight meaningful attention shifts. Extensive experiments across various MLLMs and benchmarks validate our analysis and demonstrate that the proposed approach improves <span class="search-hit mathjax">multimodal</span> reasoning performance. Code will be released.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.08151v1-abstract-full').style.display = 'none'; document.getElementById('2601.08151v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.07581">arXiv:2601.07581</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.07581">pdf</a>, <a href="https://arxiv.org/ps/2601.07581">ps</a>, <a href="https://arxiv.org/format/2601.07581">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        BenchSeg: A Large-Scale Dataset and Benchmark for Multi-View Food Video Segmentation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=AlMughrabi%2C+A">Ahmad AlMughrabi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rivo%2C+G">Guillermo Rivo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jim%C3%A9nez-Farf%C3%A1n%2C+C">Carlos Jiménez-Farfán</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Haroon%2C+U">Umair Haroon</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Al-Areqi%2C+F">Farid Al-Areqi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jung%2C+H">Hyunjun Jung</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Busam%2C+B">Benjamin Busam</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Marques%2C+R">Ricardo Marques</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Radeva%2C+P">Petia Radeva</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.07581v1-abstract-short" style="display: inline;">
        &hellip;284 meticulously annotated frames, capturing each dish under free 360° camera motion. We evaluate a diverse set of 20 state-of-the-art segmentation models (e.g., SAM-based, <span class="search-hit mathjax">transformer</span>, CNN, and large <span class="search-hit mathjax">multimodal</span>) on the existing FoodSeg103 dataset and evaluate them (alone and combined with video-memory modules) on Benc&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.07581v1-abstract-full').style.display = 'inline'; document.getElementById('2601.07581v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.07581v1-abstract-full" style="display: none;">
        Food image segmentation is a critical task for dietary analysis, enabling accurate estimation of food volume and nutrients. However, current methods suffer from limited multi-view data and poor generalization to new viewpoints. We introduce BenchSeg, a novel multi-view food video segmentation dataset and benchmark. BenchSeg aggregates 55 dish scenes (from Nutrition5k, Vegetables &amp; Fruits, MetaFood3D, and FoodKit) with 25,284 meticulously annotated frames, capturing each dish under free 360° camera motion. We evaluate a diverse set of 20 state-of-the-art segmentation models (e.g., SAM-based, <span class="search-hit mathjax">transformer</span>, CNN, and large <span class="search-hit mathjax">multimodal</span>) on the existing FoodSeg103 dataset and evaluate them (alone and combined with video-memory modules) on BenchSeg. Quantitative and qualitative results demonstrate that while standard image segmenters degrade sharply under novel viewpoints, memory-augmented methods maintain temporal consistency across frames. Our best model based on a combination of SeTR-MLA+XMem2 outperforms prior work (e.g., improving over FoodMem by ~2.63% mAP), offering new insights into food segmentation and tracking for dietary analysis. We release BenchSeg to foster future research. The project page including the dataset annotations and the food segmentation models can be found at https://amughrabi.github.io/benchseg.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.07581v1-abstract-full').style.display = 'none'; document.getElementById('2601.07581v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.07516">arXiv:2601.07516</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.07516">pdf</a>, <a href="https://arxiv.org/ps/2601.07516">ps</a>, <a href="https://arxiv.org/format/2601.07516">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Controlling <span class="search-hit mathjax">Multimodal</span> Conversational Agents with Coverage-Enhanced Latent Actions
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Y">Yongqi Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lang%2C+H">Hao Lang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qian%2C+T">Tieyun Qian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Y">Yongbin Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.07516v1-abstract-short" style="display: inline;">
        Vision-language models are increasingly employed as <span class="search-hit mathjax">multimodal</span> conversational agents (MCAs) for diverse conversational tasks. Recently, reinforcement learning (RL) has been widely explored for adapting MCAs to various human-AI interaction scenarios. Despite showing great enhancement in generalization performance, fine-tuning MCAs via RL still faces challenge&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.07516v1-abstract-full').style.display = 'inline'; document.getElementById('2601.07516v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.07516v1-abstract-full" style="display: none;">
        Vision-language models are increasingly employed as <span class="search-hit mathjax">multimodal</span> conversational agents (MCAs) for diverse conversational tasks. Recently, reinforcement learning (RL) has been widely explored for adapting MCAs to various human-AI interaction scenarios. Despite showing great enhancement in generalization performance, fine-tuning MCAs via RL still faces challenges in handling the extremely large text token space. To address this, we learn a compact latent action space for RL fine-tuning instead. Specifically, we adopt the learning from observation mechanism to construct the codebook for the latent action space, where future observations are leveraged to estimate current latent actions that could further be used to reconstruct future observations. However, the scarcity of paired image-text data hinders learning a codebook with sufficient coverage. Thus, we leverage both paired image-text data and text-only data to construct the latent action space, using a cross-modal projector for <span class="search-hit mathjax">transforming</span> text embeddings into image-text embeddings. We initialize the cross-modal projector on paired image-text data, and further train it on massive text-only data with a novel cycle consistency loss to enhance its robustness. We show that our latent action based method outperforms competitive baselines on two conversation tasks across various RL algorithms.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.07516v1-abstract-full').style.display = 'none'; document.getElementById('2601.07516v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.07235">arXiv:2601.07235</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.07235">pdf</a>, <a href="https://arxiv.org/ps/2601.07235">ps</a>, <a href="https://arxiv.org/format/2601.07235">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Gosai%2C+A">Agnivo Gosai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=De%2C+S">Shuvodeep De</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Thankachan%2C+K">Karun Thankachan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.07235v1-abstract-short" style="display: inline;">
        &hellip;widely used datasets such as IMDb, Rotten Tomatoes, and SST-2, and models ranging from Naive Bayes and support vector machines to LSTM networks, BERT, and attention-based <span class="search-hit mathjax">transformers</span>. Beyond summarizing prior work, this survey differentiates itself by offering a comparative, challenge-driven analysis of how these modeling paradigms address domain-specific i&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.07235v1-abstract-full').style.display = 'inline'; document.getElementById('2601.07235v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.07235v1-abstract-full" style="display: none;">
        This paper presents a comprehensive survey of sentiment analysis methods for movie reviews, a benchmark task that has played a central role in advancing natural language processing. We review the evolution of techniques from early lexicon-based and classical machine learning approaches to modern deep learning architectures and large language models, covering widely used datasets such as IMDb, Rotten Tomatoes, and SST-2, and models ranging from Naive Bayes and support vector machines to LSTM networks, BERT, and attention-based <span class="search-hit mathjax">transformers</span>. Beyond summarizing prior work, this survey differentiates itself by offering a comparative, challenge-driven analysis of how these modeling paradigms address domain-specific issues such as sarcasm, negation, contextual ambiguity, and domain shift, which remain open problems in existing literature. Unlike earlier reviews that focus primarily on text-only pipelines, we also synthesize recent advances in <span class="search-hit mathjax">multimodal</span> sentiment analysis that integrate textual, audio, and visual cues from movie trailers and clips. In addition, we examine emerging concerns related to interpretability, fairness, and robustness that are often underexplored in prior surveys, and we outline future research directions including zero-shot and few-shot learning, hybrid symbolic--neural models, and real-time deployment considerations. Overall, this abstract provides a domain-focused roadmap that highlights both established solutions and unresolved challenges toward building more accurate, generalizable, and explainable sentiment analysis systems for movie review data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.07235v1-abstract-full').style.display = 'none'; document.getElementById('2601.07235v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">31 Pages; 1 figure; 108 references; ongoing paper that would be submitted to suitable Wiley journal</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.06874">arXiv:2601.06874</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.06874">pdf</a>, <a href="https://arxiv.org/ps/2601.06874">ps</a>, <a href="https://arxiv.org/format/2601.06874">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MVGGT: <span class="search-hit mathjax">Multimodal</span> Visual Geometry Grounded <span class="search-hit mathjax">Transformer</span> for Multiview 3D Referring Expression Segmentation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+C">Changli Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+H">Haodong Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ji%2C+J">Jiayi Ji</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yao%2C+Y">Yutian Yao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Du%2C+C">Chunsai Du</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kang%2C+J">Jihua Kang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fu%2C+Y">Yanwei Fu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cao%2C+L">Liujuan Cao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.06874v2-abstract-short" style="display: inline;">
        &hellip;first reconstruct a point cloud and then perform segmentation, often yield low-quality geometry, produce coarse or degraded target regions, and run slowly. We propose the <span class="search-hit mathjax">Multimodal</span> Visual Geometry Grounded <span class="search-hit mathjax">Transformer</span> (MVGGT), an efficient end-to-end framework that integrates language information into sparse-view geom&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06874v2-abstract-full').style.display = 'inline'; document.getElementById('2601.06874v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.06874v2-abstract-full" style="display: none;">
        Most existing 3D referring expression segmentation (3DRES) methods rely on dense, high-quality point clouds, while real-world agents such as robots and mobile phones operate with only a few sparse RGB views and strict latency constraints. We introduce Multi-view 3D Referring Expression Segmentation (MV-3DRES), where the model must recover scene structure and segment the referred object directly from sparse multi-view images. Traditional two-stage pipelines, which first reconstruct a point cloud and then perform segmentation, often yield low-quality geometry, produce coarse or degraded target regions, and run slowly. We propose the <span class="search-hit mathjax">Multimodal</span> Visual Geometry Grounded <span class="search-hit mathjax">Transformer</span> (MVGGT), an efficient end-to-end framework that integrates language information into sparse-view geometric reasoning through a dual-branch design. Training in this setting exposes a critical optimization barrier, termed Foreground Gradient Dilution (FGD), where sparse 3D signals lead to weak supervision. To resolve this, we introduce Per-view No-target Suppression Optimization (PVSO), which provides stronger and more balanced gradients across views, enabling stable and efficient learning. To support consistent evaluation, we build MVRefer, a benchmark that defines standardized settings and metrics for MV-3DRES. Experiments show that MVGGT establishes the first strong baseline and achieves both high accuracy and fast inference, outperforming existing alternatives. Code and models are publicly available at https://mvggt.github.io.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06874v2-abstract-full').style.display = 'none'; document.getElementById('2601.06874v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 January, 2026; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 11 January, 2026;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Project Website: https://sosppxo.github.io/mvggt.github.io/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.06847">arXiv:2601.06847</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.06847">pdf</a>, <a href="https://arxiv.org/ps/2601.06847">ps</a>, <a href="https://arxiv.org/format/2601.06847">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+M">Mengmeng Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+X">Xiaoping Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Luo%2C+H">Hao Luo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+F">Fan Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lv%2C+Y">Yisheng Lv</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.06847v1-abstract-short" style="display: inline;">
        &hellip;limitation arises from the scarcity of high-quality, large-scale clinical referring-localization pairs. To address this, we introduce MedGround, an automated pipeline that <span class="search-hit mathjax">transforms</span> segmentation resources into high-quality medical referring grounding data. Leveraging expert masks as spatial anchors, MedGround precisely derives localization targets, extracts&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06847v1-abstract-full').style.display = 'inline'; document.getElementById('2601.06847v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.06847v1-abstract-full" style="display: none;">
        Vision-Language Models (VLMs) can generate convincing clinical narratives, yet frequently struggle to visually ground their statements. We posit this limitation arises from the scarcity of high-quality, large-scale clinical referring-localization pairs. To address this, we introduce MedGround, an automated pipeline that <span class="search-hit mathjax">transforms</span> segmentation resources into high-quality medical referring grounding data. Leveraging expert masks as spatial anchors, MedGround precisely derives localization targets, extracts shape and spatial cues, and guides VLMs to synthesize natural, clinically grounded queries that reflect morphology and location. To ensure data rigor, a multi-stage verification system integrates strict formatting checks, geometry- and medical-prior rules, and image-based visual judging to filter out ambiguous or visually unsupported samples. Finally, we present MedGround-35K, a novel <span class="search-hit mathjax">multimodal</span> medical dataset. Extensive experiments demonstrate that VLMs trained with MedGround-35K consistently achieve improved referring grounding performance, enhance multi-object semantic disambiguation, and exhibit strong generalization to unseen grounding settings. This work highlights MedGround as a scalable, data-driven approach to anchor medical language to verifiable visual evidence. Dataset and code will be released publicly upon acceptance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06847v1-abstract-full').style.display = 'none'; document.getElementById('2601.06847v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">18 pages, 10 figures</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          J.1
        

        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.06753">arXiv:2601.06753</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.06753">pdf</a>, <a href="https://arxiv.org/ps/2601.06753">ps</a>, <a href="https://arxiv.org/format/2601.06753">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards Computational Chinese Paleography
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ma%2C+Y+R">Yiran Rex Ma</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.06753v1-abstract-short" style="display: inline;">
        &hellip;for automated decipherment and human-AI collaboration. We examine the technological shift from classical computer vision to modern deep learning paradigms, including <span class="search-hit mathjax">transformers</span> and large&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06753v1-abstract-full').style.display = 'inline'; document.getElementById('2601.06753v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.06753v1-abstract-full" style="display: none;">
        Chinese paleography, the study of ancient Chinese writing, is undergoing a computational turn powered by artificial intelligence. This position paper charts the trajectory of this emerging field, arguing that it is evolving from automating isolated visual tasks to creating integrated digital ecosystems for scholarly research. We first map the landscape of digital resources, analyzing critical datasets for oracle bone, bronze, and bamboo slip scripts. The core of our analysis follows the field&#39;s methodological pipeline: from foundational visual processing (image restoration, character recognition), through contextual analysis (artifact rejoining, dating), to the advanced reasoning required for automated decipherment and human-AI collaboration. We examine the technological shift from classical computer vision to modern deep learning paradigms, including <span class="search-hit mathjax">transformers</span> and large <span class="search-hit mathjax">multimodal</span> models. Finally, we synthesize the field&#39;s core challenges -- notably data scarcity and a disconnect between current AI capabilities and the holistic nature of humanistic inquiry -- and advocate for a future research agenda focused on creating <span class="search-hit mathjax">multimodal</span>, few-shot, and human-centric systems to augment scholarly expertise.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06753v1-abstract-full').style.display = 'none'; document.getElementById('2601.06753v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">A position paper in progress with Peking University &amp; ByteDance Digital Humanities Open Lab</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.06616">arXiv:2601.06616</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.06616">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        LLM-Driven Accessible Interface: A Model-Based Approach
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Jerry%2C+B">Blessing Jerry</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Moreno%2C+L">Lourdes Moreno</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Francisco%2C+V">Virginia Francisco</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hervas%2C+R">Raquel Hervas</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.06616v1-abstract-short" style="display: inline;">
        &hellip;raises challenges regarding accessibility, explainability, and normative compliance. This paper presents an implemented model-driven architecture for generating personalised, <span class="search-hit mathjax">multimodal</span>, and accessibility-aligned user interfaces. The approach combines structured user profiles, declarative adaptation rules, and validated prompt templates to refine baseline ac&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06616v1-abstract-full').style.display = 'inline'; document.getElementById('2601.06616v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.06616v1-abstract-full" style="display: none;">
        The integration of Large Language Models (LLMs) into interactive systems opens new opportunities for adaptive user experiences, yet it also raises challenges regarding accessibility, explainability, and normative compliance. This paper presents an implemented model-driven architecture for generating personalised, <span class="search-hit mathjax">multimodal</span>, and accessibility-aligned user interfaces. The approach combines structured user profiles, declarative adaptation rules, and validated prompt templates to refine baseline accessible UI templates that conform to WCAG 2.2 and EN 301 549, tailored to cognitive and sensory support needs. LLMs dynamically <span class="search-hit mathjax">transform</span> language complexity, modality, and visual structure, producing outputs such as Plain-Language text, pictograms, and high-contrast layouts aligned with ISO 24495-1 and W3C COGA guidance. A healthcare use case demonstrates how the system generates accessible post-consultation medication instructions tailored to a user profile comprising cognitive disability and hearing impairment. SysML v2 models provide explicit traceability between user needs, adaptation rules, and normative requirements, ensuring explainable and auditable <span class="search-hit mathjax">transformations</span>. Grounded in Human-Centered AI (HCAI), the framework incorporates co-design processes and structured feedback mechanisms to guide iterative refinement and support trustworthy generative behaviour.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06616v1-abstract-full').style.display = 'none'; document.getElementById('2601.06616v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          H.5.2; J.3; K.4.2; H.1.2; I.2.7
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.06475">arXiv:2601.06475</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.06475">pdf</a>, <a href="https://arxiv.org/ps/2601.06475">ps</a>, <a href="https://arxiv.org/format/2601.06475">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        VVTRec: Radio Interferometric Reconstruction through Visual and Textual Modality Enrichment
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Cheng%2C+K">Kai Cheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+R">Ruoqi Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Luo%2C+Q">Qiong Luo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.06475v1-abstract-short" style="display: inline;">
        Radio astronomy is an indispensable discipline for observing distant celestial objects. Measurements of wave signals from radio telescopes, called visibility, need to be <span class="search-hit mathjax">transformed</span> into images for astronomical observations. These dirty images blend information from real sources and artifacts. Therefore, astronomers usually perform reconstruction before imag&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06475v1-abstract-full').style.display = 'inline'; document.getElementById('2601.06475v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.06475v1-abstract-full" style="display: none;">
        Radio astronomy is an indispensable discipline for observing distant celestial objects. Measurements of wave signals from radio telescopes, called visibility, need to be <span class="search-hit mathjax">transformed</span> into images for astronomical observations. These dirty images blend information from real sources and artifacts. Therefore, astronomers usually perform reconstruction before imaging to obtain cleaner images. Existing methods consider only a single modality of sparse visibility data, resulting in images with remaining artifacts and insufficient modeling of correlation. To enhance the extraction of visibility information and emphasize output quality in the image domain, we propose VVTRec, a <span class="search-hit mathjax">multimodal</span> radio interferometric data reconstruction method with visibility-guided visual and textual modality enrichment. In our VVTRec, sparse visibility is <span class="search-hit mathjax">transformed</span> into image-form and text-form features to obtain enhancements in terms of spatial and semantic information, improving the structural integrity and accuracy of images. Also, we leverage Vision-Language Models (VLMs) to achieve additional training-free performance improvements. VVTRec enables sparse visibility, as a foreign modality unseen by VLMs, to accurately extract pre-trained knowledge as a supplement. Our experiments demonstrate that VVTRec effectively enhances imaging results by exploiting <span class="search-hit mathjax">multimodal</span> information without introducing excessive computational overhead.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06475v1-abstract-full').style.display = 'none'; document.getElementById('2601.06475v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.06212">arXiv:2601.06212</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.06212">pdf</a>, <a href="https://arxiv.org/ps/2601.06212">ps</a>, <a href="https://arxiv.org/format/2601.06212">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Akasha 2: Hamiltonian State Space Duality and Visual-Language Joint Embedding Predictive Architectur
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Meziani%2C+Y">Yani Meziani</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.06212v1-abstract-short" style="display: inline;">
        We present Akasha 2, a state-of-the-art <span class="search-hit mathjax">multimodal</span> architecture that integrates Hamiltonian State Space Duality (H-SSD) with Visual-Language Joint Embedding Predictive Architecture (VL-JEPA). The system leverages the Mamba-3 Selective State Space Model (SSM) augmented by a Sparse Mixture of Hamiltonian Experts (SMoE-HE) that enforces latent physical conserva&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06212v1-abstract-full').style.display = 'inline'; document.getElementById('2601.06212v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.06212v1-abstract-full" style="display: none;">
        We present Akasha 2, a state-of-the-art <span class="search-hit mathjax">multimodal</span> architecture that integrates Hamiltonian State Space Duality (H-SSD) with Visual-Language Joint Embedding Predictive Architecture (VL-JEPA). The system leverages the Mamba-3 Selective State Space Model (SSM) augmented by a Sparse Mixture of Hamiltonian Experts (SMoE-HE) that enforces latent physical conservation laws through symplectic integration. For visual synthesis, we introduce Hamiltonian Flow Matching (HFM) and persistent 3D Gaussian Splatting (3DGS), enabling ultra-low latency (&lt;50ms) on mobile hardware. This work establishes a new paradigm in latent world models, achieving unprecedented spatiotemporal coherence through a holographic memory architecture. Our approach demonstrates that incorporating physics-inspired inductive biases into neural architectures yields significant improvements: state-of-the-art video prediction (FVD: 287), 4x faster visual synthesis than diffusion models, and 3-18x inference speedup over <span class="search-hit mathjax">transformer</span> baselines while maintaining energy conservation over extended horizons.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06212v1-abstract-full').style.display = 'none'; document.getElementById('2601.06212v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">12 pages, 6 figures, 3 tables. Includes appendices with pseudocode and implementation details. Supplementary materials eventually at github.com/yanimeziani/akasha</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68T07; 68T45; 70H05
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.6; I.2.10; I.4.8
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.06199">arXiv:2601.06199</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.06199">pdf</a>, <a href="https://arxiv.org/ps/2601.06199">ps</a>, <a href="https://arxiv.org/format/2601.06199">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        FastSLM: Hierarchical Frame Q-Former for Effective Speech Modality Adaptation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+J">Junseok Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+S">Sangyong Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chun%2C+C">Chang-Jae Chun</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.06199v1-abstract-short" style="display: inline;">
        &hellip;general intelligence (AGI). In particular, there is growing momentum in adapting LLMs to various modalities, including vision, video, and speech, through the development of <span class="search-hit mathjax">multimodal</span> LLMs (MLLMs). However, existing speech-language model (SLM) research has largely overlooked cost-effective adaptation strategies for leveraging LLMs in the speech domain. In th&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06199v1-abstract-full').style.display = 'inline'; document.getElementById('2601.06199v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.06199v1-abstract-full" style="display: none;">
        Recent advances in large language models (LLMs) have demonstrated human-expert-level capabilities, driving significant interest in their potential for achieving artificial general intelligence (AGI). In particular, there is growing momentum in adapting LLMs to various modalities, including vision, video, and speech, through the development of <span class="search-hit mathjax">multimodal</span> LLMs (MLLMs). However, existing speech-language model (SLM) research has largely overlooked cost-effective adaptation strategies for leveraging LLMs in the speech domain. In this paper, we propose FastSLM, a lightweight yet efficient SLM designed for effective understanding and reasoning over long-form speech. To address the challenge of aligning high-frame-rate speech features with LLMs, we introduce the Hierarchical Frame Querying <span class="search-hit mathjax">Transformer</span> (HFQ-Former), which compresses frame-level speech features while capturing both local and global context. Furthermore, we present a novel three-stage training strategy that enhances generalization across a wide range of speech-related tasks. Experimental results demonstrate that FastSLM achieves competitive performance compared to existing state-of-the-art models, despite operating with significantly lower FLOPs and parameter counts, while representing speech with only 1.67 tokens per second. The source code and model checkpoints are available at https://huggingface.co/okestro-ai-lab/FastSLM.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06199v1-abstract-full').style.display = 'none'; document.getElementById('2601.06199v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.06140">arXiv:2601.06140</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.06140">pdf</a>, <a href="https://arxiv.org/ps/2601.06140">ps</a>, <a href="https://arxiv.org/format/2601.06140">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Causal and Federated <span class="search-hit mathjax">Multimodal</span> Learning for Cardiovascular Risk Prediction under Heterogeneous Populations
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Kaushik%2C+R">Rohit Kaushik</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kaushik%2C+E">Eva Kaushik</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.06140v1-abstract-short" style="display: inline;">
        &hellip;calling for predictive models that not only handle diverse and high-dimensional biomedical signals but also maintain interpretability and privacy. We create a single <span class="search-hit mathjax">multimodal</span> learning framework that integrates cross modal <span class="search-hit mathjax">transformers</span> with graph neural networks and causal representation learning to measure personali&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06140v1-abstract-full').style.display = 'inline'; document.getElementById('2601.06140v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.06140v1-abstract-full" style="display: none;">
        Cardiovascular disease (CVD) continues to be the major cause of death globally, calling for predictive models that not only handle diverse and high-dimensional biomedical signals but also maintain interpretability and privacy. We create a single <span class="search-hit mathjax">multimodal</span> learning framework that integrates cross modal <span class="search-hit mathjax">transformers</span> with graph neural networks and causal representation learning to measure personalized CVD risk. The model combines genomic variation, cardiac MRI, ECG waveforms, wearable streams, and structured EHR data to predict risk while also implementing causal invariance constraints across different clinical subpopulations.
  To maintain transparency, we employ SHAP based feature attribution, counterfactual explanations and causal latent alignment for understandable risk factors. Besides, we position the design in a federated, privacy, preserving optimization protocol and establish rules for convergence, calibration and uncertainty quantification under distributional shift. Experimental studies based on large-scale biobank and multi institutional datasets reveal state discrimination and robustness, exhibiting fair performance across demographic strata and clinically distinct cohorts. This study paves the way for a principled approach to clinically trustworthy, interpretable and privacy respecting CVD prediction at the population level.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06140v1-abstract-full').style.display = 'none'; document.getElementById('2601.06140v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, 5 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.05508">arXiv:2601.05508</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.05508">pdf</a>, <a href="https://arxiv.org/ps/2601.05508">ps</a>, <a href="https://arxiv.org/format/2601.05508">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Enabling Stroke-Level Structural Analysis of Hieroglyphic Scripts without Language-Specific Priors
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Luo%2C+F">Fuwen Luo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wan%2C+Z">Zihao Wan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Z">Ziyue Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Y">Yaluo Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+P+T+L">Pau Tong Lin Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qiao%2C+X">Xuanjia Qiao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+X">Xiaolong Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+P">Peng Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Y">Yang Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.05508v1-abstract-short" style="display: inline;">
        &hellip;writing systems, encode rich semantic and cultural information within their internal structural composition. Yet, current advanced Large Language Models (LLMs) and <span class="search-hit mathjax">Multimodal</span> LLMs (MLLMs) usually remain structurally blind to this information. LLMs process characters as textual tokens, while MLLMs additionally view them as raw pixel grids. Both fall short to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.05508v1-abstract-full').style.display = 'inline'; document.getElementById('2601.05508v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.05508v1-abstract-full" style="display: none;">
        Hieroglyphs, as logographic writing systems, encode rich semantic and cultural information within their internal structural composition. Yet, current advanced Large Language Models (LLMs) and <span class="search-hit mathjax">Multimodal</span> LLMs (MLLMs) usually remain structurally blind to this information. LLMs process characters as textual tokens, while MLLMs additionally view them as raw pixel grids. Both fall short to model the underlying logic of character strokes. Furthermore, existing structural analysis methods are often script-specific and labor-intensive. In this paper, we propose Hieroglyphic Stroke Analyzer (HieroSA), a novel and generalizable framework that enables MLLMs to automatically derive stroke-level structures from character bitmaps without handcrafted data. It <span class="search-hit mathjax">transforms</span> modern logographic and ancient hieroglyphs character images into explicit, interpretable line-segment representations in a normalized coordinate space, allowing for cross-lingual generalization. Extensive experiments demonstrate that HieroSA effectively captures character-internal structures and semantics, bypassing the need for language-specific priors. Experimental results highlight the potential of our work as a graphematics analysis tool for a deeper understanding of hieroglyphic scripts. View our code at https://github.com/THUNLP-MT/HieroSA.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.05508v1-abstract-full').style.display = 'none'; document.getElementById('2601.05508v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.05470">arXiv:2601.05470</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.05470">pdf</a>, <a href="https://arxiv.org/ps/2601.05470">ps</a>, <a href="https://arxiv.org/format/2601.05470">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ROAP: A Reading-Order and Attention-Prior Pipeline for Optimizing Layout <span class="search-hit mathjax">Transformers</span> in Key Information Extraction
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Xie%2C+T">Tingwei Xie</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+J">Jinxin He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Song%2C+Y">Yonghong Song</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.05470v1-abstract-short" style="display: inline;">
        The efficacy of <span class="search-hit mathjax">Multimodal</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.05470v1-abstract-full').style.display = 'inline'; document.getElementById('2601.05470v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.05470v1-abstract-full" style="display: none;">
        The efficacy of <span class="search-hit mathjax">Multimodal</span> <span class="search-hit mathjax">Transformers</span> in visually-rich document understanding (VrDU) is critically constrained by two inherent limitations: the lack of explicit modeling for logical reading order and the interference of visual tokens that dilutes attention on textual semantics.
  To address these challenges, this paper presents ROAP, a lightweight and architecture-agnostic pipeline designed to optimize attention distributions in Layout <span class="search-hit mathjax">Transformers</span> without altering their pre-trained backbones.
  The proposed pipeline first employs an Adaptive-XY-Gap (AXG-Tree) to robustly extract hierarchical reading sequences from complex layouts. These sequences are then integrated into the attention mechanism via a Reading-Order-Aware Relative Position Bias (RO-RPB). Furthermore, a Textual-Token Sub-block Attention Prior (TT-Prior) is introduced to adaptively suppress visual noise and enhance fine-grained text-text interactions.
  Extensive experiments on the FUNSD and CORD benchmarks demonstrate that ROAP consistently improves the performance of representative backbones, including LayoutLMv3 and GeoLayoutLM.
  These findings confirm that explicitly modeling reading logic and regulating modality interference are critical for robust document understanding, offering a scalable solution for complex layout analysis. The implementation code will be released at https://github.com/KevinYuLei/ROAP.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.05470v1-abstract-full').style.display = 'none'; document.getElementById('2601.05470v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages, 4 figures, 4 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.05353">arXiv:2601.05353</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.05353">pdf</a>, <a href="https://arxiv.org/ps/2601.05353">ps</a>, <a href="https://arxiv.org/format/2601.05353">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        GlyRAG: Context-Aware Retrieval-Augmented Framework for Blood Glucose Forecasting
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Soumma%2C+S+B">Shovito Barua Soumma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ghasemzadeh%2C+H">Hassan Ghasemzadeh</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.05353v1-abstract-short" style="display: inline;">
        &hellip;an LLM as a contextualization agent to generate clinical summaries. These summaries are embedded by a language model and fused with patch-based glucose representations in a <span class="search-hit mathjax">multimodal</span> <span class="search-hit mathjax">transformer</span> architecture with a cross translation loss aligining textual and physiological embeddings. A retrieval module then identifie&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.05353v1-abstract-full').style.display = 'inline'; document.getElementById('2601.05353v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.05353v1-abstract-full" style="display: none;">
        Accurate forecasting of blood glucose from CGM is essential for preventing dysglycemic events, thus enabling proactive diabetes management. However, current forecasting models treat blood glucose readings captured using CGMs as a numerical sequence, either ignoring context or relying on additional sensors/modalities that are difficult to collect and deploy at scale. Recently, LLMs have shown promise for time-series forecasting tasks, yet their role as agentic context extractors in diabetes care remains largely unexplored. To address these limitations, we propose GlyRAG, a context-aware, retrieval-augmented forecasting framework that derives semantic understanding of blood glucose dynamics directly from CGM traces without requiring additional sensor modalities. GlyRAG employs an LLM as a contextualization agent to generate clinical summaries. These summaries are embedded by a language model and fused with patch-based glucose representations in a <span class="search-hit mathjax">multimodal</span> <span class="search-hit mathjax">transformer</span> architecture with a cross translation loss aligining textual and physiological embeddings. A retrieval module then identifies similar historical episodes in the learned embedding space and uses cross-attention to integrate these case-based analogues prior to making a forecasting inference. Extensive evaluations on two T1D cohorts show that GlyRAG consistently outperforms state-of-the art methods, achieving up to 39% lower RMSE and a further 1.7% reduction in RMSE over the baseline. Clinical evaluation shows that GlyRAG places 85% predictions in safe zones and achieves 51% improvement in predicting dysglycemic events across both cohorts. These results indicate that LLM-based contextualization and retrieval over CGM traces can enhance the accuracy and clinical reliability of long-horizon glucose forecasting without the need for extra sensors, thus supporting future agentic decision-support tools for diabetes management.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.05353v1-abstract-full').style.display = 'none'; document.getElementById('2601.05353v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.05269">arXiv:2601.05269</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.05269">pdf</a>, <a href="https://arxiv.org/ps/2601.05269">ps</a>, <a href="https://arxiv.org/format/2601.05269">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Studying Illustrations in Manuscripts: An Efficient Deep-Learning Approach
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Evron%2C+Y">Yoav Evron</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Siegal%2C+M+B">Michal Bar-Asher Siegal</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fire%2C+M">Michael Fire</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.05269v2-abstract-short" style="display: inline;">
        The recent Artificial Intelligence (AI) revolution has opened <span class="search-hit mathjax">transformative</span> possibilities for the humanities, particularly in unlocking the visual-artistic content embedded in historical illuminated manuscripts. While digital archives now offer unprecedented access to these materials, the ability to systematically locate, extract, and analyze illustrations&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.05269v2-abstract-full').style.display = 'inline'; document.getElementById('2601.05269v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.05269v2-abstract-full" style="display: none;">
        The recent Artificial Intelligence (AI) revolution has opened <span class="search-hit mathjax">transformative</span> possibilities for the humanities, particularly in unlocking the visual-artistic content embedded in historical illuminated manuscripts. While digital archives now offer unprecedented access to these materials, the ability to systematically locate, extract, and analyze illustrations at scale remains a major challenge. We present a general and scalable AI-based pipeline for large-scale visual analysis of illuminated manuscripts. The framework integrates modern deep-learning models for page-level illustration detection, illustration extraction, and <span class="search-hit mathjax">multimodal</span> description, enabling scholars to search, cluster, and study visual materials and artistic trends across entire corpora. We demonstrate the applicability of this approach on large heterogeneous collections, including the Vatican Library and richly illuminated manuscripts such as the Bible of Borso d&#39;Este. The system reveals meaningful visual patterns and cross-manuscript relationships by embedding illustrations into a shared representation space and analyzing their similarity structure (see figure 4). By harnessing recent advances in computer vision and vision-language models, our framework enables new forms of large-scale visual scholarship in historical studies, art history, and cultural heritage making it possible to explore iconography, stylistic trends, and cultural connections in ways that were previously impractical.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.05269v2-abstract-full').style.display = 'none'; document.getElementById('2601.05269v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 January, 2026; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 November, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">17 pages, 5 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.04571">arXiv:2601.04571</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.04571">pdf</a>, <a href="https://arxiv.org/ps/2601.04571">ps</a>, <a href="https://arxiv.org/format/2601.04571">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Enhancing <span class="search-hit mathjax">Multimodal</span> Retrieval via Complementary Information Extraction and Alignment
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zeng%2C+D">Delong Zeng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xie%2C+Y">Yuexiang Xie</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Y">Yaliang Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shen%2C+Y">Ying Shen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.04571v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Multimodal</span> retrieval has emerged as a promising yet challenging research direction in recent years. Most existing studies in&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.04571v1-abstract-full').style.display = 'inline'; document.getElementById('2601.04571v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.04571v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Multimodal</span> retrieval has emerged as a promising yet challenging research direction in recent years. Most existing studies in <span class="search-hit mathjax">multimodal</span> retrieval focus on capturing information in <span class="search-hit mathjax">multimodal</span> data that is similar to their paired texts, but often ignores the complementary information contained in <span class="search-hit mathjax">multimodal</span> data. In this study, we propose CIEA, a novel <span class="search-hit mathjax">multimodal</span> retrieval approach that employs Complementary Information Extraction and Alignment, which <span class="search-hit mathjax">transforms</span> both text and images in documents into a unified latent space and features a complementary information extractor designed to identify and preserve differences in the image representations. We optimize CIEA using two complementary contrastive losses to ensure semantic integrity and effectively capture the complementary information contained in images. Extensive experiments demonstrate the effectiveness of CIEA, which achieves significant improvements over both divide-and-conquer models and universal dense retrieval models. We provide an ablation study, further discussions, and case studies to highlight the advancements achieved by CIEA. To promote further research in the community, we have released the source code at https://github.com/zengdlong/CIEA.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.04571v1-abstract-full').style.display = 'none'; document.getElementById('2601.04571v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by ACL&#39;2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.04376">arXiv:2601.04376</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.04376">pdf</a>, <a href="https://arxiv.org/ps/2601.04376">ps</a>, <a href="https://arxiv.org/format/2601.04376">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Combining Facial Videos and Biosignals for Stress Estimation During Driving
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Valergaki%2C+P">Paraskevi Valergaki</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nicodemou%2C+V+C">Vassilis C. Nicodemou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Oikonomidis%2C+I">Iason Oikonomidis</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Argyros%2C+A">Antonis Argyros</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Roussos%2C+A">Anastasios Roussos</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.04376v2-abstract-short" style="display: inline;">
        &hellip;physiological signals such as perinasal perspiration and heart rate, facial activity provides complementary cues that can be captured unobtrusively from video. We propose a <span class="search-hit mathjax">multimodal</span> stress estimation framework that combines facial videos and physiological signals, remaining effective even when biosignal acquisition is challenging. Facial behavior is repres&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.04376v2-abstract-full').style.display = 'inline'; document.getElementById('2601.04376v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.04376v2-abstract-full" style="display: none;">
        Reliable stress recognition is critical in applications such as medical monitoring and safety-critical systems, including real-world driving. While stress is commonly detected using physiological signals such as perinasal perspiration and heart rate, facial activity provides complementary cues that can be captured unobtrusively from video. We propose a <span class="search-hit mathjax">multimodal</span> stress estimation framework that combines facial videos and physiological signals, remaining effective even when biosignal acquisition is challenging. Facial behavior is represented using a dense 3D Morphable Model, yielding a 56-dimensional descriptor that captures subtle expression and head-pose dynamics over time. To study how stress modulates facial motion, we perform extensive experiments alongside established physiological markers. Paired hypothesis tests between baseline and stressor phases show that 38 of 56 facial components exhibit consistent, phase-specific stress responses comparable to physiological markers. Building on these findings, we introduce a <span class="search-hit mathjax">Transformer</span>-based temporal modeling framework and evaluate unimodal, early-fusion, and cross-modal attention strategies. Cross-modal attention fusion of 3D-derived facial features with physiological signals substantially improves performance over physiological signals alone, increasing AUROC from 52.7% and accuracy from 51.0% to 92.0% and 86.7%, respectively. Although evaluated on driving data, the proposed framework and protocol may generalize to other stress estimation settings.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.04376v2-abstract-full').style.display = 'none'; document.getElementById('2601.04376v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 January, 2026; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 7 January, 2026;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Under submission to ICPR 2026</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.04359">arXiv:2601.04359</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.04359">pdf</a>, <a href="https://arxiv.org/ps/2601.04359">ps</a>, <a href="https://arxiv.org/format/2601.04359">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PackCache: A Training-Free Acceleration Method for Unified Autoregressive Video Generation via Compact KV-Cache
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+K">Kunyang Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shah%2C+M">Mubarak Shah</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shang%2C+Y">Yuzhang Shang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.04359v1-abstract-short" style="display: inline;">
        A unified autoregressive model is a <span class="search-hit mathjax">Transformer</span>-based framework that addresses diverse <span class="search-hit mathjax">multimodal</span> tasks (e.g., text, image, video) as a single sequence modeling problem under a shared token space. Such models rely on the KV-cache mechanism to reduce attention computation from O(T^2) to O(T); however, KV-cache size grow&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.04359v1-abstract-full').style.display = 'inline'; document.getElementById('2601.04359v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.04359v1-abstract-full" style="display: none;">
        A unified autoregressive model is a <span class="search-hit mathjax">Transformer</span>-based framework that addresses diverse <span class="search-hit mathjax">multimodal</span> tasks (e.g., text, image, video) as a single sequence modeling problem under a shared token space. Such models rely on the KV-cache mechanism to reduce attention computation from O(T^2) to O(T); however, KV-cache size grows linearly with the number of generated tokens, and it rapidly becomes the dominant bottleneck limiting inference efficiency and generative length. Unified autoregressive video generation inherits this limitation. Our analysis reveals that KV-cache tokens exhibit distinct spatiotemporal properties: (i) text and conditioning-image tokens act as persistent semantic anchors that consistently receive high attention, and (ii) attention to previous frames naturally decays with temporal distance. Leveraging these observations, we introduce PackCache, a training-free KV-cache management method that dynamically compacts the KV cache through three coordinated mechanisms: condition anchoring that preserves semantic references, cross-frame decay modeling that allocates cache budget according to temporal distance, and spatially preserving position embedding that maintains coherent 3D structure under cache removal. In terms of efficiency, PackCache accelerates end-to-end generation by 1.7-2.2x on 48-frame long sequences, showcasing its strong potential for enabling longer-sequence video generation. Notably, the final four frames - the portion most impacted by the progressively expanding KV-cache and thus the most expensive segment of the clip - PackCache delivers a 2.6x and 3.7x acceleration on A40 and H200, respectively, for 48-frame videos.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.04359v1-abstract-full').style.display = 'none'; document.getElementById('2601.04359v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.04299">arXiv:2601.04299</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.04299">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Quantitative Methods">q-bio.QM</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Transformer</span>-Based Multi-Modal Temporal Embeddings for Explainable Metabolic Phenotyping in Type 1 Diabetes
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Khokhar%2C+P+B">Pir Bakhsh Khokhar</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gravino%2C+C">Carmine Gravino</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Palomba%2C+F">Fabio Palomba</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yayilgan%2C+S+Y">Sule Yildrim Yayilgan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shaikh%2C+S">Sarang Shaikh</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.04299v1-abstract-short" style="display: inline;">
        &hellip;hemoglobin (HbA1c). This study proposes an explainable deep learning framework that integrates continuous glucose monitoring (CGM) data with laboratory profiles to learn <span class="search-hit mathjax">multimodal</span> temporal embeddings of individual metabolic status. Temporal dependencies across modalities are modeled using a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.04299v1-abstract-full').style.display = 'inline'; document.getElementById('2601.04299v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.04299v1-abstract-full" style="display: none;">
        Type 1 diabetes (T1D) is a highly metabolically heterogeneous disease that cannot be adequately characterized by conventional biomarkers such as glycated hemoglobin (HbA1c). This study proposes an explainable deep learning framework that integrates continuous glucose monitoring (CGM) data with laboratory profiles to learn <span class="search-hit mathjax">multimodal</span> temporal embeddings of individual metabolic status. Temporal dependencies across modalities are modeled using a <span class="search-hit mathjax">transformer</span> encoder, while latent metabolic phenotypes are identified via Gaussian mixture modeling. Model interpretability is achieved through <span class="search-hit mathjax">transformer</span> attention visualization and SHAP-based feature attribution. Five latent metabolic phenotypes, ranging from metabolic stability to elevated cardiometabolic risk, were identified among 577 individuals with T1D. These phenotypes exhibit distinct biochemical profiles, including differences in glycemic control, lipid metabolism, renal markers, and thyrotropin (TSH) levels. Attention analysis highlights glucose variability as a dominant temporal factor, while SHAP analysis identifies HbA1c, triglycerides, cholesterol, creatinine, and TSH as key contributors to phenotype differentiation. Phenotype membership shows statistically significant, albeit modest, associations with hypertension, myocardial infarction, and heart failure. Overall, this explainable <span class="search-hit mathjax">multimodal</span> temporal embedding framework reveals physiologically coherent metabolic subgroups in T1D and supports risk stratification beyond single biomarkers.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.04299v1-abstract-full').style.display = 'none'; document.getElementById('2601.04299v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.04219">arXiv:2601.04219</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.04219">pdf</a>, <a href="https://arxiv.org/ps/2601.04219">ps</a>, <a href="https://arxiv.org/format/2601.04219">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multiagent Systems">cs.MA</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AgentTutor: Empowering Personalized Learning with Multi-Turn Interactive Teaching in Intelligent Education Systems
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Y">Yuxin Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Song%2C+Z">Zeqing Song</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lou%2C+J">Jiong Lou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+C">Chentao Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+J">Jie Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.04219v1-abstract-short" style="display: inline;">
        The rapid advancement of large-scale language models (LLMs) has shown their potential to <span class="search-hit mathjax">transform</span> intelligent education systems (IESs) through automated teaching and learning support applications. However, current IESs often rely on single-turn static question-answering, which fails to assess learners&#39; cognitive levels, cannot adjust teaching strategies&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.04219v1-abstract-full').style.display = 'inline'; document.getElementById('2601.04219v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.04219v1-abstract-full" style="display: none;">
        The rapid advancement of large-scale language models (LLMs) has shown their potential to <span class="search-hit mathjax">transform</span> intelligent education systems (IESs) through automated teaching and learning support applications. However, current IESs often rely on single-turn static question-answering, which fails to assess learners&#39; cognitive levels, cannot adjust teaching strategies based on real-time feedback, and is limited to providing simple one-off responses. To address these issues, we introduce AgentTutor, a multi-turn interactive intelligent education system to empower personalized learning. It features an LLM-powered generative multi-agent system and a learner-specific personalized learning profile environment that dynamically optimizes and delivers teaching strategies based on learners&#39; learning status, personalized goals, learning preferences, and <span class="search-hit mathjax">multimodal</span> study materials. It includes five key modules: curriculum decomposition, learner assessment, dynamic strategy, teaching reflection, and knowledge &amp; experience memory. We conducted extensive experiments on multiple benchmark datasets, AgentTutor significantly enhances learners&#39; performance while demonstrating strong effectiveness in multi-turn interactions and competitiveness in teaching quality among other baselines.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.04219v1-abstract-full').style.display = 'none'; document.getElementById('2601.04219v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 December, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">AAAI2026 Workshop AI4EDU</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.04100">arXiv:2601.04100</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.04100">pdf</a>, <a href="https://arxiv.org/ps/2601.04100">ps</a>, <a href="https://arxiv.org/format/2601.04100">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Quantifying the Impact of Modules and Their Interactions in the PSO-X Framework
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Camacho-Villal%C3%B3n%2C+C+L">Christian L. Camacho-Villalón</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nikolikj%2C+A">Ana Nikolikj</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dost%2C+K">Katharina Dost</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tuba%2C+E">Eva Tuba</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=D%C5%BEeroski%2C+S">Sašo Džeroski</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Eftimov%2C+T">Tome Eftimov</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.04100v1-abstract-short" style="display: inline;">
        &hellip;in different problem classes. In practice, this allows us to identify which modules have greater influence on PSO-X performance depending on problem features such as <span class="search-hit mathjax">multimodality</span>, mathematical <span class="search-hit mathjax">transformations</span> and varying dimensionality. We then perform a cluster analysis to identify groups of problem classes that shar&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.04100v1-abstract-full').style.display = 'inline'; document.getElementById('2601.04100v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.04100v1-abstract-full" style="display: none;">
        The PSO-X framework incorporates dozens of modules that have been proposed for solving single-objective continuous optimization problems using particle swarm optimization. While modular frameworks enable users to automatically generate and configure algorithms tailored to specific optimization problems, the complexity of this process increases with the number of modules in the framework and the degrees of freedom defined for their interaction. Understanding how modules affect the performance of algorithms for different problems is critical to making the process of finding effective implementations more efficient and identifying promising areas for further investigation. Despite their practical applications and scientific relevance, there is a lack of empirical studies investigating which modules matter most in modular optimization frameworks and how they interact. In this paper, we analyze the performance of 1424 particle swarm optimization algorithms instantiated from the PSO-X framework on the 25 functions in the CEC&#39;05 benchmark suite with 10 and 30 dimensions. We use functional ANOVA to quantify the impact of modules and their combinations on performance in different problem classes. In practice, this allows us to identify which modules have greater influence on PSO-X performance depending on problem features such as <span class="search-hit mathjax">multimodality</span>, mathematical <span class="search-hit mathjax">transformations</span> and varying dimensionality. We then perform a cluster analysis to identify groups of problem classes that share similar module effect patterns. Our results show low variability in the importance of modules in all problem classes, suggesting that particle swarm optimization performance is driven by a few influential modules.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.04100v1-abstract-full').style.display = 'none'; document.getElementById('2601.04100v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.03660">arXiv:2601.03660</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.03660">pdf</a>, <a href="https://arxiv.org/ps/2601.03660">ps</a>, <a href="https://arxiv.org/format/2601.03660">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MGPC: <span class="search-hit mathjax">Multimodal</span> Network for Generalizable Point Cloud Completion With Modality Dropout and Progressive Decoding
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+J">Jiangyuan Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ma%2C+H">Hongxuan Ma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+Y">Yuhao Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Z">Zhe Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+J">Jian Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zou%2C+W">Wei Zou</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.03660v1-abstract-short" style="display: inline;">
        &hellip;from partial observations caused by limited viewpoints and occlusions. Existing learning-based works, including 3D Convolutional Neural Network (CNN)-based, point-based, and <span class="search-hit mathjax">Transformer</span>-based methods, have achieved strong performance on synthetic benchmarks. However, due to the limitations of modality, scalability, and generative capacity, their generalizati&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.03660v1-abstract-full').style.display = 'inline'; document.getElementById('2601.03660v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.03660v1-abstract-full" style="display: none;">
        Point cloud completion aims to recover complete 3D geometry from partial observations caused by limited viewpoints and occlusions. Existing learning-based works, including 3D Convolutional Neural Network (CNN)-based, point-based, and <span class="search-hit mathjax">Transformer</span>-based methods, have achieved strong performance on synthetic benchmarks. However, due to the limitations of modality, scalability, and generative capacity, their generalization to novel objects and real-world scenarios remains challenging. In this paper, we propose MGPC, a generalizable <span class="search-hit mathjax">multimodal</span> point cloud completion framework that integrates point clouds, RGB images, and text within a unified architecture. MGPC introduces an innovative modality dropout strategy, a <span class="search-hit mathjax">Transformer</span>-based fusion module, and a novel progressive generator to improve robustness, scalability, and geometric modeling capability. We further develop an automatic data generation pipeline and construct MGPC-1M, a large-scale benchmark with over 1,000 categories and one million training pairs. Extensive experiments on MGPC-1M and in-the-wild data demonstrate that the proposed method consistently outperforms prior baselines and exhibits strong generalization under real-world conditions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.03660v1-abstract-full').style.display = 'none'; document.getElementById('2601.03660v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Code and dataset are available at https://github.com/L-J-Yuan/MGPC</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.03482">arXiv:2601.03482</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.03482">pdf</a>, <a href="https://arxiv.org/ps/2601.03482">ps</a>, <a href="https://arxiv.org/format/2601.03482">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Applications">stat.AP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Personalization of Large Foundation Models for Health Interventions
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Konigorski%2C+S">Stefan Konigorski</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Vedder%2C+J+E">Johannes E. Vedder</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Owoyele%2C+B+A">Babajide Alamu Owoyele</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=%C3%96zkan%2C+%C4%B0">İbrahim Özkan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.03482v1-abstract-short" style="display: inline;">
        Large foundation models (LFMs) <span class="search-hit mathjax">transform</span> healthcare AI in prevention, diagnostics, and treatment. However, whether LFMs can provide truly personalized treatment recommendations remains an open question. Recent research has revealed multiple challenges for personalization, including the fundamental generalizability paradox: models achieving high accuracy in o&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.03482v1-abstract-full').style.display = 'inline'; document.getElementById('2601.03482v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.03482v1-abstract-full" style="display: none;">
        Large foundation models (LFMs) <span class="search-hit mathjax">transform</span> healthcare AI in prevention, diagnostics, and treatment. However, whether LFMs can provide truly personalized treatment recommendations remains an open question. Recent research has revealed multiple challenges for personalization, including the fundamental generalizability paradox: models achieving high accuracy in one clinical study perform at chance level in others, demonstrating that personalization and external validity exist in tension. This exemplifies broader contradictions in AI-driven healthcare: the privacy-performance paradox, scale-specificity paradox, and the automation-empathy paradox. As another challenge, the degree of causal understanding required for personalized recommendations, as opposed to mere predictive capacities of LFMs, remains an open question. N-of-1 trials -- crossover self-experiments and the gold standard for individual causal inference in personalized medicine -- resolve these tensions by providing within-person causal evidence while preserving privacy through local experimentation. Despite their impressive capabilities, this paper argues that LFMs cannot replace N-of-1 trials. We argue that LFMs and N-of-1 trials are complementary: LFMs excel at rapid hypothesis generation from population patterns using <span class="search-hit mathjax">multimodal</span> data, while N-of-1 trials excel at causal validation for a given individual. We propose a hybrid framework that combines the strengths of both to enable personalization and navigate the identified paradoxes: LFMs generate ranked intervention candidates with uncertainty estimates, which trigger subsequent N-of-1 trials. Clarifying the boundary between prediction and causation and explicitly addressing the paradoxical tensions are essential for responsible AI integration in personalized medicine.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.03482v1-abstract-full').style.display = 'none'; document.getElementById('2601.03482v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to the AAAI 2026 Workshop on Personalization in the Era of Large Foundation Models (PerFM)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.03464">arXiv:2601.03464</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.03464">pdf</a>, <a href="https://arxiv.org/ps/2601.03464">ps</a>, <a href="https://arxiv.org/format/2601.03464">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Prompting Underestimates LLM Capability for Time Series Classification
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Schumacher%2C+D">Dan Schumacher</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nourbakhsh%2C+E">Erfan Nourbakhsh</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Slavin%2C+R">Rocky Slavin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rios%2C+A">Anthony Rios</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.03464v1-abstract-short" style="display: inline;">
        &hellip;to 0.61-0.67, often matching or exceeding specialized time series models. Layer-wise analyses further show that class-discriminative time series information emerges in early <span class="search-hit mathjax">transformer</span> layers and is amplified by visual and <span class="search-hit mathjax">multimodal</span> inputs. Together, these results demonstrate a systematic mismatch between what LLMs i&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.03464v1-abstract-full').style.display = 'inline'; document.getElementById('2601.03464v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.03464v1-abstract-full" style="display: none;">
        Prompt-based evaluations suggest that large language models (LLMs) perform poorly on time series classification, raising doubts about whether they encode meaningful temporal structure. We show that this conclusion reflects limitations of prompt-based generation rather than the model&#39;s representational capacity by directly comparing prompt outputs with linear probes over the same internal representations. While zero-shot prompting performs near chance, linear probes improve average F1 from 0.15-0.26 to 0.61-0.67, often matching or exceeding specialized time series models. Layer-wise analyses further show that class-discriminative time series information emerges in early <span class="search-hit mathjax">transformer</span> layers and is amplified by visual and <span class="search-hit mathjax">multimodal</span> inputs. Together, these results demonstrate a systematic mismatch between what LLMs internally represent and what prompt-based evaluation reveals, leading current evaluations to underestimate their time series understanding.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.03464v1-abstract-full').style.display = 'none'; document.getElementById('2601.03464v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages + Appendix and References, 9 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.03460">arXiv:2601.03460</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.03460">pdf</a>, <a href="https://arxiv.org/ps/2601.03460">ps</a>, <a href="https://arxiv.org/format/2601.03460">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        FROST-Drive: Scalable and Efficient End-to-End Driving with a Frozen Vision Encoder
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+Z">Zeyu Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+Y">Yimin Zhu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+Y">Yu Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+Y">Yu Sun</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.03460v1-abstract-short" style="display: inline;">
        &hellip;frozen, our approach directly transfers the rich, generalized world knowledge from the VLM to the driving task. Our model architecture combines this frozen encoder with a <span class="search-hit mathjax">transformer</span>-based adapter for <span class="search-hit mathjax">multimodal</span> fusion and a GRU-based decoder for smooth waypoint generation. Furthermore, we introduce a custom loss funct&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.03460v1-abstract-full').style.display = 'inline'; document.getElementById('2601.03460v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.03460v1-abstract-full" style="display: none;">
        End-to-end (E2E) models in autonomous driving aim to directly map sensor inputs to control commands, but their ability to generalize to novel and complex scenarios remains a key challenge. The common practice of fully fine-tuning the vision encoder on driving datasets potentially limits its generalization by causing the model to specialize too heavily in the training data. This work challenges the necessity of this training paradigm. We propose FROST-Drive, a novel E2E architecture designed to preserve and leverage the powerful generalization capabilities of a pretrained vision encoder from a Vision-Language Model (VLM). By keeping the encoder&#39;s weights frozen, our approach directly transfers the rich, generalized world knowledge from the VLM to the driving task. Our model architecture combines this frozen encoder with a <span class="search-hit mathjax">transformer</span>-based adapter for <span class="search-hit mathjax">multimodal</span> fusion and a GRU-based decoder for smooth waypoint generation. Furthermore, we introduce a custom loss function designed to directly optimize for Rater Feedback Score (RFS), a metric that prioritizes robust trajectory planning. We conduct extensive experiments on Waymo Open E2E Dataset, a large-scale datasets deliberately curated to capture the long-tail scenarios, demonstrating that our frozen-encoder approach significantly outperforms models that employ full fine-tuning. Our results provide substantial evidence that preserving the broad knowledge of a capable VLM is a more effective strategy for achieving robust, generalizable driving performance than intensive domain-specific adaptation. This offers a new pathway for developing vision-based models that can better handle the complexities of real-world application domains.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.03460v1-abstract-full').style.display = 'none'; document.getElementById('2601.03460v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.03329">arXiv:2601.03329</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.03329">pdf</a>, <a href="https://arxiv.org/ps/2601.03329">ps</a>, <a href="https://arxiv.org/format/2601.03329">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Attention mechanisms in neural networks
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hays%2C+H">Hasi Hays</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.03329v1-abstract-short" style="display: inline;">
        &hellip;foundations, computational properties, and practical implementations in contemporary deep learning systems. Applications in natural language processing, computer vision, and <span class="search-hit mathjax">multimodal</span> learning demonstrate the versatility of attention mechanisms. We examine language modeling with autoregressive&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.03329v1-abstract-full').style.display = 'inline'; document.getElementById('2601.03329v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.03329v1-abstract-full" style="display: none;">
        Attention mechanisms represent a fundamental paradigm shift in neural network architectures, enabling models to selectively focus on relevant portions of input sequences through learned weighting functions. This monograph provides a comprehensive and rigorous mathematical treatment of attention mechanisms, encompassing their theoretical foundations, computational properties, and practical implementations in contemporary deep learning systems. Applications in natural language processing, computer vision, and <span class="search-hit mathjax">multimodal</span> learning demonstrate the versatility of attention mechanisms. We examine language modeling with autoregressive <span class="search-hit mathjax">transformers</span>, bidirectional encoders for representation learning, sequence-to-sequence translation, Vision <span class="search-hit mathjax">Transformers</span> for image classification, and cross-modal attention for vision-language tasks. Empirical analysis reveals training characteristics, scaling laws that relate performance to model size and computation, attention pattern visualizations, and performance benchmarks across standard datasets. We discuss the interpretability of learned attention patterns and their relationship to linguistic and visual structures. The monograph concludes with a critical examination of current limitations, including computational scalability, data efficiency, systematic generalization, and interpretability challenges.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.03329v1-abstract-full').style.display = 'none'; document.getElementById('2601.03329v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.02737">arXiv:2601.02737</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.02737">pdf</a>, <a href="https://arxiv.org/ps/2601.02737">ps</a>, <a href="https://arxiv.org/format/2601.02737">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Unveiling and Bridging the Functional Perception Gap in MLLMs: Atomic Visual Alignment and Hierarchical Evaluation via PET-Bench
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ye%2C+Z">Zanting Ye</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Niu%2C+X">Xiaolong Niu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+X">Xuanbin Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Han%2C+X">Xu Han</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+S">Shengyuan Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hao%2C+J">Jing Hao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Peng%2C+Z">Zhihao Peng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+H">Hao Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lv%2C+J">Jieqin Lv</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+F">Fanghu Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+Y">Yanchao Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+H">Hubing Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yuan%2C+Y">Yixuan Yuan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zaidi%2C+H">Habib Zaidi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rahmim%2C+A">Arman Rahmim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zheng%2C+Y">Yefeng Zheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lu%2C+L">Lijun Lu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.02737v1-abstract-short" style="display: inline;">
        While <span class="search-hit mathjax">Multimodal</span> Large Language Models (MLLMs) have demonstrated remarkable proficiency in tasks such as abnormality detection and report generation for anatomical modalities, their capability in functional imaging remains largely unexplored. In this work, we identify and quantify a fundamental functional perception gap: the inability of current vision encod&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02737v1-abstract-full').style.display = 'inline'; document.getElementById('2601.02737v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.02737v1-abstract-full" style="display: none;">
        While <span class="search-hit mathjax">Multimodal</span> Large Language Models (MLLMs) have demonstrated remarkable proficiency in tasks such as abnormality detection and report generation for anatomical modalities, their capability in functional imaging remains largely unexplored. In this work, we identify and quantify a fundamental functional perception gap: the inability of current vision encoders to decode functional tracer biodistribution independent of morphological priors. Identifying Positron Emission Tomography (PET) as the quintessential modality to investigate this disconnect, we introduce PET-Bench, the first large-scale functional imaging benchmark comprising 52,308 hierarchical QA pairs from 9,732 multi-site, multi-tracer PET studies. Extensive evaluation of 19 state-of-the-art MLLMs reveals a critical safety hazard termed the Chain-of-Thought (CoT) hallucination trap. We observe that standard CoT prompting, widely considered to enhance reasoning, paradoxically decouples linguistic generation from visual evidence in PET, producing clinically fluent but factually ungrounded diagnoses. To resolve this, we propose Atomic Visual Alignment (AVA), a simple fine-tuning strategy that enforces the mastery of low-level functional perception prior to high-level diagnostic reasoning. Our results demonstrate that AVA effectively bridges the perception gap, <span class="search-hit mathjax">transforming</span> CoT from a source of hallucination into a robust inference tool and improving diagnostic accuracy by up to 14.83%. Code and data are available at https://github.com/yezanting/PET-Bench.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02737v1-abstract-full').style.display = 'none'; document.getElementById('2601.02737v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, 6 figures, 6 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.02677">arXiv:2601.02677</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.02677">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Risk Management">q-fin.RM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Statistical Finance">q-fin.ST</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Uni-FinLLM: A Unified <span class="search-hit mathjax">Multimodal</span> Large Language Model with Modular Task Heads for Micro-Level Stock Prediction and Macro-Level Systemic Risk Assessment
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+G">Gongao Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zeng%2C+H">Haijiang Zeng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jiang%2C+L">Lu Jiang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.02677v1-abstract-short" style="display: inline;">
        &hellip;to systemic vulnerabilities. Existing approaches often treat these tasks in isolation, failing to capture cross-scale dependencies. We propose Uni-FinLLM, a unified <span class="search-hit mathjax">multimodal</span> large language model that uses a shared&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02677v1-abstract-full').style.display = 'inline'; document.getElementById('2601.02677v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.02677v1-abstract-full" style="display: none;">
        Financial institutions and regulators require systems that integrate heterogeneous data to assess risks from stock fluctuations to systemic vulnerabilities. Existing approaches often treat these tasks in isolation, failing to capture cross-scale dependencies. We propose Uni-FinLLM, a unified <span class="search-hit mathjax">multimodal</span> large language model that uses a shared <span class="search-hit mathjax">Transformer</span> backbone and modular task heads to jointly process financial text, numerical time series, fundamentals, and visual data. Through cross-modal attention and multi-task optimization, it learns a coherent representation for micro-, meso-, and macro-level predictions. Evaluated on stock forecasting, credit-risk assessment, and systemic-risk detection, Uni-FinLLM significantly outperforms baselines. It raises stock directional accuracy to 67.4% (from 61.7%), credit-risk accuracy to 84.1% (from 79.6%), and macro early-warning accuracy to 82.3%. Results validate that a unified <span class="search-hit mathjax">multimodal</span> LLM can jointly model asset behavior and systemic vulnerabilities, offering a scalable decision-support engine for finance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02677v1-abstract-full').style.display = 'none'; document.getElementById('2601.02677v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.02456">arXiv:2601.02456</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.02456">pdf</a>, <a href="https://arxiv.org/ps/2601.02456">ps</a>, <a href="https://arxiv.org/format/2601.02456">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        InternVLA-A1: Unifying Understanding, Generation and Action for Robotic Manipulation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Cai%2C+J">Junhao Cai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cai%2C+Z">Zetao Cai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cao%2C+J">Jiafei Cao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+Y">Yilun Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+Z">Zeyu He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jiang%2C+L">Lei Jiang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+H">Hang Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+H">Hengjie Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Y">Yang Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Y">Yufei Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lu%2C+Y">Yanan Lu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lv%2C+Q">Qi Lv</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ma%2C+H">Haoxiang Ma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pang%2C+J">Jiangmiao Pang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qiao%2C+Y">Yu Qiao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qiu%2C+Z">Zherui Qiu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shen%2C+Y">Yanqing Shen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+X">Xu Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tian%2C+Y">Yang Tian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+B">Bolun Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+H">Hanqing Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+J">Jiaheng Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+T">Tai Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wei%2C+X">Xueyuan Wei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+C">Chao Wu</a>
      , et al. (17 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.02456v1-abstract-short" style="display: inline;">
        Prevalent Vision-Language-Action (VLA) models are typically built upon <span class="search-hit mathjax">Multimodal</span> Large Language Models (MLLMs) and demonstrate exceptional proficiency in semantic understanding, but they inherently lack the capability to deduce physical world dynamics. Consequently, recent approaches have shifted toward World Models, typically formulated via video predictio&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02456v1-abstract-full').style.display = 'inline'; document.getElementById('2601.02456v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.02456v1-abstract-full" style="display: none;">
        Prevalent Vision-Language-Action (VLA) models are typically built upon <span class="search-hit mathjax">Multimodal</span> Large Language Models (MLLMs) and demonstrate exceptional proficiency in semantic understanding, but they inherently lack the capability to deduce physical world dynamics. Consequently, recent approaches have shifted toward World Models, typically formulated via video prediction; however, these methods often suffer from a lack of semantic grounding and exhibit brittleness when handling prediction errors. To synergize semantic understanding with dynamic predictive capabilities, we present InternVLA-A1. This model employs a unified Mixture-of-<span class="search-hit mathjax">Transformers</span> architecture, coordinating three experts for scene understanding, visual foresight generation, and action execution. These components interact seamlessly through a unified masked self-attention mechanism. Building upon InternVL3 and Qwen3-VL, we instantiate InternVLA-A1 at 2B and 3B parameter scales. We pre-train these models on hybrid synthetic-real datasets spanning InternData-A1 and Agibot-World, covering over 533M frames. This hybrid training strategy effectively harnesses the diversity of synthetic simulation data while minimizing the sim-to-real gap. We evaluated InternVLA-A1 across 12 real-world robotic tasks and simulation benchmark. It significantly outperforms leading models like pi0 and GR00T N1.5, achieving a 14.5\% improvement in daily tasks and a 40\%-73.3\% boost in dynamic settings, such as conveyor belt sorting.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02456v1-abstract-full').style.display = 'none'; document.getElementById('2601.02456v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Homepage: https://internrobotics.github.io/internvla-a1.github.io/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.02358">arXiv:2601.02358</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.02358">pdf</a>, <a href="https://arxiv.org/ps/2601.02358">ps</a>, <a href="https://arxiv.org/format/2601.02358">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        VINO: A Unified Visual Generator with Interleaved OmniModal Context
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+J">Junyi Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+T">Tong He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fu%2C+Z">Zhoujie Fu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wan%2C+P">Pengfei Wan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gai%2C+K">Kun Gai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ye%2C+W">Weicai Ye</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.02358v1-abstract-short" style="display: inline;">
        &hellip;on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a <span class="search-hit mathjax">Multimodal</span> Diffusion <span class="search-hit mathjax">Transformer</span> (MMDiT), where <span class="search-hit mathjax">multimodal</span> inputs are encoded as interleaved conditioning token&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02358v1-abstract-full').style.display = 'inline'; document.getElementById('2601.02358v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.02358v1-abstract-full" style="display: none;">
        We present VINO, a unified visual generator that performs image and video generation and editing within a single framework. Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a <span class="search-hit mathjax">Multimodal</span> Diffusion <span class="search-hit mathjax">Transformer</span> (MMDiT), where <span class="search-hit mathjax">multimodal</span> inputs are encoded as interleaved conditioning tokens, and then used to guide the diffusion process. This design supports multi-reference grounding, long-form instruction following, and coherent identity preservation across static and dynamic content, while avoiding modality-specific architectural components. To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output. Across diverse generation and editing benchmarks, VINO demonstrates strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits. Our results highlight a practical path toward scalable unified visual generation, and the promise of interleaved, in-context computation as a foundation for general-purpose visual creation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02358v1-abstract-full').style.display = 'none'; document.getElementById('2601.02358v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Project page: https://sotamak1r.github.io/VINO-web/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.02356">arXiv:2601.02356</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.02356">pdf</a>, <a href="https://arxiv.org/ps/2601.02356">ps</a>, <a href="https://arxiv.org/format/2601.02356">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric <span class="search-hit mathjax">Transformation</span> in Scenes
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Tan%2C+J">Jing Tan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Z">Zhaoyang Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shen%2C+Y">Yantao Shen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cai%2C+J">Jiarui Cai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+S">Shuo Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jiajun Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xia%2C+W">Wei Xia</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tu%2C+Z">Zhuowen Tu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Soatto%2C+S">Stefano Soatto</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.02356v2-abstract-short" style="display: inline;">
        We introduce Talk2Move, a reinforcement learning (RL) based diffusion framework for text-instructed spatial <span class="search-hit mathjax">transformation</span> of objects within scenes. Spatially manipulating objects in a scene through natural language poses a challenge for&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02356v2-abstract-full').style.display = 'inline'; document.getElementById('2601.02356v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.02356v2-abstract-full" style="display: none;">
        We introduce Talk2Move, a reinforcement learning (RL) based diffusion framework for text-instructed spatial <span class="search-hit mathjax">transformation</span> of objects within scenes. Spatially manipulating objects in a scene through natural language poses a challenge for <span class="search-hit mathjax">multimodal</span> generation systems. While existing text-based manipulation methods can adjust appearance or style, they struggle to perform object-level geometric <span class="search-hit mathjax">transformations</span>-such as translating, rotating, or resizing objects-due to scarce paired supervision and pixel-level optimization limits. Talk2Move employs Group Relative Policy Optimization (GRPO) to explore geometric actions through diverse rollouts generated from input images and lightweight textual variations, removing the need for costly paired data. A spatial reward guided model aligns geometric <span class="search-hit mathjax">transformations</span> with linguistic description, while off-policy step evaluation and active step sampling improve learning efficiency by focusing on informative <span class="search-hit mathjax">transformation</span> stages. Furthermore, we design object-centric spatial rewards that evaluate displacement, rotation, and scaling behaviors directly, enabling interpretable and coherent <span class="search-hit mathjax">transformations</span>. Experiments on curated benchmarks demonstrate that Talk2Move achieves precise, consistent, and semantically faithful object <span class="search-hit mathjax">transformations</span>, outperforming existing text-guided editing approaches in both spatial accuracy and scene coherence.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02356v2-abstract-full').style.display = 'none'; document.getElementById('2601.02356v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 January, 2026; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 January, 2026;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Project page: https://sparkstj.github.io/talk2move</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.02249">arXiv:2601.02249</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.02249">pdf</a>, <a href="https://arxiv.org/ps/2601.02249">ps</a>, <a href="https://arxiv.org/format/2601.02249">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SLGNet: Synergizing Structural Priors and Language-Guided Modulation for <span class="search-hit mathjax">Multimodal</span> Object Detection
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Xiang%2C+X">Xiantai Xiang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+G">Guangyao Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wen%2C+Z">Zixiao Wen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+W">Wenshuai Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Niu%2C+B">Ben Niu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+F">Feng Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+L">Lijia Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Q">Qiantong Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Y">Yuhan Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pan%2C+Z">Zongxu Pan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hu%2C+Y">Yuxin Hu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.02249v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Multimodal</span> object detection leveraging RGB and Infrared (IR) images is pivotal for robust perception in all-weather scenarios. While recent adapter-based approaches efficiently transfer RGB-pretrained foundation models to this task, they often prioritize model efficiency at the expense of cross-modal structural consistency. Consequently, critical structural&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02249v1-abstract-full').style.display = 'inline'; document.getElementById('2601.02249v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.02249v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Multimodal</span> object detection leveraging RGB and Infrared (IR) images is pivotal for robust perception in all-weather scenarios. While recent adapter-based approaches efficiently transfer RGB-pretrained foundation models to this task, they often prioritize model efficiency at the expense of cross-modal structural consistency. Consequently, critical structural cues are frequently lost when significant domain gaps arise, such as in high-contrast or nighttime environments. Moreover, conventional static <span class="search-hit mathjax">multimodal</span> fusion mechanisms typically lack environmental awareness, resulting in suboptimal adaptation and constrained detection performance under complex, dynamic scene variations. To address these limitations, we propose SLGNet, a parameter-efficient framework that synergizes hierarchical structural priors and language-guided modulation within a frozen Vision <span class="search-hit mathjax">Transformer</span> (ViT)-based foundation model. Specifically, we design a Structure-Aware Adapter to extract hierarchical structural representations from both modalities and dynamically inject them into the ViT to compensate for structural degradation inherent in ViT-based backbones. Furthermore, we propose a Language-Guided Modulation module that exploits VLM-driven structured captions to dynamically recalibrate visual features, thereby endowing the model with robust environmental awareness. Extensive experiments on the LLVIP, FLIR, KAIST, and DroneVehicle datasets demonstrate that SLGNet establishes new state-of-the-art performance. Notably, on the LLVIP benchmark, our method achieves an mAP of 66.1, while reducing trainable parameters by approximately 87% compared to traditional full fine-tuning. This confirms SLGNet as a robust and efficient solution for <span class="search-hit mathjax">multimodal</span> perception.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02249v1-abstract-full').style.display = 'none'; document.getElementById('2601.02249v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.02211">arXiv:2601.02211</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.02211">pdf</a>, <a href="https://arxiv.org/ps/2601.02211">ps</a>, <a href="https://arxiv.org/format/2601.02211">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Unraveling MMDiT Blocks: Training-free Analysis and Enhancement of Text-conditioned Diffusion
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+B">Binglei Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+M">Mengping Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tan%2C+Z">Zhiyu Tan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+J">Junping Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+H">Hao Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.02211v1-abstract-short" style="display: inline;">
        Recent breakthroughs of <span class="search-hit mathjax">transformer</span>-based diffusion models, particularly with <span class="search-hit mathjax">Multimodal</span> Diffusion <span class="search-hit mathjax">Transformers</span> (MMDiT) driven models like FLUX and Qwen Image, have facilitated thrilling experiences in text-to-image generation and editing. To understand the internal mechanism of&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02211v1-abstract-full').style.display = 'inline'; document.getElementById('2601.02211v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.02211v1-abstract-full" style="display: none;">
        Recent breakthroughs of <span class="search-hit mathjax">transformer</span>-based diffusion models, particularly with <span class="search-hit mathjax">Multimodal</span> Diffusion <span class="search-hit mathjax">Transformers</span> (MMDiT) driven models like FLUX and Qwen Image, have facilitated thrilling experiences in text-to-image generation and editing. To understand the internal mechanism of MMDiT-based models, existing methods tried to analyze the effect of specific components like positional encoding and attention layers. Yet, a comprehensive understanding of how different blocks and their interactions with textual conditions contribute to the synthesis process remains elusive. In this paper, we first develop a systematic pipeline to comprehensively investigate each block&#39;s functionality by removing, disabling and enhancing textual hidden-states at corresponding blocks. Our analysis reveals that 1) semantic information appears in earlier blocks and finer details are rendered in later blocks, 2) removing specific blocks is usually less disruptive than disabling text conditions, and 3) enhancing textual conditions in selective blocks improves semantic attributes. Building on these observations, we further propose novel training-free strategies for improved text alignment, precise editing, and acceleration. Extensive experiments demonstrated that our method outperforms various baselines and remains flexible across text-to-image generation, image editing, and inference acceleration. Our method improves T2I-Combench++ from 56.92% to 63.00% and GenEval from 66.42% to 71.63% on SD3.5, without sacrificing synthesis quality. These results advance understanding of MMDiT models and provide valuable insights to unlock new possibilities for further improvements.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02211v1-abstract-full').style.display = 'none'; document.getElementById('2601.02211v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">11 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.02204">arXiv:2601.02204</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.02204">pdf</a>, <a href="https://arxiv.org/ps/2601.02204">ps</a>, <a href="https://arxiv.org/format/2601.02204">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        NextFlow: Unified Sequential Modeling Activates <span class="search-hit mathjax">Multimodal</span> Understanding and Generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+H">Huichao Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qu%2C+L">Liao Qu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Y">Yiheng Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+H">Hang Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Song%2C+Y">Yangyang Song</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+Y">Yongsheng Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+S">Shikun Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+X">Xian Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+X">Xu Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jiang%2C+Y">Yi Jiang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ye%2C+H">Hu Ye</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+B">Bo Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gao%2C+Y">Yiming Gao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+P">Peng Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+A">Akide Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+Z">Zhipeng Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Deng%2C+Q">Qili Deng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xing%2C+L">Linjie Xing</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+J">Jiyang Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Z">Zhao Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+Y">Yang Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+M">Mingcong Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Y">Yi Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+Q">Qian He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hu%2C+X">Xiwei Hu</a>
      , et al. (11 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.02204v1-abstract-short" style="display: inline;">
        We present NextFlow, a unified decoder-only autoregressive <span class="search-hit mathjax">transformer</span> trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates <span class="search-hit mathjax">multimodal</span> understanding and generation capabilities, unlocking abi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02204v1-abstract-full').style.display = 'inline'; document.getElementById('2601.02204v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.02204v1-abstract-full" style="display: none;">
        We present NextFlow, a unified decoder-only autoregressive <span class="search-hit mathjax">transformer</span> trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates <span class="search-hit mathjax">multimodal</span> understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02204v1-abstract-full').style.display = 'none'; document.getElementById('2601.02204v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Project page: https://github.com/ByteVisionLab/NextFlow</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.02008">arXiv:2601.02008</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.02008">pdf</a>, <a href="https://arxiv.org/ps/2601.02008">ps</a>, <a href="https://arxiv.org/format/2601.02008">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        XAI-MeD: Explainable Knowledge Guided Neuro-Symbolic Framework for Domain Generalization and Rare Class Detection in Medical Imaging
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Urooj%2C+M">Midhat Urooj</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Banerjee%2C+A">Ayan Banerjee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gupta%2C+S">Sandeep Gupta</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.02008v1-abstract-short" style="display: inline;">
        &hellip;class sensitivity and deliver transparent clinically aligned interpretations The framework encodes clinical expertise as logical connectives over atomic medical propositions <span class="search-hit mathjax">transforming</span> them into machine checkable class specific rules Their diagnostic utility is quantified through weighted feature satisfaction scores enabling a symbolic reasoning branch tha&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02008v1-abstract-full').style.display = 'inline'; document.getElementById('2601.02008v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.02008v1-abstract-full" style="display: none;">
        Explainability domain generalization and rare class reliability are critical challenges in medical AI where deep models often fail under real world distribution shifts and exhibit bias against infrequent clinical conditions This paper introduces XAIMeD an explainable medical AI framework that integrates clinically accurate expert knowledge into deep learning through a unified neuro symbolic architecture XAIMeD is designed to improve robustness under distribution shift enhance rare class sensitivity and deliver transparent clinically aligned interpretations The framework encodes clinical expertise as logical connectives over atomic medical propositions <span class="search-hit mathjax">transforming</span> them into machine checkable class specific rules Their diagnostic utility is quantified through weighted feature satisfaction scores enabling a symbolic reasoning branch that complements neural predictions A confidence weighted fusion integrates symbolic and deep outputs while a Hunt inspired adaptive routing mechanism guided by Entropy Imbalance Gain EIG and Rare Class Gini mitigates class imbalance high intra class variability and uncertainty We evaluate XAIMeD across diverse modalities on four challenging tasks i Seizure Onset Zone SOZ localization from rs fMRI ii Diabetic Retinopathy grading across 6 multicenter datasets demonstrate substantial performance improvements including 6 percent gains in cross domain generalization and a 10 percent improved rare class F1 score far outperforming state of the art deep learning baselines Ablation studies confirm that the clinically grounded symbolic components act as effective regularizers ensuring robustness to distribution shifts XAIMeD thus provides a principled clinically faithful and interpretable approach to <span class="search-hit mathjax">multimodal</span> medical AI.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02008v1-abstract-full').style.display = 'none'; document.getElementById('2601.02008v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at AAAI Bridge Program 2026</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.01593">arXiv:2601.01593</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.01593">pdf</a>, <a href="https://arxiv.org/ps/2601.01593">ps</a>, <a href="https://arxiv.org/format/2601.01593">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Beyond Patches: Global-aware Autoregressive Model for <span class="search-hit mathjax">Multimodal</span> Few-Shot Font Generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Cai%2C+H">Haonan Cai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Luo%2C+Y">Yuxuan Luo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lian%2C+Z">Zhouhui Lian</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.01593v1-abstract-short" style="display: inline;">
        Manual font design is an intricate process that <span class="search-hit mathjax">transforms</span> a stylistic visual concept into a coherent glyph set. This challenge persists in automated Few-shot Font Generation (FFG), where models often struggle to preserve both the structural integrity and stylistic fidelity from limited references. While autoregressive (AR) models have demonstrated impressiv&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.01593v1-abstract-full').style.display = 'inline'; document.getElementById('2601.01593v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.01593v1-abstract-full" style="display: none;">
        Manual font design is an intricate process that <span class="search-hit mathjax">transforms</span> a stylistic visual concept into a coherent glyph set. This challenge persists in automated Few-shot Font Generation (FFG), where models often struggle to preserve both the structural integrity and stylistic fidelity from limited references. While autoregressive (AR) models have demonstrated impressive generative capabilities, their application to FFG is constrained by conventional patch-level tokenization, which neglects global dependencies crucial for coherent font synthesis. Moreover, existing FFG methods remain within the image-to-image paradigm, relying solely on visual references and overlooking the role of language in conveying stylistic intent during font design. To address these limitations, we propose GAR-Font, a novel AR framework for <span class="search-hit mathjax">multimodal</span> few-shot font generation. GAR-Font introduces a global-aware tokenizer that effectively captures both local structures and global stylistic patterns, a <span class="search-hit mathjax">multimodal</span> style encoder offering flexible style control through a lightweight language-style adapter without requiring intensive <span class="search-hit mathjax">multimodal</span> pretraining, and a post-refinement pipeline that further enhances structural fidelity and style coherence. Extensive experiments show that GAR-Font outperforms existing FFG methods, excelling in maintaining global style faithfulness and achieving higher-quality results with textual stylistic guidance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.01593v1-abstract-full').style.display = 'none'; document.getElementById('2601.01593v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">25 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.01322">arXiv:2601.01322</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.01322">pdf</a>, <a href="https://arxiv.org/ps/2601.01322">ps</a>, <a href="https://arxiv.org/format/2601.01322">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        LinMU: <span class="search-hit mathjax">Multimodal</span> Understanding Made Linear
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+H">Hongjie Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jha%2C+N+K">Niraj K. Jha</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.01322v1-abstract-short" style="display: inline;">
        &hellip;and makes their understanding of high-resolution images and long-context videos prohibitively expensive. To address this challenge, we introduce LinMU (Linear-complexity <span class="search-hit mathjax">Multimodal</span> Understanding), a VLM design that achieves linear complexity without using any quadratic-complexity modules while maintaining the performance of global-attention-based VLMs. LinMU&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.01322v1-abstract-full').style.display = 'inline'; document.getElementById('2601.01322v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.01322v1-abstract-full" style="display: none;">
        Modern Vision-Language Models (VLMs) achieve impressive performance but are limited by the quadratic complexity of self-attention, which prevents their deployment on edge devices and makes their understanding of high-resolution images and long-context videos prohibitively expensive. To address this challenge, we introduce LinMU (Linear-complexity <span class="search-hit mathjax">Multimodal</span> Understanding), a VLM design that achieves linear complexity without using any quadratic-complexity modules while maintaining the performance of global-attention-based VLMs. LinMU replaces every self-attention layer in the VLM with the M-MATE block: a dual-branch module that combines a bidirectional state-space model for global context (Flex-MA branch) with localized Swin-style window attention (Local-Swin branch) for adjacent correlations. To <span class="search-hit mathjax">transform</span> a pre-trained VLM into the LinMU architecture, we propose a three-stage distillation framework that (i) initializes both branches with self-attention weights and trains the Flex-MA branch alone, (ii) unfreezes the Local-Swin branch and fine-tunes it jointly with the Flex-MA branch, and (iii) unfreezes the remaining blocks and fine-tunes them using LoRA adapters, while regressing on hidden states and token-level logits of the frozen VLM teacher. On MMMU, TextVQA, LongVideoBench, Video-MME, and other benchmarks, LinMU matches the performance of teacher models, yet reduces Time-To-First-Token (TTFT) by up to 2.7$\times$ and improves token throughput by up to 9.0$\times$ on minute-length videos. Ablations confirm the importance of each distillation stage and the necessity of the two branches of the M-MATE block. The proposed framework demonstrates that state-of-the-art <span class="search-hit mathjax">multimodal</span> reasoning can be achieved without quadratic attention, thus opening up avenues for long-context VLMs that can deal with high-resolution images and long videos.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.01322v1-abstract-full').style.display = 'none'; document.getElementById('2601.01322v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">23 pages, 7 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.00907">arXiv:2601.00907</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.00907">pdf</a>, <a href="https://arxiv.org/ps/2601.00907">ps</a>, <a href="https://arxiv.org/format/2601.00907">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Placenta Accreta Spectrum Detection using <span class="search-hit mathjax">Multimodal</span> Deep Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ali%2C+S">Sumaiya Ali</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Alhothali%2C+A">Areej Alhothali</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Albasri%2C+S">Sameera Albasri</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Alzamzami%2C+O">Ohoud Alzamzami</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Abduljabbar%2C+A">Ahmed Abduljabbar</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Alwazzan%2C+M">Muhammad Alwazzan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.00907v1-abstract-short" style="display: inline;">
        &hellip;maternal and neonatal risks. This study aimed to develop and validate a deep learning framework that enhances PAS detection by integrating multiple imaging modalities. A <span class="search-hit mathjax">multimodal</span> deep learning model was designed using an intermediate feature-level fusion architecture combining 3D Magnetic Resonance Imaging (MRI) and 2D Ultrasound (US) scans. Unimodal featu&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.00907v1-abstract-full').style.display = 'inline'; document.getElementById('2601.00907v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.00907v1-abstract-full" style="display: none;">
        Placenta Accreta Spectrum (PAS) is a life-threatening obstetric complication involving abnormal placental invasion into the uterine wall. Early and accurate prenatal diagnosis is essential to reduce maternal and neonatal risks. This study aimed to develop and validate a deep learning framework that enhances PAS detection by integrating multiple imaging modalities. A <span class="search-hit mathjax">multimodal</span> deep learning model was designed using an intermediate feature-level fusion architecture combining 3D Magnetic Resonance Imaging (MRI) and 2D Ultrasound (US) scans. Unimodal feature extractors, a 3D DenseNet121-Vision <span class="search-hit mathjax">Transformer</span> for MRI and a 2D ResNet50 for US, were selected after systematic comparative analysis. Curated datasets comprising 1,293 MRI and 1,143 US scans were used to train the unimodal models and paired samples of patient-matched MRI-US scans was isolated for <span class="search-hit mathjax">multimodal</span> model development and evaluation. On an independent test set, the <span class="search-hit mathjax">multimodal</span> fusion model achieved superior performance, with an accuracy of 92.5% and an Area Under the Receiver Operating Characteristic Curve (AUC) of 0.927, outperforming the MRI-only (82.5%, AUC 0.825) and US-only (87.5%, AUC 0.879) models. Integrating MRI and US features provides complementary diagnostic information, demonstrating strong potential to enhance prenatal risk assessment and improve patient outcomes.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.00907v1-abstract-full').style.display = 'none'; document.getElementById('2601.00907v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 December, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.00670">arXiv:2601.00670</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.00670">pdf</a>, <a href="https://arxiv.org/ps/2601.00670">ps</a>, <a href="https://arxiv.org/format/2601.00670">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Wave2Word: A <span class="search-hit mathjax">Multimodal</span> <span class="search-hit mathjax">Transformer</span> Framework for Joint EEG-Text Alignment and Multi-Task Representation Learning in Neurocritical Care
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Samanta%2C+A+K">Argha Kamal Samanta</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mewada%2C+D">Deepak Mewada</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sarma%2C+M">Monalisa Sarma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Samanta%2C+D">Debasis Samanta</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.00670v1-abstract-short" style="display: inline;">
        &hellip;exhibits overlapping patterns, graded expert agreement, and temporal persistence, which are not well captured by classification objectives alone. This work proposes a <span class="search-hit mathjax">multimodal</span> EEG representation learning framework that integrates signal-domain modeling with structured clinical language supervision. First, raw EEG is&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.00670v1-abstract-full').style.display = 'inline'; document.getElementById('2601.00670v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.00670v1-abstract-full" style="display: none;">
        Continuous electroencephalography (EEG) is routinely used in neurocritical care to monitor seizures and other harmful brain activity, including rhythmic and periodic patterns that are clinically significant. Although deep learning methods have achieved high accuracy in seizure detection, most existing approaches remain seizure-centric, rely on discrete-label supervision, and are primarily evaluated using accuracy-based metrics. A central limitation of current EEG modeling practice is the weak correspondence between learned representations and how EEG findings are interpreted and summarized in clinical workflows. Harmful EEG activity exhibits overlapping patterns, graded expert agreement, and temporal persistence, which are not well captured by classification objectives alone. This work proposes a <span class="search-hit mathjax">multimodal</span> EEG representation learning framework that integrates signal-domain modeling with structured clinical language supervision. First, raw EEG is <span class="search-hit mathjax">transformed</span> into a longitudinal bipolar montage and time-frequency representations. Second, dual <span class="search-hit mathjax">transformer</span>-based encoders model complementary temporal and frequency-centric dependencies and are fused using an adaptive gating mechanism. Third, EEG embeddings are aligned with structured expert consensus descriptions through a contrastive objective. Finally, an EEG-conditioned text reconstruction loss is introduced as a representation-level constraint alongside standard classification loss. Experimental evaluation using a controlled train-validation-test split achieves a six-class test accuracy of 0.9797. Ablation analyses show that removing contrastive alignment reduces cross-modal retrieval performance from Recall@10 of 0.3390 to 0.0045, despite minimal change in classification accuracy. These findings demonstrate that discriminative accuracy does not reliably reflect representation quality for clinically meaningful EEG modeling.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.00670v1-abstract-full').style.display = 'none'; document.getElementById('2601.00670v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2512.24645">arXiv:2512.24645</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2512.24645">pdf</a>, <a href="https://arxiv.org/ps/2512.24645">ps</a>, <a href="https://arxiv.org/format/2512.24645">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3746027.3756869">10.1145/3746027.3756869 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AudioFab: Building A General and Intelligent Audio Factory through Tool Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+C">Cheng Zhu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Han%2C+J">Jing Han</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xue%2C+Q">Qianshuai Xue</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+K">Kehan Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+H">Huan Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Z">Zixing Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2512.24645v1-abstract-short" style="display: inline;">
        Currently, artificial intelligence is profoundly <span class="search-hit mathjax">transforming</span> the audio domain; however, numerous advanced algorithms and tools remain fragmented, lacking a unified and efficient framework to unlock their full potential. Existing audio agent frameworks often suffer from complex environment configurations and inefficient tool collaboration. To address these l&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.24645v1-abstract-full').style.display = 'inline'; document.getElementById('2512.24645v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2512.24645v1-abstract-full" style="display: none;">
        Currently, artificial intelligence is profoundly <span class="search-hit mathjax">transforming</span> the audio domain; however, numerous advanced algorithms and tools remain fragmented, lacking a unified and efficient framework to unlock their full potential. Existing audio agent frameworks often suffer from complex environment configurations and inefficient tool collaboration. To address these limitations, we introduce AudioFab, an open-source agent framework aimed at establishing an open and intelligent audio-processing ecosystem. Compared to existing solutions, AudioFab&#39;s modular design resolves dependency conflicts, simplifying tool integration and extension. It also optimizes tool learning through intelligent selection and few-shot learning, improving efficiency and accuracy in complex audio tasks. Furthermore, AudioFab provides a user-friendly natural language interface tailored for non-expert users. As a foundational framework, AudioFab&#39;s core contribution lies in offering a stable and extensible platform for future research and development in audio and <span class="search-hit mathjax">multimodal</span> AI. The code is available at https://github.com/SmileHnu/AudioFab.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.24645v1-abstract-full').style.display = 'none'; document.getElementById('2512.24645v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 December, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2025.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        ACM Multimedia 2025
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2512.24271">arXiv:2512.24271</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2512.24271">pdf</a>, <a href="https://arxiv.org/ps/2512.24271">ps</a>, <a href="https://arxiv.org/format/2512.24271">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Taming Hallucinations: Boosting MLLMs&#39; Video Understanding via Counterfactual Video Generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+Z">Zhe Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wen%2C+H">Hao Wen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hao%2C+A">Aiming Hao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Song%2C+B">Bingze Song</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+M">Meiqi Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jiahong Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chu%2C+X">Xiangxiang Chu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lu%2C+S">Sheng Lu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+H">Haoqian Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2512.24271v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Multimodal</span> Large Language Models (MLLMs) have made remarkable progress in video understanding. However, they suffer from a critical vulnerability: an over-reliance on language priors, which can lead to visual ungrounded hallucinations, especially when processing counterfactual videos that defy common sense. This limitation, stemming from the intrinsic data i&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.24271v1-abstract-full').style.display = 'inline'; document.getElementById('2512.24271v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2512.24271v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Multimodal</span> Large Language Models (MLLMs) have made remarkable progress in video understanding. However, they suffer from a critical vulnerability: an over-reliance on language priors, which can lead to visual ungrounded hallucinations, especially when processing counterfactual videos that defy common sense. This limitation, stemming from the intrinsic data imbalance between text and video, is challenging to address due to the substantial cost of collecting and annotating counterfactual data. To address this, we introduce DualityForge, a novel counterfactual data synthesis framework that employs controllable, diffusion-based video editing to <span class="search-hit mathjax">transform</span> real-world videos into counterfactual scenarios. By embedding structured contextual information into the video editing and QA generation processes, the framework automatically produces high-quality QA pairs together with original-edited video pairs for contrastive training. Based on this, we build DualityVidQA, a large-scale video dataset designed to reduce MLLM hallucinations. In addition, to fully exploit the contrastive nature of our paired data, we propose Duality-Normalized Advantage Training (DNA-Train), a two-stage SFT-RL training regime where the RL phase applies pair-wise $\ell_1$ advantage normalization, thereby enabling a more stable and efficient policy optimization. Experiments on DualityVidQA-Test demonstrate that our method substantially reduces model hallucinations on counterfactual videos, yielding a relative improvement of 24.0% over the Qwen2.5-VL-7B baseline. Moreover, our approach achieves significant gains across both hallucination and general-purpose benchmarks, indicating strong generalization capability. We will open-source our dataset and code.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.24271v1-abstract-full').style.display = 'none'; document.getElementById('2512.24271v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 December, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">18 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2512.24243">arXiv:2512.24243</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2512.24243">pdf</a>, <a href="https://arxiv.org/ps/2512.24243">ps</a>, <a href="https://arxiv.org/format/2512.24243">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MambaSeg: Harnessing Mamba for Accurate and Efficient Image-Event Semantic Segmentation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Gu%2C+F">Fuqiang Gu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Y">Yuanke Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Long%2C+X">Xianlei Long</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ji%2C+K">Kangping Ji</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+C">Chao Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gu%2C+Q">Qingyi Gu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ni%2C+Z">Zhenliang Ni</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2512.24243v1-abstract-short" style="display: inline;">
        &hellip;task in computer vision with wide-ranging applications, including autonomous driving and robotics. While RGB-based methods have achieved strong performance with CNNs and <span class="search-hit mathjax">Transformers</span>, their effectiveness degrades under fast motion, low-light, or high dynamic range conditions due to limitations of frame cameras. Event cameras offer complementary advantages su&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.24243v1-abstract-full').style.display = 'inline'; document.getElementById('2512.24243v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2512.24243v1-abstract-full" style="display: none;">
        Semantic segmentation is a fundamental task in computer vision with wide-ranging applications, including autonomous driving and robotics. While RGB-based methods have achieved strong performance with CNNs and <span class="search-hit mathjax">Transformers</span>, their effectiveness degrades under fast motion, low-light, or high dynamic range conditions due to limitations of frame cameras. Event cameras offer complementary advantages such as high temporal resolution and low latency, yet lack color and texture, making them insufficient on their own. To address this, recent research has explored <span class="search-hit mathjax">multimodal</span> fusion of RGB and event data; however, many existing approaches are computationally expensive and focus primarily on spatial fusion, neglecting the temporal dynamics inherent in event streams. In this work, we propose MambaSeg, a novel dual-branch semantic segmentation framework that employs parallel Mamba encoders to efficiently model RGB images and event streams. To reduce cross-modal ambiguity, we introduce the Dual-Dimensional Interaction Module (DDIM), comprising a Cross-Spatial Interaction Module (CSIM) and a Cross-Temporal Interaction Module (CTIM), which jointly perform fine-grained fusion along both spatial and temporal dimensions. This design improves cross-modal alignment, reduces ambiguity, and leverages the complementary properties of each modality. Extensive experiments on the DDD17 and DSEC datasets demonstrate that MambaSeg achieves state-of-the-art segmentation performance while significantly reducing computational cost, showcasing its promise for efficient, scalable, and robust <span class="search-hit mathjax">multimodal</span> perception.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.24243v1-abstract-full').style.display = 'none'; document.getElementById('2512.24243v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 December, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by AAAI 2026</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2512.23906">arXiv:2512.23906</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2512.23906">pdf</a>, <a href="https://arxiv.org/ps/2512.23906">ps</a>, <a href="https://arxiv.org/format/2512.23906">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A <span class="search-hit mathjax">multimodal</span> <span class="search-hit mathjax">Transformer</span> for InSAR-based ground deformation forecasting with cross-site generalization across Europe
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Yao%2C+W">Wendong Yao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+B">Binhua Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dev%2C+S">Soumyabrata Dev</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2512.23906v1-abstract-short" style="display: inline;">
        &hellip;of long-term trends, seasonal cycles, and occasional abrupt discontinuities (e.g., co-seismic steps), together with strong spatial heterogeneity. In this study we propose a <span class="search-hit mathjax">multimodal</span> patch-based&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.23906v1-abstract-full').style.display = 'inline'; document.getElementById('2512.23906v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2512.23906v1-abstract-full" style="display: none;">
        Near-real-time regional-scale monitoring of ground deformation is increasingly required to support urban planning, critical infrastructure management, and natural hazard mitigation. While Interferometric Synthetic Aperture Radar (InSAR) and continental-scale services such as the European Ground Motion Service (EGMS) provide dense observations of past motion, predicting the next observation remains challenging due to the superposition of long-term trends, seasonal cycles, and occasional abrupt discontinuities (e.g., co-seismic steps), together with strong spatial heterogeneity. In this study we propose a <span class="search-hit mathjax">multimodal</span> patch-based <span class="search-hit mathjax">Transformer</span> for single-step, fixed-interval next-epoch nowcasting of displacement maps from EGMS time series (resampled to a 64x64 grid over 100 km x 100 km tiles). The model ingests recent displacement snapshots together with (i) static kinematic indicators (mean velocity, acceleration, seasonal amplitude) computed in a leakage-safe manner from the training window only, and (ii) harmonic day-of-year encodings. On the eastern Ireland tile (E32N34), the STGCN is strongest in the displacement-only setting, whereas the <span class="search-hit mathjax">multimodal</span> <span class="search-hit mathjax">Transformer</span> clearly outperforms CNN-LSTM, CNN-LSTM+Attn, and <span class="search-hit mathjax">multimodal</span> STGCN when all models receive the same <span class="search-hit mathjax">multimodal</span> inputs, achieving RMSE = 0.90 mm and $R^2$ = 0.97 on the test set with the best threshold accuracies.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.23906v1-abstract-full').style.display = 'none'; document.getElementById('2512.23906v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 December, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">submitted to ISPRS Journal of Photogrammetry and Remote Sensing for review</span>
    </p>
    

    
      <p class="comments is-size-7">
        
          <span class="has-text-black-bis has-text-weight-semibold">Report number:</span>
          PHOTO-D-25-03411
        

        

        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2512.23903">arXiv:2512.23903</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2512.23903">pdf</a>, <a href="https://arxiv.org/ps/2512.23903">ps</a>, <a href="https://arxiv.org/format/2512.23903">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Scaling Remote Sensing Foundation Models: Data Domain Tradeoffs at the Peta-Scale
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wickrema%2C+C">Charith Wickrema</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mace%2C+E">Eliza Mace</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Brown%2C+H">Hunter Brown</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cabrera%2C+H">Heidys Cabrera</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Krall%2C+N">Nick Krall</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=O%27Neill%2C+M">Matthew O&#39;Neill</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sarkar%2C+S">Shivangi Sarkar</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Weissman%2C+L">Lowell Weissman</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hughes%2C+E">Eric Hughes</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zarrella%2C+G">Guido Zarrella</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2512.23903v1-abstract-short" style="display: inline;">
        &hellip;techniques for training foundation models on high-resolution electro-optical (EO) datasets that exceed the current state-of-the-art scale by orders of magnitude. Modern <span class="search-hit mathjax">multimodal</span> machine learning (ML) applications, such as generative artificial intelligence (GenAI) systems for image captioning, search, and reasoning, depend on robust, domain-specialized enc&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.23903v1-abstract-full').style.display = 'inline'; document.getElementById('2512.23903v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2512.23903v1-abstract-full" style="display: none;">
        We explore the scaling behaviors of artificial intelligence to establish practical techniques for training foundation models on high-resolution electro-optical (EO) datasets that exceed the current state-of-the-art scale by orders of magnitude. Modern <span class="search-hit mathjax">multimodal</span> machine learning (ML) applications, such as generative artificial intelligence (GenAI) systems for image captioning, search, and reasoning, depend on robust, domain-specialized encoders for non-text modalities. In natural-image domains where internet-scale data is plentiful, well-established scaling laws help optimize the joint scaling of model capacity, training compute, and dataset size. Unfortunately, these relationships are much less well-understood in high-value domains like remote sensing (RS). Using over a quadrillion pixels of commercial satellite EO data and the MITRE Federal AI Sandbox, we train progressively larger vision <span class="search-hit mathjax">transformer</span> (ViT) backbones, report success and failure modes observed at petascale, and analyze implications for bridging domain gaps across additional RS modalities. We observe that even at this scale, performance is consistent with a data limited regime rather than a model parameter-limited one. These practical insights are intended to inform data-collection strategies, compute budgets, and optimization schedules that advance the future development of frontier-scale RS foundation models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.23903v1-abstract-full').style.display = 'none'; document.getElementById('2512.23903v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 December, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2512.23597">arXiv:2512.23597</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2512.23597">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Scalable Residual Feature Aggregation Framework with Hybrid Metaheuristic Optimization for Robust Early Pancreatic Neoplasm Detection in <span class="search-hit mathjax">Multimodal</span> CT Imaging
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Thiruvengadam%2C+J+A">Janani Annur Thiruvengadam</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nabigaru%2C+K+M">Kiran Mayee Nabigaru</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kovi%2C+A">Anusha Kovi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2512.23597v1-abstract-short" style="display: inline;">
        &hellip;be addressed with an effective and scalable system that can assist in enhancing the salience of the subtle visual cues and provide a high level of the generalization on the <span class="search-hit mathjax">multimodal</span> imaging data. A Scalable Residual Feature Aggregation (SRFA) framework is proposed to be used to meet these conditions in this study. The framework integrates a pipeline of pre&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.23597v1-abstract-full').style.display = 'inline'; document.getElementById('2512.23597v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2512.23597v1-abstract-full" style="display: none;">
        The early detection of pancreatic neoplasm is a major clinical dilemma, and it is predominantly so because tumors are likely to occur with minimal contrast margins and a large spread anatomy-wide variation amongst patients on a CT scan. These complexities require to be addressed with an effective and scalable system that can assist in enhancing the salience of the subtle visual cues and provide a high level of the generalization on the <span class="search-hit mathjax">multimodal</span> imaging data. A Scalable Residual Feature Aggregation (SRFA) framework is proposed to be used to meet these conditions in this study. The framework integrates a pipeline of preprocessing followed by the segmentation using the MAGRes-UNet that is effective in making the pancreatic structures and isolating regions of interest more visible. DenseNet-121 performed with residual feature storage is used to extract features to allow deep hierarchical features to be aggregated without properties loss. To go further, hybrid HHO-BA metaheuristic feature selection strategy is used, which guarantees the best feature subset refinement. To be classified, the system is trained based on a new hybrid model that integrates the ability to pay attention on the world, which is the Vision <span class="search-hit mathjax">Transformer</span> (ViT) with the high representational efficiency of EfficientNet-B3. A dual optimization mechanism incorporating SSA and GWO is used to fine-tune hyperparameters to enhance greater robustness and less overfitting. Experimental results support the significant improvement in performance, with the suggested model reaching 96.23% accuracy, 95.58% F1-score and 94.83% specificity, the model is significantly better than the traditional CNNs and contemporary <span class="search-hit mathjax">transformer</span>-based models. Such results highlight the possibility of the SRFA framework as a useful instrument in the early detection of pancreatic tumors.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.23597v1-abstract-full').style.display = 'none'; document.getElementById('2512.23597v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 December, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2512.23568">arXiv:2512.23568</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2512.23568">pdf</a>, <a href="https://arxiv.org/ps/2512.23568">ps</a>, <a href="https://arxiv.org/format/2512.23568">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ThinkGen: Generalized Thinking for Visual Generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Jiao%2C+S">Siyu Jiao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+Y">Yiheng Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhong%2C+Y">Yujie Zhong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=She%2C+Q">Qi She</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+W">Wei Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lan%2C+X">Xiaohan Lan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+Z">Zilong Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+F">Fei Yu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+Y">Yingchen Yu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+Y">Yunqing Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+Y">Yao Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wei%2C+Y">Yunchao Wei</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2512.23568v1-abstract-short" style="display: inline;">
        Recent progress in <span class="search-hit mathjax">Multimodal</span> Large Language Models (MLLMs) demonstrates that Chain-of-Thought (CoT) reasoning enables systematic solutions to complex understanding tasks. However, its extension to generation tasks remains nascent and limited by scenario-specific mechanisms that hinder generalization and adaptation. In this work, we present ThinkGen, the fir&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.23568v1-abstract-full').style.display = 'inline'; document.getElementById('2512.23568v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2512.23568v1-abstract-full" style="display: none;">
        Recent progress in <span class="search-hit mathjax">Multimodal</span> Large Language Models (MLLMs) demonstrates that Chain-of-Thought (CoT) reasoning enables systematic solutions to complex understanding tasks. However, its extension to generation tasks remains nascent and limited by scenario-specific mechanisms that hinder generalization and adaptation. In this work, we present ThinkGen, the first think-driven visual generation framework that explicitly leverages MLLM&#39;s CoT reasoning in various generation scenarios. ThinkGen employs a decoupled architecture comprising a pretrained MLLM and a Diffusion <span class="search-hit mathjax">Transformer</span> (DiT), wherein the MLLM generates tailored instructions based on user intent, and DiT produces high-quality images guided by these instructions. We further propose a separable GRPO-based training paradigm (SepGRPO), alternating reinforcement learning between the MLLM and DiT modules. This flexible design enables joint training across diverse datasets, facilitating effective CoT reasoning for a wide range of generative scenarios. Extensive experiments demonstrate that ThinkGen achieves robust, state-of-the-art performance across multiple generation benchmarks. Code is available: https://github.com/jiaosiyuu/ThinkGen
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.23568v1-abstract-full').style.display = 'none'; document.getElementById('2512.23568v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 December, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2512.23380">arXiv:2512.23380</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2512.23380">pdf</a>, <a href="https://arxiv.org/ps/2512.23380">ps</a>, <a href="https://arxiv.org/format/2512.23380">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Operating Systems">cs.OS</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1038/s41598-025-27693-4">10.1038/s41598-025-27693-4 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A unified framework for detecting point and collective anomalies in operating system logs via collaborative <span class="search-hit mathjax">transformers</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Nasirzadeh%2C+M">Mohammad Nasirzadeh</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tahmoresnezhad%2C+J">Jafar Tahmoresnezhad</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rashidi-Khazaee%2C+P">Parviz Rashidi-Khazaee</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2512.23380v1-abstract-short" style="display: inline;">
        &hellip;in logs that can be considered log modalities. In light of this intuition, unimodal methods often struggle by ignoring the different modalities of log data. Meanwhile, <span class="search-hit mathjax">multimodal</span> methods fail to handle the interactions between these modalities. Applying&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.23380v1-abstract-full').style.display = 'inline'; document.getElementById('2512.23380v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2512.23380v1-abstract-full" style="display: none;">
        Log anomaly detection is crucial for preserving the security of operating systems. Depending on the source of log data collection, various information is recorded in logs that can be considered log modalities. In light of this intuition, unimodal methods often struggle by ignoring the different modalities of log data. Meanwhile, <span class="search-hit mathjax">multimodal</span> methods fail to handle the interactions between these modalities. Applying <span class="search-hit mathjax">multimodal</span> sentiment analysis to log anomaly detection, we propose CoLog, a framework that collaboratively encodes logs utilizing various modalities. CoLog utilizes collaborative <span class="search-hit mathjax">transformers</span> and multi-head impressed attention to learn interactions among several modalities, ensuring comprehensive anomaly detection. To handle the heterogeneity caused by these interactions, CoLog incorporates a modality adaptation layer, which adapts the representations from different log modalities. This methodology enables CoLog to learn nuanced patterns and dependencies within the data, enhancing its anomaly detection capabilities. Extensive experiments demonstrate CoLog&#39;s superiority over existing state-of-the-art methods. Furthermore, in detecting both point and collective anomalies, CoLog achieves a mean precision of 99.63%, a mean recall of 99.59%, and a mean F1 score of 99.61% across seven benchmark datasets for log-based anomaly detection. The comprehensive detection capabilities of CoLog make it highly suitable for cybersecurity, system monitoring, and operational efficiency. CoLog represents a significant advancement in log anomaly detection, providing a sophisticated and effective solution to point and collective anomaly detection through a unified framework and a solution to the complex challenges automatic log data analysis poses. We also provide the implementation of CoLog at https://github.com/NasirzadehMoh/CoLog.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.23380v1-abstract-full').style.display = 'none'; document.getElementById('2512.23380v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 December, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">72 pages, 19 figures, 19 tables, accepted in scientific reports on 5 November 2025</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Scientific Reports 15, 45698 (2025)
      </p>
    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                     
          
          <li>
            <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=50"
              class="pagination-link "
              aria-label="Page 2"
              aria-current="page">2
            </a>
          </li>
          
          <li>
            <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=100"
              class="pagination-link "
              aria-label="Page 3"
              aria-current="page">3
            </a>
          </li>
          
          <li>
            <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=150"
              class="pagination-link "
              aria-label="Page 4"
              aria-current="page">4
            </a>
          </li>
          
          <li>
            <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=200"
              class="pagination-link "
              aria-label="Page 5"
              aria-current="page">5
            </a>
          </li>
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  

  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  <script src="https://static.arxiv.org/static/base/1.0.0a5/js/member_acknowledgement.js"></script>
  </body>
</html>

<!-- ===== END SEARCH ===== -->

<!-- ===== ABS URL: https://arxiv.org/abs/2601.08457 | HTTP 200 ===== -->

<!DOCTYPE html>
<html lang="en">

<head>  <title>[2601.08457] An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="apple-touch-icon" sizes="180x180" href="/static/browse/0.3.4/images/icons/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/static/browse/0.3.4/images/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/static/browse/0.3.4/images/icons/favicon-16x16.png">
  <link rel="manifest" href="/static/browse/0.3.4/images/icons/site.webmanifest">
  <link rel="mask-icon" href="/static/browse/0.3.4/images/icons/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">
  <link rel="stylesheet" type="text/css" media="screen" href="/static/browse/0.3.4/css/arXiv.css?v=20241206" />
  <link rel="stylesheet" type="text/css" media="print" href="/static/browse/0.3.4/css/arXiv-print.css?v=20200611" />
  <link rel="stylesheet" type="text/css" media="screen" href="/static/browse/0.3.4/css/browse_search.css" />
  <script language="javascript" src="/static/browse/0.3.4/js/accordion.js" ></script>
  <script language="javascript" src="/static/browse/0.3.4/js/optin-modal.js?v=20250819"></script>
  
  <link rel="canonical" href="https://arxiv.org/abs/2601.08457"/>
  <meta name="description" content="Abstract page for arXiv paper 2601.08457: An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English"><meta property="og:type" content="website" />
<meta property="og:site_name" content="arXiv.org" />
<meta property="og:title" content="An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English" />
<meta property="og:url" content="https://arxiv.org/abs/2601.08457v1" />
<meta property="og:image" content="/static/browse/0.3.4/images/arxiv-logo-fb.png" />
<meta property="og:image:secure_url" content="/static/browse/0.3.4/images/arxiv-logo-fb.png" />
<meta property="og:image:width" content="1200" />
<meta property="og:image:height" content="700" />
<meta property="og:image:alt" content="arXiv logo"/>
<meta property="og:description" content="Digital platforms have an ever-expanding user base, and act as a hub for communication, business, and connectivity. However, this has also allowed for the spread of hate speech and misogyny. Artificial intelligence models have emerged as an effective solution for countering online hate speech but are under explored for low resource and code-mixed languages and suffer from a lack of interpretability. Explainable Artificial Intelligence (XAI) can enhance transparency in the decisions of deep learning models, which is crucial for a sensitive domain such as hate speech detection. In this paper, we present a multi-modal and explainable web application for detecting misogyny in text and memes in code-mixed Hindi and English. The system leverages state-of-the-art transformer-based models that support multilingual and multimodal settings. For text-based misogyny identification, the system utilizes XLM-RoBERTa (XLM-R) and multilingual Bidirectional Encoder Representations from Transformers (mBERT) on a dataset of approximately 4,193 comments. For multimodal misogyny identification from memes, the system utilizes mBERT + EfficientNet, and mBERT + ResNET trained on a dataset of approximately 4,218 memes. It also provides feature importance scores using explainability techniques including Shapley Additive Values (SHAP) and Local Interpretable Model Agnostic Explanations (LIME). The application aims to serve as a tool for both researchers and content moderators, to promote further research in the field, combat gender based digital violence, and ensure a safe digital space. The system has been evaluated using human evaluators who provided their responses on Chatbot Usability Questionnaire (CUQ) and User Experience Questionnaire (UEQ) to determine overall usability."/>
<meta name="twitter:site" content="@arxiv"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="An Under-Explored Application for Explainable Multimodal Misogyny..."/>
<meta name="twitter:description" content="Digital platforms have an ever-expanding user base, and act as a hub for communication, business, and connectivity. However, this has also allowed for the spread of hate speech and misogyny...."/>
<meta name="twitter:image" content="https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png"/>
<meta name="twitter:image:alt" content="arXiv logo"/>
  <link rel="stylesheet" media="screen" type="text/css" href="/static/browse/0.3.4/css/tooltip.css"/><link rel="stylesheet" media="screen" type="text/css" href="https://static.arxiv.org/js/bibex-dev/bibex.css?20200709"/>  <script src="/static/browse/0.3.4/js/mathjaxToggle.min.js" type="text/javascript"></script>  <script src="//code.jquery.com/jquery-latest.min.js" type="text/javascript"></script>
  <script src="//cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js" type="text/javascript"></script>
  <script src="//cdn.jsdelivr.net/npm/dompurify@2.3.5/dist/purify.min.js"></script>
  <script src="/static/browse/0.3.4/js/toggle-labs.js?20241022" type="text/javascript"></script>
  <script src="/static/browse/0.3.4/js/cite.js" type="text/javascript"></script><meta name="citation_title" content="An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English" /><meta name="citation_author" content="Yadav, Sargam" /><meta name="citation_author" content="Kaushik, Abhishek" /><meta name="citation_author" content="Daid, Kevin Mc" /><meta name="citation_date" content="2026/01/13" /><meta name="citation_online_date" content="2026/01/13" /><meta name="citation_pdf_url" content="https://arxiv.org/pdf/2601.08457" /><meta name="citation_arxiv_id" content="2601.08457" /><meta name="citation_abstract" content="Digital platforms have an ever-expanding user base, and act as a hub for communication, business, and connectivity. However, this has also allowed for the spread of hate speech and misogyny. Artificial intelligence models have emerged as an effective solution for countering online hate speech but are under explored for low resource and code-mixed languages and suffer from a lack of interpretability. Explainable Artificial Intelligence (XAI) can enhance transparency in the decisions of deep learning models, which is crucial for a sensitive domain such as hate speech detection. In this paper, we present a multi-modal and explainable web application for detecting misogyny in text and memes in code-mixed Hindi and English. The system leverages state-of-the-art transformer-based models that support multilingual and multimodal settings. For text-based misogyny identification, the system utilizes XLM-RoBERTa (XLM-R) and multilingual Bidirectional Encoder Representations from Transformers (mBERT) on a dataset of approximately 4,193 comments. For multimodal misogyny identification from memes, the system utilizes mBERT + EfficientNet, and mBERT + ResNET trained on a dataset of approximately 4,218 memes. It also provides feature importance scores using explainability techniques including Shapley Additive Values (SHAP) and Local Interpretable Model Agnostic Explanations (LIME). The application aims to serve as a tool for both researchers and content moderators, to promote further research in the field, combat gender based digital violence, and ensure a safe digital space. The system has been evaluated using human evaluators who provided their responses on Chatbot Usability Questionnaire (CUQ) and User Experience Questionnaire (UEQ) to determine overall usability." />
</head>

<body  class="with-cu-identity">
  
  
  <div class="flex-wrap-footer">
    <header>
      <a href="#content" class="is-sr-only">Skip to main content</a>
      <!-- start desktop header -->
      <div class="columns is-vcentered is-hidden-mobile" id="cu-identity">
        <div class="column" id="cu-logo">
          <a href="https://www.cornell.edu/"><img src="/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" /></a>
        </div><!-- /from April 7 at 1:00 AM to May 29 at 21:40 --><!-- /from May 2 at 1:00 AM to May 5 at 9:45 AM --><div class="column" id="support-ack">
          <span id="support-ack-url">We gratefully acknowledge support from the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors.</span>
          <a href="https://info.arxiv.org/about/donate.html" class="btn-header-donate">Donate</a>
        </div>
      </div>

      <div id="header" class="is-hidden-mobile">
<a aria-hidden="true" tabindex="-1" href="/IgnoreMe"></a>
  <div class="header-breadcrumbs is-hidden-mobile">
    <a href="/"><img src="/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg" alt="arxiv logo" style="height:40px;"/></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a> <span>&gt;</span> arXiv:2601.08457
  </div>

        <div class="columns is-vcentered is-mobile" style="justify-content: flex-end;">
        </div>

          <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
     </div><!-- /end desktop header -->

      <div class="mobile-header">
        <div class="columns is-mobile">
          <div class="column logo-arxiv"><a href="https://arxiv.org/"><img src="/static/browse/0.3.4/images/arxiv-logomark-small-white.svg" alt="arXiv logo" style="height:60px;" /></a></div>
          <div class="column logo-cornell"><a href="https://www.cornell.edu/">
            <picture>
              <source media="(min-width: 501px)"
                srcset="/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg  400w"
                sizes="400w" />
              <source srcset="/static/browse/0.3.4/images/icons/cu/cornell_seal_simple_black.svg 2x" />
              <img src="/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University Logo" />
            </picture>
          </a></div>
          <div class="column nav" id="toggle-container" role="menubar">
            <button class="toggle-control"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-white"><title>open search</title><path d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></button>
            <div class="mobile-toggle-block toggle-target">
              <form class="mobile-search-form" method="GET" action="https://arxiv.org/search">
                <div class="field has-addons">
                  <input class="input" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
                  <input type="hidden" name="source" value="header">
                  <input type="hidden" name="searchtype" value="all">
                  <button class="button">GO</button>
                </div>
              </form>
            </div>

            <button class="toggle-control"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-white" role="menu"><title>open navigation menu</title><path d="M16 132h416c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163 0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"/ ></svg></button>
            <div class="mobile-toggle-block toggle-target">
              <nav class="mobile-menu" aria-labelledby="mobilemenulabel">
                <h2 id="mobilemenulabel">quick links</h2>
                <ul>
                    <li><a href="https://arxiv.org/login">Login</a></li>
                    <li><a href="https://info.arxiv.org/help">Help Pages</a></li>
                    <li><a href="https://info.arxiv.org/about">About</a></li>
                </ul>
              </nav>
            </div>
          </div>
        </div>
      </div><!-- /end mobile-header -->
    </header>

    <main>
      <div id="content">
<!--
rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
         xmlns:dc="http://purl.org/dc/elements/1.1/"
         xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
    <rdf:Description
        rdf:about="/abs/2601.08457"
        dc:identifier="/abs/2601.08457"
        dc:title="An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English"
        trackback:ping="/trackback/2601.08457" />
    </rdf:RDF>
--><div id="abs-outer">

  <div class="leftcolumn">
    <div class="subheader">
      <h1>Computer Science > Artificial Intelligence</h1>
    </div>

    <div class="header-breadcrumbs-mobile">
      <strong>arXiv:2601.08457</strong> (cs)
    </div>
<link rel="stylesheet" type="text/css" href="/static/base/1.0.1/css/abs.css">
<div id="content-inner">
  <div id="abs">
    <div class="dateline">
  [Submitted on 13 Jan 2026]</div>
    <h1 class="title mathjax"><span class="descriptor">Title:</span>An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English</h1>
    <div class="authors"><span class="descriptor">Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yadav,+S" rel="nofollow">Sargam Yadav</a> (1), <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kaushik,+A" rel="nofollow">Abhishek Kaushik</a> (1), <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Daid,+K+M" rel="nofollow">Kevin Mc Daid</a> (1) ((1) Dundalk Institute of Technology)</div>            <div id="download-button-info" hidden>View a PDF of the paper titled An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English, by Sargam Yadav (1) and 2 other authors</div>
    <a class="mobile-submission-download" href="/pdf/2601.08457">View PDF</a>



    <blockquote class="abstract mathjax">
            <span class="descriptor">Abstract:</span>Digital platforms have an ever-expanding user base, and act as a hub for communication, business, and connectivity. However, this has also allowed for the spread of hate speech and misogyny. Artificial intelligence models have emerged as an effective solution for countering online hate speech but are under explored for low resource and code-mixed languages and suffer from a lack of interpretability. Explainable Artificial Intelligence (XAI) can enhance transparency in the decisions of deep learning models, which is crucial for a sensitive domain such as hate speech detection. In this paper, we present a multi-modal and explainable web application for detecting misogyny in text and memes in code-mixed Hindi and English. The system leverages state-of-the-art transformer-based models that support multilingual and multimodal settings. For text-based misogyny identification, the system utilizes XLM-RoBERTa (XLM-R) and multilingual Bidirectional Encoder Representations from Transformers (mBERT) on a dataset of approximately 4,193 comments. For multimodal misogyny identification from memes, the system utilizes mBERT + EfficientNet, and mBERT + ResNET trained on a dataset of approximately 4,218 memes. It also provides feature importance scores using explainability techniques including Shapley Additive Values (SHAP) and Local Interpretable Model Agnostic Explanations (LIME). The application aims to serve as a tool for both researchers and content moderators, to promote further research in the field, combat gender based digital violence, and ensure a safe digital space. The system has been evaluated using human evaluators who provided their responses on Chatbot Usability Questionnaire (CUQ) and User Experience Questionnaire (UEQ) to determine overall usability.
    </blockquote>

    <!--CONTEXT-->
    <div class="metatable">
      <table summary="Additional metadata"><tr>
          <td class="tablecell label">Subjects:</td>
          <td class="tablecell subjects">
            <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)</td>
        </tr><tr>
          <td class="tablecell label">Cite as:</td>
          <td class="tablecell arxivid"><span class="arxivid"><a href="https://arxiv.org/abs/2601.08457">arXiv:2601.08457</a> [cs.AI]</span></td>
        </tr>
        <tr>
          <td class="tablecell label">&nbsp;</td>
          <td class="tablecell arxividv">(or <span class="arxivid">
              <a href="https://arxiv.org/abs/2601.08457v1">arXiv:2601.08457v1</a> [cs.AI]</span> for this version)
          </td>
        </tr>
        <tr>
          <td class="tablecell label">&nbsp;</td>
          <td class="tablecell arxivdoi">              <a href="https://doi.org/10.48550/arXiv.2601.08457" id="arxiv-doi-link">https://doi.org/10.48550/arXiv.2601.08457</a><div class="button-and-tooltip">
              <button class="more-info" aria-describedby="more-info-desc-1">
                <svg height="15" role="presentation" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z" class=""></path></svg>
                <span class="visually-hidden">Focus to learn more</span>
              </button>
              <!-- tooltip description -->
              <div role="tooltip" id="more-info-desc-1">
                <span class="left-corner"></span>                  arXiv-issued DOI via DataCite (pending registration)</div>
            </div>
          </td>
        </tr></table>
    </div>
  </div>
</div>
    <div class="submission-history">
      <h2>Submission history</h2> From: Abhishek Kaushik Dr. [<a href="/show-email/5a561cb4/2601.08457" rel="nofollow">view email</a>]      <br/>    <strong>[v1]</strong>
        Tue, 13 Jan 2026 11:31:55 UTC (689 KB)<br/>
</div>
  </div>
  <!--end leftcolumn-->
<div class="extra-services">    <div class="full-text">
      <a name="other"></a>
      <span class="descriptor">Full-text links:</span>
      <h2>Access Paper:</h2>
      <ul>
  <div id="download-button-info" hidden>
View a PDF of the paper titled An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English, by Sargam Yadav (1) and 2 other authors</div><li><a href="/pdf/2601.08457" aria-describedby="download-button-info" accesskey="f" class="abs-button download-pdf">View PDF</a></li></ul>
      <div class="abs-license"><a href="http://creativecommons.org/licenses/by-nc-nd/4.0/" title="Rights to this article" class="has_license">
          <img alt="license icon" role="presentation" src="https://arxiv.org/icons/licenses/by-nc-nd-4.0.png"/>
          <span>view license</span>
        </a></div>
    </div>
    <!--end full-text-->    <div class="browse">
    Current browse context: <div class="current">cs.AI</div>

  <div class="prevnext">
<span class="arrow">
      <a class="abs-button prev-url" href="/prevnext?id=2601.08457&amp;function=prev&amp;context=cs.AI"
         accesskey="p" title="previous in cs.AI (accesskey p)" rel="nofollow">&lt;&nbsp;prev</a>
    </span>
    <span class="is-hidden-mobile">&nbsp; | &nbsp;</span>    <span class="arrow">
      <a class="abs-button next-url" href="/prevnext?id=2601.08457&amp;function=next&amp;context=cs.AI" accesskey="n"
         title="next in cs.AI (accesskey n)"  rel="nofollow">next&nbsp;&gt;</a>
    </span><br/>
  </div><div class="list">
    <a class="abs-button abs-button-grey abs-button-small context-new" href="/list/cs.AI/new"  rel="nofollow">new</a>
    <span class="is-hidden-mobile"> | </span>
    <a class="abs-button abs-button-grey abs-button-small context-recent" href="/list/cs.AI/recent" rel="nofollow">recent</a>
    <span class="is-hidden-mobile"> | </span><a class="abs-button abs-button-grey abs-button-small context-id" href="/list/cs.AI/2026-01" rel="nofollow">2026-01</a>
  </div><div class="abs-switch-cat">
    Change to browse by:
    <div class="switch context-change">
        <a href="/abs/2601.08457?context=cs" rel="nofollow">cs</a><br class="is-hidden-mobile">
        <a class="subclass" href="/abs/2601.08457?context=cs.CL" rel="nofollow">cs.CL</a><br class="is-hidden-mobile">
    </div>
  </div>

    </div>
      <div class="extra-ref-cite">
        <h3>References &amp; Citations</h3>
        <ul>
          <li><a  class="abs-button abs-button-small cite-ads" href="https://ui.adsabs.harvard.edu/abs/arXiv:2601.08457">NASA ADS</a></li><li><a  class="abs-button abs-button-small cite-google-scholar" href="https://scholar.google.com/scholar_lookup?arxiv_id=2601.08457" target="_blank" rel="noopener">Google Scholar</a></li>
          <li><a  class="abs-button abs-button-small cite-semantic-scholar" href="https://api.semanticscholar.org/arXiv:2601.08457" target="_blank" rel="noopener">Semantic Scholar</a></li>
        </ul>
        <div style="clear:both;"></div>
      </div>

<div class='extra-ref-cite'>
    <span id='bib-cite-trigger' class="bib-cite-button abs-button">export BibTeX citation</span>
    <span id='bib-cite-loading' hidden='true'>Loading...</span>
</div>

<div id='bib-cite-modal' class='bib-modal' hidden='true'>
    <div class='bib-modal-content'>
        <div class='bib-modal-title'>
            <h2>BibTeX formatted citation</h2>
            <span class='bib-modal-close' >&times;</span>
        </div>
        <div>
            <textarea id='bib-cite-target' class="bib-citation-content" aria-label="loading the citation">loading...</textarea>
        </div>
        <div>
            <span>Data provided by: </span>
            <a id='bib-cite-source-api'></a>
        </div>
    </div>
</div><div class="bookmarks">
  <div><h3>Bookmark</h3></div><a class="abs-button abs-button-grey abs-button-small" href="http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2601.08457&amp;description=An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English"
     title="Bookmark on BibSonomy">
    <img src="/static/browse/0.3.4/images/icons/social/bibsonomy.png"
         alt="BibSonomy logo"/>
  </a>
  <a class="abs-button abs-button-grey abs-button-small" href="https://reddit.com/submit?url=https://arxiv.org/abs/2601.08457&amp;title=An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English"
     title="Bookmark on Reddit">
    <img src="/static/browse/0.3.4/images/icons/social/reddit.png"
         alt="Reddit logo"/>
  </a>
</div>  </div>
  <!--end extra-services-->
<!-- LABS AREA -->
<div id="labstabs">
  <div class="labstabs"><input type="radio" name="tabs" id="tabone"checked="checked">
    <label for="tabone">Bibliographic Tools</label>
    <div class="tab labs-display-bib">
      <h1>Bibliographic and Citation Tools</h1>
      <div class="toggle">
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input id="bibex-toggle" type="checkbox" class="lab-toggle"
                     data-script-url="/static/browse/0.3.4/bibex/bibex.js?20241202">
              <span class="slider"></span>
              <span class="is-sr-only">Bibliographic Explorer Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-bibex">Bibliographic Explorer</span> <em>(<a href="https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer">What is the Explorer?</a>)</em>
          </div>
        </div>
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="connectedpapers-toggle"
                type="checkbox"
                class="lab-toggle"
                data-script-url="/static/browse/0.3.4/js/connectedpapers.js"
                aria-labelledby="label-for-connected-papers">
              <span class="slider"></span>
              <span class="is-sr-only">Connected Papers Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-connected-papers">Connected Papers</span> <em>(<a href="https://www.connectedpapers.com/about" target="_blank">What is Connected Papers?</a>)</em>
          </div>
        </div><div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="litmaps-toggle"
                type="checkbox"
                class="lab-toggle"
                data-script-url="/static/browse/0.3.4/js/litmaps.js?20210617"
                aria-labelledby="label-for-litmaps">
              <span class="slider"></span>
              <span class="is-sr-only">Litmaps Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-litmaps">Litmaps</span> <em>(<a href="https://www.litmaps.co/" target="_blank">What is Litmaps?</a>)</em>
          </div>
        </div>
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="scite-toggle"
                type="checkbox"
                class="lab-toggle"
                data-script-url="/static/browse/0.3.4/js/scite.js?20210617"
                aria-labelledby="label-for-scite">
              <span class="slider"></span>
              <span class="is-sr-only">scite.ai Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-scite">scite Smart Citations</span> <em>(<a href="https://www.scite.ai/" target="_blank">What are Smart Citations?</a>)</em>
          </div>
        </div>
      </div>
        <div class="labs-content-placeholder labs-display" style="display: none;"></div>
        <div style="min-height: 15px" id="connectedpapers-output"></div>
        <div style="min-height: 15px" id="litmaps-open-in"></div>
        <div style="min-height: 15px" id="scite-open-in"></div>
    </div>


    <input type="radio" name="tabs" id="tabtwo">
    <label for="tabtwo">Code, Data, Media</label>
    <div class="tab">
      <h1>Code, Data and Media Associated with this Article</h1>
      <div class="toggle">
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="alphaxiv-toggle"
                data-script-url="/static/browse/0.3.4/js/alphaxiv.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-alphaxiv">
              <span class="slider"></span>
              <span class="is-sr-only">alphaXiv Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-alphaxiv">alphaXiv</span> <em>(<a href="https://alphaxiv.org/" target="_blank">What is alphaXiv?</a>)</em>
          </div>
        </div>

        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input        
                id="catalyzex-toggle"
                data-script-url="/static/browse/0.3.4/js/catalyzex.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-cx">
              <span class="slider"></span>
              <span class="is-sr-only">Links to Code Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-cx">CatalyzeX Code Finder for Papers</span> <em>(<a href="https://www.catalyzex.com" target="_blank">What is CatalyzeX?</a>)</em>
          </div>
        </div>

        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="dagshub-toggle"
                data-script-url="/static/browse/0.3.4/js/dagshub.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-dagshub">
              <span class="slider"></span>
              <span class="is-sr-only">DagsHub Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-dagshub">DagsHub</span> <em>(<a href="https://dagshub.com/" target="_blank">What is DagsHub?</a>)</em>
          </div>
        </div>
  
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="gotitpub-toggle"
                data-script-url="/static/browse/0.3.4/js/gotitpub.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-gotitpub">
              <span class="slider"></span>
              <span class="is-sr-only">GotitPub Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-gotitpub">Gotit.pub</span> <em>(<a href="http://gotit.pub/faq" target="_blank">What is GotitPub?</a>)</em>
          </div>
        </div>

        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="huggingface-toggle"
                data-script-url="/static/browse/0.3.4/js/huggingface.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-huggingface">
              <span class="slider"></span>
              <span class="is-sr-only">Huggingface Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-huggingface">Hugging Face</span> <em>(<a href="https://huggingface.co/huggingface" target="_blank">What is Huggingface?</a>)</em>
          </div>
        </div>

        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="paperwithcode-toggle"
                data-script-url="/static/browse/0.3.4/js/paperswithcode.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-pwc">
              <span class="slider"></span>
              <span class="is-sr-only">Links to Code Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-pwc">Papers with Code</span> <em>(<a href="https://paperswithcode.com/" target="_blank">What is Papers with Code?</a>)</em>
          </div>
        </div>


        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="sciencecast-toggle"
                data-script-url="/static/browse/0.3.4/js/sciencecast.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-sciencecast">
              <span class="slider"></span>
              <span class="is-sr-only">ScienceCast Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-sciencecast">ScienceCast</span> <em>(<a href="https://sciencecast.org/welcome" target="_blank">What is ScienceCast?</a>)</em>
          </div>
        </div>
      </div>

      <div id="alphaxiv-output" style="display:none"></div>
      <div id="catalyzex-output" style="display:none"></div>
      <div id="dagshub-output" style="display:none"></div>
      <div id="gotitpub-output" style="display:none"></div>
      <div id="pwc-output" style="display:none"></div>
      <div id="pwc-data-output" style="display:none"></div>
      <div id="sciencecast-output" style="display:none"></div>
      <div id="huggingface-output" style="display:none"></div>
    </div>


      <input type="radio" name="tabs" id="labstabs-demos-input">
      <label for="labstabs-demos-input" id="labstabs-demos-label">Demos</label>
      <div class="tab">
        <h1>Demos</h1>
        <div class="toggle">
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input
                  id="replicate-toggle"
                  data-script-url="/static/browse/0.3.4/js/replicate.js"
                  type="checkbox" class="lab-toggle" aria-labelledby="label-for-replicate">
                <span class="slider"></span>
                <span class="is-sr-only">Replicate Toggle</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-replicate">Replicate</span> <em>(<a href="https://replicate.com/docs/arxiv/about" target="_blank">What is Replicate?</a>)</em>
            </div>
          </div>
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input
                  id="spaces-toggle"
                  data-script-url="/static/browse/0.3.4/js/spaces.js"
                  type="checkbox" class="lab-toggle" aria-labelledby="label-for-spaces">
                <span class="slider"></span>
                <span class="is-sr-only">Spaces Toggle</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-spaces">Hugging Face Spaces</span> <em>(<a href="https://huggingface.co/docs/hub/spaces" target="_blank">What is Spaces?</a>)</em>
            </div>
          </div>
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input
                  id="txyz-toggle"
                  data-script-url="/static/browse/0.3.4/js/txyz.js"
                  type="checkbox" class="lab-toggle" aria-labelledby="label-for-txyz">
                <span class="slider"></span>
                <span class="is-sr-only">Spaces Toggle</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-txyz">TXYZ.AI</span> <em>(<a href="https://txyz.ai" target="_blank">What is TXYZ.AI?</a>)</em>
            </div>
          </div>
        </div>
        <div id="replicate-output"></div>
        <div id="spaces-output"></div>
        <div id="txyz-output"></div>
      </div>
      <input type="radio" name="tabs" id="tabfour">
      <label for="tabfour">Related Papers</label>
      <div class="tab">
        <h1>Recommenders and Search Tools</h1>
        <div class="toggle">
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input id="influenceflower-toggle"
                data-script-url="/static/browse/0.3.4/js/influenceflower.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-influenceflower">
                <span class="slider"></span>
                <span class="is-sr-only">Link to Influence Flower</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-influenceflower">Influence Flower</span> <em>(<a href="https://influencemap.cmlab.dev/" target="_blank">What are Influence Flowers?</a>)</em>
            </div>
          </div>
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input id="core-recommender-toggle" type="checkbox" class="lab-toggle" aria-labelledby="label-for-core">
                <span class="slider"></span>
                <span class="is-sr-only">Core recommender toggle</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-core">CORE Recommender</span> <em>(<a href="https://core.ac.uk/services/recommender">What is CORE?</a>)</em>
            </div>
          </div></div>
        <div id="influenceflower-output"></div>
        <div id="influenceflower-output-graph" style="display:none">
          <ul class="flower-tabs">
            <li class="active"><a class="btn tab-btn" onclick="openTab(event, 'tab-author')">Author</a></li>
            <li><a class="btn tab-btn" onclick="openTab(event, 'tab-venue')">Venue</a></li>
            <li><a class="btn tab-btn" onclick="openTab(event, 'tab-inst')">Institution</a></li>
            <li><a class="btn tab-btn" onclick="openTab(event, 'tab-topic')">Topic</a></li>
          </ul>
          <div class="flower-tab-content">
            <div class="tab-flower active" id="tab-author"><svg id="flower-graph-author"></svg></div>
            <div class="tab-flower" id="tab-venue"><svg id="flower-graph-venue"></svg></div>
            <div class="tab-flower" id="tab-inst"><svg id="flower-graph-inst"></svg></div>
            <div class="tab-flower" id="tab-topic"><svg id="flower-graph-topic"></svg></div>
          </div>
        </div>
        <div id="coreRecommenderOutput"></div>
        <div id="iarxivOutput"></div>
      </div>

      <input type="radio" name="tabs" id="tabfive">
      <label for="tabfive">
        About arXivLabs
      </label>
      <div class="tab">
        <div class="columns">
          <div class="column">
            <h1>arXivLabs: experimental projects with community collaborators</h1>
            <p>arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.</p>
            <p>Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.</p>
            <p>Have an idea for a project that will add value for arXiv's community? <a href="https://info.arxiv.org/labs/index.html"><strong>Learn more about arXivLabs</strong></a>.</p>
          </div>
          <div class="column is-narrow is-full-mobile">
            <p class="icon-labs"><svg xmlns="http://www.w3.org/2000/svg" role="presentation" viewBox="0 0 635.572 811"><path d="M175.6 676v27h-27v-27zm-54 27v27h27v-27zm-27 27v27h27v-27zm396-54v27h-27v-27zm0 27v27h27v-27zm27 27v27h27v-27zm-27-414h27v27h-27zm27 0h27v-27h-27zm27-27h27v-27h-27zm-396 45h-27v-27h27zm-27-54h-27v27h27zm-27-27h-27v27h27z"/><path d="M94.6 730v27h-27v-27zm477 0v27h-27v-27zm-27-495h27v27h-27zm-450 18h-27v-27h27zm477 9h27v27h-27zm-54 495h27v27h-27zm-423 0h27v27h-27zm-54-504h27v27h-27z" fill="#666"/><path d="M67.6 730v27h-27v-27zm54 54v27h-27v-27zm0-108v27h27v-27zm-27 27v27h27v-27zm-81 0v27h27v-27zm585 27v27h-27v-27zm-108-54v27h27v-27zm27 27v27h27v-27zm81 0v27h27v-27zm-54-495h27v27h-27zm-54 108h27v-27h-27zm27-27h27v-27h-27zm0-81h27v-27h-27zm-423 18h-27v-27h27zm54 54h-27v27h27zm-27-27h-27v27h27zm0-81h-27v27h27zm423 612v27h-27v-27zm81-522v27h-27v-27zm-585-9v27h-27v-27z" fill="#999"/><path d="M94.6 784v27h-27v-27zm-27-27v27h27v-27zm-27-54v27h27v-27zm27 0v27h27v-27zm0-27v27h27v-27zm27 0v27h27v-27zm0-27v27h27v-27zm27 0v27h27v-27zm-108 81v27h27v-27zm558 54v27h-27v-27zm-27-27v27h27v-27zm27-54v27h27v-27zm-27 0v27h27v-27zm0-27v27h27v-27zm-27 0v27h27v-27zm0-27v27h27v-27zm-27 0v27h27v-27zm108 81v27h27v-27zm0-495h27v27h-27zm-27 27h27v-27h-27zm-54-27h27v-27h-27zm0 27h27v-27h-27zm-27 0h27v-27h-27zm0 27h27v-27h-27zm-27 0h27v-27h-27zm0 27h27v-27h-27zm81-108h27v-27h-27zm-504 45h-27v-27h27zm27-27h-27v27h27zm54-27h-27v27h27zm0 27h-27v27h27zm27 0h-27v27h27zm0 27h-27v27h27zm27 0h-27v27h27zm0 27h-27v27h27zm-81-108h-27v27h27z" fill="#ccc"/><path d="M598.6 665.1H41.5C-76.5 667 176 280.2 176 280.2h53a46.5 46.5 0 0162.8-56.3 29.2 29.2 0 1128.5 35.9h-1a46.5 46.5 0 01-1.5 20.3l142.5-.1s255.3 387 138.3 385.1zM291 181a29.3 29.3 0 10-29.2-29.3A29.3 29.3 0 00291 181zm65.4-66.8a22.4 22.4 0 10-22.5-22.4 22.4 22.4 0 0022.5 22.4z" fill="#fc0"/><path d="M245.5 172V10h153v162s324 495 198 495h-558c-126 0 207-495 207-495zm126 54h56m-13 72h56m-9 72h56m-20 72h56m-22 72h56m-29 72h56m-457-45c20.8 41.7 87.3 81 160.7 81 72.1 0 142.1-38.2 163.4-81" fill="none" stroke="#000" stroke-miterlimit="10" stroke-width="20"/><path d="M273.3 421.7c0 31-9.8 56.3-21.9 56.3s-21.8-25.2-21.8-56.3 9.8-56.3 21.8-56.3 21.9 25.2 21.9 56.3zm114.4-56.3c-12 0-21.8 25.2-21.8 56.3s9.7 56.3 21.8 56.3 21.9-25.2 21.9-56.3-9.8-56.3-21.9-56.3zM150.1 526.6c-18.2 6.7-27.5 22.9-23.2 30.2s14.8-5.5 33-12.2 37.4-4.9 33-12.2-24.5-12.6-42.8-5.8zm296 5.8c-4.2 7.3 14.9 5.5 33.1 12.2s28.7 19.5 33 12.2-5-23.5-23.2-30.2-38.5-1.5-42.8 5.8z"/></svg></p>
          </div>
        </div>
      </div>

    </div>
</div>
<!-- END LABS AREA -->
  <div class="endorsers">
    <a href="/auth/show-endorsers/2601.08457" class="endorser-who" rel="nofollow">Which authors of this paper are endorsers?</a> |
    <a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="https://info.arxiv.org/help/mathjax.html">What is MathJax?</a>)
    <span class="help" style="font-style: normal; float: right; margin-top: 0; margin-right: 1em;"></span>
  </div>
  <script type="text/javascript" language="javascript">mathjaxToggle();</script>
</div>
      </div>
    </main>

    <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://info.arxiv.org/about">About</a></li>
                <li><a href="https://info.arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
                <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
  </div>

  <script src="/static/base/1.0.1/js/member_acknowledgement.js"></script>

</body>

</html>

<!-- ===== END ABS ===== -->

<!-- ===== HTML URL: https://arxiv.org/html/2601.08457 | HTTP 404 ===== -->

<!DOCTYPE html>
<html lang="en">
  <head>
<meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="/static/base/1.0.1/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/static/base/1.0.1/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/static/base/1.0.1/images/icons/favicon-16x16.png">
<link rel="manifest" href="/static/base/1.0.1/images/icons/site.webmanifest">
<link rel="mask-icon" href="/static/base/1.0.1/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="/static/base/1.0.1/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title> | arXiv e-print repository</title>
<script defer src="/static/base/1.0.1/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="/static/base/1.0.1/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="/static/base/1.0.1/js/notification.js"></script>
  </head>
  <body>
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
<!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="/static/base/1.0.1/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="/static/base/1.0.1/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
    <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
<a href="https://arxiv.org/login">Login</a>    </div>
</div>  </header>
  <main class="container" id="main-container">
<h1>No HTML for '2601.08457'</h1>
<p>HTML is not available for the source.</p>
<p>This could be due to the source files not being HTML, LaTeX, or a conversion failure.</p>
<p>If you are an author, learn how you can help <a href="https://info.arxiv.org/about/accessibility_html_error_messages.html">HTML conversions for your papers</a>.</p>  </main>
  <footer>
<div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>  </footer>
  <script src="/static/base/1.0.1/js/member_acknowledgement.js"></script>
  </body>
</html>

<!-- ===== END HTML ===== -->

<!-- ===== ABS URL: https://arxiv.org/abs/2601.08179 | HTTP 200 ===== -->

<!DOCTYPE html>
<html lang="en">

<head>  <title>[2601.08179] Instruction-Driven 3D Facial Expression Generation and Transition</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="apple-touch-icon" sizes="180x180" href="/static/browse/0.3.4/images/icons/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/static/browse/0.3.4/images/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/static/browse/0.3.4/images/icons/favicon-16x16.png">
  <link rel="manifest" href="/static/browse/0.3.4/images/icons/site.webmanifest">
  <link rel="mask-icon" href="/static/browse/0.3.4/images/icons/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">
  <link rel="stylesheet" type="text/css" media="screen" href="/static/browse/0.3.4/css/arXiv.css?v=20241206" />
  <link rel="stylesheet" type="text/css" media="print" href="/static/browse/0.3.4/css/arXiv-print.css?v=20200611" />
  <link rel="stylesheet" type="text/css" media="screen" href="/static/browse/0.3.4/css/browse_search.css" />
  <script language="javascript" src="/static/browse/0.3.4/js/accordion.js" ></script>
  <script language="javascript" src="/static/browse/0.3.4/js/optin-modal.js?v=20250819"></script>
  
  <link rel="canonical" href="https://arxiv.org/abs/2601.08179"/>
  <meta name="description" content="Abstract page for arXiv paper 2601.08179: Instruction-Driven 3D Facial Expression Generation and Transition"><meta property="og:type" content="website" />
<meta property="og:site_name" content="arXiv.org" />
<meta property="og:title" content="Instruction-Driven 3D Facial Expression Generation and Transition" />
<meta property="og:url" content="https://arxiv.org/abs/2601.08179v1" />
<meta property="og:image" content="/static/browse/0.3.4/images/arxiv-logo-fb.png" />
<meta property="og:image:secure_url" content="/static/browse/0.3.4/images/arxiv-logo-fb.png" />
<meta property="og:image:width" content="1200" />
<meta property="og:image:height" content="700" />
<meta property="og:image:alt" content="arXiv logo"/>
<meta property="og:description" content="A 3D avatar typically has one of six cardinal facial expressions. To simulate realistic emotional variation, we should be able to render a facial transition between two arbitrary expressions. This study presents a new framework for instruction-driven facial expression generation that produces a 3D face and, starting from an image of the face, transforms the facial expression from one designated facial expression to another. The Instruction-driven Facial Expression Decomposer (IFED) module is introduced to facilitate multimodal data learning and capture the correlation between textual descriptions and facial expression features. Subsequently, we propose the Instruction to Facial Expression Transition (I2FET) method, which leverages IFED and a vertex reconstruction loss function to refine the semantic comprehension of latent vectors, thus generating a facial expression sequence according to the given instruction. Lastly, we present the Facial Expression Transition model to generate smooth transitions between facial expressions. Extensive evaluation suggests that the proposed model outperforms state-of-the-art methods on the CK+ and CelebV-HQ datasets. The results show that our framework can generate facial expression trajectories according to text instruction. Considering that text prompts allow us to make diverse descriptions of human emotional states, the repertoire of facial expressions and the transitions between them can be expanded greatly. We expect our framework to find various practical applications More information about our project can be found at https://vohoanganh.github.io/tg3dfet/"/>
<meta name="twitter:site" content="@arxiv"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Instruction-Driven 3D Facial Expression Generation and Transition"/>
<meta name="twitter:description" content="A 3D avatar typically has one of six cardinal facial expressions. To simulate realistic emotional variation, we should be able to render a facial transition between two arbitrary expressions. This..."/>
<meta name="twitter:image" content="https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png"/>
<meta name="twitter:image:alt" content="arXiv logo"/>
  <link rel="stylesheet" media="screen" type="text/css" href="/static/browse/0.3.4/css/tooltip.css"/><link rel="stylesheet" media="screen" type="text/css" href="https://static.arxiv.org/js/bibex-dev/bibex.css?20200709"/>  <script src="/static/browse/0.3.4/js/mathjaxToggle.min.js" type="text/javascript"></script>  <script src="//code.jquery.com/jquery-latest.min.js" type="text/javascript"></script>
  <script src="//cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js" type="text/javascript"></script>
  <script src="//cdn.jsdelivr.net/npm/dompurify@2.3.5/dist/purify.min.js"></script>
  <script src="/static/browse/0.3.4/js/toggle-labs.js?20241022" type="text/javascript"></script>
  <script src="/static/browse/0.3.4/js/cite.js" type="text/javascript"></script><meta name="citation_title" content="Instruction-Driven 3D Facial Expression Generation and Transition" /><meta name="citation_author" content="Vo, Anh H." /><meta name="citation_author" content="Kim, Tae-Seok" /><meta name="citation_author" content="Jin, Hulin" /><meta name="citation_author" content="Choi, Soo-Mi" /><meta name="citation_author" content="Kim, Yong-Guk" /><meta name="citation_doi" content="10.1109/TMM.2025.3565929" /><meta name="citation_date" content="2026/01/13" /><meta name="citation_online_date" content="2026/01/13" /><meta name="citation_pdf_url" content="https://arxiv.org/pdf/2601.08179" /><meta name="citation_arxiv_id" content="2601.08179" /><meta name="citation_abstract" content="A 3D avatar typically has one of six cardinal facial expressions. To simulate realistic emotional variation, we should be able to render a facial transition between two arbitrary expressions. This study presents a new framework for instruction-driven facial expression generation that produces a 3D face and, starting from an image of the face, transforms the facial expression from one designated facial expression to another. The Instruction-driven Facial Expression Decomposer (IFED) module is introduced to facilitate multimodal data learning and capture the correlation between textual descriptions and facial expression features. Subsequently, we propose the Instruction to Facial Expression Transition (I2FET) method, which leverages IFED and a vertex reconstruction loss function to refine the semantic comprehension of latent vectors, thus generating a facial expression sequence according to the given instruction. Lastly, we present the Facial Expression Transition model to generate smooth transitions between facial expressions. Extensive evaluation suggests that the proposed model outperforms state-of-the-art methods on the CK+ and CelebV-HQ datasets. The results show that our framework can generate facial expression trajectories according to text instruction. Considering that text prompts allow us to make diverse descriptions of human emotional states, the repertoire of facial expressions and the transitions between them can be expanded greatly. We expect our framework to find various practical applications More information about our project can be found at https://vohoanganh.github.io/tg3dfet/" />
</head>

<body  class="with-cu-identity">
  
  
  <div class="flex-wrap-footer">
    <header>
      <a href="#content" class="is-sr-only">Skip to main content</a>
      <!-- start desktop header -->
      <div class="columns is-vcentered is-hidden-mobile" id="cu-identity">
        <div class="column" id="cu-logo">
          <a href="https://www.cornell.edu/"><img src="/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" /></a>
        </div><!-- /from April 7 at 1:00 AM to May 29 at 21:40 --><!-- /from May 2 at 1:00 AM to May 5 at 9:45 AM --><div class="column" id="support-ack">
          <span id="support-ack-url">We gratefully acknowledge support from the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors.</span>
          <a href="https://info.arxiv.org/about/donate.html" class="btn-header-donate">Donate</a>
        </div>
      </div>

      <div id="header" class="is-hidden-mobile">
<a aria-hidden="true" tabindex="-1" href="/IgnoreMe"></a>
  <div class="header-breadcrumbs is-hidden-mobile">
    <a href="/"><img src="/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg" alt="arxiv logo" style="height:40px;"/></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a> <span>&gt;</span> arXiv:2601.08179
  </div>

        <div class="columns is-vcentered is-mobile" style="justify-content: flex-end;">
        </div>

          <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
     </div><!-- /end desktop header -->

      <div class="mobile-header">
        <div class="columns is-mobile">
          <div class="column logo-arxiv"><a href="https://arxiv.org/"><img src="/static/browse/0.3.4/images/arxiv-logomark-small-white.svg" alt="arXiv logo" style="height:60px;" /></a></div>
          <div class="column logo-cornell"><a href="https://www.cornell.edu/">
            <picture>
              <source media="(min-width: 501px)"
                srcset="/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg  400w"
                sizes="400w" />
              <source srcset="/static/browse/0.3.4/images/icons/cu/cornell_seal_simple_black.svg 2x" />
              <img src="/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University Logo" />
            </picture>
          </a></div>
          <div class="column nav" id="toggle-container" role="menubar">
            <button class="toggle-control"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-white"><title>open search</title><path d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></button>
            <div class="mobile-toggle-block toggle-target">
              <form class="mobile-search-form" method="GET" action="https://arxiv.org/search">
                <div class="field has-addons">
                  <input class="input" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
                  <input type="hidden" name="source" value="header">
                  <input type="hidden" name="searchtype" value="all">
                  <button class="button">GO</button>
                </div>
              </form>
            </div>

            <button class="toggle-control"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-white" role="menu"><title>open navigation menu</title><path d="M16 132h416c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163 0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"/ ></svg></button>
            <div class="mobile-toggle-block toggle-target">
              <nav class="mobile-menu" aria-labelledby="mobilemenulabel">
                <h2 id="mobilemenulabel">quick links</h2>
                <ul>
                    <li><a href="https://arxiv.org/login">Login</a></li>
                    <li><a href="https://info.arxiv.org/help">Help Pages</a></li>
                    <li><a href="https://info.arxiv.org/about">About</a></li>
                </ul>
              </nav>
            </div>
          </div>
        </div>
      </div><!-- /end mobile-header -->
    </header>

    <main>
      <div id="content">
<!--
rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
         xmlns:dc="http://purl.org/dc/elements/1.1/"
         xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
    <rdf:Description
        rdf:about="/abs/2601.08179"
        dc:identifier="/abs/2601.08179"
        dc:title="Instruction-Driven 3D Facial Expression Generation and Transition"
        trackback:ping="/trackback/2601.08179" />
    </rdf:RDF>
--><div id="abs-outer">

  <div class="leftcolumn">
    <div class="subheader">
      <h1>Computer Science > Computer Vision and Pattern Recognition</h1>
    </div>

    <div class="header-breadcrumbs-mobile">
      <strong>arXiv:2601.08179</strong> (cs)
    </div>
<link rel="stylesheet" type="text/css" href="/static/base/1.0.1/css/abs.css">
<div id="content-inner">
  <div id="abs">
    <div class="dateline">
  [Submitted on 13 Jan 2026]</div>
    <h1 class="title mathjax"><span class="descriptor">Title:</span>Instruction-Driven 3D Facial Expression Generation and Transition</h1>
    <div class="authors"><span class="descriptor">Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Vo,+A+H" rel="nofollow">Anh H. Vo</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kim,+T" rel="nofollow">Tae-Seok Kim</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jin,+H" rel="nofollow">Hulin Jin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Choi,+S" rel="nofollow">Soo-Mi Choi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kim,+Y" rel="nofollow">Yong-Guk Kim</a></div>            <div id="download-button-info" hidden>View a PDF of the paper titled Instruction-Driven 3D Facial Expression Generation and Transition, by Anh H. Vo and 4 other authors</div>
    <a class="mobile-submission-download" href="/pdf/2601.08179">View PDF</a>
    <a class="mobile-submission-download" href="https://arxiv.org/html/2601.08179v1">HTML (experimental)</a>



    <blockquote class="abstract mathjax">
            <span class="descriptor">Abstract:</span>A 3D avatar typically has one of six cardinal facial expressions. To simulate realistic emotional variation, we should be able to render a facial transition between two arbitrary expressions. This study presents a new framework for instruction-driven facial expression generation that produces a 3D face and, starting from an image of the face, transforms the facial expression from one designated facial expression to another. The Instruction-driven Facial Expression Decomposer (IFED) module is introduced to facilitate multimodal data learning and capture the correlation between textual descriptions and facial expression features. Subsequently, we propose the Instruction to Facial Expression Transition (I2FET) method, which leverages IFED and a vertex reconstruction loss function to refine the semantic comprehension of latent vectors, thus generating a facial expression sequence according to the given instruction. Lastly, we present the Facial Expression Transition model to generate smooth transitions between facial expressions. Extensive evaluation suggests that the proposed model outperforms state-of-the-art methods on the CK+ and CelebV-HQ datasets. The results show that our framework can generate facial expression trajectories according to text instruction. Considering that text prompts allow us to make diverse descriptions of human emotional states, the repertoire of facial expressions and the transitions between them can be expanded greatly. We expect our framework to find various practical applications More information about our project can be found at <a href="https://vohoanganh.github.io/tg3dfet/" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
    </blockquote>

    <!--CONTEXT-->
    <div class="metatable">
      <table summary="Additional metadata"><tr>
          <td class="tablecell label">Subjects:</td>
          <td class="tablecell subjects">
            <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Graphics (cs.GR); Machine Learning (cs.LG); Multimedia (cs.MM)</td>
        </tr><tr>
          <td class="tablecell label">Cite as:</td>
          <td class="tablecell arxivid"><span class="arxivid"><a href="https://arxiv.org/abs/2601.08179">arXiv:2601.08179</a> [cs.CV]</span></td>
        </tr>
        <tr>
          <td class="tablecell label">&nbsp;</td>
          <td class="tablecell arxividv">(or <span class="arxivid">
              <a href="https://arxiv.org/abs/2601.08179v1">arXiv:2601.08179v1</a> [cs.CV]</span> for this version)
          </td>
        </tr>
        <tr>
          <td class="tablecell label">&nbsp;</td>
          <td class="tablecell arxivdoi">              <a href="https://doi.org/10.48550/arXiv.2601.08179" id="arxiv-doi-link">https://doi.org/10.48550/arXiv.2601.08179</a><div class="button-and-tooltip">
              <button class="more-info" aria-describedby="more-info-desc-1">
                <svg height="15" role="presentation" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z" class=""></path></svg>
                <span class="visually-hidden">Focus to learn more</span>
              </button>
              <!-- tooltip description -->
              <div role="tooltip" id="more-info-desc-1">
                <span class="left-corner"></span>                  arXiv-issued DOI via DataCite (pending registration)</div>
            </div>
          </td>
        </tr>        <tr>
          <td class="tablecell label">Journal&nbsp;reference:</td>
          <td class="tablecell jref">IEEE Transactions on Multimedia, 2025</td>
        </tr>
        <tr>
          <td class="tablecell label">
            <abbr title="Digital Object Identifier">Related DOI</abbr>:
          </td>
          <td class="tablecell doi"><a href="https://doi.org/10.1109/TMM.2025.3565929" data-doi="10.1109/TMM.2025.3565929" class="link-https link-external" rel="external noopener nofollow">https://doi.org/10.1109/TMM.2025.3565929</a>

            <!-- accessible tooltip example -->
            <div class="button-and-tooltip">
              <button class="more-info" aria-describedby="more-info-desc-1">
                <svg height="15" role="presentation" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z" class=""></path></svg>
                <span class="visually-hidden">Focus to learn more</span>
              </button>
              <!-- tooltip description -->
              <div role="tooltip" id="more-info-desc-1">
                <span class="left-corner"></span>
                DOI(s) linking to related resources
              </div>
            </div>
          </td>
        </tr>
</table>
    </div>
  </div>
</div>
    <div class="submission-history">
      <h2>Submission history</h2> From: Anh Vo Hoang [<a href="/show-email/e26e56e3/2601.08179" rel="nofollow">view email</a>]      <br/>    <strong>[v1]</strong>
        Tue, 13 Jan 2026 03:12:48 UTC (6,582 KB)<br/>
</div>
  </div>
  <!--end leftcolumn-->
<div class="extra-services">    <div class="full-text">
      <a name="other"></a>
      <span class="descriptor">Full-text links:</span>
      <h2>Access Paper:</h2>
      <ul>
  <div id="download-button-info" hidden>
View a PDF of the paper titled Instruction-Driven 3D Facial Expression Generation and Transition, by Anh H. Vo and 4 other authors</div><li><a href="/pdf/2601.08179" aria-describedby="download-button-info" accesskey="f" class="abs-button download-pdf">View PDF</a></li><li><a href="https://arxiv.org/html/2601.08179v1" class="abs-button" id="latexml-download-link">HTML (experimental)</a></li><li><a href="/src/2601.08179" class="abs-button download-eprint">TeX Source
 </a></li></ul>
      <div class="abs-license"><a href="http://arxiv.org/licenses/nonexclusive-distrib/1.0/" title="Rights to this article">view license</a></div>
    </div>
    <!--end full-text-->    <div class="browse">
    Current browse context: <div class="current">cs.CV</div>

  <div class="prevnext">
<span class="arrow">
      <a class="abs-button prev-url" href="/prevnext?id=2601.08179&amp;function=prev&amp;context=cs.CV"
         accesskey="p" title="previous in cs.CV (accesskey p)" rel="nofollow">&lt;&nbsp;prev</a>
    </span>
    <span class="is-hidden-mobile">&nbsp; | &nbsp;</span>    <span class="arrow">
      <a class="abs-button next-url" href="/prevnext?id=2601.08179&amp;function=next&amp;context=cs.CV" accesskey="n"
         title="next in cs.CV (accesskey n)"  rel="nofollow">next&nbsp;&gt;</a>
    </span><br/>
  </div><div class="list">
    <a class="abs-button abs-button-grey abs-button-small context-new" href="/list/cs.CV/new"  rel="nofollow">new</a>
    <span class="is-hidden-mobile"> | </span>
    <a class="abs-button abs-button-grey abs-button-small context-recent" href="/list/cs.CV/recent" rel="nofollow">recent</a>
    <span class="is-hidden-mobile"> | </span><a class="abs-button abs-button-grey abs-button-small context-id" href="/list/cs.CV/2026-01" rel="nofollow">2026-01</a>
  </div><div class="abs-switch-cat">
    Change to browse by:
    <div class="switch context-change">
        <a href="/abs/2601.08179?context=cs" rel="nofollow">cs</a><br class="is-hidden-mobile">
        <a class="subclass" href="/abs/2601.08179?context=cs.AI" rel="nofollow">cs.AI</a><br class="is-hidden-mobile">
        <a class="subclass" href="/abs/2601.08179?context=cs.GR" rel="nofollow">cs.GR</a><br class="is-hidden-mobile">
        <a class="subclass" href="/abs/2601.08179?context=cs.LG" rel="nofollow">cs.LG</a><br class="is-hidden-mobile">
        <a class="subclass" href="/abs/2601.08179?context=cs.MM" rel="nofollow">cs.MM</a><br class="is-hidden-mobile">
    </div>
  </div>

    </div>
      <div class="extra-ref-cite">
        <h3>References &amp; Citations</h3>
        <ul>
          <li><a  class="abs-button abs-button-small cite-ads" href="https://ui.adsabs.harvard.edu/abs/arXiv:2601.08179">NASA ADS</a></li><li><a  class="abs-button abs-button-small cite-google-scholar" href="https://scholar.google.com/scholar_lookup?arxiv_id=2601.08179" target="_blank" rel="noopener">Google Scholar</a></li>
          <li><a  class="abs-button abs-button-small cite-semantic-scholar" href="https://api.semanticscholar.org/arXiv:2601.08179" target="_blank" rel="noopener">Semantic Scholar</a></li>
        </ul>
        <div style="clear:both;"></div>
      </div>

<div class='extra-ref-cite'>
    <span id='bib-cite-trigger' class="bib-cite-button abs-button">export BibTeX citation</span>
    <span id='bib-cite-loading' hidden='true'>Loading...</span>
</div>

<div id='bib-cite-modal' class='bib-modal' hidden='true'>
    <div class='bib-modal-content'>
        <div class='bib-modal-title'>
            <h2>BibTeX formatted citation</h2>
            <span class='bib-modal-close' >&times;</span>
        </div>
        <div>
            <textarea id='bib-cite-target' class="bib-citation-content" aria-label="loading the citation">loading...</textarea>
        </div>
        <div>
            <span>Data provided by: </span>
            <a id='bib-cite-source-api'></a>
        </div>
    </div>
</div><div class="bookmarks">
  <div><h3>Bookmark</h3></div><a class="abs-button abs-button-grey abs-button-small" href="http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2601.08179&amp;description=Instruction-Driven 3D Facial Expression Generation and Transition"
     title="Bookmark on BibSonomy">
    <img src="/static/browse/0.3.4/images/icons/social/bibsonomy.png"
         alt="BibSonomy logo"/>
  </a>
  <a class="abs-button abs-button-grey abs-button-small" href="https://reddit.com/submit?url=https://arxiv.org/abs/2601.08179&amp;title=Instruction-Driven 3D Facial Expression Generation and Transition"
     title="Bookmark on Reddit">
    <img src="/static/browse/0.3.4/images/icons/social/reddit.png"
         alt="Reddit logo"/>
  </a>
</div>  </div>
  <!--end extra-services-->
<!-- LABS AREA -->
<div id="labstabs">
  <div class="labstabs"><input type="radio" name="tabs" id="tabone"checked="checked">
    <label for="tabone">Bibliographic Tools</label>
    <div class="tab labs-display-bib">
      <h1>Bibliographic and Citation Tools</h1>
      <div class="toggle">
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input id="bibex-toggle" type="checkbox" class="lab-toggle"
                     data-script-url="/static/browse/0.3.4/bibex/bibex.js?20241202">
              <span class="slider"></span>
              <span class="is-sr-only">Bibliographic Explorer Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-bibex">Bibliographic Explorer</span> <em>(<a href="https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer">What is the Explorer?</a>)</em>
          </div>
        </div>
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="connectedpapers-toggle"
                type="checkbox"
                class="lab-toggle"
                data-script-url="/static/browse/0.3.4/js/connectedpapers.js"
                aria-labelledby="label-for-connected-papers">
              <span class="slider"></span>
              <span class="is-sr-only">Connected Papers Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-connected-papers">Connected Papers</span> <em>(<a href="https://www.connectedpapers.com/about" target="_blank">What is Connected Papers?</a>)</em>
          </div>
        </div><div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="litmaps-toggle"
                type="checkbox"
                class="lab-toggle"
                data-script-url="/static/browse/0.3.4/js/litmaps.js?20210617"
                aria-labelledby="label-for-litmaps">
              <span class="slider"></span>
              <span class="is-sr-only">Litmaps Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-litmaps">Litmaps</span> <em>(<a href="https://www.litmaps.co/" target="_blank">What is Litmaps?</a>)</em>
          </div>
        </div>
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="scite-toggle"
                type="checkbox"
                class="lab-toggle"
                data-script-url="/static/browse/0.3.4/js/scite.js?20210617"
                aria-labelledby="label-for-scite">
              <span class="slider"></span>
              <span class="is-sr-only">scite.ai Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-scite">scite Smart Citations</span> <em>(<a href="https://www.scite.ai/" target="_blank">What are Smart Citations?</a>)</em>
          </div>
        </div>
      </div>
        <div class="labs-content-placeholder labs-display" style="display: none;"></div>
        <div style="min-height: 15px" id="connectedpapers-output"></div>
        <div style="min-height: 15px" id="litmaps-open-in"></div>
        <div style="min-height: 15px" id="scite-open-in"></div>
    </div>


    <input type="radio" name="tabs" id="tabtwo">
    <label for="tabtwo">Code, Data, Media</label>
    <div class="tab">
      <h1>Code, Data and Media Associated with this Article</h1>
      <div class="toggle">
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="alphaxiv-toggle"
                data-script-url="/static/browse/0.3.4/js/alphaxiv.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-alphaxiv">
              <span class="slider"></span>
              <span class="is-sr-only">alphaXiv Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-alphaxiv">alphaXiv</span> <em>(<a href="https://alphaxiv.org/" target="_blank">What is alphaXiv?</a>)</em>
          </div>
        </div>

        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input        
                id="catalyzex-toggle"
                data-script-url="/static/browse/0.3.4/js/catalyzex.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-cx">
              <span class="slider"></span>
              <span class="is-sr-only">Links to Code Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-cx">CatalyzeX Code Finder for Papers</span> <em>(<a href="https://www.catalyzex.com" target="_blank">What is CatalyzeX?</a>)</em>
          </div>
        </div>

        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="dagshub-toggle"
                data-script-url="/static/browse/0.3.4/js/dagshub.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-dagshub">
              <span class="slider"></span>
              <span class="is-sr-only">DagsHub Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-dagshub">DagsHub</span> <em>(<a href="https://dagshub.com/" target="_blank">What is DagsHub?</a>)</em>
          </div>
        </div>
  
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="gotitpub-toggle"
                data-script-url="/static/browse/0.3.4/js/gotitpub.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-gotitpub">
              <span class="slider"></span>
              <span class="is-sr-only">GotitPub Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-gotitpub">Gotit.pub</span> <em>(<a href="http://gotit.pub/faq" target="_blank">What is GotitPub?</a>)</em>
          </div>
        </div>

        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="huggingface-toggle"
                data-script-url="/static/browse/0.3.4/js/huggingface.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-huggingface">
              <span class="slider"></span>
              <span class="is-sr-only">Huggingface Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-huggingface">Hugging Face</span> <em>(<a href="https://huggingface.co/huggingface" target="_blank">What is Huggingface?</a>)</em>
          </div>
        </div>

        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="paperwithcode-toggle"
                data-script-url="/static/browse/0.3.4/js/paperswithcode.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-pwc">
              <span class="slider"></span>
              <span class="is-sr-only">Links to Code Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-pwc">Papers with Code</span> <em>(<a href="https://paperswithcode.com/" target="_blank">What is Papers with Code?</a>)</em>
          </div>
        </div>


        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="sciencecast-toggle"
                data-script-url="/static/browse/0.3.4/js/sciencecast.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-sciencecast">
              <span class="slider"></span>
              <span class="is-sr-only">ScienceCast Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-sciencecast">ScienceCast</span> <em>(<a href="https://sciencecast.org/welcome" target="_blank">What is ScienceCast?</a>)</em>
          </div>
        </div>
      </div>

      <div id="alphaxiv-output" style="display:none"></div>
      <div id="catalyzex-output" style="display:none"></div>
      <div id="dagshub-output" style="display:none"></div>
      <div id="gotitpub-output" style="display:none"></div>
      <div id="pwc-output" style="display:none"></div>
      <div id="pwc-data-output" style="display:none"></div>
      <div id="sciencecast-output" style="display:none"></div>
      <div id="huggingface-output" style="display:none"></div>
    </div>


      <input type="radio" name="tabs" id="labstabs-demos-input">
      <label for="labstabs-demos-input" id="labstabs-demos-label">Demos</label>
      <div class="tab">
        <h1>Demos</h1>
        <div class="toggle">
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input
                  id="replicate-toggle"
                  data-script-url="/static/browse/0.3.4/js/replicate.js"
                  type="checkbox" class="lab-toggle" aria-labelledby="label-for-replicate">
                <span class="slider"></span>
                <span class="is-sr-only">Replicate Toggle</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-replicate">Replicate</span> <em>(<a href="https://replicate.com/docs/arxiv/about" target="_blank">What is Replicate?</a>)</em>
            </div>
          </div>
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input
                  id="spaces-toggle"
                  data-script-url="/static/browse/0.3.4/js/spaces.js"
                  type="checkbox" class="lab-toggle" aria-labelledby="label-for-spaces">
                <span class="slider"></span>
                <span class="is-sr-only">Spaces Toggle</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-spaces">Hugging Face Spaces</span> <em>(<a href="https://huggingface.co/docs/hub/spaces" target="_blank">What is Spaces?</a>)</em>
            </div>
          </div>
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input
                  id="txyz-toggle"
                  data-script-url="/static/browse/0.3.4/js/txyz.js"
                  type="checkbox" class="lab-toggle" aria-labelledby="label-for-txyz">
                <span class="slider"></span>
                <span class="is-sr-only">Spaces Toggle</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-txyz">TXYZ.AI</span> <em>(<a href="https://txyz.ai" target="_blank">What is TXYZ.AI?</a>)</em>
            </div>
          </div>
        </div>
        <div id="replicate-output"></div>
        <div id="spaces-output"></div>
        <div id="txyz-output"></div>
      </div>
      <input type="radio" name="tabs" id="tabfour">
      <label for="tabfour">Related Papers</label>
      <div class="tab">
        <h1>Recommenders and Search Tools</h1>
        <div class="toggle">
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input id="influenceflower-toggle"
                data-script-url="/static/browse/0.3.4/js/influenceflower.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-influenceflower">
                <span class="slider"></span>
                <span class="is-sr-only">Link to Influence Flower</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-influenceflower">Influence Flower</span> <em>(<a href="https://influencemap.cmlab.dev/" target="_blank">What are Influence Flowers?</a>)</em>
            </div>
          </div>
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input id="core-recommender-toggle" type="checkbox" class="lab-toggle" aria-labelledby="label-for-core">
                <span class="slider"></span>
                <span class="is-sr-only">Core recommender toggle</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-core">CORE Recommender</span> <em>(<a href="https://core.ac.uk/services/recommender">What is CORE?</a>)</em>
            </div>
          </div></div>
        <div id="influenceflower-output"></div>
        <div id="influenceflower-output-graph" style="display:none">
          <ul class="flower-tabs">
            <li class="active"><a class="btn tab-btn" onclick="openTab(event, 'tab-author')">Author</a></li>
            <li><a class="btn tab-btn" onclick="openTab(event, 'tab-venue')">Venue</a></li>
            <li><a class="btn tab-btn" onclick="openTab(event, 'tab-inst')">Institution</a></li>
            <li><a class="btn tab-btn" onclick="openTab(event, 'tab-topic')">Topic</a></li>
          </ul>
          <div class="flower-tab-content">
            <div class="tab-flower active" id="tab-author"><svg id="flower-graph-author"></svg></div>
            <div class="tab-flower" id="tab-venue"><svg id="flower-graph-venue"></svg></div>
            <div class="tab-flower" id="tab-inst"><svg id="flower-graph-inst"></svg></div>
            <div class="tab-flower" id="tab-topic"><svg id="flower-graph-topic"></svg></div>
          </div>
        </div>
        <div id="coreRecommenderOutput"></div>
        <div id="iarxivOutput"></div>
      </div>

      <input type="radio" name="tabs" id="tabfive">
      <label for="tabfive">
        About arXivLabs
      </label>
      <div class="tab">
        <div class="columns">
          <div class="column">
            <h1>arXivLabs: experimental projects with community collaborators</h1>
            <p>arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.</p>
            <p>Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.</p>
            <p>Have an idea for a project that will add value for arXiv's community? <a href="https://info.arxiv.org/labs/index.html"><strong>Learn more about arXivLabs</strong></a>.</p>
          </div>
          <div class="column is-narrow is-full-mobile">
            <p class="icon-labs"><svg xmlns="http://www.w3.org/2000/svg" role="presentation" viewBox="0 0 635.572 811"><path d="M175.6 676v27h-27v-27zm-54 27v27h27v-27zm-27 27v27h27v-27zm396-54v27h-27v-27zm0 27v27h27v-27zm27 27v27h27v-27zm-27-414h27v27h-27zm27 0h27v-27h-27zm27-27h27v-27h-27zm-396 45h-27v-27h27zm-27-54h-27v27h27zm-27-27h-27v27h27z"/><path d="M94.6 730v27h-27v-27zm477 0v27h-27v-27zm-27-495h27v27h-27zm-450 18h-27v-27h27zm477 9h27v27h-27zm-54 495h27v27h-27zm-423 0h27v27h-27zm-54-504h27v27h-27z" fill="#666"/><path d="M67.6 730v27h-27v-27zm54 54v27h-27v-27zm0-108v27h27v-27zm-27 27v27h27v-27zm-81 0v27h27v-27zm585 27v27h-27v-27zm-108-54v27h27v-27zm27 27v27h27v-27zm81 0v27h27v-27zm-54-495h27v27h-27zm-54 108h27v-27h-27zm27-27h27v-27h-27zm0-81h27v-27h-27zm-423 18h-27v-27h27zm54 54h-27v27h27zm-27-27h-27v27h27zm0-81h-27v27h27zm423 612v27h-27v-27zm81-522v27h-27v-27zm-585-9v27h-27v-27z" fill="#999"/><path d="M94.6 784v27h-27v-27zm-27-27v27h27v-27zm-27-54v27h27v-27zm27 0v27h27v-27zm0-27v27h27v-27zm27 0v27h27v-27zm0-27v27h27v-27zm27 0v27h27v-27zm-108 81v27h27v-27zm558 54v27h-27v-27zm-27-27v27h27v-27zm27-54v27h27v-27zm-27 0v27h27v-27zm0-27v27h27v-27zm-27 0v27h27v-27zm0-27v27h27v-27zm-27 0v27h27v-27zm108 81v27h27v-27zm0-495h27v27h-27zm-27 27h27v-27h-27zm-54-27h27v-27h-27zm0 27h27v-27h-27zm-27 0h27v-27h-27zm0 27h27v-27h-27zm-27 0h27v-27h-27zm0 27h27v-27h-27zm81-108h27v-27h-27zm-504 45h-27v-27h27zm27-27h-27v27h27zm54-27h-27v27h27zm0 27h-27v27h27zm27 0h-27v27h27zm0 27h-27v27h27zm27 0h-27v27h27zm0 27h-27v27h27zm-81-108h-27v27h27z" fill="#ccc"/><path d="M598.6 665.1H41.5C-76.5 667 176 280.2 176 280.2h53a46.5 46.5 0 0162.8-56.3 29.2 29.2 0 1128.5 35.9h-1a46.5 46.5 0 01-1.5 20.3l142.5-.1s255.3 387 138.3 385.1zM291 181a29.3 29.3 0 10-29.2-29.3A29.3 29.3 0 00291 181zm65.4-66.8a22.4 22.4 0 10-22.5-22.4 22.4 22.4 0 0022.5 22.4z" fill="#fc0"/><path d="M245.5 172V10h153v162s324 495 198 495h-558c-126 0 207-495 207-495zm126 54h56m-13 72h56m-9 72h56m-20 72h56m-22 72h56m-29 72h56m-457-45c20.8 41.7 87.3 81 160.7 81 72.1 0 142.1-38.2 163.4-81" fill="none" stroke="#000" stroke-miterlimit="10" stroke-width="20"/><path d="M273.3 421.7c0 31-9.8 56.3-21.9 56.3s-21.8-25.2-21.8-56.3 9.8-56.3 21.8-56.3 21.9 25.2 21.9 56.3zm114.4-56.3c-12 0-21.8 25.2-21.8 56.3s9.7 56.3 21.8 56.3 21.9-25.2 21.9-56.3-9.8-56.3-21.9-56.3zM150.1 526.6c-18.2 6.7-27.5 22.9-23.2 30.2s14.8-5.5 33-12.2 37.4-4.9 33-12.2-24.5-12.6-42.8-5.8zm296 5.8c-4.2 7.3 14.9 5.5 33.1 12.2s28.7 19.5 33 12.2-5-23.5-23.2-30.2-38.5-1.5-42.8 5.8z"/></svg></p>
          </div>
        </div>
      </div>

    </div>
</div>
<!-- END LABS AREA -->
  <div class="endorsers">
    <a href="/auth/show-endorsers/2601.08179" class="endorser-who" rel="nofollow">Which authors of this paper are endorsers?</a> |
    <a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="https://info.arxiv.org/help/mathjax.html">What is MathJax?</a>)
    <span class="help" style="font-style: normal; float: right; margin-top: 0; margin-right: 1em;"></span>
  </div>
  <script type="text/javascript" language="javascript">mathjaxToggle();</script>
</div>
      </div>
    </main>

    <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://info.arxiv.org/about">About</a></li>
                <li><a href="https://info.arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
                <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
  </div>

  <script src="/static/base/1.0.1/js/member_acknowledgement.js"></script>

</body>

</html>

<!-- ===== END ABS ===== -->

<!-- ===== HTML URL: https://arxiv.org/html/2601.08179v1 | HTTP 200 ===== -->

<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Instruction-Driven 3D Facial Expression Generation and Transition</title>
<!--Generated on Tue Jan 13 03:04:03 2026 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="/static/browse/0.3.4/css/arxiv-html-papers-20250916.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Instruction-Driven,  Facial Expression and Transition,  Controllable Avatar,  CK+ and CelebV-HQ datasets.
" lang="en" name="keywords"/>
<base href="/html/2601.08179v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S1" title="In Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S2" title="In Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Work</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S2.SS1" title="In II Related Work ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Facial Expression Transition</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S2.SS2" title="In II Related Work ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Face Rendering</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S3" title="In Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Method</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S3.SS1" title="In III Method ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Problem Statement</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S3.SS2" title="In III Method ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">The Proposed Method</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S3.SS2.SSS1" title="In III-B The Proposed Method ‣ III Method ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span>1 </span>Facial Expression Transition</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S3.SS2.SSS2" title="In III-B The Proposed Method ‣ III Method ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span>2 </span>Face Rendering</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4" title="In Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Experiments</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS1" title="In IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Dataset</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS1.SSS1" title="In IV-A Dataset ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>1 </span>CK+ dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS1.SSS2" title="In IV-A Dataset ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span>2 </span>CelebV-HQ dataset</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS2" title="In IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Implementation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS3" title="In IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">Classification of Facial Expressions</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS4" title="In IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span> </span><span class="ltx_text ltx_font_italic">Results</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS4.SSS1" title="In IV-D Results ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span>1 </span>Quantitative Comparison</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS4.SSS2" title="In IV-D Results ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span>2 </span>Qualitative Comparison</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS4.SSS3" title="In IV-D Results ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span>3 </span>Inference Time</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS5" title="In IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-E</span> </span><span class="ltx_text ltx_font_italic">Ablation Study</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS5.SSS1" title="In IV-E Ablation Study ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-E</span>1 </span>Effectiveness of the Proposed Components</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS5.SSS2" title="In IV-E Ablation Study ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-E</span>2 </span>Impact of Three Loss Functions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS5.SSS3" title="In IV-E Ablation Study ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-E</span>3 </span>Impact of IFED on the Training Process</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS5.SSS4" title="In IV-E Ablation Study ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-E</span>4 </span>Effect of IFED on Intensity of Facial Expression</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS5.SSS5" title="In IV-E Ablation Study ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-E</span>5 </span>Impact of Quantity of CAFT Layer</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS5.SSS6" title="In IV-E Ablation Study ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-E</span>6 </span>Impact of Quantity of Transformer Encoder in Facial Parameter Branch and Text Branch</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS6" title="In IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-F</span> </span><span class="ltx_text ltx_font_italic">Longer Facial Expression Generation with Neutral Expressions</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS6.SSS1" title="In IV-F Longer Facial Expression Generation with Neutral Expressions ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-F</span>1 </span>Implementation Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS6.SSS2" title="In IV-F Longer Facial Expression Generation with Neutral Expressions ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-F</span>2 </span>Network Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS6.SSS3" title="In IV-F Longer Facial Expression Generation with Neutral Expressions ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-F</span>3 </span>Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS7" title="In IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-G</span> </span><span class="ltx_text ltx_font_italic">Facial Expression Classification </span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS7.SSS1" title="In IV-G Facial Expression Classification ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-G</span>1 </span>Implementation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS7.SSS2" title="In IV-G Facial Expression Classification ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-G</span>2 </span>Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS8" title="In IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-H</span> </span><span class="ltx_text ltx_font_italic">Discussion</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS8.SSS1" title="In IV-H Discussion ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-H</span>1 </span>Diversity of Facial Expression Sequences</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS8.SSS2" title="In IV-H Discussion ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-H</span>2 </span>Results on Instructions with Changes in Template Sentences</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS8.SSS3" title="In IV-H Discussion ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-H</span>3 </span>Failure Cases</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S5" title="In Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Instruction-Driven 3D Facial Expression Generation and Transition</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Anh H. Vo, 
Tae-Seok Kim, 
Hulin Jin, 
Soo-Mi Choi, 
and Yong-Guk Kim
</span><span class="ltx_author_notes">A H Vo, T-S Kim, S-M Choi, and Y-G Kim are with the Department of Computer Engineering, Sejong University, Seoul, Republic of Korea.H Jin is with the School of Computer Science and Technology, Anhui University, Hefei, China.* Corresponding Author: ykim@sejong.ac.kr.
This is the author’s accepted manuscript.
The final version is published in IEEE Transactions on Multimedia, 2025
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">A 3D avatar typically has one of six cardinal facial expressions. To simulate realistic emotional variation, we should be able to render a facial transition between two arbitrary expressions. This study presents a new framework for instruction-driven facial expression generation that produces a 3D face and, starting from an image of the face, transforms the facial expression from one designated facial expression to another. The Instruction-driven Facial Expression Decomposer (IFED) module is introduced to facilitate multimodal data learning and capture the correlation between textual descriptions and facial expression features. Subsequently, we propose the Instruction to Facial Expression Transition (I2FET) method, which leverages IFED and a vertex reconstruction loss function to refine the semantic comprehension of latent vectors, thus generating a facial expression sequence according to the given instruction. Lastly, we present the Facial Expression Transition model to generate smooth transitions between facial expressions.
Extensive evaluation suggests that the proposed model outperforms state-of-the-art methods on the CK+ and CelebV-HQ datasets. The results show that our framework can generate facial expression trajectories according to text instruction. Considering that text prompts allow us to make diverse descriptions of human emotional states, the repertoire of facial expressions and the transitions between them can be expanded greatly. We expect our framework to find various practical applications. More information about our project can be found at <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://vohoanganh.github.io/tg3dfet/</span></p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Instruction-Driven, Facial Expression and Transition, Controllable Avatar, CK+ and CelebV-HQ datasets.

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Face and facial expression generation have attracted much attention in recent years due to their widespread application in many fields such as augmented reality/virtual reality, the gaming industry, movie production, or human-computer interfaces. Face generation can be divided into two categories depending on whether the face is two or three-dimensional (2D or 3D faces). For the 2D case, several methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib6" title="Collaborative diffusion for multi-modal face generation and editing">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib7" title="TediGAN: text-guided diverse face image generation and manipulation">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib10" title="Talk-to-edit: fine-grained facial editing via dialog">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib5" title="DCFace: synthetic face generation with dual condition diffusion model">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib71" title="Utilizing greedy nature for multimodal conditional image synthesis in transformers">36</a>]</cite> have been proposed to generate facial images using multimodal input, such as text, sketches, and segmentation. Previous studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib9" title="Exprgan: facial expression editing with controllable expression intensity">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib7" title="TediGAN: text-guided diverse face image generation and manipulation">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib52" title="FaceCLIP: facial image-to-video translation via a brief text description">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib62" title="Language-guided face animation by recurrent stylegan-based generator">13</a>]</cite> have attempted to solve facial expression generation in response to input provided as an emotion label or text prompts. However, these methods have limited ability to control the geometrical information of the output image and are effective only for specific applications. In the case of a 3D face, several studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib37" title="Realistic one-shot mesh-based head avatars">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib42" title="Instant volumetric head avatars">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib50" title="CVTHead: one-shot controllable head avatar with vertex-feature transformer">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib63" title="A unified framework for high fidelity face swap and expression reenactment">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib64" title="Rethinking one-shot face reenactment: a spatial–temporal reconstruction view">4</a>]</cite> have demonstrated methods for generating 3D faces from 2D ones. The models generate the output image combining geometrical information from the source image, such as the pose, shape, and texture.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="173" id="S1.F1.g1" src="x1.png" width="252"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;">Illustration of our framework wherein it accepts a text instruction with a face image (source) as input and generates a 3D face from it. Then, it transforms the facial expression, from disgust (expression 1) to happiness (expression 2) via a neutral expression.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Other researchers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib8" title="FaceCLIPNeRF: text-driven 3d face manipulation using deformable neural radiance fields">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib57" title="ClipFace: text-guided editing of textured 3d morphable models">1</a>]</cite> generate a specific 3D facial expression based on text prompts. They utilize CLIP (Contrastive Language-Image Pretraining) or NeRF (Neural Radiance Field) to learn the latent code for manipulating facial expressions. Yet, NeRF requires substantial computational resources and time. For instance, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib57" title="ClipFace: text-guided editing of textured 3d morphable models">1</a>]</cite> learns facial parameters and generates facial texture and expression based on text prompts. Although these methods can generate specific facial expressions, they do not address facial expression transitions based on instruction provided. Meanwhile, other studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib53" title="HeadGAN: one-shot neural head synthesis and editing">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib51" title="VariTex: variational neural face textures">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib41" title="Neural head avatars from monocular rgb videos">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib11" title="Neural emotion director: speech-preserving semantic control of facial expressions in ”in-the-wild” videos">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib56" title="GaussianAvatars: photorealistic head avatars with rigged 3d gaussians">32</a>]</cite> proposed to control facial expressions from an RGB video or generate portrait images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib15" title="AniPortraitGAN: animatable 3d portrait generation from 2d image collections">40</a>]</cite> with controllable facial expression, head pose, and shoulder movements. Alternatively, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib13" title="Generating complex 4d expression transitions by learning face landmark trajectories">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib12" title="4D facial expression diffusion model">46</a>]</cite> utilized the facial landmark trajectories to generate multiple 4D expression transitions on a meshed face.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">These studies led us to propose a new framework that enables a 3D face to be generated from a source image. Then, the facial expression undergoes transition in response to prompt text provided by the user to request a facial transition between two arbitrary expressions. Recent studies have used text-driven methods to generate a deformed face via NeRF or create an animated face via StyleGAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib62" title="Language-guided face animation by recurrent stylegan-based generator">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib8" title="FaceCLIPNeRF: text-driven 3d face manipulation using deformable neural radiance fields">16</a>]</cite>. They generate sequence of facial expressions by specifying a certain emotion, such as ’the person is happy’. In contrast, our study utilizes a text instruction to control the variation in the facial expression, such as ’Turn this face from disgust to happiness’, as illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S1.F1" title="Figure 1 ‣ I Introduction ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">1</span></a>. Yet, understanding the target expressions specified by text instructions presents a challenge when relying solely on global text embedding. Indeed, even though CLIP is a notable method for the language-conditioning module in the text-to-image task <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib62" title="Language-guided face animation by recurrent stylegan-based generator">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib8" title="FaceCLIPNeRF: text-driven 3d face manipulation using deformable neural radiance fields">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib65" title="Seer: language instructed video prediction with latent diffusion models">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib57" title="ClipFace: text-guided editing of textured 3d morphable models">1</a>]</cite>, it still faces challenges related to bias in understanding the text despite its large-scale training or the provision of global instruction, which may overlook fine-grained sub-instructions. Moreover, a text-only approach could be sensitive in generative models because changes in the structure of instructions can significantly impact their performance.
The additional challenge lies therein to ensure that the facial motion described in the instruction is aligned with the semantics of the language. A straightforward approach to generating the facial motion would entail interpolating between the source code and the edited code from StyleClip <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib66" title="StyleCLIP: text-driven manipulation of stylegan imagery">30</a>]</cite>.
Nevertheless, the additional challenge of controlling the latent code to adjust the head pose of the source image while generating the facial animation makes it difficult to achieve the intended goal.

<br class="ltx_break"/>  To make the connection between the text and facial expressions, we supplement a text instruction for making a transition between facial expressions into two standard datasets containing facial images, i.e., CelebV-HQ (High-Quality Celebrity Video) and CK+ (Extended Cohn-Kanade). To control the facial model in detail, FLAME (Faces Learned with an Articulated Model and Expressions) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib47" title="Learning a model of facial shape and expression from 4d scans">22</a>]</cite> is adopted and then fine-tuned with basic facial expressions and corresponding poses to capture the movement of the head, eyes, and mouth.
<br class="ltx_break"/>  Furthermore, integrating feature representations from different modalities minimizes ambiguity in redundant resources and effectively handles missing modalities, particularly in generative models, by improving inter-modality relations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib67" title="Multimodal generative learning utilizing jensen-shannon-divergence">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib68" title="A survey of multimodal deep generative models">38</a>]</cite>.
The proposed approach allows our model, i.e. Instruction to Facial Expression Transition (I2FET), to enhance the relationships between facial expression features and textual descriptions, thereby improving the latent representation for accurately generating target facial expressions and reducing the reliance solely on instructional text.
Then, facial expression trajectories are created using the target facial expressions, which are generated by the facial parameters of the source image using I2FET. Using this framework, one can generate a 3D facial expression avatar from a photograph of a face and create any facial transition based on a text prompt instruction that contains the initial and final facial expressions.

<br class="ltx_break"/>  The main contributions of our study are given as follows:</p>
</div>
<div class="ltx_para" id="S1.p4">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We present an instruction-driven 3D facial expression generation and transition framework to generate transitions in facial expressions based on instructions, starting from a photograph of a face.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We propose an Instruction-Driven Facial Expression Decomposer (IFED), designed to learn multimodal data and capture the correlation between facial expression features and textual descriptions.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We propose an Instruction to Facial Expression Transition method (I2FET), which utilizes conditional variational autoencoders, integrated with IFED and a vertex reconstruction loss function, to enhance the semantic comprehension of the latent vectors.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">Extensive evaluation of the proposed model on the CK+ and CelebV-HQ datasets suggests that it outperforms SOTA methods.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related Work</span>
</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.4.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.5.2">Facial Expression Transition</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Generative Adversarial Networks (GANs) have been favored for generating facial expressions. For instance, Motion3DGAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib13" title="Generating complex 4d expression transitions by learning face landmark trajectories">27</a>]</cite>, an extension of MotionGAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib55" title="Sparse to dense dynamic 3d facial expression generation">28</a>]</cite>, addresses the dynamics of 3D landmarks, whereas WGAN learns the distribution of 3D expression dynamics across the hypersphere space, sampled with a condition to generate landmark sequences. In this case, a neutral 3D face mesh, frame by frame, is deformed by a mesh decoder using the generated landmark sequences although the disadvantage of this work is that the start and end expressions have to be provided to generate a concatenated expression transition.
On the other hand, a diffusion model is adopted for facial expression generation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib12" title="4D facial expression diffusion model">46</a>]</cite> by modeling the input distribution. The deformation of the 3D mesh is guided by a specific landmark, and the original shape of the input facial mesh is considered to apply displacement to each vertex.
In this work, MotionClip <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib31" title="Motionclip: exposing human motion generation to clip space">39</a>]</cite>, which was originally designed for human motion synthesis, is compared with the task of generating the expression transition conditions given by the text.
EMOTE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib48" title="Emotional speech-driven animation with content-emotion disentanglement">6</a>]</cite> utilizes emotion labels and various inputs to generate a talking-head avatar that maintains lip-sync from speech as well as facial expression. In particular, the facial motion prior, called FLINT (FLAME IN TIME), uses the expression and pose parameters of FLAME <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib47" title="Learning a model of facial shape and expression from 4d scans">22</a>]</cite> to represent facial motion sequences.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Although several studies have demonstrated methods to produce transitions between facial expressions under diverse conditions, the generated faces are typically conditioned on the front face by neglecting the head pose variations. Inspired by previous works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib13" title="Generating complex 4d expression transitions by learning face landmark trajectories">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib12" title="4D facial expression diffusion model">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib48" title="Emotional speech-driven animation with content-emotion disentanglement">6</a>]</cite>, the present study proposes a model for facial expression transition that learns the expression and pose parameters. The additional merit of our model would be that it can easily be plugged into the models that generate the facial appearance.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.4.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.5.2">Face Rendering</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">In the face rendering area, 3D Morphable Model (3DMM) has been widely used for rendering realistic faces <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib51" title="VariTex: variational neural face textures">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib38" title="Exp-gan: 3d-aware facial image generation with expression control">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib37" title="Realistic one-shot mesh-based head avatars">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib50" title="CVTHead: one-shot controllable head avatar with vertex-feature transformer">26</a>]</cite>.
Among these methods, VariTex <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib51" title="VariTex: variational neural face textures">2</a>]</cite> employs a neural texture obtained by a face texture decoder to sample with a UV map. Subsequently, this neural face feature image is merged with an additive feature image to serve as the input for a U-Net to generate the desired image. Exp-GAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib38" title="Exp-gan: 3d-aware facial image generation with expression control">21</a>]</cite> employs a neural volume generator and StyleGAN2 to generate images at various resolutions. This process combines different feature components, such as face features, depth images, and feature volumes. The facial feature is also obtained by sampling the neural texture using the UV map. However, this work is computationally intensive and time-consuming.
ROME <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib37" title="Realistic one-shot mesh-based head avatars">18</a>]</cite> can synthesize realistic face images by estimating a specific head mesh and neural texture from an input image, whereupon a neural rendering technique is adapted to render the rigged mesh. Next, the U-net-like neural rendering network, which accepts inputs such as neural texture and surface normals, is utilized to acquire both the target image and its corresponding mask.
Similarly, CVTHead <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib50" title="CVTHead: one-shot controllable head avatar with vertex-feature transformer">26</a>]</cite> also estimates a neural texture from an input image, and yet, in this case, the neural texture is flattened to a feature vector form before being passed to transformers along with vertex tokens to output a vertex descriptor. Specifically, each of the vertices is considered a query token, and a transformer is employed to learn the canonical vertex feature from the source image. To generate the desired image and its mask, the feature descriptor of each vertex and depth are projected into image space before being processed by a U-Net.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">In the current study, we employ ROME and CVTHead to generate facial appearances by leveraging DECA for the extraction of facial expression coefficients. This streamlines the integration into a facial expression transition model and enhances the applicability of the proposed framework to practical scenarios.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Method</span>
</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.4.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.5.2">Problem Statement</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.21">We aim to develop a model to generate 3D facial expression with two inputs: a face image <math alttext="I_{s}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1" intent=":literal"><semantics><msub><mi>I</mi><mi>s</mi></msub><annotation encoding="application/x-tex">I_{s}</annotation></semantics></math> and a text instruction <math alttext="t" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>. The facial expression transition model <math alttext="\mathcal{T}_{w}(t,I_{s})" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3" intent=":literal"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒯</mi><mi>w</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>I</mi><mi>s</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{T}_{w}(t,I_{s})</annotation></semantics></math>, having weights <math alttext="w" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4" intent=":literal"><semantics><mi>w</mi><annotation encoding="application/x-tex">w</annotation></semantics></math>, is trained with a training set <math alttext="\mathcal{X}=\{(t,(e_{0},\theta_{0}),(e_{1},\theta_{1}))^{(j)}\}_{j=1}^{n}" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m5" intent=":literal"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒳</mi><mo>=</mo><msubsup><mrow><mo stretchy="false">{</mo><msup><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><mrow><mo stretchy="false">(</mo><msub><mi>e</mi><mn>0</mn></msub><mo>,</mo><msub><mi>θ</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow><mo>,</mo><mrow><mo stretchy="false">(</mo><msub><mi>e</mi><mn>1</mn></msub><mo>,</mo><msub><mi>θ</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">}</mo></mrow><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup></mrow><annotation encoding="application/x-tex">\mathcal{X}=\{(t,(e_{0},\theta_{0}),(e_{1},\theta_{1}))^{(j)}\}_{j=1}^{n}</annotation></semantics></math>, where <math alttext="e_{0},e_{1}\in R^{1\times 50}" class="ltx_Math" display="inline" id="S3.SS1.p1.6.m6" intent=":literal"><semantics><mrow><mrow><msub><mi>e</mi><mn>0</mn></msub><mo>,</mo><msub><mi>e</mi><mn>1</mn></msub></mrow><mo>∈</mo><msup><mi>R</mi><mrow><mn>1</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>50</mn></mrow></msup></mrow><annotation encoding="application/x-tex">e_{0},e_{1}\in R^{1\times 50}</annotation></semantics></math> denote expression vectors with 50 dimensions and <math alttext="\theta_{0},\theta_{1}\in R^{1\times 6}" class="ltx_Math" display="inline" id="S3.SS1.p1.7.m7" intent=":literal"><semantics><mrow><mrow><msub><mi>θ</mi><mn>0</mn></msub><mo>,</mo><msub><mi>θ</mi><mn>1</mn></msub></mrow><mo>∈</mo><msup><mi>R</mi><mrow><mn>1</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>6</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\theta_{0},\theta_{1}\in R^{1\times 6}</annotation></semantics></math> represent pose vectors with 6 dimensions corresponding to the first and next expressions in the facial expression sequence specified in instruction <math alttext="t" class="ltx_Math" display="inline" id="S3.SS1.p1.8.m8" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>, respectively.
<math alttext="n" class="ltx_Math" display="inline" id="S3.SS1.p1.9.m9" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math> is the number of samples in the training dataset. Our network learns <math alttext="\mathcal{T}_{w}(.)" class="ltx_math_unparsed" display="inline" id="S3.SS1.p1.10.m10" intent=":literal"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒯</mi><mi>w</mi></msub><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0.167em">.</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{T}_{w}(.)</annotation></semantics></math> that approximates the data distribution <math alttext="p(\mathcal{X})" class="ltx_Math" display="inline" id="S3.SS1.p1.11.m11" intent=":literal"><semantics><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic">𝒳</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">p(\mathcal{X})</annotation></semantics></math> and then generates a facial expression coefficient sequence <math alttext="\{(e_{0},\theta_{0}),(e_{1},\theta_{1})\}" class="ltx_Math" display="inline" id="S3.SS1.p1.12.m12" intent=":literal"><semantics><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><msub><mi>e</mi><mn>0</mn></msub><mo>,</mo><msub><mi>θ</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow><mo>,</mo><mrow><mo stretchy="false">(</mo><msub><mi>e</mi><mn>1</mn></msub><mo>,</mo><msub><mi>θ</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{(e_{0},\theta_{0}),(e_{1},\theta_{1})\}</annotation></semantics></math> with a given instruction <math alttext="t" class="ltx_Math" display="inline" id="S3.SS1.p1.13.m13" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>.
Next, a face rendering module <math alttext="\mathcal{G}(I_{s},\mathcal{S})" class="ltx_Math" display="inline" id="S3.SS1.p1.14.m14" intent=":literal"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒢</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>I</mi><mi>s</mi></msub><mo>,</mo><mi class="ltx_font_mathcaligraphic">𝒮</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{G}(I_{s},\mathcal{S})</annotation></semantics></math> is used to render a sequence of facial appearances <math alttext="\mathcal{Y}=\{y_{s}^{(1)},y^{(i)},y_{e_{0}}^{(k)},y^{(j)},y_{e_{1}}^{(T)}\}" class="ltx_Math" display="inline" id="S3.SS1.p1.15.m15" intent=":literal"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒴</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><msubsup><mi>y</mi><mi>s</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msubsup><mi>y</mi><msub><mi>e</mi><mn>0</mn></msub><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msubsup><mi>y</mi><msub><mi>e</mi><mn>1</mn></msub><mrow><mo stretchy="false">(</mo><mi>T</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{Y}=\{y_{s}^{(1)},y^{(i)},y_{e_{0}}^{(k)},y^{(j)},y_{e_{1}}^{(T)}\}</annotation></semantics></math> , with inputs, consisting of the texture of source image <math alttext="I_{s}" class="ltx_Math" display="inline" id="S3.SS1.p1.16.m16" intent=":literal"><semantics><msub><mi>I</mi><mi>s</mi></msub><annotation encoding="application/x-tex">I_{s}</annotation></semantics></math> and facial expression trajectories
<math alttext="S=\{s_{s}^{(1)},s^{(i)},s_{e_{0}}^{(k)},s^{(j)},s_{e_{1}}^{(T)}\}" class="ltx_Math" display="inline" id="S3.SS1.p1.17.m17" intent=":literal"><semantics><mrow><mi>S</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><msubsup><mi>s</mi><mi>s</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><msup><mi>s</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msubsup><mi>s</mi><msub><mi>e</mi><mn>0</mn></msub><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><msup><mi>s</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msubsup><mi>s</mi><msub><mi>e</mi><mn>1</mn></msub><mrow><mo stretchy="false">(</mo><mi>T</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">S=\{s_{s}^{(1)},s^{(i)},s_{e_{0}}^{(k)},s^{(j)},s_{e_{1}}^{(T)}\}</annotation></semantics></math>,
where <math alttext="\{s_{s}^{(1)},s_{e_{0}}^{(k)},s_{e_{1}}^{(T)}\}" class="ltx_Math" display="inline" id="S3.SS1.p1.18.m18" intent=":literal"><semantics><mrow><mo stretchy="false">{</mo><msubsup><mi>s</mi><mi>s</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>s</mi><msub><mi>e</mi><mn>0</mn></msub><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>s</mi><msub><mi>e</mi><mn>1</mn></msub><mrow><mo stretchy="false">(</mo><mi>T</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{s_{s}^{(1)},s_{e_{0}}^{(k)},s_{e_{1}}^{(T)}\}</annotation></semantics></math> are anchor facial expression sequences corresponding to the source image, and specific facial expressions, generated by <math alttext="\mathcal{T}_{w}(.)" class="ltx_math_unparsed" display="inline" id="S3.SS1.p1.19.m19" intent=":literal"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒯</mi><mi>w</mi></msub><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0.167em">.</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{T}_{w}(.)</annotation></semantics></math>.
Then, facial expression sequences <math alttext="s^{(i)}" class="ltx_Math" display="inline" id="S3.SS1.p1.20.m20" intent=":literal"><semantics><msup><mi>s</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><annotation encoding="application/x-tex">s^{(i)}</annotation></semantics></math> and <math alttext="s^{(j)}" class="ltx_Math" display="inline" id="S3.SS1.p1.21.m21" intent=":literal"><semantics><msup><mi>s</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msup><annotation encoding="application/x-tex">s^{(j)}</annotation></semantics></math> transitioning between these anchor facial expressions can be inferred by a linear interpolation function to ensure the smoothness of the sequence of facial expressions.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.4.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.5.2">The Proposed Method</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.6">Figure <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S3.F2" title="Figure 2 ‣ III-B The Proposed Method ‣ III Method ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">2</span></a> provides an overview of the proposed framework whereby one can control 3D facial expressions of the monocular image <math alttext="I_{s}" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1" intent=":literal"><semantics><msub><mi>I</mi><mi>s</mi></msub><annotation encoding="application/x-tex">I_{s}</annotation></semantics></math> following an input instruction <math alttext="t" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>.
The framework consists of two major modules: (1) Facial Expression Transition (FET), <math alttext="\mathcal{T}_{w}(t,I_{s})" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3" intent=":literal"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒯</mi><mi>w</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>t</mi><mo>,</mo><msub><mi>I</mi><mi>s</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{T}_{w}(t,I_{s})</annotation></semantics></math>, for generating diverse expression trajectories; (2) Face Rendering (FR), <math alttext="\mathcal{G}(I_{s},S)" class="ltx_Math" display="inline" id="S3.SS2.p1.4.m4" intent=":literal"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒢</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>I</mi><mi>s</mi></msub><mo>,</mo><mi>S</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{G}(I_{s},S)</annotation></semantics></math>, for rendering facial appearances based on the texture of the source image <math alttext="I_{s}" class="ltx_Math" display="inline" id="S3.SS2.p1.5.m5" intent=":literal"><semantics><msub><mi>I</mi><mi>s</mi></msub><annotation encoding="application/x-tex">I_{s}</annotation></semantics></math> and the expression trajectories <math alttext="S" class="ltx_Math" display="inline" id="S3.SS2.p1.6.m6" intent=":literal"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math> generated by the FET module.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="278" id="S3.F2.g1" src="x2.png" width="350"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.34.17.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S3.F2.32.16" style="font-size:90%;">Overview of the proposed framework consisting of two major modules: Facial Expression Transition (FET) and Face Rendering (FR). First, textual vector <math alttext="x_{t}" class="ltx_Math" display="inline" id="S3.F2.17.1.m1" intent=":literal"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding="application/x-tex">x_{t}</annotation></semantics></math> is obtained from the pre-trained CLIP encoder for the FET module. Simultaneously, latent representations <math alttext="z_{p}" class="ltx_Math" display="inline" id="S3.F2.18.2.m2" intent=":literal"><semantics><msub><mi>z</mi><mi>p</mi></msub><annotation encoding="application/x-tex">z_{p}</annotation></semantics></math> and <math alttext="z_{e}" class="ltx_Math" display="inline" id="S3.F2.19.3.m3" intent=":literal"><semantics><msub><mi>z</mi><mi>e</mi></msub><annotation encoding="application/x-tex">z_{e}</annotation></semantics></math> are drawn from <math alttext="\mathcal{N}(0,I)" class="ltx_Math" display="inline" id="S3.F2.20.4.m4" intent=":literal"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mi>I</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{N}(0,I)</annotation></semantics></math>. Then, the latent vectors are processed and concatenated to obtain <math alttext="\hat{x}^{f}" class="ltx_Math" display="inline" id="S3.F2.21.5.m5" intent=":literal"><semantics><msup><mover accent="true"><mi>x</mi><mo>^</mo></mover><mi>f</mi></msup><annotation encoding="application/x-tex">\hat{x}^{f}</annotation></semantics></math>. Afterward, the IFED module utilizes <math alttext="x^{t}" class="ltx_Math" display="inline" id="S3.F2.22.6.m6" intent=":literal"><semantics><msup><mi>x</mi><mi>t</mi></msup><annotation encoding="application/x-tex">x^{t}</annotation></semantics></math> and <math alttext="\hat{x}^{f}" class="ltx_Math" display="inline" id="S3.F2.23.7.m7" intent=":literal"><semantics><msup><mover accent="true"><mi>x</mi><mo>^</mo></mover><mi>f</mi></msup><annotation encoding="application/x-tex">\hat{x}^{f}</annotation></semantics></math> as inputs to create conditional feature vectors <math alttext="\hat{x}_{emb}^{e}" class="ltx_Math" display="inline" id="S3.F2.24.8.m8" intent=":literal"><semantics><msubsup><mover accent="true"><mi>x</mi><mo>^</mo></mover><mrow><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mi>b</mi></mrow><mi>e</mi></msubsup><annotation encoding="application/x-tex">\hat{x}_{emb}^{e}</annotation></semantics></math>, and <math alttext="\hat{x}_{emb}^{p}" class="ltx_Math" display="inline" id="S3.F2.25.9.m9" intent=":literal"><semantics><msubsup><mover accent="true"><mi>x</mi><mo>^</mo></mover><mrow><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mi>b</mi></mrow><mi>p</mi></msubsup><annotation encoding="application/x-tex">\hat{x}_{emb}^{p}</annotation></semantics></math> for the pose and expression decoders. Subsequently, for smoothness of the facial expression sequence, the facial expression in the source image <math alttext="(e_{s},\theta_{s})" class="ltx_Math" display="inline" id="S3.F2.26.10.m10" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>e</mi><mi>s</mi></msub><mo>,</mo><msub><mi>θ</mi><mi>s</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(e_{s},\theta_{s})</annotation></semantics></math> is subjected to linear interpolation and the specific facial expressions <math alttext="(e_{0},\theta_{0}),(e_{1},\theta_{1})" class="ltx_Math" display="inline" id="S3.F2.27.11.m11" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><msub><mi>e</mi><mn>0</mn></msub><mo>,</mo><msub><mi>θ</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow><mo>,</mo><mrow><mo stretchy="false">(</mo><msub><mi>e</mi><mn>1</mn></msub><mo>,</mo><msub><mi>θ</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">(e_{0},\theta_{0}),(e_{1},\theta_{1})</annotation></semantics></math>, generated by the I2FET decoders.
These sequences are then combined with the shape <math alttext="\phi_{s}" class="ltx_Math" display="inline" id="S3.F2.28.12.m12" intent=":literal"><semantics><msub><mi>ϕ</mi><mi>s</mi></msub><annotation encoding="application/x-tex">\phi_{s}</annotation></semantics></math> and camera <math alttext="c_{s}" class="ltx_Math" display="inline" id="S3.F2.29.13.m13" intent=":literal"><semantics><msub><mi>c</mi><mi>s</mi></msub><annotation encoding="application/x-tex">c_{s}</annotation></semantics></math> parameters of the source image, obtained from DECA, to form facial expression trajectories <math alttext="\{s_{s}^{(1)},s^{(i)},s^{(k)}_{e_{0}},s^{(j)},s_{e_{1}}^{(T)}\}" class="ltx_Math" display="inline" id="S3.F2.30.14.m14" intent=":literal"><semantics><mrow><mo stretchy="false">{</mo><msubsup><mi>s</mi><mi>s</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><msup><mi>s</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msubsup><mi>s</mi><msub><mi>e</mi><mn>0</mn></msub><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><msup><mi>s</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msubsup><mi>s</mi><msub><mi>e</mi><mn>1</mn></msub><mrow><mo stretchy="false">(</mo><mi>T</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{s_{s}^{(1)},s^{(i)},s^{(k)}_{e_{0}},s^{(j)},s_{e_{1}}^{(T)}\}</annotation></semantics></math>. For FR, head mesh reconstruction produces a flame mesh sequence <math alttext="\{m_{s}^{(1)},m^{(i)},m^{(k)}_{e_{0}},m^{(j)},m_{e_{1}}^{(T)}\}" class="ltx_Math" display="inline" id="S3.F2.31.15.m15" intent=":literal"><semantics><mrow><mo stretchy="false">{</mo><msubsup><mi>m</mi><mi>s</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><msup><mi>m</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msubsup><mi>m</mi><msub><mi>e</mi><mn>0</mn></msub><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><msup><mi>m</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msubsup><mi>m</mi><msub><mi>e</mi><mn>1</mn></msub><mrow><mo stretchy="false">(</mo><mi>T</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{m_{s}^{(1)},m^{(i)},m^{(k)}_{e_{0}},m^{(j)},m_{e_{1}}^{(T)}\}</annotation></semantics></math> aligned with the facial expression trajectories and the texture encoder extracts the facial appearance from the source image. Finally, the rendering module generates the facial appearances <math alttext="\{y_{s}^{(1)},y^{(i)},y^{(k)}_{e_{0}},y^{(j)},y_{e_{1}}^{(T)}\}" class="ltx_Math" display="inline" id="S3.F2.32.16.m16" intent=":literal"><semantics><mrow><mo stretchy="false">{</mo><msubsup><mi>y</mi><mi>s</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msubsup><mi>y</mi><msub><mi>e</mi><mn>0</mn></msub><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msubsup><mi>y</mi><msub><mi>e</mi><mn>1</mn></msub><mrow><mo stretchy="false">(</mo><mi>T</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{y_{s}^{(1)},y^{(i)},y^{(k)}_{e_{0}},y^{(j)},y_{e_{1}}^{(T)}\}</annotation></semantics></math> within the facial expression trajectories using the flame mesh sequence and the extracted facial appearance.
</span></figcaption>
</figure>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS2.SSS1.4.1.1">III-B</span>1 </span>Facial Expression Transition</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.3">This module is designed to generate diverse expression trajectories that correspond to the facial parameters of the source face image <math alttext="I_{s}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.1.m1" intent=":literal"><semantics><msub><mi>I</mi><mi>s</mi></msub><annotation encoding="application/x-tex">I_{s}</annotation></semantics></math> and the parameters of specific facial expressions, described in an instruction <math alttext="t" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.2.m2" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>. The module has two primary branches: the first receives an instruction to generate the corresponding sequence of facial expression based on instruction <math alttext="t" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.3.m3" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>, whereas the second takes a 2D face image to encode its facial coefficients and create the facial expression trajectories.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="313" id="S3.F3.g1" src="x3.png" width="330"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.14.7.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.12.6" style="font-size:90%;">Flow diagram of IFED (Instruction-Driven Facial Expression Decomposer). It consists of CAFT and the decomposition of parameters. The major function of CAFT is to introduce cross-attention between the facial parameters and text instruction. The two outputs from CAFT pass through layer normalization and are combined to create the fused feature vector <math alttext="x^{fused}" class="ltx_Math" display="inline" id="S3.F3.7.1.m1" intent=":literal"><semantics><msup><mi>x</mi><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mi>u</mi><mo lspace="0em" rspace="0em">​</mo><mi>s</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msup><annotation encoding="application/x-tex">x^{fused}</annotation></semantics></math>. Then, pose projection function <math alttext="\mathcal{P}_{p}" class="ltx_Math" display="inline" id="S3.F3.8.2.m2" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mi>p</mi></msub><annotation encoding="application/x-tex">\mathcal{P}_{p}</annotation></semantics></math> and expression projection function <math alttext="\mathcal{P}_{e}" class="ltx_Math" display="inline" id="S3.F3.9.3.m3" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mi>e</mi></msub><annotation encoding="application/x-tex">\mathcal{P}_{e}</annotation></semantics></math> are used to decompose <math alttext="x^{fused}" class="ltx_Math" display="inline" id="S3.F3.10.4.m4" intent=":literal"><semantics><msup><mi>x</mi><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mi>u</mi><mo lspace="0em" rspace="0em">​</mo><mi>s</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msup><annotation encoding="application/x-tex">x^{fused}</annotation></semantics></math> into two conditional vectors for facial expression (<math alttext="x_{emb}^{e}" class="ltx_Math" display="inline" id="S3.F3.11.5.m5" intent=":literal"><semantics><msubsup><mi>x</mi><mrow><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mi>b</mi></mrow><mi>e</mi></msubsup><annotation encoding="application/x-tex">x_{emb}^{e}</annotation></semantics></math>) and pose (<math alttext="x_{emb}^{p}" class="ltx_Math" display="inline" id="S3.F3.12.6.m6" intent=":literal"><semantics><msubsup><mi>x</mi><mrow><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mi>b</mi></mrow><mi>p</mi></msubsup><annotation encoding="application/x-tex">x_{emb}^{p}</annotation></semantics></math>).</span></figcaption>
</figure>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="466" id="S3.F4.g1" src="x4.png" width="718"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.24.12.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S3.F4.22.11" style="font-size:90%;">Detailed architecture of the I2FET (Instruction to Facial Expression Transition) method. Given an instruction and a sequence of facial parameters, including pose and expression, the IFED module utilizes the facial parameters and textual description extracted by CLIP to produce the corresponding conditional feature vectors for pose <math alttext="x_{emb}^{p}" class="ltx_Math" display="inline" id="S3.F4.12.1.m1" intent=":literal"><semantics><msubsup><mi>x</mi><mrow><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mi>b</mi></mrow><mi>p</mi></msubsup><annotation encoding="application/x-tex">x_{emb}^{p}</annotation></semantics></math> and expression <math alttext="x_{emb}^{e}" class="ltx_Math" display="inline" id="S3.F4.13.2.m2" intent=":literal"><semantics><msubsup><mi>x</mi><mrow><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mi>b</mi></mrow><mi>e</mi></msubsup><annotation encoding="application/x-tex">x_{emb}^{e}</annotation></semantics></math>, respectively. The pose and expression encoders then map this sequence of facial parameters and conditional feature vectors to latent representations corresponding to pose and expression. These latent vectors, along with the textual description, are passed through the IFED module to generate the conditional feature vectors <math alttext="\hat{x}_{emb}^{e}" class="ltx_Math" display="inline" id="S3.F4.14.3.m3" intent=":literal"><semantics><msubsup><mover accent="true"><mi>x</mi><mo>^</mo></mover><mrow><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mi>b</mi></mrow><mi>e</mi></msubsup><annotation encoding="application/x-tex">\hat{x}_{emb}^{e}</annotation></semantics></math> and <math alttext="\hat{x}_{emb}^{p}" class="ltx_Math" display="inline" id="S3.F4.15.4.m4" intent=":literal"><semantics><msubsup><mover accent="true"><mi>x</mi><mo>^</mo></mover><mrow><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mi>b</mi></mrow><mi>p</mi></msubsup><annotation encoding="application/x-tex">\hat{x}_{emb}^{p}</annotation></semantics></math>. Following this, the expression and pose decoders reconstruct expression <math alttext="\hat{e}" class="ltx_Math" display="inline" id="S3.F4.16.5.m5" intent=":literal"><semantics><mover accent="true"><mi>e</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{e}</annotation></semantics></math> and pose <math alttext="\hat{\theta}" class="ltx_Math" display="inline" id="S3.F4.17.6.m6" intent=":literal"><semantics><mover accent="true"><mi>θ</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\theta}</annotation></semantics></math> parameters using these latent representations and the conditional feature vectors <math alttext="\hat{x}_{emb}^{e}" class="ltx_Math" display="inline" id="S3.F4.18.7.m7" intent=":literal"><semantics><msubsup><mover accent="true"><mi>x</mi><mo>^</mo></mover><mrow><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mi>b</mi></mrow><mi>e</mi></msubsup><annotation encoding="application/x-tex">\hat{x}_{emb}^{e}</annotation></semantics></math> and <math alttext="\hat{x}_{emb}^{p}" class="ltx_Math" display="inline" id="S3.F4.19.8.m8" intent=":literal"><semantics><msubsup><mover accent="true"><mi>x</mi><mo>^</mo></mover><mrow><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mi>b</mi></mrow><mi>p</mi></msubsup><annotation encoding="application/x-tex">\hat{x}_{emb}^{p}</annotation></semantics></math>, respectively. Finally, the pre-trained FLAME decoder reconstructs the vertex coordinates <math alttext="\hat{v}" class="ltx_Math" display="inline" id="S3.F4.20.9.m9" intent=":literal"><semantics><mover accent="true"><mi>v</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{v}</annotation></semantics></math> based on <math alttext="\hat{e}" class="ltx_Math" display="inline" id="S3.F4.21.10.m10" intent=":literal"><semantics><mover accent="true"><mi>e</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{e}</annotation></semantics></math> and <math alttext="\hat{\theta}" class="ltx_Math" display="inline" id="S3.F4.22.11.m11" intent=":literal"><semantics><mover accent="true"><mi>θ</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{\theta}</annotation></semantics></math>.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.SSS1.p2">
<p class="ltx_p" id="S3.SS2.SSS1.p2.7">In the former, the pre-trained CLIP encoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib43" title="Learning transferable visual models from natural language supervision">33</a>]</cite> extracts text description <math alttext="x^{t}\in R^{77\times 768}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p2.1.m1" intent=":literal"><semantics><mrow><msup><mi>x</mi><mi>t</mi></msup><mo>∈</mo><msup><mi>R</mi><mrow><mn>77</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>768</mn></mrow></msup></mrow><annotation encoding="application/x-tex">x^{t}\in R^{77\times 768}</annotation></semantics></math> according to the given instruction <math alttext="t" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p2.2.m2" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>. Then, the I2FET model generates a sequence of expression and pose coefficients for the facial expression trajectories.

<br class="ltx_break"/> <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p2.7.1">Instruction-driven Facial Expression Decomposer</span>:
A dual-branch vision transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib59" title="CrossViT: cross-attention multi-scale vision transformer for image classification">3</a>]</cite> is adopted for extracting multi-scale features and fusing cross-attention information at different scales because it is efficient for the image classification task. Here, the Cross-Attention for Facial Expression Parameter and Text (CAFT) is used to learn the relationship between instruction and facial expression parameters by enhancing the understanding of multimodal data and refining the latent representation for the Instruction to Facial Expression Transition model. The proposed IFED module is illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S3.F3" title="Figure 3 ‣ III-B1 Facial Expression Transition ‣ III-B The Proposed Method ‣ III Method ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">3</span></a>. In particular, we directly incorporate facial expression parameters and textual descriptions into CAFT instead of adding class tokens as in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib59" title="CrossViT: cross-attention multi-scale vision transformer for image classification">3</a>]</cite>. With respect to the transformer encoders, the first branch learns the correlation between the facial expression parameters, while the second branch focuses on learning the textual descriptions. To begin with, each vector representing facial expression parameters comprises two elements: expression and pose parameters, denoted as <math alttext="x^{f}=\{e,\theta\}\in R^{m\times 56}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p2.3.m3" intent=":literal"><semantics><mrow><msup><mi>x</mi><mi>f</mi></msup><mo>=</mo><mrow><mo stretchy="false">{</mo><mi>e</mi><mo>,</mo><mi>θ</mi><mo stretchy="false">}</mo></mrow><mo>∈</mo><msup><mi>R</mi><mrow><mi>m</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mn>56</mn></mrow></msup></mrow><annotation encoding="application/x-tex">x^{f}=\{e,\theta\}\in R^{m\times 56}</annotation></semantics></math>, where <math alttext="m" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p2.4.m4" intent=":literal"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> denotes the number of expressions in a given instruction. As <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib58" title="PVP: Personalized Video Prior for Editable Dynamic Portraits using StyleGAN">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib48" title="Emotional speech-driven animation with content-emotion disentanglement">6</a>]</cite>, we only utilize jaw pose <math alttext="\theta^{jaw}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p2.5.m5" intent=":literal"><semantics><msup><mi>θ</mi><mrow><mi>j</mi><mo lspace="0em" rspace="0em">​</mo><mi>a</mi><mo lspace="0em" rspace="0em">​</mo><mi>w</mi></mrow></msup><annotation encoding="application/x-tex">\theta^{jaw}</annotation></semantics></math> and expression <math alttext="e" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p2.6.m6" intent=":literal"><semantics><mi>e</mi><annotation encoding="application/x-tex">e</annotation></semantics></math> to convey facial expression information. Therefore, we rewrite <math alttext="x^{f}=\{e,\theta^{jaw}\}\in R^{m\times 53}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p2.7.m7" intent=":literal"><semantics><mrow><msup><mi>x</mi><mi>f</mi></msup><mo>=</mo><mrow><mo stretchy="false">{</mo><mi>e</mi><mo>,</mo><msup><mi>θ</mi><mrow><mi>j</mi><mo lspace="0em" rspace="0em">​</mo><mi>a</mi><mo lspace="0em" rspace="0em">​</mo><mi>w</mi></mrow></msup><mo stretchy="false">}</mo></mrow><mo>∈</mo><msup><mi>R</mi><mrow><mi>m</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mn>53</mn></mrow></msup></mrow><annotation encoding="application/x-tex">x^{f}=\{e,\theta^{jaw}\}\in R^{m\times 53}</annotation></semantics></math> as the input of the first branch. Then, the transformer encoder is utilized as</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E1">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E1X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle y^{f}=x^{f}+MSA(LN(x^{f}))" class="ltx_Math" display="inline" id="S3.E1X.2.1.1.m1" intent=":literal"><semantics><mrow><msup><mi>y</mi><mi>f</mi></msup><mo>=</mo><mrow><msup><mi>x</mi><mi>f</mi></msup><mo>+</mo><mrow><mi>M</mi><mo lspace="0em" rspace="0em">​</mo><mi>S</mi><mo lspace="0em" rspace="0em">​</mo><mi>A</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>L</mi><mo lspace="0em" rspace="0em">​</mo><mi>N</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mi>f</mi></msup><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle y^{f}=x^{f}+MSA(LN(x^{f}))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="2"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(1)</span></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E1Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\hat{x}^{f}=y^{f}+FFN(LN(y^{f}))" class="ltx_Math" display="inline" id="S3.E1Xa.2.1.1.m1" intent=":literal"><semantics><mrow><msup><mover accent="true"><mi>x</mi><mo>^</mo></mover><mi>f</mi></msup><mo>=</mo><mrow><msup><mi>y</mi><mi>f</mi></msup><mo>+</mo><mrow><mi>F</mi><mo lspace="0em" rspace="0em">​</mo><mi>F</mi><mo lspace="0em" rspace="0em">​</mo><mi>N</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>L</mi><mo lspace="0em" rspace="0em">​</mo><mi>N</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>y</mi><mi>f</mi></msup><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\hat{x}^{f}=y^{f}+FFN(LN(y^{f}))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS1.p2.18">where <math alttext="\hat{x}^{f}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p2.8.m1" intent=":literal"><semantics><msup><mover accent="true"><mi>x</mi><mo>^</mo></mover><mi>f</mi></msup><annotation encoding="application/x-tex">\hat{x}^{f}</annotation></semantics></math> denotes the output of the transformer encoder in the facial expression parameters branch <math alttext="\mathcal{E}_{P}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p2.9.m2" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℰ</mi><mi>P</mi></msub><annotation encoding="application/x-tex">\mathcal{E}_{P}</annotation></semantics></math>. <math alttext="LN(.)" class="ltx_math_unparsed" display="inline" id="S3.SS2.SSS1.p2.10.m3" intent=":literal"><semantics><mrow><mi>L</mi><mi>N</mi><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0.167em">.</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">LN(.)</annotation></semantics></math>, <math alttext="MSA(.)" class="ltx_math_unparsed" display="inline" id="S3.SS2.SSS1.p2.11.m4" intent=":literal"><semantics><mrow><mi>M</mi><mi>S</mi><mi>A</mi><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0.167em">.</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">MSA(.)</annotation></semantics></math>, and <math alttext="FFN(.)" class="ltx_math_unparsed" display="inline" id="S3.SS2.SSS1.p2.12.m5" intent=":literal"><semantics><mrow><mi>F</mi><mi>F</mi><mi>N</mi><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0.167em">.</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">FFN(.)</annotation></semantics></math> are the layer norm, multi-head self-attention, and feed-forward network, respectively. Likewise, in the text branch, we replicate <math alttext="x^{t}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p2.13.m6" intent=":literal"><semantics><msup><mi>x</mi><mi>t</mi></msup><annotation encoding="application/x-tex">x^{t}</annotation></semantics></math> to create a vector <math alttext="\{x\}^{t}_{i=\{0,1\}}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p2.14.m7" intent=":literal"><semantics><msubsup><mrow><mo stretchy="false">{</mo><mi>x</mi><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">}</mo></mrow></mrow><mi>t</mi></msubsup><annotation encoding="application/x-tex">\{x\}^{t}_{i=\{0,1\}}</annotation></semantics></math> for computational purposes. Subsequently, the input for the transformer encoder is generated using a text projection function <math alttext="P_{t}(.)" class="ltx_math_unparsed" display="inline" id="S3.SS2.SSS1.p2.15.m8" intent=":literal"><semantics><mrow><msub><mi>P</mi><mi>t</mi></msub><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0.167em">.</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">P_{t}(.)</annotation></semantics></math> on <math alttext="\{x\}^{t}_{i=\{0,1\}}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p2.16.m9" intent=":literal"><semantics><msubsup><mrow><mo stretchy="false">{</mo><mi>x</mi><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">}</mo></mrow></mrow><mi>t</mi></msubsup><annotation encoding="application/x-tex">\{x\}^{t}_{i=\{0,1\}}</annotation></semantics></math>. As a result, the textual description <math alttext="\hat{x}^{t}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p2.17.m10" intent=":literal"><semantics><msup><mover accent="true"><mi>x</mi><mo>^</mo></mover><mi>t</mi></msup><annotation encoding="application/x-tex">\hat{x}^{t}</annotation></semantics></math> is obtained through the transformer encoder <math alttext="\mathcal{E}_{t}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p2.18.m11" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℰ</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\mathcal{E}_{t}</annotation></semantics></math>.
Both branches are then fused by cross-attention to capture the relationship between the facial expression parameters and textual descriptions. The fusion process can be expressed as</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E2">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E2X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle y_{c}^{f}=h^{f2t}(\hat{x}_{0}^{f}),\quad y_{c}^{t}=h^{t2f}(\hat{x}_{0}^{t})," class="ltx_Math" display="inline" id="S3.E2X.2.1.1.m1" intent=":literal"><semantics><mrow><mrow><mrow><msubsup><mi>y</mi><mi>c</mi><mi>f</mi></msubsup><mo>=</mo><mrow><msup><mi>h</mi><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mover accent="true"><mi>x</mi><mo>^</mo></mover><mn>0</mn><mi>f</mi></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><msubsup><mi>y</mi><mi>c</mi><mi>t</mi></msubsup><mo>=</mo><mrow><msup><mi>h</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mover accent="true"><mi>x</mi><mo>^</mo></mover><mn>0</mn><mi>t</mi></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle y_{c}^{f}=h^{f2t}(\hat{x}_{0}^{f}),\quad y_{c}^{t}=h^{t2f}(\hat{x}_{0}^{t}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="3"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(2)</span></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E2Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle x_{o}^{f}=g^{t2f}([CA(y^{f}_{c}\otimes x_{1}^{t})+y_{c}^{f}])\otimes\hat{x}_{1}^{f}" class="ltx_Math" display="inline" id="S3.E2Xa.2.1.1.m1" intent=":literal"><semantics><mrow><msubsup><mi>x</mi><mi>o</mi><mi>f</mi></msubsup><mo>=</mo><mrow><mrow><msup><mi>g</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mi>C</mi><mo lspace="0em" rspace="0em">​</mo><mi>A</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>y</mi><mi>c</mi><mi>f</mi></msubsup><mo lspace="0.222em" rspace="0.222em">⊗</mo><msubsup><mi>x</mi><mn>1</mn><mi>t</mi></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><msubsup><mi>y</mi><mi>c</mi><mi>f</mi></msubsup></mrow><mo stretchy="false">]</mo></mrow><mo rspace="0.055em" stretchy="false">)</mo></mrow></mrow><mo rspace="0.222em">⊗</mo><msubsup><mover accent="true"><mi>x</mi><mo>^</mo></mover><mn>1</mn><mi>f</mi></msubsup></mrow></mrow><annotation encoding="application/x-tex">\displaystyle x_{o}^{f}=g^{t2f}([CA(y^{f}_{c}\otimes x_{1}^{t})+y_{c}^{f}])\otimes\hat{x}_{1}^{f}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E2Xb">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle x_{o}^{t}=g^{f2t}([CA(y_{c}^{t}\otimes x_{1}^{f})+y_{c}^{t}])\otimes\hat{x}_{1}^{t}" class="ltx_Math" display="inline" id="S3.E2Xb.2.1.1.m1" intent=":literal"><semantics><mrow><msubsup><mi>x</mi><mi>o</mi><mi>t</mi></msubsup><mo>=</mo><mrow><mrow><msup><mi>g</mi><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mo stretchy="false">[</mo><mrow><mrow><mi>C</mi><mo lspace="0em" rspace="0em">​</mo><mi>A</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>y</mi><mi>c</mi><mi>t</mi></msubsup><mo lspace="0.222em" rspace="0.222em">⊗</mo><msubsup><mi>x</mi><mn>1</mn><mi>f</mi></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><msubsup><mi>y</mi><mi>c</mi><mi>t</mi></msubsup></mrow><mo stretchy="false">]</mo></mrow><mo rspace="0.055em" stretchy="false">)</mo></mrow></mrow><mo rspace="0.222em">⊗</mo><msubsup><mover accent="true"><mi>x</mi><mo>^</mo></mover><mn>1</mn><mi>t</mi></msubsup></mrow></mrow><annotation encoding="application/x-tex">\displaystyle x_{o}^{t}=g^{f2t}([CA(y_{c}^{t}\otimes x_{1}^{f})+y_{c}^{t}])\otimes\hat{x}_{1}^{t}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS1.p2.27">where <math alttext="h^{t2f},h^{f2t},g^{t2f}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p2.19.m1" intent=":literal"><semantics><mrow><msup><mi>h</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msup><mo>,</mo><msup><mi>h</mi><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></msup><mo>,</mo><msup><mi>g</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>f</mi></mrow></msup></mrow><annotation encoding="application/x-tex">h^{t2f},h^{f2t},g^{t2f}</annotation></semantics></math>, and <math alttext="g^{f2t}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p2.20.m2" intent=":literal"><semantics><msup><mi>g</mi><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow></msup><annotation encoding="application/x-tex">g^{f2t}</annotation></semantics></math> are the projection function and back-projection function between the vectors that represent the text and facial expression parameters, <math alttext="x_{o}^{f}\in R^{m\times 53}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p2.21.m3" intent=":literal"><semantics><mrow><msubsup><mi>x</mi><mi>o</mi><mi>f</mi></msubsup><mo>∈</mo><msup><mi>R</mi><mrow><mi>m</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mn>53</mn></mrow></msup></mrow><annotation encoding="application/x-tex">x_{o}^{f}\in R^{m\times 53}</annotation></semantics></math> and <math alttext="x_{o}^{t}\in R^{m\times 768}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p2.22.m4" intent=":literal"><semantics><mrow><msubsup><mi>x</mi><mi>o</mi><mi>t</mi></msubsup><mo>∈</mo><msup><mi>R</mi><mrow><mi>m</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mn>768</mn></mrow></msup></mrow><annotation encoding="application/x-tex">x_{o}^{t}\in R^{m\times 768}</annotation></semantics></math> are the outputs of CAFT, <math alttext="CA" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p2.23.m5" intent=":literal"><semantics><mrow><mi>C</mi><mo lspace="0em" rspace="0em">​</mo><mi>A</mi></mrow><annotation encoding="application/x-tex">CA</annotation></semantics></math> denotes cross-attention and <math alttext="\otimes" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p2.24.m6" intent=":literal"><semantics><mo>⊗</mo><annotation encoding="application/x-tex">\otimes</annotation></semantics></math> denotes the concatenation operator.
Next, the fused feature vector is obtained by concatenating <math alttext="x_{o}^{f}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p2.25.m7" intent=":literal"><semantics><msubsup><mi>x</mi><mi>o</mi><mi>f</mi></msubsup><annotation encoding="application/x-tex">x_{o}^{f}</annotation></semantics></math> and <math alttext="x_{o}^{t}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p2.26.m8" intent=":literal"><semantics><msubsup><mi>x</mi><mi>o</mi><mi>t</mi></msubsup><annotation encoding="application/x-tex">x_{o}^{t}</annotation></semantics></math>, after passing through <math alttext="LN(.)" class="ltx_math_unparsed" display="inline" id="S3.SS2.SSS1.p2.27.m9" intent=":literal"><semantics><mrow><mi>L</mi><mi>N</mi><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0.167em">.</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">LN(.)</annotation></semantics></math>, as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="x^{fused}=LN(x_{o}^{t})\otimes LN(x_{o}^{f})" class="ltx_Math" display="block" id="S3.E3.m1" intent=":literal"><semantics><mrow><msup><mi>x</mi><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mi>u</mi><mo lspace="0em" rspace="0em">​</mo><mi>s</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msup><mo>=</mo><mrow><mrow><mrow><mi>L</mi><mo lspace="0em" rspace="0em">​</mo><mi>N</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>x</mi><mi>o</mi><mi>t</mi></msubsup><mo rspace="0.055em" stretchy="false">)</mo></mrow></mrow><mo rspace="0.222em">⊗</mo><mi>L</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mi>N</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>x</mi><mi>o</mi><mi>f</mi></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">x^{fused}=LN(x_{o}^{t})\otimes LN(x_{o}^{f})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS1.p2.29">Finally, the projection functions for pose <math alttext="\mathcal{P}_{p}(.)" class="ltx_math_unparsed" display="inline" id="S3.SS2.SSS1.p2.28.m1" intent=":literal"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mi>p</mi></msub><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0.167em">.</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{P}_{p}(.)</annotation></semantics></math> and expression <math alttext="\mathcal{P}_{e}(.)" class="ltx_math_unparsed" display="inline" id="S3.SS2.SSS1.p2.29.m2" intent=":literal"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mi>e</mi></msub><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0.167em">.</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{P}_{e}(.)</annotation></semantics></math> are applied to the fused feature vector as follows:</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E4">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E4X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle x^{e}_{emb}=\mathcal{P}_{e}(x^{fused})" class="ltx_Math" display="inline" id="S3.E4X.2.1.1.m1" intent=":literal"><semantics><mrow><msubsup><mi>x</mi><mrow><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mi>b</mi></mrow><mi>e</mi></msubsup><mo>=</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mi>e</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mi>u</mi><mo lspace="0em" rspace="0em">​</mo><mi>s</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle x^{e}_{emb}=\mathcal{P}_{e}(x^{fused})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="2"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(4)</span></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E4Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle x^{p}_{emb}=\mathcal{P}_{p}(x^{fused})" class="ltx_Math" display="inline" id="S3.E4Xa.2.1.1.m1" intent=":literal"><semantics><mrow><msubsup><mi>x</mi><mrow><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mi>b</mi></mrow><mi>p</mi></msubsup><mo>=</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mi>p</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mi>u</mi><mo lspace="0em" rspace="0em">​</mo><mi>s</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle x^{p}_{emb}=\mathcal{P}_{p}(x^{fused})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS1.p2.38">where <math alttext="x^{e}_{emb}\in R^{m\times 50}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p2.30.m1" intent=":literal"><semantics><mrow><msubsup><mi>x</mi><mrow><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mi>b</mi></mrow><mi>e</mi></msubsup><mo>∈</mo><msup><mi>R</mi><mrow><mi>m</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mn>50</mn></mrow></msup></mrow><annotation encoding="application/x-tex">x^{e}_{emb}\in R^{m\times 50}</annotation></semantics></math> and <math alttext="x^{p}_{emb}\in R^{m\times 6}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p2.31.m2" intent=":literal"><semantics><mrow><msubsup><mi>x</mi><mrow><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mi>b</mi></mrow><mi>p</mi></msubsup><mo>∈</mo><msup><mi>R</mi><mrow><mi>m</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mn>6</mn></mrow></msup></mrow><annotation encoding="application/x-tex">x^{p}_{emb}\in R^{m\times 6}</annotation></semantics></math> are the conditional feature vectors for pose and expression, respectively. In this work, <math alttext="\mathcal{P}_{p}(.)" class="ltx_math_unparsed" display="inline" id="S3.SS2.SSS1.p2.32.m3" intent=":literal"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mi>p</mi></msub><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0.167em">.</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{P}_{p}(.)</annotation></semantics></math> and <math alttext="\mathcal{P}_{e}(.)" class="ltx_math_unparsed" display="inline" id="S3.SS2.SSS1.p2.33.m4" intent=":literal"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒫</mi><mi>e</mi></msub><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0.167em">.</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{P}_{e}(.)</annotation></semantics></math> are designed as linear transformation functions to map the feature <math alttext="x^{fused}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p2.34.m5" intent=":literal"><semantics><msup><mi>x</mi><mrow><mi>f</mi><mo lspace="0em" rspace="0em">​</mo><mi>u</mi><mo lspace="0em" rspace="0em">​</mo><mi>s</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msup><annotation encoding="application/x-tex">x^{fused}</annotation></semantics></math> to the corresponding pose and expression feature vectors.

<br class="ltx_break"/> <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p2.38.1">Instruction to Facial Expression Transition</span>:
The proposed I2FET method is based on a conditional variational autoencoder architecture. The method includes encoders <math alttext="\mathcal{E}_{e}^{t}(.)" class="ltx_math_unparsed" display="inline" id="S3.SS2.SSS1.p2.35.m6" intent=":literal"><semantics><mrow><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mi>e</mi><mi>t</mi></msubsup><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0.167em">.</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{E}_{e}^{t}(.)</annotation></semantics></math> and <math alttext="\mathcal{E}_{p}^{t}(.)" class="ltx_math_unparsed" display="inline" id="S3.SS2.SSS1.p2.36.m7" intent=":literal"><semantics><mrow><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mi>p</mi><mi>t</mi></msubsup><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0.167em">.</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{E}_{p}^{t}(.)</annotation></semantics></math>, as well as decoders <math alttext="\mathcal{D}^{t}_{e}(.)" class="ltx_math_unparsed" display="inline" id="S3.SS2.SSS1.p2.37.m8" intent=":literal"><semantics><mrow><msubsup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>e</mi><mi>t</mi></msubsup><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0.167em">.</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{D}^{t}_{e}(.)</annotation></semantics></math> and <math alttext="\mathcal{D}^{t}_{p}(.)" class="ltx_math_unparsed" display="inline" id="S3.SS2.SSS1.p2.38.m9" intent=":literal"><semantics><mrow><msubsup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>p</mi><mi>t</mi></msubsup><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0.167em">.</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{D}^{t}_{p}(.)</annotation></semantics></math>, which are defined using three Multi-Layer Perceptrons (MLPs). These components are conditioned on feature vectors that represent expression and pose information, respectively. An overview of the proposed I2FET architecture is presented in Fig.<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S3.F4" title="Figure 4 ‣ III-B1 Facial Expression Transition ‣ III-B The Proposed Method ‣ III Method ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p3">
<p class="ltx_p" id="S3.SS2.SSS1.p3.1">Here, the encoders are defined as follows:</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p4">
<table class="ltx_equation ltx_eqn_table" id="S3.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{E}_{e}^{t}(e\otimes x^{e}_{emb})\longrightarrow(\mu_{e},\sigma_{e})" class="ltx_Math" display="block" id="S3.E5.m1" intent=":literal"><semantics><mrow><mrow><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mi>e</mi><mi>t</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>e</mi><mo lspace="0.222em" rspace="0.222em">⊗</mo><msubsup><mi>x</mi><mrow><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mi>b</mi></mrow><mi>e</mi></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">⟶</mo><mrow><mo stretchy="false">(</mo><msub><mi>μ</mi><mi>e</mi></msub><mo>,</mo><msub><mi>σ</mi><mi>e</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{E}_{e}^{t}(e\otimes x^{e}_{emb})\longrightarrow(\mu_{e},\sigma_{e})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS1.p4.33">and</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{E}_{p}^{t}(\theta\otimes x^{p}_{emb})\longrightarrow(\mu_{p},\sigma_{p})" class="ltx_Math" display="block" id="S3.E6.m1" intent=":literal"><semantics><mrow><mrow><msubsup><mi class="ltx_font_mathcaligraphic">ℰ</mi><mi>p</mi><mi>t</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>θ</mi><mo lspace="0.222em" rspace="0.222em">⊗</mo><msubsup><mi>x</mi><mrow><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mi>b</mi></mrow><mi>p</mi></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">⟶</mo><mrow><mo stretchy="false">(</mo><msub><mi>μ</mi><mi>p</mi></msub><mo>,</mo><msub><mi>σ</mi><mi>p</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{E}_{p}^{t}(\theta\otimes x^{p}_{emb})\longrightarrow(\mu_{p},\sigma_{p})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS1.p4.16">where <math alttext="\otimes" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p4.1.m1" intent=":literal"><semantics><mo>⊗</mo><annotation encoding="application/x-tex">\otimes</annotation></semantics></math> denotes the concatenation operator and <math alttext="x^{e}_{emb}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p4.2.m2" intent=":literal"><semantics><msubsup><mi>x</mi><mrow><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mi>b</mi></mrow><mi>e</mi></msubsup><annotation encoding="application/x-tex">x^{e}_{emb}</annotation></semantics></math> and <math alttext="x^{p}_{emb}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p4.3.m3" intent=":literal"><semantics><msubsup><mi>x</mi><mrow><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mi>b</mi></mrow><mi>p</mi></msubsup><annotation encoding="application/x-tex">x^{p}_{emb}</annotation></semantics></math> are conditional feature vectors for expression and pose, respectively. For the encoder stage, <math alttext="x^{e}_{emb}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p4.4.m4" intent=":literal"><semantics><msubsup><mi>x</mi><mrow><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mi>b</mi></mrow><mi>e</mi></msubsup><annotation encoding="application/x-tex">x^{e}_{emb}</annotation></semantics></math> and <math alttext="x^{p}_{emb}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p4.5.m5" intent=":literal"><semantics><msubsup><mi>x</mi><mrow><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mi>b</mi></mrow><mi>p</mi></msubsup><annotation encoding="application/x-tex">x^{p}_{emb}</annotation></semantics></math> are created by textual description <math alttext="x_{t}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p4.6.m6" intent=":literal"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding="application/x-tex">x_{t}</annotation></semantics></math> and the facial expression parameters <math alttext="(e,\theta)" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p4.7.m7" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mi>e</mi><mo>,</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(e,\theta)</annotation></semantics></math>. <math alttext="\mu_{e}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p4.8.m8" intent=":literal"><semantics><msub><mi>μ</mi><mi>e</mi></msub><annotation encoding="application/x-tex">\mu_{e}</annotation></semantics></math> and <math alttext="\mu_{p}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p4.9.m9" intent=":literal"><semantics><msub><mi>μ</mi><mi>p</mi></msub><annotation encoding="application/x-tex">\mu_{p}</annotation></semantics></math> denote the mean and standard deviation of the expression distribution, whereas <math alttext="\sigma_{e}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p4.10.m10" intent=":literal"><semantics><msub><mi>σ</mi><mi>e</mi></msub><annotation encoding="application/x-tex">\sigma_{e}</annotation></semantics></math>, and <math alttext="\sigma_{p}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p4.11.m11" intent=":literal"><semantics><msub><mi>σ</mi><mi>p</mi></msub><annotation encoding="application/x-tex">\sigma_{p}</annotation></semantics></math> are the mean and standard deviation of the pose distribution. In this study,
the conditional feature vectors <math alttext="\hat{x}^{e}_{emb}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p4.12.m12" intent=":literal"><semantics><msubsup><mover accent="true"><mi>x</mi><mo>^</mo></mover><mrow><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mi>b</mi></mrow><mi>e</mi></msubsup><annotation encoding="application/x-tex">\hat{x}^{e}_{emb}</annotation></semantics></math> and <math alttext="\hat{x}^{p}_{emb}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p4.13.m13" intent=":literal"><semantics><msubsup><mover accent="true"><mi>x</mi><mo>^</mo></mover><mrow><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mi>b</mi></mrow><mi>p</mi></msubsup><annotation encoding="application/x-tex">\hat{x}^{p}_{emb}</annotation></semantics></math> in the decoder are computed by textual description <math alttext="x_{t}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p4.14.m14" intent=":literal"><semantics><msub><mi>x</mi><mi>t</mi></msub><annotation encoding="application/x-tex">x_{t}</annotation></semantics></math> and the latent vectors <math alttext="\hat{z}_{p}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p4.15.m15" intent=":literal"><semantics><msub><mover accent="true"><mi>z</mi><mo>^</mo></mover><mi>p</mi></msub><annotation encoding="application/x-tex">\hat{z}_{p}</annotation></semantics></math> and <math alttext="\hat{z}_{e}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p4.16.m16" intent=":literal"><semantics><msub><mover accent="true"><mi>z</mi><mo>^</mo></mover><mi>e</mi></msub><annotation encoding="application/x-tex">\hat{z}_{e}</annotation></semantics></math> representing pose and expression, respectively. These latent vectors are calculated as follows:</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E7">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E7X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\tilde{z}_{p}=\sigma_{p}*z_{p}+\mu_{p}" class="ltx_Math" display="inline" id="S3.E7X.2.1.1.m1" intent=":literal"><semantics><mrow><msub><mover accent="true"><mi>z</mi><mo>~</mo></mover><mi>p</mi></msub><mo>=</mo><mrow><mrow><msub><mi>σ</mi><mi>p</mi></msub><mo lspace="0.222em" rspace="0.222em">∗</mo><msub><mi>z</mi><mi>p</mi></msub></mrow><mo>+</mo><msub><mi>μ</mi><mi>p</mi></msub></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\tilde{z}_{p}=\sigma_{p}*z_{p}+\mu_{p}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="4"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(7)</span></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E7Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\hat{z}_{p}=\mathcal{T}_{p}(\tilde{z}_{p})" class="ltx_Math" display="inline" id="S3.E7Xa.2.1.1.m1" intent=":literal"><semantics><mrow><msub><mover accent="true"><mi>z</mi><mo>^</mo></mover><mi>p</mi></msub><mo>=</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒯</mi><mi>p</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mover accent="true"><mi>z</mi><mo>~</mo></mover><mi>p</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\hat{z}_{p}=\mathcal{T}_{p}(\tilde{z}_{p})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E7Xb">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\tilde{z}_{e}=\sigma_{e}*z_{e}+\mu_{e}" class="ltx_Math" display="inline" id="S3.E7Xb.2.1.1.m1" intent=":literal"><semantics><mrow><msub><mover accent="true"><mi>z</mi><mo>~</mo></mover><mi>e</mi></msub><mo>=</mo><mrow><mrow><msub><mi>σ</mi><mi>e</mi></msub><mo lspace="0.222em" rspace="0.222em">∗</mo><msub><mi>z</mi><mi>e</mi></msub></mrow><mo>+</mo><msub><mi>μ</mi><mi>e</mi></msub></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\tilde{z}_{e}=\sigma_{e}*z_{e}+\mu_{e}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E7Xc">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\hat{z}_{e}=\mathcal{T}_{e}(\tilde{z}_{e})" class="ltx_Math" display="inline" id="S3.E7Xc.2.1.1.m1" intent=":literal"><semantics><mrow><msub><mover accent="true"><mi>z</mi><mo>^</mo></mover><mi>e</mi></msub><mo>=</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒯</mi><mi>e</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mover accent="true"><mi>z</mi><mo>~</mo></mover><mi>e</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\hat{z}_{e}=\mathcal{T}_{e}(\tilde{z}_{e})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS1.p4.32">where <math alttext="z_{p}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p4.17.m1" intent=":literal"><semantics><msub><mi>z</mi><mi>p</mi></msub><annotation encoding="application/x-tex">z_{p}</annotation></semantics></math> and <math alttext="z_{e}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p4.18.m2" intent=":literal"><semantics><msub><mi>z</mi><mi>e</mi></msub><annotation encoding="application/x-tex">z_{e}</annotation></semantics></math> are the latent vectors sampled from <math alttext="\mathcal{N}(0,I)" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p4.19.m3" intent=":literal"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mi>I</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{N}(0,I)</annotation></semantics></math>.
In this case, we utilize linear transformation functions <math alttext="\mathcal{T}_{p}(.)" class="ltx_math_unparsed" display="inline" id="S3.SS2.SSS1.p4.20.m4" intent=":literal"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒯</mi><mi>p</mi></msub><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0.167em">.</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{T}_{p}(.)</annotation></semantics></math> and <math alttext="\mathcal{T}_{e}(.)" class="ltx_math_unparsed" display="inline" id="S3.SS2.SSS1.p4.21.m5" intent=":literal"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">𝒯</mi><mi>e</mi></msub><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0.167em">.</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{T}_{e}(.)</annotation></semantics></math> to transform <math alttext="\tilde{z}_{p}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p4.22.m6" intent=":literal"><semantics><msub><mover accent="true"><mi>z</mi><mo>~</mo></mover><mi>p</mi></msub><annotation encoding="application/x-tex">\tilde{z}_{p}</annotation></semantics></math> and <math alttext="\tilde{z}_{e}\in R^{m\times 16}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p4.23.m7" intent=":literal"><semantics><mrow><msub><mover accent="true"><mi>z</mi><mo>~</mo></mover><mi>e</mi></msub><mo>∈</mo><msup><mi>R</mi><mrow><mi>m</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mn>16</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\tilde{z}_{e}\in R^{m\times 16}</annotation></semantics></math> into <math alttext="\hat{z}_{p}\in R^{m\times 6}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p4.24.m8" intent=":literal"><semantics><mrow><msub><mover accent="true"><mi>z</mi><mo>^</mo></mover><mi>p</mi></msub><mo>∈</mo><msup><mi>R</mi><mrow><mi>m</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mn>6</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\hat{z}_{p}\in R^{m\times 6}</annotation></semantics></math> and <math alttext="\hat{z}_{e}\in R^{m\times 50}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p4.25.m9" intent=":literal"><semantics><mrow><msub><mover accent="true"><mi>z</mi><mo>^</mo></mover><mi>e</mi></msub><mo>∈</mo><msup><mi>R</mi><mrow><mi>m</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mn>50</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\hat{z}_{e}\in R^{m\times 50}</annotation></semantics></math>, respectively, similar to the expression and pose dimensions.
Thus, the latent vectors <math alttext="\tilde{z}_{e}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p4.26.m10" intent=":literal"><semantics><msub><mover accent="true"><mi>z</mi><mo>~</mo></mover><mi>e</mi></msub><annotation encoding="application/x-tex">\tilde{z}_{e}</annotation></semantics></math> and <math alttext="\tilde{z}_{p}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p4.27.m11" intent=":literal"><semantics><msub><mover accent="true"><mi>z</mi><mo>~</mo></mover><mi>p</mi></msub><annotation encoding="application/x-tex">\tilde{z}_{p}</annotation></semantics></math> can be used as input for the IFED module. Then, the latent vectors <math alttext="\hat{z}_{e}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p4.28.m12" intent=":literal"><semantics><msub><mover accent="true"><mi>z</mi><mo>^</mo></mover><mi>e</mi></msub><annotation encoding="application/x-tex">\hat{z}_{e}</annotation></semantics></math> and <math alttext="\hat{z}_{p}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p4.29.m13" intent=":literal"><semantics><msub><mover accent="true"><mi>z</mi><mo>^</mo></mover><mi>p</mi></msub><annotation encoding="application/x-tex">\hat{z}_{p}</annotation></semantics></math> are concatenated together, and along with the text vector <math alttext="x^{t}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p4.30.m14" intent=":literal"><semantics><msup><mi>x</mi><mi>t</mi></msup><annotation encoding="application/x-tex">x^{t}</annotation></semantics></math>, they are passed through the IFED module to obtain the conditional feature vectors <math alttext="\hat{x}_{emb}^{e}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p4.31.m15" intent=":literal"><semantics><msubsup><mover accent="true"><mi>x</mi><mo>^</mo></mover><mrow><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mi>b</mi></mrow><mi>e</mi></msubsup><annotation encoding="application/x-tex">\hat{x}_{emb}^{e}</annotation></semantics></math> and <math alttext="\hat{x}_{emb}^{p}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p4.32.m16" intent=":literal"><semantics><msubsup><mover accent="true"><mi>x</mi><mo>^</mo></mover><mrow><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mi>b</mi></mrow><mi>p</mi></msubsup><annotation encoding="application/x-tex">\hat{x}_{emb}^{p}</annotation></semantics></math> as</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E8">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E8X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\hat{x}_{emb}^{e},\hat{x}_{emb}^{p}=IFED(\hat{z}_{p}\otimes\hat{z}_{e},x^{t})" class="ltx_Math" display="inline" id="S3.E8X.2.1.1.m1" intent=":literal"><semantics><mrow><mrow><msubsup><mover accent="true"><mi>x</mi><mo>^</mo></mover><mrow><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mi>b</mi></mrow><mi>e</mi></msubsup><mo>,</mo><msubsup><mover accent="true"><mi>x</mi><mo>^</mo></mover><mrow><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mi>b</mi></mrow><mi>p</mi></msubsup></mrow><mo>=</mo><mrow><mi>I</mi><mo lspace="0em" rspace="0em">​</mo><mi>F</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi><mo lspace="0em" rspace="0em">​</mo><mi>D</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mover accent="true"><mi>z</mi><mo>^</mo></mover><mi>p</mi></msub><mo lspace="0.222em" rspace="0.222em">⊗</mo><msub><mover accent="true"><mi>z</mi><mo>^</mo></mover><mi>e</mi></msub></mrow><mo>,</mo><msup><mi>x</mi><mi>t</mi></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\hat{x}_{emb}^{e},\hat{x}_{emb}^{p}=IFED(\hat{z}_{p}\otimes\hat{z}_{e},x^{t})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(8)</span></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p5">
<p class="ltx_p" id="S3.SS2.SSS1.p5.1">This approach enhances the semantic representation of latent vectors by refining the conditional feature vectors for decoders based on the learned latent vectors throughout the training process.
The decoders are defined as follows</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{D}^{t}_{p}(\tilde{z}_{p}\otimes\hat{x}^{p}_{emb})\longrightarrow(\hat{\theta}_{0},\hat{\theta}_{1})" class="ltx_Math" display="block" id="S3.E9.m1" intent=":literal"><semantics><mrow><mrow><msubsup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>p</mi><mi>t</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mover accent="true"><mi>z</mi><mo>~</mo></mover><mi>p</mi></msub><mo lspace="0.222em" rspace="0.222em">⊗</mo><msubsup><mover accent="true"><mi>x</mi><mo>^</mo></mover><mrow><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mi>b</mi></mrow><mi>p</mi></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">⟶</mo><mrow><mo stretchy="false">(</mo><msub><mover accent="true"><mi>θ</mi><mo>^</mo></mover><mn>0</mn></msub><mo>,</mo><msub><mover accent="true"><mi>θ</mi><mo>^</mo></mover><mn>1</mn></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{D}^{t}_{p}(\tilde{z}_{p}\otimes\hat{x}^{p}_{emb})\longrightarrow(\hat{\theta}_{0},\hat{\theta}_{1})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS1.p5.2">and</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E10">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{D}^{t}_{e}(\tilde{z}_{e}\otimes\hat{x}^{e}_{emb})\longrightarrow(\hat{e}_{0},\hat{e}_{1})" class="ltx_Math" display="block" id="S3.E10.m1" intent=":literal"><semantics><mrow><mrow><msubsup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>e</mi><mi>t</mi></msubsup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mover accent="true"><mi>z</mi><mo>~</mo></mover><mi>e</mi></msub><mo lspace="0.222em" rspace="0.222em">⊗</mo><msubsup><mover accent="true"><mi>x</mi><mo>^</mo></mover><mrow><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>m</mi><mo lspace="0em" rspace="0em">​</mo><mi>b</mi></mrow><mi>e</mi></msubsup></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">⟶</mo><mrow><mo stretchy="false">(</mo><msub><mover accent="true"><mi>e</mi><mo>^</mo></mover><mn>0</mn></msub><mo>,</mo><msub><mover accent="true"><mi>e</mi><mo>^</mo></mover><mn>1</mn></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{D}^{t}_{e}(\tilde{z}_{e}\otimes\hat{x}^{e}_{emb})\longrightarrow(\hat{e}_{0},\hat{e}_{1})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(10)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p6">
<p class="ltx_p" id="S3.SS2.SSS1.p6.4">The parameters of the I2FET model are optimized by minimizing the reconstruction loss function <math alttext="\mathcal{L}_{MSE}(.,.)" class="ltx_math_unparsed" display="inline" id="S3.SS2.SSS1.p6.1.m1" intent=":literal"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mrow><mi>M</mi><mo lspace="0em" rspace="0em">​</mo><mi>S</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></msub><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0.167em">.</mo><mo>,</mo><mo lspace="0em" rspace="0.167em">.</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}_{MSE}(.,.)</annotation></semantics></math> between the ground truth coefficients <math alttext="e" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p6.2.m2" intent=":literal"><semantics><mi>e</mi><annotation encoding="application/x-tex">e</annotation></semantics></math>, <math alttext="\theta" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p6.3.m3" intent=":literal"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>, and the predicted coefficients <math alttext="\hat{e},\hat{\theta}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p6.4.m4" intent=":literal"><semantics><mrow><mover accent="true"><mi>e</mi><mo>^</mo></mover><mo>,</mo><mover accent="true"><mi>θ</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{e},\hat{\theta}</annotation></semantics></math>. In addition, the Kullback-Leibler divergence loss function is utilized to minimize the distance between the probability distribution of the ground truth and that of the predicted values.</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E11">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E11X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{L}_{e}=\mathcal{L}_{MSE}(e,\hat{e})" class="ltx_Math" display="inline" id="S3.E11X.2.1.1.m1" intent=":literal"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>e</mi></msub><mo>=</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mrow><mi>M</mi><mo lspace="0em" rspace="0em">​</mo><mi>S</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>e</mi><mo>,</mo><mover accent="true"><mi>e</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\mathcal{L}_{e}=\mathcal{L}_{MSE}(e,\hat{e})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="2"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(11)</span></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E11Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle+5\left[-\sum_{i}(log\&gt;\sigma_{i}^{2}+1)+\sum_{i}\sigma^{2}_{i}+\sum_{i}\mu_{i}^{2}\right]" class="ltx_Math" display="inline" id="S3.E11Xa.2.1.1.m1" intent=":literal"><semantics><mrow><mo>+</mo><mrow><mn>0.5</mn><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><mrow><mo>−</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mi>i</mi></munder></mstyle><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>g</mi><mo lspace="0.220em" rspace="0em">​</mo><msubsup><mi>σ</mi><mi>i</mi><mn>2</mn></msubsup></mrow><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mi>i</mi></munder></mstyle><msubsup><mi>σ</mi><mi>i</mi><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mi>i</mi></munder></mstyle><msubsup><mi>μ</mi><mi>i</mi><mn>2</mn></msubsup></mrow></mrow><mo>]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle+5\left[-\sum_{i}(log\&gt;\sigma_{i}^{2}+1)+\sum_{i}\sigma^{2}_{i}+\sum_{i}\mu_{i}^{2}\right]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E12">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E12X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{L}_{p}=\mathcal{L}_{MSE}(\theta,\hat{\theta})" class="ltx_Math" display="inline" id="S3.E12X.2.1.1.m1" intent=":literal"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>p</mi></msub><mo>=</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mrow><mi>M</mi><mo lspace="0em" rspace="0em">​</mo><mi>S</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo>,</mo><mover accent="true"><mi>θ</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\mathcal{L}_{p}=\mathcal{L}_{MSE}(\theta,\hat{\theta})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="2"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(12)</span></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E12Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle+5\left[-\sum_{i}(log\&gt;\sigma_{i}^{2}+1)+\sum_{i}\sigma^{2}_{i}+\sum_{i}\mu_{i}^{2}\right]" class="ltx_Math" display="inline" id="S3.E12Xa.2.1.1.m1" intent=":literal"><semantics><mrow><mo>+</mo><mrow><mn>0.5</mn><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><mrow><mo>−</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mi>i</mi></munder></mstyle><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>l</mi><mo lspace="0em" rspace="0em">​</mo><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>g</mi><mo lspace="0.220em" rspace="0em">​</mo><msubsup><mi>σ</mi><mi>i</mi><mn>2</mn></msubsup></mrow><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mi>i</mi></munder></mstyle><msubsup><mi>σ</mi><mi>i</mi><mn>2</mn></msubsup></mrow><mo>+</mo><mrow><mstyle displaystyle="true"><munder><mo movablelimits="false">∑</mo><mi>i</mi></munder></mstyle><msubsup><mi>μ</mi><mi>i</mi><mn>2</mn></msubsup></mrow></mrow><mo>]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle+5\left[-\sum_{i}(log\&gt;\sigma_{i}^{2}+1)+\sum_{i}\sigma^{2}_{i}+\sum_{i}\mu_{i}^{2}\right]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS1.p6.6">where <math alttext="\mathcal{L}_{e}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p6.5.m1" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>e</mi></msub><annotation encoding="application/x-tex">\mathcal{L}_{e}</annotation></semantics></math> and <math alttext="\mathcal{L}_{p}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p6.6.m2" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>p</mi></msub><annotation encoding="application/x-tex">\mathcal{L}_{p}</annotation></semantics></math> denote the expression and pose loss function, respectively.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p7">
<p class="ltx_p" id="S3.SS2.SSS1.p7.8">Furthermore, we utilize the pretrained FLAME <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib47" title="Learning a model of facial shape and expression from 4d scans">22</a>]</cite>, a parametric model of a 3D head that is integrated into DECA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib40" title="Learning an animatable detailed 3D face model from in-the-wild images">9</a>]</cite>, which has <math alttext="N=5023" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p7.1.m1" intent=":literal"><semantics><mrow><mi>N</mi><mo>=</mo><mn>5023</mn></mrow><annotation encoding="application/x-tex">N=5023</annotation></semantics></math> base vertices <math alttext="V_{b}\in\mathcal{R}^{N\times 3}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p7.2.m2" intent=":literal"><semantics><mrow><msub><mi>V</mi><mi>b</mi></msub><mo>∈</mo><msup><mi class="ltx_font_mathcaligraphic">ℛ</mi><mrow><mi>N</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mn>3</mn></mrow></msup></mrow><annotation encoding="application/x-tex">V_{b}\in\mathcal{R}^{N\times 3}</annotation></semantics></math> and two sets of <math alttext="M" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p7.3.m3" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math> and <math alttext="K" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p7.4.m4" intent=":literal"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math> basis vectors that encode shape blendshapes <math alttext="\mathcal{S}\in R^{N\times 3\times M}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p7.5.m5" intent=":literal"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒮</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>N</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mn>3</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mi>M</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathcal{S}\in R^{N\times 3\times M}</annotation></semantics></math>, and expression blendshapes <math alttext="B\in R^{N\times 3\times K}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p7.6.m6" intent=":literal"><semantics><mrow><mi>B</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>N</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mn>3</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mi>K</mi></mrow></msup></mrow><annotation encoding="application/x-tex">B\in R^{N\times 3\times K}</annotation></semantics></math>. The basic vectors are first blended and then Linear Blend Skinning (LBS) <math alttext="\mathcal{W}(.)" class="ltx_math_unparsed" display="inline" id="S3.SS2.SSS1.p7.7.m7" intent=":literal"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒲</mi><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0.167em">.</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{W}(.)</annotation></semantics></math> is applied to rotate the vertices following the pose <math alttext="\theta" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p7.8.m8" intent=":literal"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math>.
The final reconstruction in world coordinates can be computed as follows</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E13">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{V}(\phi,e,\theta)=\mathcal{W}(V_{b}+\mathcal{S}\phi+\mathcal{B}e,\theta)" class="ltx_Math" display="block" id="S3.E13.m1" intent=":literal"><semantics><mrow><mrow><mi class="ltx_font_mathcaligraphic">𝒱</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>ϕ</mi><mo>,</mo><mi>e</mi><mo>,</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒲</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>V</mi><mi>b</mi></msub><mo>+</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒮</mi><mo lspace="0em" rspace="0em">​</mo><mi>ϕ</mi></mrow><mo>+</mo><mrow><mi class="ltx_font_mathcaligraphic">ℬ</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi></mrow></mrow><mo>,</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathcal{V}(\phi,e,\theta)=\mathcal{W}(V_{b}+\mathcal{S}\phi+\mathcal{B}e,\theta)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(13)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS1.p7.11">where <math alttext="\phi" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p7.9.m1" intent=":literal"><semantics><mi>ϕ</mi><annotation encoding="application/x-tex">\phi</annotation></semantics></math>, <math alttext="e" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p7.10.m2" intent=":literal"><semantics><mi>e</mi><annotation encoding="application/x-tex">e</annotation></semantics></math>, and <math alttext="\theta" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p7.11.m3" intent=":literal"><semantics><mi>θ</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math> denote the identity shape, facial expression, and head pose parameters, respectively.
In this case, FLAME is used to reconstruct vertices to improve the performance of I2FET.</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E14">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{v}=\mathcal{L}_{MSE}(v,\hat{v})" class="ltx_Math" display="block" id="S3.E14.m1" intent=":literal"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>v</mi></msub><mo>=</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mrow><mi>M</mi><mo lspace="0em" rspace="0em">​</mo><mi>S</mi><mo lspace="0em" rspace="0em">​</mo><mi>E</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>v</mi><mo>,</mo><mover accent="true"><mi>v</mi><mo>^</mo></mover><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}_{v}=\mathcal{L}_{MSE}(v,\hat{v})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(14)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS1.p7.15">where the vertex coordinates <math alttext="v" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p7.12.m1" intent=":literal"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math> and <math alttext="\hat{v}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p7.13.m2" intent=":literal"><semantics><mover accent="true"><mi>v</mi><mo>^</mo></mover><annotation encoding="application/x-tex">\hat{v}</annotation></semantics></math> are computed by feeding the ground truth <math alttext="\{\theta,e,\phi\}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p7.14.m3" intent=":literal"><semantics><mrow><mo stretchy="false">{</mo><mi>θ</mi><mo>,</mo><mi>e</mi><mo>,</mo><mi>ϕ</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\theta,e,\phi\}</annotation></semantics></math> and the reconstructed <math alttext="\{\hat{\theta},\hat{e},\phi\}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p7.15.m4" intent=":literal"><semantics><mrow><mo stretchy="false">{</mo><mover accent="true"><mi>θ</mi><mo>^</mo></mover><mo>,</mo><mover accent="true"><mi>e</mi><mo>^</mo></mover><mo>,</mo><mi>ϕ</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\hat{\theta},\hat{e},\phi\}</annotation></semantics></math> through FLAME.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p8">
<p class="ltx_p" id="S3.SS2.SSS1.p8.15">Finally, the total loss function is given as</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E15">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{total}=\mathcal{L}_{e}+\mathcal{L}_{p}+\mathcal{L}_{v}" class="ltx_Math" display="block" id="S3.E15.m1" intent=":literal"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>a</mi><mo lspace="0em" rspace="0em">​</mo><mi>l</mi></mrow></msub><mo>=</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>e</mi></msub><mo>+</mo><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>p</mi></msub><mo>+</mo><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>v</mi></msub></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}_{total}=\mathcal{L}_{e}+\mathcal{L}_{p}+\mathcal{L}_{v}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(15)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS1.p8.12">For the second branch, the pre-trained DECA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib40" title="Learning an animatable detailed 3D face model from in-the-wild images">9</a>]</cite> is used to encode the facial coefficients of the source face <math alttext="I_{s}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p8.1.m1" intent=":literal"><semantics><msub><mi>I</mi><mi>s</mi></msub><annotation encoding="application/x-tex">I_{s}</annotation></semantics></math> including shape <math alttext="\phi_{s}\in R^{1\times 100}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p8.2.m2" intent=":literal"><semantics><mrow><msub><mi>ϕ</mi><mi>s</mi></msub><mo>∈</mo><msup><mi>R</mi><mrow><mn>1</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>100</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\phi_{s}\in R^{1\times 100}</annotation></semantics></math>, expression <math alttext="e_{s}\in R^{1\times 50}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p8.3.m3" intent=":literal"><semantics><mrow><msub><mi>e</mi><mi>s</mi></msub><mo>∈</mo><msup><mi>R</mi><mrow><mn>1</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>50</mn></mrow></msup></mrow><annotation encoding="application/x-tex">e_{s}\in R^{1\times 50}</annotation></semantics></math>, pose <math alttext="\theta_{s}\in R^{1\times 6}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p8.4.m4" intent=":literal"><semantics><mrow><msub><mi>θ</mi><mi>s</mi></msub><mo>∈</mo><msup><mi>R</mi><mrow><mn>1</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>6</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\theta_{s}\in R^{1\times 6}</annotation></semantics></math> and camera <math alttext="c_{s}\in R^{1\times 3}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p8.5.m5" intent=":literal"><semantics><mrow><msub><mi>c</mi><mi>s</mi></msub><mo>∈</mo><msup><mi>R</mi><mrow><mn>1</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>3</mn></mrow></msup></mrow><annotation encoding="application/x-tex">c_{s}\in R^{1\times 3}</annotation></semantics></math>.
Following this step, <math alttext="\{\phi_{s},e_{s},\theta_{s},c_{s}\}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p8.6.m6" intent=":literal"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>ϕ</mi><mi>s</mi></msub><mo>,</mo><msub><mi>e</mi><mi>s</mi></msub><mo>,</mo><msub><mi>θ</mi><mi>s</mi></msub><mo>,</mo><msub><mi>c</mi><mi>s</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{\phi_{s},e_{s},\theta_{s},c_{s}\}</annotation></semantics></math> is integrated with the sequence of facial expression coefficients <math alttext="\{(e_{0},\theta_{0}),(e_{1},\theta_{1})\}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p8.7.m7" intent=":literal"><semantics><mrow><mo stretchy="false">{</mo><mrow><mo stretchy="false">(</mo><msub><mi>e</mi><mn>0</mn></msub><mo>,</mo><msub><mi>θ</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow><mo>,</mo><mrow><mo stretchy="false">(</mo><msub><mi>e</mi><mn>1</mn></msub><mo>,</mo><msub><mi>θ</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{(e_{0},\theta_{0}),(e_{1},\theta_{1})\}</annotation></semantics></math>, generated by the first branch to generate the anchor facial expressions <math alttext="\{s^{(1)}_{s},s^{(k)}_{e_{0}},s^{(T)}_{e_{1}}\}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p8.8.m8" intent=":literal"><semantics><mrow><mo stretchy="false">{</mo><msubsup><mi>s</mi><mi>s</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>s</mi><msub><mi>e</mi><mn>0</mn></msub><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><msubsup><mi>s</mi><msub><mi>e</mi><mn>1</mn></msub><mrow><mo stretchy="false">(</mo><mi>T</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{s^{(1)}_{s},s^{(k)}_{e_{0}},s^{(T)}_{e_{1}}\}</annotation></semantics></math>.
Then, a linear interpolation function <math alttext="\Psi(.)" class="ltx_math_unparsed" display="inline" id="S3.SS2.SSS1.p8.9.m9" intent=":literal"><semantics><mrow><mi mathvariant="normal">Ψ</mi><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0.167em">.</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Psi(.)</annotation></semantics></math> is applied to smoothen the transition between the two anchored facial expressions. The <math alttext="\Psi(s^{(l)},s^{(n)})" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p8.10.m10" intent=":literal"><semantics><mrow><mi mathvariant="normal">Ψ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>s</mi><mrow><mo stretchy="false">(</mo><mi>l</mi><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msup><mi>s</mi><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\Psi(s^{(l)},s^{(n)})</annotation></semantics></math> function for interpolation between facial expression sequences <math alttext="s^{(l)}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p8.11.m11" intent=":literal"><semantics><msup><mi>s</mi><mrow><mo stretchy="false">(</mo><mi>l</mi><mo stretchy="false">)</mo></mrow></msup><annotation encoding="application/x-tex">s^{(l)}</annotation></semantics></math> and <math alttext="s^{(n)}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p8.12.m12" intent=":literal"><semantics><msup><mi>s</mi><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msup><annotation encoding="application/x-tex">s^{(n)}</annotation></semantics></math> can be defined as</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E16">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E16X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle e^{(k)}=\delta*e^{(l)}+(1-\delta)*e^{(n)}" class="ltx_Math" display="inline" id="S3.E16X.2.1.1.m1" intent=":literal"><semantics><mrow><msup><mi>e</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mrow><mrow><mi>δ</mi><mo lspace="0.222em" rspace="0.222em">∗</mo><msup><mi>e</mi><mrow><mo stretchy="false">(</mo><mi>l</mi><mo stretchy="false">)</mo></mrow></msup></mrow><mo>+</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mi>δ</mi></mrow><mo rspace="0.055em" stretchy="false">)</mo></mrow><mo rspace="0.222em">∗</mo><msup><mi>e</mi><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msup></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle e^{(k)}=\delta*e^{(l)}+(1-\delta)*e^{(n)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="2"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(16)</span></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E16Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\theta^{(k)}=\delta*\theta^{(l)}+(1-\delta)*\theta^{(n)}" class="ltx_Math" display="inline" id="S3.E16Xa.2.1.1.m1" intent=":literal"><semantics><mrow><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mrow><mrow><mi>δ</mi><mo lspace="0.222em" rspace="0.222em">∗</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>l</mi><mo stretchy="false">)</mo></mrow></msup></mrow><mo>+</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>−</mo><mi>δ</mi></mrow><mo rspace="0.055em" stretchy="false">)</mo></mrow><mo rspace="0.222em">∗</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msup></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\theta^{(k)}=\delta*\theta^{(l)}+(1-\delta)*\theta^{(n)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS1.p8.14">where <math alttext="0\leq\delta\leq 1" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p8.13.m1" intent=":literal"><semantics><mrow><mn>0</mn><mo>≤</mo><mi>δ</mi><mo>≤</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">0\leq\delta\leq 1</annotation></semantics></math> is a linear coefficient and <math alttext="\{e^{(l)},\theta^{(l)}\}\in s^{(l)}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p8.14.m2" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">{</mo><msup><mi>e</mi><mrow><mo stretchy="false">(</mo><mi>l</mi><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msup><mi>θ</mi><mrow><mo stretchy="false">(</mo><mi>l</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">}</mo></mrow><mo>∈</mo><msup><mi>s</mi><mrow><mo stretchy="false">(</mo><mi>l</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">\{e^{(l)},\theta^{(l)}\}\in s^{(l)}</annotation></semantics></math>. This allows our framework to ensure temporal consistency when the motion changes are sampled uniformly.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S3.SS2.SSS2.4.1.1">III-B</span>2 </span>Face Rendering</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">This module renders the facial appearance for the facial expression trajectories, generated by FET. In this work, the face rendering approach is based on the facial parameters of FLAME for integration with FET.

<br class="ltx_break"/> <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS2.p1.1.1">Head Mesh Reconstruction:</span></p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p2">
<p class="ltx_p" id="S3.SS2.SSS2.p2.2">It generates the vertex coordinates corresponding to the facial expressions using Eq.<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S3.E13" title="In III-B1 Facial Expression Transition ‣ III-B The Proposed Method ‣ III Method ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">13</span></a> by using the sequence of facial expression transitions as the input. Then, the target vertex coordinates are obtained by combining these expressions with the deformation of hair and shoulder regions <math alttext="f_{H}(.)" class="ltx_math_unparsed" display="inline" id="S3.SS2.SSS2.p2.1.m1" intent=":literal"><semantics><mrow><msub><mi>f</mi><mi>H</mi></msub><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0.167em">.</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{H}(.)</annotation></semantics></math> from <math alttext="I_{s}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p2.2.m2" intent=":literal"><semantics><msub><mi>I</mi><mi>s</mi></msub><annotation encoding="application/x-tex">I_{s}</annotation></semantics></math> as follows</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E17">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{M}^{t}=\mathcal{V}(\phi,e,\theta)+f_{H}(I_{s})" class="ltx_Math" display="block" id="S3.E17.m1" intent=":literal"><semantics><mrow><msup><mi class="ltx_font_mathcaligraphic">ℳ</mi><mi>t</mi></msup><mo>=</mo><mrow><mrow><mi class="ltx_font_mathcaligraphic">𝒱</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>ϕ</mi><mo>,</mo><mi>e</mi><mo>,</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><msub><mi>f</mi><mi>H</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>I</mi><mi>s</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathcal{M}^{t}=\mathcal{V}(\phi,e,\theta)+f_{H}(I_{s})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(17)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS2.p2.3">where <math alttext="f_{H}(.)" class="ltx_math_unparsed" display="inline" id="S3.SS2.SSS2.p2.3.m1" intent=":literal"><semantics><mrow><msub><mi>f</mi><mi>H</mi></msub><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0.167em">.</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{H}(.)</annotation></semantics></math> is the pre-trained linear deformation model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib37" title="Realistic one-shot mesh-based head avatars">18</a>]</cite> that is used to refine the vertices locations.

<br class="ltx_break"/> <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS2.p2.3.1">Rendering</span>:</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p3">
<p class="ltx_p" id="S3.SS2.SSS2.p3.4">Following the head mesh reconstruction step, facial appearances can be rendered from the reconstructed head vertices <math alttext="\mathcal{M}=\{m_{s}^{(1)},m^{(i)},m_{e_{0}}^{(k)},m^{(j)},m_{e_{1}}^{(T)}\}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p3.1.m1" intent=":literal"><semantics><mrow><mi class="ltx_font_mathcaligraphic">ℳ</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><msubsup><mi>m</mi><mi>s</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><msup><mi>m</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msubsup><mi>m</mi><msub><mi>e</mi><mn>0</mn></msub><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><msup><mi>m</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msubsup><mi>m</mi><msub><mi>e</mi><mn>1</mn></msub><mrow><mo stretchy="false">(</mo><mi>T</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{M}=\{m_{s}^{(1)},m^{(i)},m_{e_{0}}^{(k)},m^{(j)},m_{e_{1}}^{(T)}\}</annotation></semantics></math> and the extracted neutral texture of the source image <math alttext="I_{s}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p3.2.m2" intent=":literal"><semantics><msub><mi>I</mi><mi>s</mi></msub><annotation encoding="application/x-tex">I_{s}</annotation></semantics></math> by the texture decoder <math alttext="\mathcal{E}_{tex}(I_{s})" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p3.3.m3" intent=":literal"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">ℰ</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>e</mi><mo lspace="0em" rspace="0em">​</mo><mi>x</mi></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>I</mi><mi>s</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{E}_{tex}(I_{s})</annotation></semantics></math>.
The present work explores a face-rendering model suitable for facial expression transition. Therefore, we utilize pre-trained facial rendering models, known as state-of-the-art models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib37" title="Realistic one-shot mesh-based head avatars">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib50" title="CVTHead: one-shot controllable head avatar with vertex-feature transformer">26</a>]</cite>, capable of rendering facial appearances based on the facial parameters calculated with FLAME.
Subsequently, the sequence of facial appearances <math alttext="\mathcal{Y}=\{y_{s}^{(1)},y^{(i)},y_{e_{0}}^{(k)},y^{(j)},y_{e_{1}}^{(T)}\}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p3.4.m4" intent=":literal"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒴</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><msubsup><mi>y</mi><mi>s</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msubsup><mi>y</mi><msub><mi>e</mi><mn>0</mn></msub><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msubsup><mo>,</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>j</mi><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msubsup><mi>y</mi><msub><mi>e</mi><mn>1</mn></msub><mrow><mo stretchy="false">(</mo><mi>T</mi><mo stretchy="false">)</mo></mrow></msubsup><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{Y}=\{y_{s}^{(1)},y^{(i)},y_{e_{0}}^{(k)},y^{(j)},y_{e_{1}}^{(T)}\}</annotation></semantics></math> is generated as the output of the proposed framework.</p>
</div>
<figure class="ltx_figure" id="S3.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F5.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="139" id="S3.F5.sf1.g1" src="x5.png" width="190"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S3.F5.sf1.3.2" style="font-size:90%;"> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F5.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="142" id="S3.F5.sf2.g1" src="x6.png" width="177"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S3.F5.sf2.3.2" style="font-size:90%;"> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F5.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="136" id="S3.F5.sf3.g1" src="x7.png" width="136"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.sf3.2.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="S3.F5.sf3.3.2" style="font-size:90%;"> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S3.F5.3.2" style="font-size:90%;">Statistics of the facial expressions utilized in this study for the CK+ (a) and CelebV-HQ (b) datasets, respectively, and a collection of sampled words extracted from the text instructions (c).</span></figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Experiments</span>
</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.4.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.5.2">Dataset</span>
</h3>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.20.5.1" style="font-size:90%;">TABLE I</span>: </span><span class="ltx_text" id="S4.T1.8.4" style="font-size:90%;">
<math alttext="N_{1},N_{2}\in\{" class="ltx_math_unparsed" display="inline" id="S4.T1.5.1.m1" intent=":literal"><semantics><mrow><msub><mi>N</mi><mn>1</mn></msub><mo>,</mo><msub><mi>N</mi><mn>2</mn></msub><mo>∈</mo><mo stretchy="false">{</mo></mrow><annotation encoding="application/x-tex">N_{1},N_{2}\in\{</annotation></semantics></math> HAPPINESS, DISGUST, ANGER, FEAR, SURPRISE, CONTEMPT, SADNESS<math alttext="\}" class="ltx_Math" display="inline" id="S4.T1.6.2.m2" intent=":literal"><semantics><mo stretchy="false">}</mo><annotation encoding="application/x-tex">\}</annotation></semantics></math> IN THE CK+ DATASET AND <math alttext="N_{1},N_{2}\in\{" class="ltx_math_unparsed" display="inline" id="S4.T1.7.3.m3" intent=":literal"><semantics><mrow><msub><mi>N</mi><mn>1</mn></msub><mo>,</mo><msub><mi>N</mi><mn>2</mn></msub><mo>∈</mo><mo stretchy="false">{</mo></mrow><annotation encoding="application/x-tex">N_{1},N_{2}\in\{</annotation></semantics></math> HAPPINESS, DISGUST, ANGER, FEAR, SURPRISE, CONTEMPT, SADNESS, NEUTRAL<math alttext="\}" class="ltx_Math" display="inline" id="S4.T1.8.4.m4" intent=":literal"><semantics><mo stretchy="false">}</mo><annotation encoding="application/x-tex">\}</annotation></semantics></math> IN THE CELEBV-HQ DATASET.
</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.18">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.18.11.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.18.11.1.1">No</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T1.18.11.1.2">Instruction/ Prompts</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.10.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.10.2.3">1</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.10.2.2">Turn this face from [<math alttext="N_{1}" class="ltx_Math" display="inline" id="S4.T1.9.1.1.m1" intent=":literal"><semantics><msub><mi>N</mi><mn>1</mn></msub><annotation encoding="application/x-tex">N_{1}</annotation></semantics></math>] to [<math alttext="N_{2}" class="ltx_Math" display="inline" id="S4.T1.10.2.2.m2" intent=":literal"><semantics><msub><mi>N</mi><mn>2</mn></msub><annotation encoding="application/x-tex">N_{2}</annotation></semantics></math>].</td>
</tr>
<tr class="ltx_tr" id="S4.T1.12.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.12.4.3">2</th>
<td class="ltx_td ltx_align_left" id="S4.T1.12.4.2">Change this face from [<math alttext="N_{1}" class="ltx_Math" display="inline" id="S4.T1.11.3.1.m1" intent=":literal"><semantics><msub><mi>N</mi><mn>1</mn></msub><annotation encoding="application/x-tex">N_{1}</annotation></semantics></math>] to [<math alttext="N_{2}" class="ltx_Math" display="inline" id="S4.T1.12.4.2.m2" intent=":literal"><semantics><msub><mi>N</mi><mn>2</mn></msub><annotation encoding="application/x-tex">N_{2}</annotation></semantics></math>].</td>
</tr>
<tr class="ltx_tr" id="S4.T1.14.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.14.6.3">3</th>
<td class="ltx_td ltx_align_left" id="S4.T1.14.6.2">Transform this face from [<math alttext="N_{1}" class="ltx_Math" display="inline" id="S4.T1.13.5.1.m1" intent=":literal"><semantics><msub><mi>N</mi><mn>1</mn></msub><annotation encoding="application/x-tex">N_{1}</annotation></semantics></math>] to [<math alttext="N_{2}" class="ltx_Math" display="inline" id="S4.T1.14.6.2.m2" intent=":literal"><semantics><msub><mi>N</mi><mn>2</mn></msub><annotation encoding="application/x-tex">N_{2}</annotation></semantics></math>].</td>
</tr>
<tr class="ltx_tr" id="S4.T1.15.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.15.7.2">4</th>
<td class="ltx_td ltx_align_left" id="S4.T1.15.7.1">Modify this face, changing it from [<math alttext="N_{1}" class="ltx_Math" display="inline" id="S4.T1.15.7.1.m1" intent=":literal"><semantics><msub><mi>N</mi><mn>1</mn></msub><annotation encoding="application/x-tex">N_{1}</annotation></semantics></math>]</td>
</tr>
<tr class="ltx_tr" id="S4.T1.16.8">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S4.T1.16.8.2"></th>
<td class="ltx_td ltx_align_left" id="S4.T1.16.8.1">to [<math alttext="N_{2}" class="ltx_Math" display="inline" id="S4.T1.16.8.1.m1" intent=":literal"><semantics><msub><mi>N</mi><mn>2</mn></msub><annotation encoding="application/x-tex">N_{2}</annotation></semantics></math>].</td>
</tr>
<tr class="ltx_tr" id="S4.T1.18.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S4.T1.18.10.3">5</th>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T1.18.10.2">Replace this face from [<math alttext="N_{1}" class="ltx_Math" display="inline" id="S4.T1.17.9.1.m1" intent=":literal"><semantics><msub><mi>N</mi><mn>1</mn></msub><annotation encoding="application/x-tex">N_{1}</annotation></semantics></math> to [<math alttext="N_{2}" class="ltx_Math" display="inline" id="S4.T1.18.10.2.m2" intent=":literal"><semantics><msub><mi>N</mi><mn>2</mn></msub><annotation encoding="application/x-tex">N_{2}</annotation></semantics></math>].</td>
</tr>
</tbody>
</table>
</figure>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS1.4.1.1">IV-A</span>1 </span>CK+ dataset</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">The CK+ dataset was supplemented with text instructions for facial expression generation and transition. The original CK+ dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib3" title="The extended cohn-kanade dataset (ck+): a complete dataset for action unit and emotion-specified expression">25</a>]</cite> includes 117 subjects and 7 emotions, such as <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p1.1.1">happiness, disgust, anger, fear, surprise, contempt, and sadness</span>. The instructions are created to describe the transitions between these facial expressions.
To extract the facial parameters and the shape image corresponding to the facial expression in the image, we utilize the pre-trained DECA model. For each sample, a tuple is created with an instruction, the target shape image, and facial parameters that correspond to the same subject. The CK+ dataset, in this study, contains a total of 26,352 samples, and the composition of this dataset in terms of the seven facial expressions is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S3.F5.sf1" title="In Figure 5 ‣ III-B2 Face Rendering ‣ III-B The Proposed Method ‣ III Method ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">5(a)</span></a>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS1.SSS2.4.1.1">IV-A</span>2 </span>CelebV-HQ dataset</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.2">Similar to the CK+ case, text instructions were added to the CelebV-HQ dataset for our experiments<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib2" title="CelebV-HQ: a large-scale video facial attributes dataset">44</a>]</cite>.
The original CelebV-HQ dataset included 15,653 subjects and 8 expressions of <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS2.p1.2.1">neutral, happiness, disgust, anger, fear, surprise, contempt, and sadness</span>. The images in the CelebV-HQ dataset were spontaneously collected and presented many challenges such as highly non-frontal head poses, blur, and brightness. This problem was overcome by using a head pose algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib4" title="Accurate head pose estimation using image rectification and a lightweight convolutional neural network">23</a>]</cite> to estimate parameters such as the <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS2.p1.2.2">yaw, roll, and pitch</span>, of the head pose ranging from -15 to 15 degrees. Given that the subjects were chosen indiscriminately, we select those with more than two facial expressions. Then, the pre-trained DECA was applied to extract the corresponding shape images and facial parameters. The CelebV-HQ dataset adopted in this study includes 614 subjects and 28,335 samples, and Fig. <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S3.F5.sf2" title="In Figure 5 ‣ III-B2 Face Rendering ‣ III-B The Proposed Method ‣ III Method ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">5(b)</span></a> shows a statistical breakdown of the eight facial expressions. 
<br class="ltx_break"/>  Table <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.T1" title="TABLE I ‣ IV-A Dataset ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">I</span></a> lists the template sentences that are used to generate guided instructions for facial expression transitions. We generated five instructions for each sample and depicted the transition from expression <math alttext="N_{1}" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.1.m1" intent=":literal"><semantics><msub><mi>N</mi><mn>1</mn></msub><annotation encoding="application/x-tex">N_{1}</annotation></semantics></math> to expression <math alttext="N_{2}" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.2.m2" intent=":literal"><semantics><msub><mi>N</mi><mn>2</mn></msub><annotation encoding="application/x-tex">N_{2}</annotation></semantics></math>. Subsequently, one presentation sentence is randomly chosen from these instructions. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S3.F5.sf3" title="In Figure 5 ‣ III-B2 Face Rendering ‣ III-B The Proposed Method ‣ III Method ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">5(c)</span></a> illustrates some sampled words used in the text instructions.</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="116" id="S4.F6.g1" src="figs/ck+pose-samples.jpg" width="175"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.2.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S4.F6.3.2" style="font-size:90%;">The shapes of facial expression are generated using images from the CK+ dataset. The top and bottom rows illustrate surprised and angry expressions with various head poses.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.4.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.5.2">Implementation</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">The I2FET model was trained for 200 epochs with a batch size of 128 and a learning rate of <math alttext="8e^{-4}" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1" intent=":literal"><semantics><mrow><mn>8</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>e</mi><mrow><mo>−</mo><mn>4</mn></mrow></msup></mrow><annotation encoding="application/x-tex">8e^{-4}</annotation></semantics></math>. The Adam optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib49" title="Adam: a method for stochastic optimization">20</a>]</cite> was adopted to optimize the model. For the experiments, 10% of the dataset was allocated to the testing set, whereas the remaining portion was used for training. Then, a validation set was drawn from 10% of the training set. In this work, we repeated each experiment ten times and used the average results to evaluate the overall performance of the proposed I2FET.
Because our task is new and a specific method that would allow a fair comparison is not available, we chose MotionClip <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib31" title="Motionclip: exposing human motion generation to clip space">39</a>]</cite> as a baseline. For the face rendering experiment, the configuration by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib37" title="Realistic one-shot mesh-based head avatars">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib50" title="CVTHead: one-shot controllable head avatar with vertex-feature transformer">26</a>]</cite> was used.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.4.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.5.2">Classification of Facial Expressions</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">In this study, it was important to employ an appropriate classifier to evaluate the method that was used to generate facial expressions as prior studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib13" title="Generating complex 4d expression transitions by learning face landmark trajectories">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib12" title="4D facial expression diffusion model">46</a>]</cite>. Given that the CK+ and CelebV-HQ datasets had skewed distributions, the performance for minor classes could be affected by the facial expression classifiers trained on these imbalanced datasets, in which the results would be biased towards the major class. Moreover, the effectiveness of the classification model could also be affected by head movements, which were commonly encountered as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.F6" title="Figure 6 ‣ IV-A2 CelebV-HQ dataset ‣ IV-A Dataset ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.2">These problems were addressed by conducting an experiment to identify an appropriate classifier that can handle not only imbalanced datasets but also invariant rotation transformations. ResNet-101 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib21" title="Deep Residual Learning for Image Recognition">14</a>]</cite>, MEK <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib35" title="Leave no stone unturned: mine extra knowledge for imbalanced facial expression recognition">43</a>]</cite>, and ResNet-101 integrated with Reweighted Focal Loss (RFL) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib24" title="Class-balanced loss based on effective number of samples">5</a>]</cite> were utilized to evaluate the performance of facial expression classification.
For this purpose, augmented datasets were created by rotating the original CK+ and CelebV-HQ datasets at angles of <math alttext="\pm 10^{o},\pm 15^{o}" class="ltx_Math" display="inline" id="S4.SS3.p2.1.m1" intent=":literal"><semantics><mrow><mrow><mo>±</mo><msup><mn>10</mn><mi>o</mi></msup></mrow><mo>,</mo><mrow><mo>±</mo><msup><mn>15</mn><mi>o</mi></msup></mrow></mrow><annotation encoding="application/x-tex">\pm 10^{o},\pm 15^{o}</annotation></semantics></math>, and <math alttext="\pm 30^{o}" class="ltx_Math" display="inline" id="S4.SS3.p2.2.m2" intent=":literal"><semantics><mrow><mo>±</mo><msup><mn>30</mn><mi>o</mi></msup></mrow><annotation encoding="application/x-tex">\pm 30^{o}</annotation></semantics></math>. Then, the three networks are trained using the original datasets and evaluated on all datasets. The Accuracy was employed to assess whether samples had been correctly classified across all classes, while G-mean was used to evaluate the ability of the model to effectively recognize minor classes that are predicted by the facial expression classification model.
The supplementary data in the ablation study in Section <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS5" title="IV-E Ablation Study ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-E</span></span></a> suggests that MEK can address imbalanced datasets but is sensitive to rotational transformations. ResNet101 demonstrates positive results with rotation transformations but performs worse on datasets that are highly imbalanced. ResNet101 integrated with RFL outperforms ResNet101 and MEK in both scenarios. As a result, ResNet101 with RFL was selected as our classifier for evaluating the ability of our models to generate facial expressions.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS4.4.1.1">IV-D</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS4.5.2">Results</span>
</h3>
<figure class="ltx_figure" id="S4.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S4.F7.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="128" id="S4.F7.sf1.g1" src="x8.png" width="128"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F7.sf1.2.1.1" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S4.F7.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="128" id="S4.F7.sf2.g1" src="x9.png" width="128"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F7.sf2.2.1.1" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel" id="S4.F7.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="103" id="S4.F7.sf3.g1" src="x10.png" width="141"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F7.sf3.2.1.1" style="font-size:90%;">(c)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F7.2.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text" id="S4.F7.3.2" style="font-size:90%;">Visualization of the learned latent space for seven facial expressions by (a) MotionClip, (b) I2FET without IFED, and (c) I2FET with IFED, respectively, using t-SNE.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F8">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="S4.F8.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="165" id="S4.F8.sf1.g1" src="x11.png" width="206"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F8.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S4.F8.sf1.3.2" style="font-size:90%;"> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="S4.F8.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="165" id="S4.F8.sf2.g1" src="x12.png" width="203"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F8.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S4.F8.sf2.3.2" style="font-size:90%;"> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F8.2.1.1" style="font-size:90%;">Figure 8</span>: </span><span class="ltx_text" id="S4.F8.3.2" style="font-size:90%;">Confusion matrices for seven facial expressions by (a) MotionClip and (b) our method on CK+.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F9">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="S4.F9.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="167" id="S4.F9.sf1.g1" src="x13.png" width="209"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F9.sf1.2.1.1" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="S4.F9.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="167" id="S4.F9.sf2.g1" src="x14.png" width="210"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F9.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S4.F9.sf2.3.2" style="font-size:90%;"> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F9.2.1.1" style="font-size:90%;">Figure 9</span>: </span><span class="ltx_text" id="S4.F9.3.2" style="font-size:90%;">Confusion matrices for eight facial expressions by (a) MotionClip and (b) our method on CelebV-HQ.</span></figcaption>
</figure>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.17.1.1" style="font-size:90%;">TABLE II</span>: </span><span class="ltx_text" id="S4.T2.18.2" style="font-size:90%;">COMPARISON OF ACCURACY BETWEEN OUR METHOD AND MOTIONCLIP ON THE CK+ DATASET</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.15">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T2.5.5.6"><span class="ltx_text ltx_font_bold" id="S4.T2.5.5.6.1">Method</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T2.2.2.2">
<span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1">Acc<sub class="ltx_sub" id="S4.T2.1.1.1.1.1"><span class="ltx_text ltx_font_medium" id="S4.T2.1.1.1.1.1.1">1</span></sub></span> (%) <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.2.2.2.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T2.4.4.4">
<span class="ltx_text ltx_font_bold" id="S4.T2.3.3.3.1">Acc<sub class="ltx_sub" id="S4.T2.3.3.3.1.1"><span class="ltx_text ltx_font_medium" id="S4.T2.3.3.3.1.1.1">2</span></sub></span> (%) <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.4.4.4.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T2.5.5.5">
<span class="ltx_text ltx_font_bold" id="S4.T2.5.5.5.1">Gmean</span> (%) <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.5.5.5.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.9.9.5">Ground-truth</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.6.6.1">99.69 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T2.6.6.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0001</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.7.7.2">99.39 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T2.7.7.2.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0002</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.9.9.4">99.69 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T2.8.8.3.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 9.3e<sup class="ltx_sup" id="S4.T2.9.9.4.1"><span class="ltx_text ltx_font_italic" id="S4.T2.9.9.4.1.1">-5</span></sup>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.12.12.4">MotionClip</th>
<td class="ltx_td ltx_align_left" id="S4.T2.10.10.1">52.00 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T2.10.10.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0058</td>
<td class="ltx_td ltx_align_left" id="S4.T2.11.11.2">20.00 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T2.11.11.2.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0082</td>
<td class="ltx_td ltx_align_left" id="S4.T2.12.12.3">40.48 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T2.12.12.3.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0060</td>
</tr>
<tr class="ltx_tr" id="S4.T2.15.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T2.15.15.4"><span class="ltx_text ltx_font_bold" id="S4.T2.15.15.4.1">Ours</span></th>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T2.13.13.1"><span class="ltx_text ltx_font_bold" id="S4.T2.13.13.1.1">91.44 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T2.13.13.1.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0024</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T2.14.14.2"><span class="ltx_text ltx_font_bold" id="S4.T2.14.14.2.1">84.03 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T2.14.14.2.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0034</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T2.15.15.3"><span class="ltx_text ltx_font_bold" id="S4.T2.15.15.3.1">80.30 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T2.15.15.3.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0064</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T3.17.1.1" style="font-size:90%;">TABLE III</span>: </span><span class="ltx_text" id="S4.T3.18.2" style="font-size:90%;">COMPARISON OF ACCURACY BETWEEN OUR METHOD AND MOTIONCLIP ON THE CELEBV-HQ DATASET</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.15">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T3.5.5.6"><span class="ltx_text ltx_font_bold" id="S4.T3.5.5.6.1">Method</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T3.2.2.2">
<span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1">Acc<sub class="ltx_sub" id="S4.T3.1.1.1.1.1"><span class="ltx_text ltx_font_medium" id="S4.T3.1.1.1.1.1.1">1</span></sub></span> (%) <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.2.2.2.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T3.4.4.4">
<span class="ltx_text ltx_font_bold" id="S4.T3.3.3.3.1">Acc<sub class="ltx_sub" id="S4.T3.3.3.3.1.1"><span class="ltx_text ltx_font_medium" id="S4.T3.3.3.3.1.1.1">2</span></sub></span> (%) <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.4.4.4.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T3.5.5.5">
<span class="ltx_text ltx_font_bold" id="S4.T3.5.5.5.1">Gmean</span> (%) <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.5.5.5.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.9.9.5">Ground-truth</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.6.6.1">93.67 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T3.6.6.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0000</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.7.7.2">87.89 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T3.7.7.2.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0000</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.9.9.4">95.04 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T3.8.8.3.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 1.1e<sup class="ltx_sup" id="S4.T3.9.9.4.1"><span class="ltx_text ltx_font_italic" id="S4.T3.9.9.4.1.1">-16</span></sup>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.12.12.4">MotionClip</th>
<td class="ltx_td ltx_align_left" id="S4.T3.10.10.1">40.00 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T3.10.10.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0039</td>
<td class="ltx_td ltx_align_left" id="S4.T3.11.11.2">13.73 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T3.11.11.2.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0056</td>
<td class="ltx_td ltx_align_left" id="S4.T3.12.12.3">34.42 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T3.12.12.3.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0069</td>
</tr>
<tr class="ltx_tr" id="S4.T3.15.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T3.15.15.4"><span class="ltx_text ltx_font_bold" id="S4.T3.15.15.4.1">Ours</span></th>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T3.13.13.1"><span class="ltx_text ltx_font_bold" id="S4.T3.13.13.1.1">58.24 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T3.13.13.1.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0028</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T3.14.14.2"><span class="ltx_text ltx_font_bold" id="S4.T3.14.14.2.1">33.45 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T3.14.14.2.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0052</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T3.15.15.3"><span class="ltx_text ltx_font_bold" id="S4.T3.15.15.3.1">46.47 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T3.15.15.3.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0065</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T4.10.3.1" style="font-size:90%;">TABLE IV</span>: </span><span class="ltx_text" id="S4.T4.4.2" style="font-size:90%;">RESULT OBTAINED BY COMBINING OUR METHOD WITH ROME OR CVTHEAD FOR TEXTURE GENERATION ON THE CK+ AND CELEBV-HQ DATASETS, RESPECTIVELY. NOTE THAT <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T4.3.1.m1" intent=":literal"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math> INDICATES THAT LOWER IS BETTER, WHEREAS <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T4.4.2.m2" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math> DENOTES THAT THE HIGHER IS BETTER.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T4.8">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.8.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T4.8.4.5"><span class="ltx_text ltx_font_bold" id="S4.T4.8.4.5.1">Dataset</span></th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T4.8.4.6"><span class="ltx_text ltx_font_bold" id="S4.T4.8.4.6.1">Method</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T4.5.1.1">L<math alttext="{}_{1}\downarrow" class="ltx_math_unparsed" display="inline" id="S4.T4.5.1.1.m1" intent=":literal"><semantics><mmultiscripts><mo stretchy="false">↓</mo><mprescripts></mprescripts><mn>1</mn><mrow></mrow></mmultiscripts><annotation encoding="application/x-tex">{}_{1}\downarrow</annotation></semantics></math>
</th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T4.6.2.2">PSNR<math alttext="{\uparrow}" class="ltx_Math" display="inline" id="S4.T4.6.2.2.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">{\uparrow}</annotation></semantics></math>
</th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T4.7.3.3">LPIPS<math alttext="{\downarrow}" class="ltx_Math" display="inline" id="S4.T4.7.3.3.m1" intent=":literal"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">{\downarrow}</annotation></semantics></math>
</th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T4.8.4.4">MS_SSIM<math alttext="{\uparrow}" class="ltx_Math" display="inline" id="S4.T4.8.4.4.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">{\uparrow}</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.8.5.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.8.5.1.1">CK+</td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T4.8.5.1.2">Ours+ROME</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.8.5.1.3">0.199</td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T4.8.5.1.4">9.939</td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T4.8.5.1.5">0.490</td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T4.8.5.1.6">0.347</td>
</tr>
<tr class="ltx_tr" id="S4.T4.8.6.2">
<td class="ltx_td" id="S4.T4.8.6.2.1"></td>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T4.8.6.2.2"><span class="ltx_text ltx_font_bold" id="S4.T4.8.6.2.2.1">Ours+CVTHead</span></td>
<td class="ltx_td ltx_align_left" id="S4.T4.8.6.2.3"><span class="ltx_text ltx_font_bold" id="S4.T4.8.6.2.3.1">0.005</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T4.8.6.2.4"><span class="ltx_text ltx_font_bold" id="S4.T4.8.6.2.4.1">33.74</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T4.8.6.2.5"><span class="ltx_text ltx_font_bold" id="S4.T4.8.6.2.5.1">0.021</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T4.8.6.2.6"><span class="ltx_text ltx_font_bold" id="S4.T4.8.6.2.6.1">0.978</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.8.7.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.8.7.3.1">CelebV-HQ</td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T4.8.7.3.2">Ours+ROME</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.8.7.3.3">0.168</td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T4.8.7.3.4">11.316</td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T4.8.7.3.5">0.505</td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T4.8.7.3.6">0.418</td>
</tr>
<tr class="ltx_tr" id="S4.T4.8.8.4">
<td class="ltx_td ltx_border_b" id="S4.T4.8.8.4.1"></td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_b" id="S4.T4.8.8.4.2"><span class="ltx_text ltx_font_bold" id="S4.T4.8.8.4.2.1">Ours+CVTHead</span></td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T4.8.8.4.3"><span class="ltx_text ltx_font_bold" id="S4.T4.8.8.4.3.1">0.003</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_b" id="S4.T4.8.8.4.4"><span class="ltx_text ltx_font_bold" id="S4.T4.8.8.4.4.1">37.08</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_b" id="S4.T4.8.8.4.5"><span class="ltx_text ltx_font_bold" id="S4.T4.8.8.4.5.1">0.010</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_b" id="S4.T4.8.8.4.6"><span class="ltx_text ltx_font_bold" id="S4.T4.8.8.4.6.1">0.990</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S4.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="218" id="S4.F10.g1" src="x15.png" width="696"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F10.2.1.1" style="font-size:90%;">Figure 10</span>: </span><span class="ltx_text" id="S4.F10.3.2" style="font-size:90%;">Qualitative comparison between 3D expressive heads that were generated by MotionClip and our method, respectively. The input, consisting of a shape image and instruction, was used for generating expressions 1 and 2 using either MotionClip (b) or Ours (c) on CK+ and CelebV-HQ (d, e), respectively.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="230" id="S4.F11.g1" src="figs/ck-celebv-texture.jpg" width="484"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F11.2.1.1" style="font-size:90%;">Figure 11</span>: </span><span class="ltx_text" id="S4.F11.3.2" style="font-size:90%;">
Qualitative comparison between generated 3D expressive faces when our method is combined with texture generation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib37" title="Realistic one-shot mesh-based head avatars">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib50" title="CVTHead: one-shot controllable head avatar with vertex-feature transformer">26</a>]</cite> to generate the desired expressions. The input (a), consisting of the source image and instruction, was used for generating expressions 1 and 2 using either Ours+ROME (b) or Ours+CVTHead (c) on CK+ and CelebV-HQ (d, e), respectively.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="110" id="S4.F12.g1" src="figs/test-FET.jpg" width="494"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F12.2.1.1" style="font-size:90%;">Figure 12</span>: </span><span class="ltx_text" id="S4.F12.3.2" style="font-size:90%;">Successive snapshots of the generated 3D expressive faces that include a neutral expression between two facial expressions instructed by text prompts.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">As the present study aims to generate an avatar with a 3D facial expression from a 2D image of a face and transform the facial emotion according to the given instructions, two major challenges arise (1) facial expression transition based on expression information given by the instruction and (2) rendering the facial appearance such that it corresponds with the desired facial expression. Two experiments were prepared to evaluate the results obtained with the proposed framework both quantitatively and qualitatively.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS4.SSS1.4.1.1">IV-D</span>1 </span>Quantitative Comparison</h4>
<div class="ltx_para" id="S4.SS4.SSS1.p1">
<p class="ltx_p" id="S4.SS4.SSS1.p1.2">Two tasks are prepared for quantitative comparison.

<br class="ltx_break"/> <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS1.p1.2.1">Facial Expression Transition</span>
<br class="ltx_break"/>The DECA encoder extracts the shape, facial expression, and pose from the source image. At the same time, the CLIP encoder combined with IFED and I2FET was used to interpret the text instruction: the initial facial expression, the final facial expression, and pose variation. To compare our approach with MotionClip, the first accuracy metric (Acc<sub class="ltx_sub" id="S4.SS4.SSS1.p1.2.2">1</sub>) was used to count the correctly classified samples for all expression labels in the instructions. The other one (Acc<sub class="ltx_sub" id="S4.SS4.SSS1.p1.2.3">2</sub>) was used to measure the correctly classified samples corresponding to both expression labels presented in each input instruction. The result suggests that the accuracy of our model is higher than that of MotionClip on both datasets as summarized in Table <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.T2" title="TABLE II ‣ IV-D Results ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">II</span></a>. For example, our model reaches 91.44 % accuracy for generating an independent expression and 84.03 % for generating expression sequences on the CK+ dataset. The Gmean of our model is 80.30%.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS1.p2">
<p class="ltx_p" id="S4.SS4.SSS1.p2.2">For the CelebV-HQ dataset, which is the real-world dataset, our model scores 58.24 %, 33.45 %, and 46.47 % for Acc<sub class="ltx_sub" id="S4.SS4.SSS1.p2.2.1">1</sub>, Acc<sub class="ltx_sub" id="S4.SS4.SSS1.p2.2.2">2</sub>, and Gmean, respectively, as indicated in Table <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.T3" title="TABLE III ‣ IV-D Results ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">III</span></a>. This result suggests that our model outperforms MotionClip for learning the facial expression parameters.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS1.p3">
<p class="ltx_p" id="S4.SS4.SSS1.p3.1"><span class="ltx_text ltx_font_italic" id="S4.SS4.SSS1.p3.1.1">Visualization with t-SNE</span>: Fig. <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.F7" title="Figure 7 ‣ IV-D Results ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">7</span></a> shows three learned latent representations, i.e. with MotionClip, I2FET, and I2FET with IFED, for seven facial expressions. These visual representations indicate that segregation between facial expressions is highly distinctive when IFED is combined with I2FET, compared with either MotionClip or I2FET alone.

<br class="ltx_break"/> 
<span class="ltx_text ltx_font_italic" id="S4.SS4.SSS1.p3.1.2">Confusion Matrices Analysis</span>: Fig. <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.F8" title="Figure 8 ‣ IV-D Results ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">8</span></a> and Fig. <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.F9" title="Figure 9 ‣ IV-D Results ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">9</span></a> show that IFED enhances the accuracy across most classes compared to MotionClip on both datasets. Yet, a few case of misclassification occurred between certain classes of expressions, such as anger and sadness, or contempt and anger with sadness on CK+. In real-world datasets such as CelebV-HQ, some misclassification occurs between classes, particularly those with challenging expressions such as fear and surprise, or contempt and neutral.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS1.p4">
<p class="ltx_p" id="S4.SS4.SSS1.p4.1"><span class="ltx_text ltx_font_italic" id="S4.SS4.SSS1.p4.1.1">Face Rendering</span>:
For the face-rendering task, FET was combined with state-of-the-art models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib37" title="Realistic one-shot mesh-based head avatars">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib50" title="CVTHead: one-shot controllable head avatar with vertex-feature transformer">26</a>]</cite> for rendering facial appearances that correspond to specific expressions requested by instruction. This was accomplished by integrating whether ROME <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib37" title="Realistic one-shot mesh-based head avatars">18</a>]</cite> or CVTHead <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib50" title="CVTHead: one-shot controllable head avatar with vertex-feature transformer">26</a>]</cite> with facial expression transition. In this case, DECA extracts facial parameters to render geometrical information for a specific face. Instead of shapes, the RGB images and instructions are used to evaluate the performance. The L<sub class="ltx_sub" id="S4.SS4.SSS1.p4.1.2">1</sub> loss, Peak Signal to Noise Ratio (PSNR), Learned Perceptual Image Patch Similarity (LPIPS) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib54" title="The unreasonable effectiveness of deep features as a perceptual metric">42</a>]</cite>, and Multi-Scale Structured Similarity (MS-SSIM) were used to measure the quality of reconstruction between the target image and synthesized image in the generated expression. As indicated by Table <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.T4" title="TABLE IV ‣ IV-D Results ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">IV</span></a>, our model with CVTHead produces the best result.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS4.SSS2.4.1.1">IV-D</span>2 </span>Qualitative Comparison</h4>
<div class="ltx_para" id="S4.SS4.SSS2.p1">
<p class="ltx_p" id="S4.SS4.SSS2.p1.1">In a similar vein, two tasks were prepared for qualitative comparisons.
<br class="ltx_break"/>  <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS2.p1.1.1">Facial Expression Transition</span>:
Fig. <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.F10" title="Figure 10 ‣ IV-D Results ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">10</span></a> compares sequences of facial expressions, generated by Motionclip and our method on CK+ and CelebV-HQ, respectively. These results clearly show that our method can generate facial expressions that convey more accurate expressions than MotionClip.

<br class="ltx_break"/> <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS2.p1.1.2">Face Rendering</span>:
The resulting expressions can be qualitatively compared by viewing and deciding which expression looks better and more natural than the other. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.F11" title="Figure 11 ‣ IV-D Results ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">11</span></a> shows the generated facial expressions based on learning the latent space of facial parameters on both datasets. The viewer may notice the difference between (b) and (c) for the CK+ dataset, and between (d) and (e) for the CelebV-HQ dataset, respectively, in terms of facial expressions and facial texture.
For instance, the facial transition from surprise to happiness seen in the 2nd row is clearer in (c) than in (b), and the facial texture seen in the 4th row is much smoother in (c) than in (b). A similar observation can be made between (d) and (e), suggesting that Ours+CVTHead is superior to than Ours+ROME.
For instance, in some cases, those enclosed within dashed boxes in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.F11" title="Figure 11 ‣ IV-D Results ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">11</span></a>, a subtle distortion arose either when the facial region of the source image was occluded or when the instructions describe expressions with significant pose changes, such as transitioning from surprise to happiness or from fear to surprise. This led to inconsistencies, and missing details in the generated facial regions compared to Ours+CVTHead.
A comparison between the two datasets, such as between (b) and (d) or between (c) and (e), reveals that the visual quality of CK+ is better than that of CelebV-HQ.
These results confirm that our model has outstanding ability to transform facial expressions, learned on CK+ or CelebV-HQ, to a specific facial image.

<br class="ltx_break"/>  In addition, a user study was carried out by requesting 24 subjects to view 60 randomly paired images of various identities selected from either CVTHead
+ MotionClip or CVTHead + Ours. Subjects were asked to choose the expression closest to the expressions specified in the instructions or indicate whether none of them matched the provided expressions. Naturally, they were unaware as to which algorithm was used to the presented images. Four subjects viewed each video and voted for the one that more closely resembled the expression in the instructions. In cases in which the votes were tied, an additional subject was asked to make the final decision. Based on these outcomes, CVTHead + Ours achieved 73% accuracy, whereas CVTHead + MotionClip yielded 58% accuracy for CK+. For CelebV-HQ, CVTHead + Ours and CVTHead + MotionClip achieved 77% and 70.5% accuracy, respectively.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS4.SSS3.4.1.1">IV-D</span>3 </span>Inference Time</h4>
<div class="ltx_para" id="S4.SS4.SSS3.p1">
<p class="ltx_p" id="S4.SS4.SSS3.p1.1">During inference, CVTHead + MotionClip takes 3.97 seconds to generate a video, whereas CVTHead + Ours only takes 3.92 seconds on a single NVIDIA RTX A6000, suggesting that our model is a slightly faster than MotionClip.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS5.4.1.1">IV-E</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS5.5.2">Ablation Study</span>
</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">This section presents a series of ablation studies that were carried out to evaluate the performance of our model.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS5.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS5.SSS1.4.1.1">IV-E</span>1 </span>Effectiveness of the Proposed Components</h4>
<div class="ltx_para" id="S4.SS5.SSS1.p1">
<p class="ltx_p" id="S4.SS5.SSS1.p1.2">To demonstrate the effectiveness of each proposed component, we conducted a comparative experiment, as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.T5" title="TABLE V ‣ IV-E1 Effectiveness of the Proposed Components ‣ IV-E Ablation Study ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">V</span></a>.
It can be observed that the IFED module significantly improves the performance of I2FET compared to I2FET without IFED on both CK+ and CelebV-HQ datasets. Additionally, integration of the vertex loss function with IFED increases Acc<sub class="ltx_sub" id="S4.SS5.SSS1.p1.2.1">1</sub> and Gmean on both datasets. Despite a slight decrease in Acc<sub class="ltx_sub" id="S4.SS5.SSS1.p1.2.2">2</sub> on CK+, it still performs well on CelebV-HQ.</p>
</div>
<figure class="ltx_table" id="S4.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T5.27.1.1" style="font-size:90%;">TABLE V</span>: </span><span class="ltx_text" id="S4.T5.28.2" style="font-size:90%;">COMPARISON OF THE ACCURACY BETWEEN THE PROPOSED DIFFERENT COMPONENTS FOR OUR METHOD (I2FET) ON THE TWO DATASETS</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T5.25">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T5.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T5.5.5.6"><span class="ltx_text ltx_font_bold" id="S4.T5.5.5.6.1">Dataset</span></th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T5.5.5.7"><span class="ltx_text ltx_font_bold" id="S4.T5.5.5.7.1">Method</span></th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T5.2.2.2">
<span class="ltx_text ltx_font_bold" id="S4.T5.1.1.1.1">Acc<sub class="ltx_sub" id="S4.T5.1.1.1.1.1"><span class="ltx_text ltx_font_medium" id="S4.T5.1.1.1.1.1.1">1</span></sub></span> (%) <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T5.2.2.2.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T5.4.4.4">
<span class="ltx_text ltx_font_bold" id="S4.T5.3.3.3.1">Acc<sub class="ltx_sub" id="S4.T5.3.3.3.1.1"><span class="ltx_text ltx_font_medium" id="S4.T5.3.3.3.1.1.1">2</span></sub></span> (%) <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T5.4.4.4.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T5.5.5.5">
<span class="ltx_text ltx_font_bold" id="S4.T5.5.5.5.1">Gmean</span> (%) <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T5.5.5.5.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T5.8.8">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T5.8.8.4"></th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T5.8.8.5">w/o IFED</th>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T5.6.6.1">76.89 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T5.6.6.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0029</td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T5.7.7.2">63.23 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T5.7.7.2.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0047</td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T5.8.8.3">63.59 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T5.8.8.3.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0059</td>
</tr>
<tr class="ltx_tr" id="S4.T5.11.11">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T5.11.11.4"></th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row" id="S4.T5.11.11.5">w/ IFED</th>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T5.9.9.1">91.30 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T5.9.9.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0017</td>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T5.10.10.2"><span class="ltx_text ltx_font_bold" id="S4.T5.10.10.2.1">84.06 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T5.10.10.2.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0026</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T5.11.11.3">79.41 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T5.11.11.3.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0061</td>
</tr>
<tr class="ltx_tr" id="S4.T5.14.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.14.14.4">CK+</th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row" id="S4.T5.14.14.5"><span class="ltx_text ltx_font_bold" id="S4.T5.14.14.5.1">w/ IFED</span></th>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T5.12.12.1"><span class="ltx_text ltx_font_bold" id="S4.T5.12.12.1.1">91.44 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T5.12.12.1.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0024</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T5.13.13.2">84.03 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T5.13.13.2.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0034</td>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T5.14.14.3"><span class="ltx_text ltx_font_bold" id="S4.T5.14.14.3.1">80.30 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T5.14.14.3.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0064</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.15.15">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T5.15.15.2"></th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row" id="S4.T5.15.15.1">+<span class="ltx_text ltx_font_bold" id="S4.T5.15.15.1.1">w/ <math alttext="\mathcal{L}_{v}" class="ltx_Math" display="inline" id="S4.T5.15.15.1.1.m1" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>v</mi></msub><annotation encoding="application/x-tex">\mathcal{L}_{v}</annotation></semantics></math></span>
</th>
<td class="ltx_td ltx_nopad_l" id="S4.T5.15.15.3"></td>
<td class="ltx_td ltx_nopad_l" id="S4.T5.15.15.4"></td>
<td class="ltx_td ltx_nopad_l" id="S4.T5.15.15.5"></td>
</tr>
<tr class="ltx_tr" id="S4.T5.18.18">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T5.18.18.4"></th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T5.18.18.5">w/o IFED</th>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T5.16.16.1">48.35 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T5.16.16.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0037</td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T5.17.17.2">22.1 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T5.17.17.2.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0040</td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T5.18.18.3">32.88 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T5.18.18.3.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0058</td>
</tr>
<tr class="ltx_tr" id="S4.T5.21.21">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T5.21.21.4"></th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row" id="S4.T5.21.21.5">w/ IFED</th>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T5.19.19.1">57.80 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T5.19.19.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0020</td>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T5.20.20.2">32.60 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T5.20.20.2.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0037</td>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T5.21.21.3">46.04 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T5.21.21.3.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0062</td>
</tr>
<tr class="ltx_tr" id="S4.T5.24.24">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T5.24.24.4">CelebV-HQ</th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row" id="S4.T5.24.24.5"><span class="ltx_text ltx_font_bold" id="S4.T5.24.24.5.1">w/ IFED</span></th>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T5.22.22.1"><span class="ltx_text ltx_font_bold" id="S4.T5.22.22.1.1">58.24 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T5.22.22.1.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0028</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T5.23.23.2"><span class="ltx_text ltx_font_bold" id="S4.T5.23.23.2.1">33.45 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T5.23.23.2.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0052</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T5.24.24.3"><span class="ltx_text ltx_font_bold" id="S4.T5.24.24.3.1">46.47 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T5.24.24.3.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0065</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.25.25">
<th class="ltx_td ltx_th ltx_th_row ltx_border_b" id="S4.T5.25.25.2"></th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T5.25.25.1">+<span class="ltx_text ltx_font_bold" id="S4.T5.25.25.1.1">w/ <math alttext="\mathcal{L}_{v}" class="ltx_Math" display="inline" id="S4.T5.25.25.1.1.m1" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>v</mi></msub><annotation encoding="application/x-tex">\mathcal{L}_{v}</annotation></semantics></math></span>
</th>
<td class="ltx_td ltx_nopad_l ltx_border_b" id="S4.T5.25.25.3"></td>
<td class="ltx_td ltx_nopad_l ltx_border_b" id="S4.T5.25.25.4"></td>
<td class="ltx_td ltx_nopad_l ltx_border_b" id="S4.T5.25.25.5"></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS5.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS5.SSS2.4.1.1">IV-E</span>2 </span>Impact of Three Loss Functions</h4>
<div class="ltx_para" id="S4.SS5.SSS2.p1">
<p class="ltx_p" id="S4.SS5.SSS2.p1.3">To analyze the impact of each loss function in I2FET, an experiment was carried out to compare the performance of the model in various scenarios. Specifically, IFED was integrated with the I2FET model and each loss function was incrementally added to determine its impact on the model performance of the model. The results in Table <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.T6" title="TABLE VI ‣ IV-E2 Impact of Three Loss Functions ‣ IV-E Ablation Study ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">VI</span></a> indicate that the performance improved when the pose loss function integrated with the expression loss function, whereas the vertex loss function yielded a slight improvement when used alongside both the pose and expression loss functions.
In this study, the vertex loss was integrated with other loss functions to capture the geometry of the face, to enhance the accuracy of facial expression recognition. Fig <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.F13" title="Figure 13 ‣ IV-E2 Impact of Three Loss Functions ‣ IV-E Ablation Study ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">13</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.F14" title="Figure 14 ‣ IV-E2 Impact of Three Loss Functions ‣ IV-E Ablation Study ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">14</span></a> are the confusion matrix of our model when the total loss function integrated with <math alttext="\mathcal{L}_{v}" class="ltx_Math" display="inline" id="S4.SS5.SSS2.p1.1.m1" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>v</mi></msub><annotation encoding="application/x-tex">\mathcal{L}_{v}</annotation></semantics></math> or without <math alttext="\mathcal{L}_{v}" class="ltx_Math" display="inline" id="S4.SS5.SSS2.p1.2.m2" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>v</mi></msub><annotation encoding="application/x-tex">\mathcal{L}_{v}</annotation></semantics></math> on the CK+ and CelebV-HQ datasets. Note that the fear accuracies for both datasets rather decrease slightly when the vertex loss is applied. It is known that the fear expression often overlaps with other expressions (e.g., surprise and happiness), thereby making it challenging to distinguish. Moreover, since the fear portion of CK+ is much larger than that of CelebV-HQ in Fig. 5, it appears that confusion during the facial expression transition has a reverse impact on Acc<sub class="ltx_sub" id="S4.SS5.SSS2.p1.3.1">2</sub> as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.T6" title="TABLE VI ‣ IV-E2 Impact of Three Loss Functions ‣ IV-E Ablation Study ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">VI</span></a></p>
</div>
<figure class="ltx_figure" id="S4.F13">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="S4.F13.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="166" id="S4.F13.sf1.g1" src="x16.png" width="206"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F13.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S4.F13.sf1.3.2" style="font-size:90%;"> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="S4.F13.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="165" id="S4.F13.sf2.g1" src="x17.png" width="203"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F13.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S4.F13.sf2.3.2" style="font-size:90%;"> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F13.6.3.1" style="font-size:90%;">Figure 13</span>: </span><span class="ltx_text" id="S4.F13.4.2" style="font-size:90%;">Confusion matrices for seven facial expressions by I2FET (a) without <math alttext="\mathcal{L}_{v}" class="ltx_Math" display="inline" id="S4.F13.3.1.m1" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>v</mi></msub><annotation encoding="application/x-tex">\mathcal{L}_{v}</annotation></semantics></math> and (b) with <math alttext="\mathcal{L}_{v}" class="ltx_Math" display="inline" id="S4.F13.4.2.m2" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>v</mi></msub><annotation encoding="application/x-tex">\mathcal{L}_{v}</annotation></semantics></math> on CK+.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F14">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="S4.F14.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="167" id="S4.F14.sf1.g1" src="x18.png" width="210"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F14.sf1.2.1.1" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel" id="S4.F14.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="167" id="S4.F14.sf2.g1" src="x19.png" width="210"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F14.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S4.F14.sf2.3.2" style="font-size:90%;"> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F14.6.3.1" style="font-size:90%;">Figure 14</span>: </span><span class="ltx_text" id="S4.F14.4.2" style="font-size:90%;">Confusion matrices for eight facial expressions by I2FET (a) without <math alttext="\mathcal{L}_{v}" class="ltx_Math" display="inline" id="S4.F14.3.1.m1" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>v</mi></msub><annotation encoding="application/x-tex">\mathcal{L}_{v}</annotation></semantics></math> and (b) with <math alttext="\mathcal{L}_{v}" class="ltx_Math" display="inline" id="S4.F14.4.2.m2" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>v</mi></msub><annotation encoding="application/x-tex">\mathcal{L}_{v}</annotation></semantics></math> on CelebV-HQ.</span></figcaption>
</figure>
<figure class="ltx_table" id="S4.T6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T6.37.1.1" style="font-size:90%;">TABLE VI</span>: </span><span class="ltx_text" id="S4.T6.38.2" style="font-size:90%;">COMPARISON OF THE IMPACT OF LOSS FUNCTIONS ON THE PERFORMANCE OF OUR METHOD (I2FET) ACROSS THE TWO DATASETS</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T6.35">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T6.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T6.5.5.6"><span class="ltx_text ltx_font_bold" id="S4.T6.5.5.6.1">Dataset</span></th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T6.5.5.7"><span class="ltx_text ltx_font_bold" id="S4.T6.5.5.7.1">Method</span></th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T6.2.2.2">
<span class="ltx_text ltx_font_bold" id="S4.T6.1.1.1.1">Acc<sub class="ltx_sub" id="S4.T6.1.1.1.1.1"><span class="ltx_text ltx_font_medium" id="S4.T6.1.1.1.1.1.1">1</span></sub></span> (%) <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T6.2.2.2.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T6.4.4.4">
<span class="ltx_text ltx_font_bold" id="S4.T6.3.3.3.1">Acc<sub class="ltx_sub" id="S4.T6.3.3.3.1.1"><span class="ltx_text ltx_font_medium" id="S4.T6.3.3.3.1.1.1">2</span></sub></span> (%) <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T6.4.4.4.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T6.5.5.5">
<span class="ltx_text ltx_font_bold" id="S4.T6.5.5.5.1">Gmean</span> (%) <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T6.5.5.5.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T6.9.9">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T6.9.9.5"></th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T6.6.6.1"><math alttext="\mathcal{L}_{e}" class="ltx_Math" display="inline" id="S4.T6.6.6.1.m1" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>e</mi></msub><annotation encoding="application/x-tex">\mathcal{L}_{e}</annotation></semantics></math></th>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T6.7.7.2">66.80 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T6.7.7.2.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0056</td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T6.8.8.3">45.96 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T6.8.8.3.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0112</td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T6.9.9.4">52.42 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T6.9.9.4.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0730</td>
</tr>
<tr class="ltx_tr" id="S4.T6.14.14">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T6.14.14.6"></th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row" id="S4.T6.11.11.2">
<math alttext="\mathcal{L}_{e}" class="ltx_Math" display="inline" id="S4.T6.10.10.1.m1" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>e</mi></msub><annotation encoding="application/x-tex">\mathcal{L}_{e}</annotation></semantics></math>+<math alttext="\mathcal{L}_{p}" class="ltx_Math" display="inline" id="S4.T6.11.11.2.m2" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>p</mi></msub><annotation encoding="application/x-tex">\mathcal{L}_{p}</annotation></semantics></math>
</th>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T6.12.12.3">91.30 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T6.12.12.3.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0017</td>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T6.13.13.4"><span class="ltx_text ltx_font_bold" id="S4.T6.13.13.4.1">84.06 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T6.13.13.4.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0026</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T6.14.14.5">79.41 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T6.14.14.5.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0061</td>
</tr>
<tr class="ltx_tr" id="S4.T6.20.20">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T6.20.20.7">CK+</th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row" id="S4.T6.17.17.3">
<math alttext="\mathcal{L}_{e}" class="ltx_Math" display="inline" id="S4.T6.15.15.1.m1" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>e</mi></msub><annotation encoding="application/x-tex">\mathcal{L}_{e}</annotation></semantics></math>+<math alttext="\mathcal{L}_{p}" class="ltx_Math" display="inline" id="S4.T6.16.16.2.m2" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>p</mi></msub><annotation encoding="application/x-tex">\mathcal{L}_{p}</annotation></semantics></math>+<math alttext="\mathcal{L}_{v}" class="ltx_Math" display="inline" id="S4.T6.17.17.3.m3" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>v</mi></msub><annotation encoding="application/x-tex">\mathcal{L}_{v}</annotation></semantics></math>
</th>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T6.18.18.4"><span class="ltx_text ltx_font_bold" id="S4.T6.18.18.4.1">91.44 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T6.18.18.4.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0024</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T6.19.19.5">84.03 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T6.19.19.5.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0034</td>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T6.20.20.6"><span class="ltx_text ltx_font_bold" id="S4.T6.20.20.6.1">80.30 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T6.20.20.6.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0064</span></td>
</tr>
<tr class="ltx_tr" id="S4.T6.24.24">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T6.24.24.5"></th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T6.21.21.1"><math alttext="\mathcal{L}_{e}" class="ltx_Math" display="inline" id="S4.T6.21.21.1.m1" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>e</mi></msub><annotation encoding="application/x-tex">\mathcal{L}_{e}</annotation></semantics></math></th>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T6.22.22.2">37.57 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T6.22.22.2.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0052</td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T6.23.23.3">11.99 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T6.23.23.3.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0326</td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T6.24.24.4">25.82 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T6.24.24.4.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0114</td>
</tr>
<tr class="ltx_tr" id="S4.T6.29.29">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T6.29.29.6"></th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row" id="S4.T6.26.26.2">
<math alttext="\mathcal{L}_{e}" class="ltx_Math" display="inline" id="S4.T6.25.25.1.m1" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>e</mi></msub><annotation encoding="application/x-tex">\mathcal{L}_{e}</annotation></semantics></math>+<math alttext="\mathcal{L}_{p}" class="ltx_Math" display="inline" id="S4.T6.26.26.2.m2" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>p</mi></msub><annotation encoding="application/x-tex">\mathcal{L}_{p}</annotation></semantics></math>
</th>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T6.27.27.3">57.80 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T6.27.27.3.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0020</td>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T6.28.28.4">32.60 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T6.28.28.4.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0037</td>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T6.29.29.5">46.04 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T6.29.29.5.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0062</td>
</tr>
<tr class="ltx_tr" id="S4.T6.35.35">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T6.35.35.7">CelebV-HQ</th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T6.32.32.3">
<math alttext="\mathcal{L}_{e}" class="ltx_Math" display="inline" id="S4.T6.30.30.1.m1" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>e</mi></msub><annotation encoding="application/x-tex">\mathcal{L}_{e}</annotation></semantics></math>+<math alttext="\mathcal{L}_{p}" class="ltx_Math" display="inline" id="S4.T6.31.31.2.m2" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>p</mi></msub><annotation encoding="application/x-tex">\mathcal{L}_{p}</annotation></semantics></math>+<math alttext="\mathcal{L}_{v}" class="ltx_Math" display="inline" id="S4.T6.32.32.3.m3" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mi>v</mi></msub><annotation encoding="application/x-tex">\mathcal{L}_{v}</annotation></semantics></math>
</th>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_b" id="S4.T6.33.33.4"><span class="ltx_text ltx_font_bold" id="S4.T6.33.33.4.1">58.24 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T6.33.33.4.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0028</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_b" id="S4.T6.34.34.5"><span class="ltx_text ltx_font_bold" id="S4.T6.34.34.5.1">33.45 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T6.34.34.5.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0052</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_b" id="S4.T6.35.35.6"><span class="ltx_text ltx_font_bold" id="S4.T6.35.35.6.1">46.47 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T6.35.35.6.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0065</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS5.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS5.SSS3.4.1.1">IV-E</span>3 </span>Impact of IFED on the Training Process</h4>
<div class="ltx_para" id="S4.SS5.SSS3.p1">
<p class="ltx_p" id="S4.SS5.SSS3.p1.1">This experiment was conducted to compare the two loss functions with and without incorporating the IFED module in I2FET. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.F15.sf1" title="In Figure 15 ‣ IV-E4 Effect of IFED on Intensity of Facial Expression ‣ IV-E Ablation Study ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">15(a)</span></a> and Fig. <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.F15.sf2" title="In Figure 15 ‣ IV-E4 Effect of IFED on Intensity of Facial Expression ‣ IV-E Ablation Study ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">15(b)</span></a> show the results for CK+ and CelebV-HQ, respectively, confirming that I2FED plays an important role.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS5.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS5.SSS4.4.1.1">IV-E</span>4 </span>Effect of IFED on Intensity of Facial Expression</h4>
<div class="ltx_para" id="S4.SS5.SSS4.p1">
<p class="ltx_p" id="S4.SS5.SSS4.p1.1">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.F16" title="Figure 16 ‣ IV-E4 Effect of IFED on Intensity of Facial Expression ‣ IV-E Ablation Study ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">16</span></a> demonstrates that IFED helps I2FED to generate high-intensity facial expressions.</p>
</div>
<figure class="ltx_figure" id="S4.F15">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F15.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="139" id="S4.F15.sf1.g1" src="x20.png" width="185"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F15.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S4.F15.sf1.3.2" style="font-size:90%;"> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F15.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="139" id="S4.F15.sf2.g1" src="x21.png" width="185"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F15.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S4.F15.sf2.3.2" style="font-size:90%;"> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F15.2.1.1" style="font-size:90%;">Figure 15</span>: </span><span class="ltx_text" id="S4.F15.3.2" style="font-size:90%;">Comparison of the validation loss during the training process of I2FET w/ and w/o IFED, respectively, on (a) CK+ and (b) CelebV-HQ. The green curves represent I2FET without IFED, whereas the blue curves are the results for I2FET with IFED.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F16"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="130" id="S4.F16.g1" src="x22.png" width="342"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F16.2.1.1" style="font-size:90%;">Figure 16</span>: </span><span class="ltx_text" id="S4.F16.3.2" style="font-size:90%;">Qualitative comparison between 3D expressive faces generated (a) w/o and (b) w/ IFED.</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS5.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS5.SSS5.4.1.1">IV-E</span>5 </span>Impact of Quantity of CAFT Layer</h4>
<div class="ltx_para" id="S4.SS5.SSS5.p1">
<p class="ltx_p" id="S4.SS5.SSS5.p1.1">The experiments were conducted to evaluate the results of the proposed I2FET model by designing the IFED module with varying numbers of CAFT layers. We fixed the number of transformer layers to 1 in both branches and adjusted the number of CAFT layers. The results in Table <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.T7" title="TABLE VII ‣ IV-E6 Impact of Quantity of Transformer Encoder in Facial Parameter Branch and Text Branch ‣ IV-E Ablation Study ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">VII</span></a> indicate that increasing the quantity of CAFT layers does not improve the performance of the proposed I2FET.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS5.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS5.SSS6.4.1.1">IV-E</span>6 </span>Impact of Quantity of Transformer Encoder in Facial Parameter Branch and Text Branch</h4>
<div class="ltx_para" id="S4.SS5.SSS6.p1">
<p class="ltx_p" id="S4.SS5.SSS6.p1.1">In this experiment, we utilized different configurations by adjusting the number of transformer encoder layers of IFED.
The findings suggest that the I2FET model performs better when <span class="ltx_text ltx_font_bold" id="S4.SS5.SSS6.p1.1.1">N</span> = 2 and <span class="ltx_text ltx_font_bold" id="S4.SS5.SSS6.p1.1.2">M</span> = 1 compared to other configurations, particularly in terms of Acc<sub class="ltx_sub" id="S4.SS5.SSS6.p1.1.3">1</sub> and G-mean, as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.T8" title="TABLE VIII ‣ IV-E6 Impact of Quantity of Transformer Encoder in Facial Parameter Branch and Text Branch ‣ IV-E Ablation Study ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">VIII</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T7">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T7.19.1.1" style="font-size:90%;">TABLE VII</span>: </span><span class="ltx_text" id="S4.T7.20.2" style="font-size:90%;">COMPARISON OF THE ACCURACY OF THE I2FET METHOD ON BOTH DATASETS BY VARYING THE NUMBER OF CAFT LAYERS IN I2FED</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T7.17">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T7.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T7.5.5.6"><span class="ltx_text ltx_font_bold" id="S4.T7.5.5.6.1">Dataset</span></th>
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T7.5.5.7"><span class="ltx_text ltx_font_bold" id="S4.T7.5.5.7.1">#Layers</span></th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T7.2.2.2">
<span class="ltx_text ltx_font_bold" id="S4.T7.1.1.1.1">Acc<sub class="ltx_sub" id="S4.T7.1.1.1.1.1"><span class="ltx_text ltx_font_medium" id="S4.T7.1.1.1.1.1.1">1</span></sub></span> (%) <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T7.2.2.2.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T7.4.4.4">
<span class="ltx_text ltx_font_bold" id="S4.T7.3.3.3.1">Acc<sub class="ltx_sub" id="S4.T7.3.3.3.1.1"><span class="ltx_text ltx_font_medium" id="S4.T7.3.3.3.1.1.1">2</span></sub></span> (%) <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T7.4.4.4.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T7.5.5.5">
<span class="ltx_text ltx_font_bold" id="S4.T7.5.5.5.1">Gmean</span> (%) <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T7.5.5.5.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T7.8.8">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T7.8.8.4"></th>
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T7.8.8.5">1</th>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T7.6.6.1"><span class="ltx_text ltx_font_bold" id="S4.T7.6.6.1.1">91.36 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T7.6.6.1.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0023</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T7.7.7.2"><span class="ltx_text ltx_font_bold" id="S4.T7.7.7.2.1">84.13 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T7.7.7.2.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0046</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T7.8.8.3"><span class="ltx_text ltx_font_bold" id="S4.T7.8.8.3.1">79.65 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T7.8.8.3.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0080</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T7.11.11.4">CK+</th>
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row" id="S4.T7.11.11.5">2</th>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T7.9.9.1">88.95 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T7.9.9.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0019</td>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T7.10.10.2">80.95 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T7.10.10.2.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0038</td>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T7.11.11.3">78.72 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T7.11.11.3.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0071</td>
</tr>
<tr class="ltx_tr" id="S4.T7.14.14">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T7.14.14.4"></th>
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T7.14.14.5">1</th>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T7.12.12.1"><span class="ltx_text ltx_font_bold" id="S4.T7.12.12.1.1">58.25 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T7.12.12.1.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0034</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T7.13.13.2"><span class="ltx_text ltx_font_bold" id="S4.T7.13.13.2.1">33.03 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T7.13.13.2.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0056</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T7.14.14.3"><span class="ltx_text ltx_font_bold" id="S4.T7.14.14.3.1">46.46 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T7.14.14.3.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0066</span></td>
</tr>
<tr class="ltx_tr" id="S4.T7.17.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T7.17.17.4">CelebV-HQ</th>
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row ltx_border_b" id="S4.T7.17.17.5">2</th>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_b" id="S4.T7.15.15.1">55.68 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T7.15.15.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0034</td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_b" id="S4.T7.16.16.2">29.82 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T7.16.16.2.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0066</td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_b" id="S4.T7.17.17.3">42.59 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T7.17.17.3.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0085</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S4.T8">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T8.21.1.1" style="font-size:90%;">TABLE VIII</span>: </span><span class="ltx_text" id="S4.T8.22.2" style="font-size:90%;">COMPARISON OF THE ACCURACY OF THE PROPOSED METHOD WITH THE NUMBER OF TRANSFORMER LAYERS FOR FACIAL PARAMETERS BRANCH (<span class="ltx_text ltx_font_bold" id="S4.T8.22.2.1">N</span>) AND TEXT BRANCH (<span class="ltx_text ltx_font_bold" id="S4.T8.22.2.2">M</span>) IN TFED ON CK+</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T8.17">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T8.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T8.5.5.6">Mode</th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T8.5.5.7"><span class="ltx_text ltx_font_bold" id="S4.T8.5.5.7.1">N</span></th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T8.5.5.8"><span class="ltx_text ltx_font_bold" id="S4.T8.5.5.8.1">M</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T8.2.2.2">
<span class="ltx_text ltx_font_bold" id="S4.T8.1.1.1.1">Acc<sub class="ltx_sub" id="S4.T8.1.1.1.1.1"><span class="ltx_text ltx_font_medium" id="S4.T8.1.1.1.1.1.1">1</span></sub></span> (%) <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T8.2.2.2.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T8.4.4.4">
<span class="ltx_text ltx_font_bold" id="S4.T8.3.3.3.1">Acc<sub class="ltx_sub" id="S4.T8.3.3.3.1.1"><span class="ltx_text ltx_font_medium" id="S4.T8.3.3.3.1.1.1">2</span></sub></span> (%) <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T8.4.4.4.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T8.5.5.5">
<span class="ltx_text ltx_font_bold" id="S4.T8.5.5.5.1">Gmean</span> (%) <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T8.5.5.5.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T8.8.8">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T8.8.8.4">A</td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T8.8.8.5">1</td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T8.8.8.6">1</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T8.6.6.1">91.36 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T8.6.6.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0023</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T8.7.7.2"><span class="ltx_text ltx_font_bold" id="S4.T8.7.7.2.1">84.13 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T8.7.7.2.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0046</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T8.8.8.3">79.67 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T8.8.8.3.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.008</td>
</tr>
<tr class="ltx_tr" id="S4.T8.11.11">
<td class="ltx_td ltx_align_left" id="S4.T8.11.11.4">B</td>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T8.11.11.5">2</td>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T8.11.11.6">1</td>
<td class="ltx_td ltx_align_left" id="S4.T8.9.9.1"><span class="ltx_text ltx_font_bold" id="S4.T8.9.9.1.1">91.44 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T8.9.9.1.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0024</span></td>
<td class="ltx_td ltx_align_left" id="S4.T8.10.10.2">84.03 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T8.10.10.2.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0034</td>
<td class="ltx_td ltx_align_left" id="S4.T8.11.11.3"><span class="ltx_text ltx_font_bold" id="S4.T8.11.11.3.1">80.30 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T8.11.11.3.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0064</span></td>
</tr>
<tr class="ltx_tr" id="S4.T8.14.14">
<td class="ltx_td ltx_align_left" id="S4.T8.14.14.4">C</td>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T8.14.14.5">1</td>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T8.14.14.6">2</td>
<td class="ltx_td ltx_align_left" id="S4.T8.12.12.1">91.27 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T8.12.12.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0019</td>
<td class="ltx_td ltx_align_left" id="S4.T8.13.13.2">83.84 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T8.13.13.2.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0037</td>
<td class="ltx_td ltx_align_left" id="S4.T8.14.14.3">79.69 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T8.14.14.3.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0052</td>
</tr>
<tr class="ltx_tr" id="S4.T8.17.17">
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T8.17.17.4">D</td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_b" id="S4.T8.17.17.5">2</td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_b" id="S4.T8.17.17.6">2</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T8.15.15.1">89.07 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T8.15.15.1.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0021</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T8.16.16.2">81.13 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T8.16.16.2.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0034</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T8.17.17.3">78.71 <math alttext="\pm" class="ltx_Math" display="inline" id="S4.T8.17.17.3.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math> 0.0070</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS6.4.1.1">IV-F</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS6.5.2">Longer Facial Expression Generation with Neutral Expressions</span>
</h3>
<div class="ltx_para" id="S4.SS6.p1">
<p class="ltx_p" id="S4.SS6.p1.1">According to appraisal theory <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib70" title="Appraisal considered as a process of multilevel sequential checking.">35</a>]</cite>, which is one of the major emotion theories, the dynamic of emotion is described as the sequential check theory of emotion differentiation. In other words, the differentiation of emotional states of humans is the result of a sequence of specific stimulus evaluation appraisal checks, rather than an action of hopping from one emotional state to another in a discrete manner. In our framework, the transition of the facial expression from A to B is similar to the differentiation of emotional states in a sequence: starting from A facial expression with strong emotion a neutral expression and then moving to strong facial expression B. In addition to appraisal theory, the present approach is suitable for investigating facial behaviors within the pleasure-arousal (P-A) space <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib60" title="Reading emotions from and into faces: resurrecting dimensional-contextual perspective">34</a>]</cite>, where the neutral expression serves as a transitional expression between two specific expressions. For this purpose, a neutral encoder-decoder (NED) was adopted to generate neutral faces. The aim thereof was to improve the performance of our framework when a neutral expression is not available in the instruction to increase its flexibility for extending facial expression sequences.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS6.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS6.SSS1.4.1.1">IV-F</span>1 </span>Implementation Details</h4>
<div class="ltx_para" id="S4.SS6.SSS1.p1">
<p class="ltx_p" id="S4.SS6.SSS1.p1.1">In this scenario, two datasets containing the parameters of neutral expressions and poses were constructed the pretrained DECA to extract the data from the CelebV-HQ and CK+ datasets. As the CK+ dataset does not contain neutral expressions, the first frame of each video was extracted and considered as the representative neutral case. Afterward, NED was trained on these two neutral datasets.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS6.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS6.SSS2.4.1.1">IV-F</span>2 </span>Network Architecture</h4>
<div class="ltx_para" id="S4.SS6.SSS2.p1">
<p class="ltx_p" id="S4.SS6.SSS2.p1.1">The neutral encoder-decoder model was constructed with fully connected layers. The encoder consisted of four fully connected layers, whereas the decoder was structured with four fully connected layers dedicated to pose and expression, respectively.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS6.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS6.SSS3.4.1.1">IV-F</span>3 </span>Analysis</h4>
<div class="ltx_para" id="S4.SS6.SSS3.p1">
<p class="ltx_p" id="S4.SS6.SSS3.p1.1">Similarly, we also conducted experiments to evaluate neutral expression generation for both quantitative and qualitative comparisons.

<br class="ltx_break"/>  <span class="ltx_text ltx_font_italic" id="S4.SS6.SSS3.p1.1.1">Quantitative comparison</span>:
Specifically, the accuracy of NED on the CK+ and CelebV-HQ datasets was 49.5% and 51.33% for generating the neutral expression, respectively. These results are presented in
Table <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.T9" title="TABLE IX ‣ IV-F3 Analysis ‣ IV-F Longer Facial Expression Generation with Neutral Expressions ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">IX</span></a>, which confirms that NED + CVTHead outperforms NED + ROME.</p>
</div>
<div class="ltx_para" id="S4.SS6.SSS3.p2">
<p class="ltx_p" id="S4.SS6.SSS3.p2.1"><span class="ltx_text ltx_font_italic" id="S4.SS6.SSS3.p2.1.1">Qualitative comparison</span>:
Fig. <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.F17" title="Figure 17 ‣ IV-F3 Analysis ‣ IV-F Longer Facial Expression Generation with Neutral Expressions ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">17</span></a> visualizes neutral expressions generated by NED + CVTHead on CK+ and CelebV-HQ. Moreover, we also illustrate neutral expressions generated by FLAME with the expression parameter set to zero, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.F17" title="Figure 17 ‣ IV-F3 Analysis ‣ IV-F Longer Facial Expression Generation with Neutral Expressions ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">17</span></a>c.</p>
</div>
<div class="ltx_para" id="S4.SS6.SSS3.p3">
<p class="ltx_p" id="S4.SS6.SSS3.p3.1">The results show that the facial units differ, depending on whether the environment is controllable or non-controllable, or whether FLAME was used. This indicates that neutral expressions are more complex in real-world scenarios, leading us to prefer using a neural encoder-decoder instead of the expression parameter settings provided by FLAME</p>
</div>
<figure class="ltx_figure" id="S4.F17"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="175" id="S4.F17.g1" src="x23.png" width="218"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F17.2.1.1" style="font-size:90%;">Figure 17</span>: </span><span class="ltx_text" id="S4.F17.3.2" style="font-size:90%;">Samples of facial images with a neutral expression generated by NED+CVTHead. The input images are shown in the leftmost column (Source). The images generated using CK+ dataset (controllable environment) are shown in (a), whereas those using CelebV-HQ dataset (non-controllable environment) are shown in (b). On the other hand, the generated 3D face with FLAME is depicted by setting the expression parameter to zero in (c). Note that subtle differences exist in the image quality between the three generated cases.</span></figcaption>
</figure>
<figure class="ltx_table" id="S4.T9">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T9.6.1.1" style="font-size:90%;">TABLE IX</span>: </span><span class="ltx_text" id="S4.T9.7.2" style="font-size:90%;">RESULT OBTAINED WHEN NED IS COMBINED WITH ROME OR CVTHEAD FOR TEXTURE GENERATION ON CK+ AND CELEBV-HQ, RESPECTIVELY.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T9.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T9.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T9.4.4.5"><span class="ltx_text ltx_font_bold" id="S4.T9.4.4.5.1">Dataset</span></th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T9.4.4.6"><span class="ltx_text ltx_font_bold" id="S4.T9.4.4.6.1">Method</span></th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T9.1.1.1">L<math alttext="{}_{1}\downarrow" class="ltx_math_unparsed" display="inline" id="S4.T9.1.1.1.m1" intent=":literal"><semantics><mmultiscripts><mo stretchy="false">↓</mo><mprescripts></mprescripts><mn>1</mn><mrow></mrow></mmultiscripts><annotation encoding="application/x-tex">{}_{1}\downarrow</annotation></semantics></math>
</th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T9.2.2.2">PSNR<math alttext="{\uparrow}" class="ltx_Math" display="inline" id="S4.T9.2.2.2.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">{\uparrow}</annotation></semantics></math>
</th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T9.3.3.3">LPIPS<math alttext="{\downarrow}" class="ltx_Math" display="inline" id="S4.T9.3.3.3.m1" intent=":literal"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">{\downarrow}</annotation></semantics></math>
</th>
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T9.4.4.4">MS_SSIM<math alttext="{\uparrow}" class="ltx_Math" display="inline" id="S4.T9.4.4.4.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">{\uparrow}</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T9.4.5.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T9.4.5.1.1">CK+</td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T9.4.5.1.2">NED + ROME</td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T9.4.5.1.3">0.193</td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T9.4.5.1.4">10.207</td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T9.4.5.1.5">0.516</td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T9.4.5.1.6">0.348</td>
</tr>
<tr class="ltx_tr" id="S4.T9.4.6.2">
<td class="ltx_td" id="S4.T9.4.6.2.1"></td>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T9.4.6.2.2"><span class="ltx_text ltx_font_bold" id="S4.T9.4.6.2.2.1">NED + CVTHead</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T9.4.6.2.3"><span class="ltx_text ltx_font_bold" id="S4.T9.4.6.2.3.1">0.004</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T9.4.6.2.4"><span class="ltx_text ltx_font_bold" id="S4.T9.4.6.2.4.1">35.158</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T9.4.6.2.5"><span class="ltx_text ltx_font_bold" id="S4.T9.4.6.2.5.1">0.014</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_left" id="S4.T9.4.6.2.6"><span class="ltx_text ltx_font_bold" id="S4.T9.4.6.2.6.1">0.987</span></td>
</tr>
<tr class="ltx_tr" id="S4.T9.4.7.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T9.4.7.3.1">CelebV-HQ</td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T9.4.7.3.2">NED + ROME</td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T9.4.7.3.3">0.183</td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T9.4.7.3.4">10.818</td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T9.4.7.3.5">0.498</td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" id="S4.T9.4.7.3.6">0.407</td>
</tr>
<tr class="ltx_tr" id="S4.T9.4.8.4">
<td class="ltx_td ltx_border_b" id="S4.T9.4.8.4.1"></td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_b" id="S4.T9.4.8.4.2"><span class="ltx_text ltx_font_bold" id="S4.T9.4.8.4.2.1">NED + CVTHead</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_b" id="S4.T9.4.8.4.3"><span class="ltx_text ltx_font_bold" id="S4.T9.4.8.4.3.1">0.004</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_b" id="S4.T9.4.8.4.4"><span class="ltx_text ltx_font_bold" id="S4.T9.4.8.4.4.1">36.347</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_b" id="S4.T9.4.8.4.5"><span class="ltx_text ltx_font_bold" id="S4.T9.4.8.4.5.1">0.0117</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_left ltx_border_b" id="S4.T9.4.8.4.6"><span class="ltx_text ltx_font_bold" id="S4.T9.4.8.4.6.1">0.989</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS6.SSS3.p4">
<p class="ltx_p" id="S4.SS6.SSS3.p4.1">In a user study similar to that described in Section <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS4.SSS2" title="IV-D2 Qualitative Comparison ‣ IV-D Results ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-D</span>2</span></a>, we asked subjects to determine whether the generated images display neutral expressions. Most subjects succeeded in identifying the neutral expressions produced by our model on CK+, and they had difficulties with only 4% of the samples generated by the learned model on CelebV-HQ.</p>
</div>
<figure class="ltx_table" id="S4.T10">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T10.23.2.1" style="font-size:90%;">TABLE X</span>: </span><span class="ltx_text" id="S4.T10.2.1" style="font-size:90%;">COMPARISON OF FACIAL EXPRESSION CLASSIFIERS ON CK+ AND CELEBV-HQ WITH SAMPLES ROTATED AT VARIOUS ANGLES <math alttext="\{0^{o},\pm 10^{o},\pm 15^{o},\pm 30^{o}\}" class="ltx_Math" display="inline" id="S4.T10.2.1.m1" intent=":literal"><semantics><mrow><mo stretchy="false">{</mo><msup><mn>0</mn><mi>o</mi></msup><mo>,</mo><mrow><mo>±</mo><msup><mn>10</mn><mi>o</mi></msup></mrow><mo>,</mo><mrow><mo>±</mo><msup><mn>15</mn><mi>o</mi></msup></mrow><mo>,</mo><mrow><mo>±</mo><msup><mn>30</mn><mi>o</mi></msup></mrow><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{0^{o},\pm 10^{o},\pm 15^{o},\pm 30^{o}\}</annotation></semantics></math></span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T10.21">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T10.9.7">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T10.9.7.8"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T10.9.7.9"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" colspan="2" id="S4.T10.3.1.1"><span class="ltx_text ltx_font_bold" id="S4.T10.3.1.1.1">0<sup class="ltx_sup" id="S4.T10.3.1.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T10.3.1.1.1.1.1">o</span></sup></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2" id="S4.T10.5.3.3">
<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T10.4.2.2.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S4.T10.5.3.3.1">10<sup class="ltx_sup" id="S4.T10.5.3.3.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T10.5.3.3.1.1.1">o</span></sup></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2" id="S4.T10.7.5.5">
<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T10.6.4.4.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S4.T10.7.5.5.1">15<sup class="ltx_sup" id="S4.T10.7.5.5.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T10.7.5.5.1.1.1">o</span></sup></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2" id="S4.T10.9.7.7">
<math alttext="\pm" class="ltx_Math" display="inline" id="S4.T10.8.6.6.m1" intent=":literal"><semantics><mo>±</mo><annotation encoding="application/x-tex">\pm</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="S4.T10.9.7.7.1">30<sup class="ltx_sup" id="S4.T10.9.7.7.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T10.9.7.7.1.1.1">o</span></sup></span>
</th>
</tr>
<tr class="ltx_tr" id="S4.T10.21.19">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S4.T10.21.19.13"><span class="ltx_text ltx_font_bold" id="S4.T10.21.19.13.1">Dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S4.T10.21.19.14"><span class="ltx_text ltx_font_bold" id="S4.T10.21.19.14.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T10.11.9.2">
<span class="ltx_text ltx_font_bold" id="S4.T10.10.8.1.1">Acc<sub class="ltx_sub" id="S4.T10.10.8.1.1.1"><span class="ltx_text ltx_font_medium" id="S4.T10.10.8.1.1.1.1">1</span></sub></span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T10.11.9.2.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T10.12.10.3">
<span class="ltx_text ltx_font_bold" id="S4.T10.12.10.3.1">G-mean</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T10.12.10.3.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T10.14.12.5">
<span class="ltx_text ltx_font_bold" id="S4.T10.13.11.4.1">Acc<sub class="ltx_sub" id="S4.T10.13.11.4.1.1"><span class="ltx_text ltx_font_medium" id="S4.T10.13.11.4.1.1.1">1</span></sub></span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T10.14.12.5.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T10.15.13.6">
<span class="ltx_text ltx_font_bold" id="S4.T10.15.13.6.1">G-mean</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T10.15.13.6.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T10.17.15.8">
<span class="ltx_text ltx_font_bold" id="S4.T10.16.14.7.1">Acc<sub class="ltx_sub" id="S4.T10.16.14.7.1.1"><span class="ltx_text ltx_font_medium" id="S4.T10.16.14.7.1.1.1">1</span></sub></span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T10.17.15.8.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T10.18.16.9">
<span class="ltx_text ltx_font_bold" id="S4.T10.18.16.9.1">G-mean</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T10.18.16.9.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T10.20.18.11">
<span class="ltx_text ltx_font_bold" id="S4.T10.19.17.10.1">Acc<sub class="ltx_sub" id="S4.T10.19.17.10.1.1"><span class="ltx_text ltx_font_medium" id="S4.T10.19.17.10.1.1.1">1</span></sub></span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T10.20.18.11.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T10.21.19.12">
<span class="ltx_text ltx_font_bold" id="S4.T10.21.19.12.1">G-mean</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T10.21.19.12.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T10.21.20.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T10.21.20.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T10.21.20.1.2">MEK <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib35" title="Leave no stone unturned: mine extra knowledge for imbalanced facial expression recognition">43</a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T10.21.20.1.3"><span class="ltx_text ltx_font_bold" id="S4.T10.21.20.1.3.1">100</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T10.21.20.1.4"><span class="ltx_text ltx_font_bold" id="S4.T10.21.20.1.4.1">100</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.21.20.1.5">86.41</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.21.20.1.6"><span class="ltx_text ltx_font_bold" id="S4.T10.21.20.1.6.1">76.20</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.21.20.1.7">76.01</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.21.20.1.8">52.16</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.21.20.1.9">32.56</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.21.20.1.10">0.0</td>
</tr>
<tr class="ltx_tr" id="S4.T10.21.21.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T10.21.21.2.1">CK+</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T10.21.21.2.2">ResNet-101 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib21" title="Deep Residual Learning for Image Recognition">14</a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T10.21.21.2.3">99.56</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T10.21.21.2.4">99.4</th>
<td class="ltx_td ltx_align_center" id="S4.T10.21.21.2.5">84.40</td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.21.2.6">73.28</td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.21.2.7">79.50</td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.21.2.8"><span class="ltx_text ltx_font_bold" id="S4.T10.21.21.2.8.1">67.6</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.21.2.9">75.29</td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.21.2.10"><span class="ltx_text ltx_font_bold" id="S4.T10.21.21.2.10.1">33.39</span></td>
</tr>
<tr class="ltx_tr" id="S4.T10.21.22.3">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T10.21.22.3.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T10.21.22.3.2">ResNet-101 with RFL</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T10.21.22.3.3">99.69</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T10.21.22.3.4">99.69</th>
<td class="ltx_td ltx_align_center" id="S4.T10.21.22.3.5"><span class="ltx_text ltx_font_bold" id="S4.T10.21.22.3.5.1">87.18</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.22.3.6">75.85</td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.22.3.7"><span class="ltx_text ltx_font_bold" id="S4.T10.21.22.3.7.1">83.66</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.22.3.8">66.13</td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.22.3.9"><span class="ltx_text ltx_font_bold" id="S4.T10.21.22.3.9.1">77.48</span></td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.22.3.10">31.5</td>
</tr>
<tr class="ltx_tr" id="S4.T10.21.23.4">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T10.21.23.4.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T10.21.23.4.2">MEK <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib35" title="Leave no stone unturned: mine extra knowledge for imbalanced facial expression recognition">43</a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T10.21.23.4.3"><span class="ltx_text ltx_font_bold" id="S4.T10.21.23.4.3.1">99.75</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T10.21.23.4.4"><span class="ltx_text ltx_font_bold" id="S4.T10.21.23.4.4.1">99.83</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.21.23.4.5"><span class="ltx_text ltx_font_bold" id="S4.T10.21.23.4.5.1">60.49</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.21.23.4.6"><span class="ltx_text ltx_font_bold" id="S4.T10.21.23.4.6.1">55.07</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.21.23.4.7">49.08</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.21.23.4.8">33.84</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.21.23.4.9">32.05</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T10.21.23.4.10">9.64</td>
</tr>
<tr class="ltx_tr" id="S4.T10.21.24.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T10.21.24.5.1">CelebV-HQ</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T10.21.24.5.2">ResNet-101 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib21" title="Deep Residual Learning for Image Recognition">14</a>]</cite>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T10.21.24.5.3">37</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T10.21.24.5.4">12.77</th>
<td class="ltx_td ltx_align_center" id="S4.T10.21.24.5.5">35.77</td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.24.5.6">13.39</td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.24.5.7">34.97</td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.24.5.8">0.0</td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.24.5.9">28.92</td>
<td class="ltx_td ltx_align_center" id="S4.T10.21.24.5.10">0.0</td>
</tr>
<tr class="ltx_tr" id="S4.T10.21.25.6">
<th class="ltx_td ltx_th ltx_th_row ltx_border_b" id="S4.T10.21.25.6.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="S4.T10.21.25.6.2">ResNet-101 with RFL</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="S4.T10.21.25.6.3">93.67</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="S4.T10.21.25.6.4">95.04</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T10.21.25.6.5">58.54</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T10.21.25.6.6">49.92</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T10.21.25.6.7"><span class="ltx_text ltx_font_bold" id="S4.T10.21.25.6.7.1">51.53</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T10.21.25.6.8"><span class="ltx_text ltx_font_bold" id="S4.T10.21.25.6.8.1">41.22</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T10.21.25.6.9"><span class="ltx_text ltx_font_bold" id="S4.T10.21.25.6.9.1">40.71</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T10.21.25.6.10"><span class="ltx_text ltx_font_bold" id="S4.T10.21.25.6.10.1">26.80</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS7.4.1.1">IV-G</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS7.5.2">Facial Expression Classification </span>
</h3>
<section class="ltx_subsubsection" id="S4.SS7.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS7.SSS1.4.1.1">IV-G</span>1 </span>Implementation</h4>
<div class="ltx_para" id="S4.SS7.SSS1.p1">
<p class="ltx_p" id="S4.SS7.SSS1.p1.1">For ResNet-101 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib21" title="Deep Residual Learning for Image Recognition">14</a>]</cite> and ResNet-101 with RFL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib24" title="Class-balanced loss based on effective number of samples">5</a>]</cite>, the learning rate was <math alttext="1e^{-4}" class="ltx_Math" display="inline" id="S4.SS7.SSS1.p1.1.m1" intent=":literal"><semantics><mrow><mn>1</mn><mo lspace="0em" rspace="0em">​</mo><msup><mi>e</mi><mrow><mo>−</mo><mn>4</mn></mrow></msup></mrow><annotation encoding="application/x-tex">1e^{-4}</annotation></semantics></math> and the batch size was 64. Adam was used to optimize the models with 50 epochs per cycle. For the MEK model, the configuration in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib35" title="Leave no stone unturned: mine extra knowledge for imbalanced facial expression recognition">43</a>]</cite> was used to optimize the training process for both the CK+ and CelebV-HQ datasets.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS7.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS7.SSS2.4.1.1">IV-G</span>2 </span>Analysis</h4>
<div class="ltx_para" id="S4.SS7.SSS2.p1">
<p class="ltx_p" id="S4.SS7.SSS2.p1.1">Based on the results in Table <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.T10" title="TABLE X ‣ IV-F3 Analysis ‣ IV-F Longer Facial Expression Generation with Neutral Expressions ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">X</span></a>, ResNet-101 with RFL was more stable compared to ResNet-101 or MEK in terms of their classification performance, suggesting that ResNet-101 with RFL can handle both imbalanced datasets and rotational transformations fairly well. In particular, MEK achieved the best result on the original images although its performance was affected by rotated images. ResNet-101 with RFL demonstrated more stable performance than ResNet-101 and MEK on CK+. For real-world scenarios such as those in the CelebV-HQ dataset, we obtained similar results. These findings indicate that the performance of facial expression classifiers deteriorates for faces that are rotated by <math alttext="\pm 15" class="ltx_Math" display="inline" id="S4.SS7.SSS2.p1.1.m1" intent=":literal"><semantics><mrow><mo>±</mo><mn>15</mn></mrow><annotation encoding="application/x-tex">\pm 15</annotation></semantics></math> degrees or more. In addition, MEK performs better for the original datasets, but worse for the augmented datasets. The performance of ResNet-101 is high on datasets with low imbalance rates, whereas it is low on datasets with high imbalance rates.
Meanwhile, ResNet-101 with RFL delivers stable performance on imbalanced and rotated datasets. Based on this result, we adopted ResNet-101 by default during evaluation.</p>
</div>
<figure class="ltx_figure" id="S4.F18">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F18.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="161" id="S4.F18.sf1.g1" src="figs/vocab.jpg" width="195"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F18.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S4.F18.sf1.3.2" style="font-size:90%;"> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F18.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="198" id="S4.F18.sf2.g1" src="x24.png" width="295"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F18.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S4.F18.sf2.3.2" style="font-size:90%;"> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F18.2.1.1" style="font-size:90%;">Figure 18</span>: </span><span class="ltx_text" id="S4.F18.3.2" style="font-size:90%;">Generated 3D faces (a) by changing the template sentences and (b) a few unsuccessful cases.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS8">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS8.4.1.1">IV-H</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS8.5.2">Discussion</span>
</h3>
<section class="ltx_subsubsection" id="S4.SS8.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS8.SSS1.4.1.1">IV-H</span>1 </span>Diversity of Facial Expression Sequences</h4>
<div class="ltx_para" id="S4.SS8.SSS1.p1">
<p class="ltx_p" id="S4.SS8.SSS1.p1.1">Our framework is capable of animating various facial expressions guided by instruction and facial images with pose variation, as illustrated in Fig <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.F12" title="Figure 12 ‣ IV-D Results ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">12</span></a>. Moreover, it can generate facial expressions corresponding to both controlled and real-world environments, enabling the generation of a diverse range of expressions for creating a long video.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS8.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS8.SSS2.4.1.1">IV-H</span>2 </span>Results on Instructions with Changes in Template Sentences</h4>
<div class="ltx_para" id="S4.SS8.SSS2.p1">
<p class="ltx_p" id="S4.SS8.SSS2.p1.1">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.F18.sf1" title="In Figure 18 ‣ IV-G2 Analysis ‣ IV-G Facial Expression Classification ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">18(a)</span></a> shows the outcomes obtained by changing the template sentences. The result suggests that our model can successfully generate facial expressions that closely resemble specific emotions described in the instruction.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS8.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS8.SSS3.4.1.1">IV-H</span>3 </span>Failure Cases</h4>
<div class="ltx_para" id="S4.SS8.SSS3.p1">
<p class="ltx_p" id="S4.SS8.SSS3.p1.1">Although our model performs well in many cases, the model failed in certain instances as exemplified in Fig <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.F18.sf2" title="In Figure 18 ‣ IV-G2 Analysis ‣ IV-G Facial Expression Classification ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">18(b)</span></a>. In the top row, significant pose variation in the input image leads to regions of the shoulders being missing from the resulting image due to constraints within the pre-trained face-rendering model. Likewise, images in which regions of the head and those containing hair were missing from the source image also led to the lack of information in the face rendering process because of this constraint, as shown in the bottom row.
In the second row, the generated result is restricted by the size of the expression vocabulary, causing a <span class="ltx_text ltx_font_italic" id="S4.SS8.SSS3.p1.1.1">scared</span> face to be transformed into an <span class="ltx_text ltx_font_italic" id="S4.SS8.SSS3.p1.1.2">angry</span> face.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this study, we present a novel framework for generating 3D facial expressions from an RGB image and animating facial transitions between two expressions as specified by entering text instructions.
First, FET was introduced to produce a sequence of facial expressions. The Instruction-Driven Facial Expression Decomposer was designed to learn from multimodal data and capture correlations between textual descriptions and facial expression features. We proposed an Instruction to Facial Expression Transition model to generate target facial expressions guided by a single instruction. Our framework integrates FET and a pre-trained face rendering model to generate facial appearances aligned with the expected expression sequence.
Furthermore, our framework can be expanded to include the generation of a neutral expression, thereby enabling the creation of diverse expressions in a video, thus closely simulating facial expression behaviors in real-world scenarios. Extensive experiments were conducted on the CK+ and CelebV-HQ datasets to demonstrate the effectiveness of our framework.
Although our proposed approach demonstrates positive outcomes in terms of controlling the facial expression of an RGB image according to the provided instructions, it does have certain limitations. Similar to previous studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib37" title="Realistic one-shot mesh-based head avatars">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib50" title="CVTHead: one-shot controllable head avatar with vertex-feature transformer">26</a>]</cite> in which facial parameters were used, the performance of our model relies on the precision of 3D face coefficients, especially using DECA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#bib.bib40" title="Learning an animatable detailed 3D face model from in-the-wild images">9</a>]</cite> within our configurations since it may encounter difficulty, often seen in challenging scenarios, in disentangling facial factors. Also, even though linear interpolation can maintain temporal consistency, it often compromises the realism of the synthesized videos. Although the present work uses either basic facial expressions or two designated expressions, the vocabulary number can be easily expanded by combining a Large Language Model (LLM) with our model. Given that text prompting is a powerful tool by which various emotional states can be expressed on a 3D avatar via our framework, we expect it to find various applications in the future.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgment</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This work was supported by the Information Technology Research Center (ITRC) support program (IITP-2022-RS-2022-00156354)
and a Korean government grant (MSIT) (No.RS-2019-II190231) from the Institute of Information &amp; Communications Technology Planning &amp; Evaluation (IITP) as well as by the Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (2020R1A6A1A03038540).</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist" id="bib.L1">
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib57">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Aneja, J. Thies, A. Dai, and M. Nießner</span><span class="ltx_text ltx_bib_year"> (2023)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">ClipFace: text-guided editing of textured 3d morphable models</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">SIGGRAPH ’23 Conference Proceedings</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S1.p2.1" title="I Introduction ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§I</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S1.p3.1" title="I Introduction ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§I</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib51">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. C. Buehler, A. Meka, G. Li, T. Beeler, and O. Hilliges</span><span class="ltx_text ltx_bib_year"> (2021)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">VariTex: variational neural face textures</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S1.p2.1" title="I Introduction ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§I</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S2.SS2.p1.1" title="II-B Face Rendering ‣ II Related Work ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">II-B</span></span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib59">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">C. Chen, Q. Fan, and R. Panda</span><span class="ltx_text ltx_bib_year"> (2021)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">CrossViT: cross-attention multi-scale vision transformer for image classification</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">2021 IEEE/CVF International Conference on Computer Vision (ICCV)</span>, <span class="ltx_text ltx_bib_pages"> pp. 347–356</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S3.SS2.SSS1.p2.7" title="III-B1 Facial Expression Transition ‣ III-B The Proposed Method ‣ III Method ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">III-B</span>1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib64">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Chen and S. Xiong</span><span class="ltx_text ltx_bib_year"> (2023)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Rethinking one-shot face reenactment: a spatial–temporal reconstruction view</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Knowledge Based Systems</span> <span class="ltx_text ltx_bib_volume">277</span> (<span class="ltx_text ltx_bib_number">C</span>).
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S1.p1.1" title="I Introduction ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§I</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib24">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Cui, M. Jia, T. Lin, Y. Song, and S. J. Belongie</span><span class="ltx_text ltx_bib_year"> (2019)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Class-balanced loss based on effective number of samples</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>, <span class="ltx_text ltx_bib_pages"> pp. 9260–9269</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS3.p2.2" title="IV-C Classification of Facial Expressions ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">IV-C</span></span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS7.SSS1.p1.1" title="IV-G1 Implementation ‣ IV-G Facial Expression Classification ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">IV-G</span>1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib48">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. Daněček, K. Chhatre, S. Tripathi, Y. Wen, M. Black, and T. Bolkart</span><span class="ltx_text ltx_bib_year"> (2023)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Emotional speech-driven animation with content-emotion disentanglement</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">SIGGRAPH Asia 2023 Conference Papers</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S2.SS1.p1.1" title="II-A Facial Expression Transition ‣ II Related Work ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">II-A</span></span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S2.SS1.p2.1" title="II-A Facial Expression Transition ‣ II Related Work ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">II-A</span></span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S3.SS2.SSS1.p2.7" title="III-B1 Facial Expression Transition ‣ III-B The Proposed Method ‣ III Method ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">III-B</span>1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib9">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">H. Ding, K. Sricharan, and R. Chellappa</span><span class="ltx_text ltx_bib_year"> (2018)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Exprgan: facial expression editing with controllable expression intensity</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">AAAI</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S1.p1.1" title="I Introduction ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§I</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib53">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. C. Doukas, S. Zafeiriou, and V. Sharmanska</span><span class="ltx_text ltx_bib_year"> (2021-10)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">HeadGAN: one-shot neural head synthesis and editing</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 14398–14407</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S1.p2.1" title="I Introduction ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§I</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib40">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Feng, H. Feng, M. J. Black, and T. Bolkart</span><span class="ltx_text ltx_bib_year"> (2021)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Learning an animatable detailed 3D face model from in-the-wild images</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">ACM Transactions on Graphics, (Proc. SIGGRAPH)</span>,
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">40</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S3.SS2.SSS1.p7.8" title="III-B1 Facial Expression Transition ‣ III-B The Proposed Method ‣ III Method ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">III-B</span>1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S3.SS2.SSS1.p8.12" title="III-B1 Facial Expression Transition ‣ III-B The Proposed Method ‣ III Method ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">III-B</span>1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S5.p1.1" title="V Conclusion ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§V</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib41">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. Grassal, M. Prinzler, T. Leistner, C. Rother, M. Nießner, and J. Thies</span><span class="ltx_text ltx_bib_year"> (2022)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Neural head avatars from monocular rgb videos</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 18653–18664</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S1.p2.1" title="I Introduction ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§I</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib65">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">X. Gu, C. Wen, W. Ye, J. Song, and Y. Gao</span><span class="ltx_text ltx_bib_year"> (2023)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Seer: language instructed video prediction with latent diffusion models</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">ICLR 2024</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S1.p3.1" title="I Introduction ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§I</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib52">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Guo, H. Manukyan, C. Yang, C. Wang, L. Khachatryan, S. Navasardyan, S. Song, H. Shi, and G. Huang</span><span class="ltx_text ltx_bib_year"> (2023)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">FaceCLIP: facial image-to-video translation via a brief text description</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">IEEE Transactions on Circuits and Systems for Video Technology</span>, <span class="ltx_text ltx_bib_pages"> pp. 1–1</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S1.p1.1" title="I Introduction ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§I</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib62">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. Hang, H. Yang, B. Liu, J. Fu, X. Geng, and B. Guo</span><span class="ltx_text ltx_bib_year"> (2023)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Language-guided face animation by recurrent stylegan-based generator</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">IEEE Transactions on Multimedia</span> <span class="ltx_text ltx_bib_volume">25</span> (<span class="ltx_text ltx_bib_number"></span>), <span class="ltx_text ltx_bib_pages"> pp. 1–12</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S1.p1.1" title="I Introduction ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§I</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S1.p3.1" title="I Introduction ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§I</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib21">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. He, X. Zhang, S. Ren, and J. Sun</span><span class="ltx_text ltx_bib_year"> (2016)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Deep Residual Learning for Image Recognition</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_series">CVPR ’16</span>, <span class="ltx_text ltx_bib_pages"> pp. 770–778</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.1109/CVPR.2016.90" title="">Document</a>,
<span class="ltx_text issn ltx_bib_external">ISSN 1063-6919</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS3.p2.2" title="IV-C Classification of Facial Expressions ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">IV-C</span></span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS7.SSS1.p1.1" title="IV-G1 Implementation ‣ IV-G Facial Expression Classification ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">IV-G</span>1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.T10.21.21.2.2" title="In IV-F3 Analysis ‣ IV-F Longer Facial Expression Generation with Neutral Expressions ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">TABLE X</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.T10.21.24.5.2" title="In IV-F3 Analysis ‣ IV-F Longer Facial Expression Generation with Neutral Expressions ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">TABLE X</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib6">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Z. Huang, K. C.K. Chan, Y. Jiang, and Z. Liu</span><span class="ltx_text ltx_bib_year"> (2023)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Collaborative diffusion for multi-modal face generation and editing</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S1.p1.1" title="I Introduction ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§I</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib8">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Hwang, J. Hyung, D. Kim, M. Kim, and J. Choo</span><span class="ltx_text ltx_bib_year"> (2023-10)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">FaceCLIPNeRF: text-driven 3d face manipulation using deformable neural radiance fields</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 3469–3479</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S1.p2.1" title="I Introduction ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§I</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S1.p3.1" title="I Introduction ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§I</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib10">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Jiang, Z. Huang, X. Pan, C. C. Loy, and Z. Liu</span><span class="ltx_text ltx_bib_year"> (2021)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Talk-to-edit: fine-grained facial editing via dialog</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S1.p1.1" title="I Introduction ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§I</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib37">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. Khakhulin, V. Sklyarova, V. Lempitsky, and E. Zakharov</span><span class="ltx_text ltx_bib_year"> (2022)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Realistic one-shot mesh-based head avatars</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">European Conference of Computer vision (ECCV)</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S1.p1.1" title="I Introduction ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§I</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S2.SS2.p1.1" title="II-B Face Rendering ‣ II Related Work ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">II-B</span></span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S3.SS2.SSS2.p2.3" title="III-B2 Face Rendering ‣ III-B The Proposed Method ‣ III Method ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">III-B</span>2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S3.SS2.SSS2.p3.4" title="III-B2 Face Rendering ‣ III-B The Proposed Method ‣ III Method ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">III-B</span>2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.F11" title="In IV-D Results ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">Figure 11</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.F11.3.2" title="In IV-D Results ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">Figure 11</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS2.p1.1" title="IV-B Implementation ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">IV-B</span></span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS4.SSS1.p4.1" title="IV-D1 Quantitative Comparison ‣ IV-D Results ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">IV-D</span>1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S5.p1.1" title="V Conclusion ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§V</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib5">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Kim, F. Liu, A. Jain, and X. Liu</span><span class="ltx_text ltx_bib_year"> (2023-06)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">DCFace: synthetic face generation with dual condition diffusion model</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 12715–12725</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S1.p1.1" title="I Introduction ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§I</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib49">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. P. Kingma and J. Ba</span><span class="ltx_text ltx_bib_year"> (2014)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Adam: a method for stochastic optimization</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">ICLR</span> <span class="ltx_text ltx_bib_volume">abs/1412.6980</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS2.p1.1" title="IV-B Implementation ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">IV-B</span></span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib38">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Lee, T. Choi, H. Go, H. Lee, S. Cho, and J. Kim</span><span class="ltx_text ltx_bib_year"> (2023)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Exp-gan: 3d-aware facial image generation with expression control</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Computer Vision – ACCV 2022: 16th Asian Conference on Computer Vision</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 151–167</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text isbn ltx_bib_external">ISBN 978-3-031-26292-0</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S2.SS2.p1.1" title="II-B Face Rendering ‣ II Related Work ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">II-B</span></span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib47">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. Li, T. Bolkart, M. J. Black, H. Li, and J. Romero</span><span class="ltx_text ltx_bib_year"> (2017)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Learning a model of facial shape and expression from 4d scans</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">ACM Transactions on Graphics (TOG)</span> <span class="ltx_text ltx_bib_volume">36</span>, <span class="ltx_text ltx_bib_pages"> pp. 1 – 17</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S1.p3.1" title="I Introduction ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§I</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S2.SS1.p1.1" title="II-A Facial Expression Transition ‣ II Related Work ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">II-A</span></span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S3.SS2.SSS1.p7.8" title="III-B1 Facial Expression Transition ‣ III-B The Proposed Method ‣ III Method ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">III-B</span>1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib4">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">X. Li, D. Zhang, M. Li, and D. Lee</span><span class="ltx_text ltx_bib_year"> (2023)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Accurate head pose estimation using image rectification and a lightweight convolutional neural network</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">IEEE Transactions on Multimedia</span> <span class="ltx_text ltx_bib_volume">25</span> (<span class="ltx_text ltx_bib_number"></span>), <span class="ltx_text ltx_bib_pages"> pp. 2239–2251</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS1.SSS2.p1.2" title="IV-A2 CelebV-HQ dataset ‣ IV-A Dataset ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">IV-A</span>2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib58">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. Lin, A. Trevithick, K. Cheng, M. Sarkis, M. Ghafoorian, N. Bi, G. Reitmayr, and R. Ramamoorthi</span><span class="ltx_text ltx_bib_year"> (2023)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">PVP: Personalized Video Prior for Editable Dynamic Portraits using StyleGAN</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Computer Graphics Forum</span> <span class="ltx_text ltx_bib_volume">42</span> (<span class="ltx_text ltx_bib_number">4</span>), <span class="ltx_text ltx_bib_pages"> pp. 13 pages</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S3.SS2.SSS1.p2.7" title="III-B1 Facial Expression Transition ‣ III-B The Proposed Method ‣ III Method ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">III-B</span>1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib3">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and I. Matthews</span><span class="ltx_text ltx_bib_year"> (2010)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The extended cohn-kanade dataset (ck+): a complete dataset for action unit and emotion-specified expression</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops</span>,
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume"></span>, <span class="ltx_text ltx_bib_pages"> pp. 94–101</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS1.SSS1.p1.1" title="IV-A1 CK+ dataset ‣ IV-A Dataset ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">IV-A</span>1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib50">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">H. Ma, T. Zhang, S. Sun, X. Yan, K. Han, and X. Xie</span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">CVTHead: one-shot controllable head avatar with vertex-feature transformer</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S1.p1.1" title="I Introduction ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§I</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S2.SS2.p1.1" title="II-B Face Rendering ‣ II Related Work ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">II-B</span></span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S3.SS2.SSS2.p3.4" title="III-B2 Face Rendering ‣ III-B The Proposed Method ‣ III Method ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">III-B</span>2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.F11" title="In IV-D Results ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">Figure 11</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.F11.3.2" title="In IV-D Results ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">Figure 11</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS2.p1.1" title="IV-B Implementation ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">IV-B</span></span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS4.SSS1.p4.1" title="IV-D1 Quantitative Comparison ‣ IV-D Results ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">IV-D</span>1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S5.p1.1" title="V Conclusion ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§V</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib13">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">N. Otberdout, C. Ferrari, M. Daoudi, S. Berretti, and A. Bimbo</span><span class="ltx_text ltx_bib_year"> (2023)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Generating complex 4d expression transitions by learning face landmark trajectories</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">IEEE Transactions on Affective Computing</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S1.p2.1" title="I Introduction ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§I</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S2.SS1.p1.1" title="II-A Facial Expression Transition ‣ II Related Work ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">II-A</span></span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S2.SS1.p2.1" title="II-A Facial Expression Transition ‣ II Related Work ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">II-A</span></span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS3.p1.1" title="IV-C Classification of Facial Expressions ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">IV-C</span></span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib55">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">N. Otberdout, C. Ferrari, M. Daoudi, S. Berritti, and A. Del Bimbo</span><span class="ltx_text ltx_bib_year"> (2022-06)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Sparse to dense dynamic 3d facial expression generation</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S2.SS1.p1.1" title="II-A Facial Expression Transition ‣ II Related Work ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">II-A</span></span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib11">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">F. Paraperas Papantoniou, P. P. Filntisis, P. Maragos, and A. Roussos</span><span class="ltx_text ltx_bib_year"> (2022)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Neural emotion director: speech-preserving semantic control of facial expressions in ”in-the-wild” videos</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S1.p2.1" title="I Introduction ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§I</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib66">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">O. Patashnik, Z. Wu, E. Shechtman, D. Cohen-Or, and D. Lischinski</span><span class="ltx_text ltx_bib_year"> (2021-10)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">StyleCLIP: text-driven manipulation of stylegan imagery</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 2085–2094</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S1.p3.1" title="I Introduction ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§I</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib63">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">B. Peng, H. Fan, W. Wang, J. Dong, and S. Lyu</span><span class="ltx_text ltx_bib_year"> (2022)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A unified framework for high fidelity face swap and expression reenactment</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">IEEE Transactions on Circuits and Systems for Video Technology</span> <span class="ltx_text ltx_bib_volume">32</span> (<span class="ltx_text ltx_bib_number">6</span>), <span class="ltx_text ltx_bib_pages"> pp. 3673–3684</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S1.p1.1" title="I Introduction ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§I</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib56">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Qian, T. Kirschstein, L. Schoneveld, D. Davoli, S. Giebenhain, and M. Nießner</span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">GaussianAvatars: photorealistic head avatars with rigged 3d gaussians</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S1.p2.1" title="I Introduction ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§I</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib43">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, <span class="ltx_text ltx_bib_etal">et al.</span></span><span class="ltx_text ltx_bib_year"> (2021)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Learning transferable visual models from natural language supervision</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">International conference on machine learning</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 8748–8763</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S3.SS2.SSS1.p2.7" title="III-B1 Facial Expression Transition ‣ III-B The Proposed Method ‣ III Method ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">III-B</span>1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib60">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Russell and J. M. F. dez Dols</span><span class="ltx_text ltx_bib_year"> (1997)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Reading emotions from and into faces: resurrecting dimensional-contextual perspective</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">The psychology of facial expression</span>, <span class="ltx_text ltx_bib_pages"> pp. 295–320</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">Cambridge University Press</span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS6.p1.1" title="IV-F Longer Facial Expression Generation with Neutral Expressions ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">IV-F</span></span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib70">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. R. Scherer</span><span class="ltx_text ltx_bib_year"> (2001)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Appraisal considered as a process of multilevel sequential checking.</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Oxford University Press</span>, <span class="ltx_text ltx_bib_pages"> pp. 92 – 120</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS6.p1.1" title="IV-F Longer Facial Expression Generation with Neutral Expressions ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">IV-F</span></span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib71">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Su, J. Zhu, L. Gao, and J. Song</span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Utilizing greedy nature for multimodal conditional image synthesis in transformers</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">IEEE Transactions on Multimedia</span> <span class="ltx_text ltx_bib_volume">26</span> (<span class="ltx_text ltx_bib_number"></span>), <span class="ltx_text ltx_bib_pages"> pp. 2354–2366</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S1.p1.1" title="I Introduction ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§I</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib67">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. Sutter, I. Daunhawer, and J. Vogt</span><span class="ltx_text ltx_bib_year"> (2020)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Multimodal generative learning utilizing jensen-shannon-divergence</span>.
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">33</span>, <span class="ltx_text ltx_bib_pages"> pp. 6100–6110</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S1.p3.1" title="I Introduction ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§I</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib68">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Suzuki and Y. Matsuo</span><span class="ltx_text ltx_bib_year"> (2022)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A survey of multimodal deep generative models</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Advanced Robotics</span> <span class="ltx_text ltx_bib_volume">36</span>, <span class="ltx_text ltx_bib_pages"> pp. 261 – 278</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S1.p3.1" title="I Introduction ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§I</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib31">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">G. Tevet, B. Gordon, A. Hertz, A. H. Bermano, and D. Cohen-Or</span><span class="ltx_text ltx_bib_year"> (2022)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Motionclip: exposing human motion generation to clip space</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Computer Vision–ECCV 2022: 17th European Conference</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 358–374</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S2.SS1.p1.1" title="II-A Facial Expression Transition ‣ II Related Work ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">II-A</span></span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS2.p1.1" title="IV-B Implementation ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">IV-B</span></span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib15">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Wu, S. Xu, J. Xiang, F. Wei, Q. Chen, J. Yang, and X. Tong</span><span class="ltx_text ltx_bib_year"> (2023)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">AniPortraitGAN: animatable 3d portrait generation from 2d image collections</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">SIGGRAPH Asia 2023 Conference Proceedings</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S1.p2.1" title="I Introduction ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§I</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib7">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">W. Xia, Y. Yang, J. Xue, and B. Wu</span><span class="ltx_text ltx_bib_year"> (2021)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">TediGAN: text-guided diverse face image generation and manipulation</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S1.p1.1" title="I Introduction ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§I</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib54">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang</span><span class="ltx_text ltx_bib_year"> (2018)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The unreasonable effectiveness of deep features as a perceptual metric</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">CVPR</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS4.SSS1.p4.1" title="IV-D1 Quantitative Comparison ‣ IV-D Results ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">IV-D</span>1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib35">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Zhang, Y. Li, lixiong Qin, X. Liu, and W. Deng</span><span class="ltx_text ltx_bib_year"> (2023)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Leave no stone unturned: mine extra knowledge for imbalanced facial expression recognition</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Thirty-seventh Conference on Neural Information Processing Systems</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS3.p2.2" title="IV-C Classification of Facial Expressions ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">IV-C</span></span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS7.SSS1.p1.1" title="IV-G1 Implementation ‣ IV-G Facial Expression Classification ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">IV-G</span>1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.T10.21.20.1.2" title="In IV-F3 Analysis ‣ IV-F Longer Facial Expression Generation with Neutral Expressions ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">TABLE X</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.T10.21.23.4.2" title="In IV-F3 Analysis ‣ IV-F Longer Facial Expression Generation with Neutral Expressions ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">TABLE X</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib2">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">H. Zhu, W. Wu, W. Zhu, L. Jiang, S. Tang, L. Zhang, Z. Liu, and C. C. Loy</span><span class="ltx_text ltx_bib_year"> (2022)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">CelebV-HQ: a large-scale video facial attributes dataset</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">ECCV</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS1.SSS2.p1.2" title="IV-A2 CelebV-HQ dataset ‣ IV-A Dataset ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">IV-A</span>2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib42">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">W. Zielonka, T. Bolkart, and J. Thies</span><span class="ltx_text ltx_bib_year"> (2023-06)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Instant volumetric head avatars</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR)</span>,
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/" title="">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S1.p1.1" title="I Introduction ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§I</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib12">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. Zou, S. Faisan, B. Yu, S. Valette, and H. Seo</span><span class="ltx_text ltx_bib_year"> (2023)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">4D facial expression diffusion model</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">2303.16611</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S1.p2.1" title="I Introduction ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§I</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S2.SS1.p1.1" title="II-A Facial Expression Transition ‣ II Related Work ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">II-A</span></span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S2.SS1.p2.1" title="II-A Facial Expression Transition ‣ II Related Work ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">II-A</span></span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08179v1#S4.SS3.p1.1" title="IV-C Classification of Facial Expressions ‣ IV Experiments ‣ Instruction-Driven 3D Facial Expression Generation and Transition"><span class="ltx_text ltx_ref_tag">§<span class="ltx_text">IV-C</span></span></a>.
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jan 13 03:04:03 2026 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>


<!-- ===== END HTML ===== -->

<!-- ===== ABS URL: https://arxiv.org/abs/2601.08151 | HTTP 200 ===== -->

<!DOCTYPE html>
<html lang="en">

<head>  <title>[2601.08151] Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="apple-touch-icon" sizes="180x180" href="/static/browse/0.3.4/images/icons/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/static/browse/0.3.4/images/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/static/browse/0.3.4/images/icons/favicon-16x16.png">
  <link rel="manifest" href="/static/browse/0.3.4/images/icons/site.webmanifest">
  <link rel="mask-icon" href="/static/browse/0.3.4/images/icons/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">
  <link rel="stylesheet" type="text/css" media="screen" href="/static/browse/0.3.4/css/arXiv.css?v=20241206" />
  <link rel="stylesheet" type="text/css" media="print" href="/static/browse/0.3.4/css/arXiv-print.css?v=20200611" />
  <link rel="stylesheet" type="text/css" media="screen" href="/static/browse/0.3.4/css/browse_search.css" />
  <script language="javascript" src="/static/browse/0.3.4/js/accordion.js" ></script>
  <script language="javascript" src="/static/browse/0.3.4/js/optin-modal.js?v=20250819"></script>
  
  <link rel="canonical" href="https://arxiv.org/abs/2601.08151"/>
  <meta name="description" content="Abstract page for arXiv paper 2601.08151: Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><meta property="og:type" content="website" />
<meta property="og:site_name" content="arXiv.org" />
<meta property="og:title" content="Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention" />
<meta property="og:url" content="https://arxiv.org/abs/2601.08151v1" />
<meta property="og:image" content="/static/browse/0.3.4/images/arxiv-logo-fb.png" />
<meta property="og:image:secure_url" content="/static/browse/0.3.4/images/arxiv-logo-fb.png" />
<meta property="og:image:width" content="1200" />
<meta property="og:image:height" content="700" />
<meta property="og:image:alt" content="arXiv logo"/>
<meta property="og:description" content="Multimodal Large Language Models (MLLMs) have achieved remarkable progress in vision-language understanding, yet how they internally integrate visual and textual information remains poorly understood. To bridge this gap, we perform a systematic layer-wise masking analysis across multiple architectures, revealing how visual-text fusion evolves within MLLMs. The results show that fusion emerges at several specific layers rather than being uniformly distributed across the network, and certain models exhibit a late-stage &#34;review&#34; phenomenon where visual signals are reactivated before output generation. Besides, we further analyze layer-wise attention evolution and observe persistent high-attention noise on irrelevant regions, along with gradually increasing attention on text-aligned areas. Guided by these insights, we introduce a training-free contrastive attention framework that models the transformation between early fusion and final layers to highlight meaningful attention shifts. Extensive experiments across various MLLMs and benchmarks validate our analysis and demonstrate that the proposed approach improves multimodal reasoning performance. Code will be released."/>
<meta name="twitter:site" content="@arxiv"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Where Does Vision Meet Language? Understanding and Refining Visual..."/>
<meta name="twitter:description" content="Multimodal Large Language Models (MLLMs) have achieved remarkable progress in vision-language understanding, yet how they internally integrate visual and textual information remains poorly..."/>
<meta name="twitter:image" content="https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png"/>
<meta name="twitter:image:alt" content="arXiv logo"/>
  <link rel="stylesheet" media="screen" type="text/css" href="/static/browse/0.3.4/css/tooltip.css"/><link rel="stylesheet" media="screen" type="text/css" href="https://static.arxiv.org/js/bibex-dev/bibex.css?20200709"/>  <script src="/static/browse/0.3.4/js/mathjaxToggle.min.js" type="text/javascript"></script>  <script src="//code.jquery.com/jquery-latest.min.js" type="text/javascript"></script>
  <script src="//cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js" type="text/javascript"></script>
  <script src="//cdn.jsdelivr.net/npm/dompurify@2.3.5/dist/purify.min.js"></script>
  <script src="/static/browse/0.3.4/js/toggle-labs.js?20241022" type="text/javascript"></script>
  <script src="/static/browse/0.3.4/js/cite.js" type="text/javascript"></script><meta name="citation_title" content="Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention" /><meta name="citation_author" content="Song, Shezheng" /><meta name="citation_author" content="Li, Shasha" /><meta name="citation_author" content="Yu, Jie" /><meta name="citation_date" content="2026/01/13" /><meta name="citation_online_date" content="2026/01/13" /><meta name="citation_pdf_url" content="https://arxiv.org/pdf/2601.08151" /><meta name="citation_arxiv_id" content="2601.08151" /><meta name="citation_abstract" content="Multimodal Large Language Models (MLLMs) have achieved remarkable progress in vision-language understanding, yet how they internally integrate visual and textual information remains poorly understood. To bridge this gap, we perform a systematic layer-wise masking analysis across multiple architectures, revealing how visual-text fusion evolves within MLLMs. The results show that fusion emerges at several specific layers rather than being uniformly distributed across the network, and certain models exhibit a late-stage &#34;review&#34; phenomenon where visual signals are reactivated before output generation. Besides, we further analyze layer-wise attention evolution and observe persistent high-attention noise on irrelevant regions, along with gradually increasing attention on text-aligned areas. Guided by these insights, we introduce a training-free contrastive attention framework that models the transformation between early fusion and final layers to highlight meaningful attention shifts. Extensive experiments across various MLLMs and benchmarks validate our analysis and demonstrate that the proposed approach improves multimodal reasoning performance. Code will be released." />
</head>

<body  class="with-cu-identity">
  
  
  <div class="flex-wrap-footer">
    <header>
      <a href="#content" class="is-sr-only">Skip to main content</a>
      <!-- start desktop header -->
      <div class="columns is-vcentered is-hidden-mobile" id="cu-identity">
        <div class="column" id="cu-logo">
          <a href="https://www.cornell.edu/"><img src="/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" /></a>
        </div><!-- /from April 7 at 1:00 AM to May 29 at 21:40 --><!-- /from May 2 at 1:00 AM to May 5 at 9:45 AM --><div class="column" id="support-ack">
          <span id="support-ack-url">We gratefully acknowledge support from the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors.</span>
          <a href="https://info.arxiv.org/about/donate.html" class="btn-header-donate">Donate</a>
        </div>
      </div>

      <div id="header" class="is-hidden-mobile">
<a aria-hidden="true" tabindex="-1" href="/IgnoreMe"></a>
  <div class="header-breadcrumbs is-hidden-mobile">
    <a href="/"><img src="/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg" alt="arxiv logo" style="height:40px;"/></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a> <span>&gt;</span> arXiv:2601.08151
  </div>

        <div class="columns is-vcentered is-mobile" style="justify-content: flex-end;">
        </div>

          <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
     </div><!-- /end desktop header -->

      <div class="mobile-header">
        <div class="columns is-mobile">
          <div class="column logo-arxiv"><a href="https://arxiv.org/"><img src="/static/browse/0.3.4/images/arxiv-logomark-small-white.svg" alt="arXiv logo" style="height:60px;" /></a></div>
          <div class="column logo-cornell"><a href="https://www.cornell.edu/">
            <picture>
              <source media="(min-width: 501px)"
                srcset="/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg  400w"
                sizes="400w" />
              <source srcset="/static/browse/0.3.4/images/icons/cu/cornell_seal_simple_black.svg 2x" />
              <img src="/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University Logo" />
            </picture>
          </a></div>
          <div class="column nav" id="toggle-container" role="menubar">
            <button class="toggle-control"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-white"><title>open search</title><path d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></button>
            <div class="mobile-toggle-block toggle-target">
              <form class="mobile-search-form" method="GET" action="https://arxiv.org/search">
                <div class="field has-addons">
                  <input class="input" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
                  <input type="hidden" name="source" value="header">
                  <input type="hidden" name="searchtype" value="all">
                  <button class="button">GO</button>
                </div>
              </form>
            </div>

            <button class="toggle-control"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-white" role="menu"><title>open navigation menu</title><path d="M16 132h416c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163 0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"/ ></svg></button>
            <div class="mobile-toggle-block toggle-target">
              <nav class="mobile-menu" aria-labelledby="mobilemenulabel">
                <h2 id="mobilemenulabel">quick links</h2>
                <ul>
                    <li><a href="https://arxiv.org/login">Login</a></li>
                    <li><a href="https://info.arxiv.org/help">Help Pages</a></li>
                    <li><a href="https://info.arxiv.org/about">About</a></li>
                </ul>
              </nav>
            </div>
          </div>
        </div>
      </div><!-- /end mobile-header -->
    </header>

    <main>
      <div id="content">
<!--
rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
         xmlns:dc="http://purl.org/dc/elements/1.1/"
         xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
    <rdf:Description
        rdf:about="/abs/2601.08151"
        dc:identifier="/abs/2601.08151"
        dc:title="Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"
        trackback:ping="/trackback/2601.08151" />
    </rdf:RDF>
--><div id="abs-outer">

  <div class="leftcolumn">
    <div class="subheader">
      <h1>Computer Science > Computer Vision and Pattern Recognition</h1>
    </div>

    <div class="header-breadcrumbs-mobile">
      <strong>arXiv:2601.08151</strong> (cs)
    </div>
<link rel="stylesheet" type="text/css" href="/static/base/1.0.1/css/abs.css">
<div id="content-inner">
  <div id="abs">
    <div class="dateline">
  [Submitted on 13 Jan 2026]</div>
    <h1 class="title mathjax"><span class="descriptor">Title:</span>Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention</h1>
    <div class="authors"><span class="descriptor">Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Song,+S" rel="nofollow">Shezheng Song</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+S" rel="nofollow">Shasha Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yu,+J" rel="nofollow">Jie Yu</a></div>            <div id="download-button-info" hidden>View a PDF of the paper titled Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention, by Shezheng Song and 2 other authors</div>
    <a class="mobile-submission-download" href="/pdf/2601.08151">View PDF</a>
    <a class="mobile-submission-download" href="https://arxiv.org/html/2601.08151v1">HTML (experimental)</a>



    <blockquote class="abstract mathjax">
            <span class="descriptor">Abstract:</span>Multimodal Large Language Models (MLLMs) have achieved remarkable progress in vision-language understanding, yet how they internally integrate visual and textual information remains poorly understood. To bridge this gap, we perform a systematic layer-wise masking analysis across multiple architectures, revealing how visual-text fusion evolves within MLLMs. The results show that fusion emerges at several specific layers rather than being uniformly distributed across the network, and certain models exhibit a late-stage &#34;review&#34; phenomenon where visual signals are reactivated before output generation. Besides, we further analyze layer-wise attention evolution and observe persistent high-attention noise on irrelevant regions, along with gradually increasing attention on text-aligned areas. Guided by these insights, we introduce a training-free contrastive attention framework that models the transformation between early fusion and final layers to highlight meaningful attention shifts. Extensive experiments across various MLLMs and benchmarks validate our analysis and demonstrate that the proposed approach improves multimodal reasoning performance. Code will be released.
    </blockquote>

    <!--CONTEXT-->
    <div class="metatable">
      <table summary="Additional metadata"><tr>
          <td class="tablecell label">Subjects:</td>
          <td class="tablecell subjects">
            <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Multimedia (cs.MM)</td>
        </tr><tr>
          <td class="tablecell label">Cite as:</td>
          <td class="tablecell arxivid"><span class="arxivid"><a href="https://arxiv.org/abs/2601.08151">arXiv:2601.08151</a> [cs.CV]</span></td>
        </tr>
        <tr>
          <td class="tablecell label">&nbsp;</td>
          <td class="tablecell arxividv">(or <span class="arxivid">
              <a href="https://arxiv.org/abs/2601.08151v1">arXiv:2601.08151v1</a> [cs.CV]</span> for this version)
          </td>
        </tr>
        <tr>
          <td class="tablecell label">&nbsp;</td>
          <td class="tablecell arxivdoi">              <a href="https://doi.org/10.48550/arXiv.2601.08151" id="arxiv-doi-link">https://doi.org/10.48550/arXiv.2601.08151</a><div class="button-and-tooltip">
              <button class="more-info" aria-describedby="more-info-desc-1">
                <svg height="15" role="presentation" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z" class=""></path></svg>
                <span class="visually-hidden">Focus to learn more</span>
              </button>
              <!-- tooltip description -->
              <div role="tooltip" id="more-info-desc-1">
                <span class="left-corner"></span>                  arXiv-issued DOI via DataCite (pending registration)</div>
            </div>
          </td>
        </tr></table>
    </div>
  </div>
</div>
    <div class="submission-history">
      <h2>Submission history</h2> From: Shezheng Song [<a href="/show-email/e8419b04/2601.08151" rel="nofollow">view email</a>]      <br/>    <strong>[v1]</strong>
        Tue, 13 Jan 2026 02:26:21 UTC (1,092 KB)<br/>
</div>
  </div>
  <!--end leftcolumn-->
<div class="extra-services">    <div class="full-text">
      <a name="other"></a>
      <span class="descriptor">Full-text links:</span>
      <h2>Access Paper:</h2>
      <ul>
  <div id="download-button-info" hidden>
View a PDF of the paper titled Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention, by Shezheng Song and 2 other authors</div><li><a href="/pdf/2601.08151" aria-describedby="download-button-info" accesskey="f" class="abs-button download-pdf">View PDF</a></li><li><a href="https://arxiv.org/html/2601.08151v1" class="abs-button" id="latexml-download-link">HTML (experimental)</a></li><li><a href="/src/2601.08151" class="abs-button download-eprint">TeX Source
 </a></li></ul>
      <div class="abs-license"><a href="http://creativecommons.org/licenses/by/4.0/" title="Rights to this article" class="has_license">
          <img alt="license icon" role="presentation" src="https://arxiv.org/icons/licenses/by-4.0.png"/>
          <span>view license</span>
        </a></div>
    </div>
    <!--end full-text-->    <div class="browse">
    Current browse context: <div class="current">cs.CV</div>

  <div class="prevnext">
<span class="arrow">
      <a class="abs-button prev-url" href="/prevnext?id=2601.08151&amp;function=prev&amp;context=cs.CV"
         accesskey="p" title="previous in cs.CV (accesskey p)" rel="nofollow">&lt;&nbsp;prev</a>
    </span>
    <span class="is-hidden-mobile">&nbsp; | &nbsp;</span>    <span class="arrow">
      <a class="abs-button next-url" href="/prevnext?id=2601.08151&amp;function=next&amp;context=cs.CV" accesskey="n"
         title="next in cs.CV (accesskey n)"  rel="nofollow">next&nbsp;&gt;</a>
    </span><br/>
  </div><div class="list">
    <a class="abs-button abs-button-grey abs-button-small context-new" href="/list/cs.CV/new"  rel="nofollow">new</a>
    <span class="is-hidden-mobile"> | </span>
    <a class="abs-button abs-button-grey abs-button-small context-recent" href="/list/cs.CV/recent" rel="nofollow">recent</a>
    <span class="is-hidden-mobile"> | </span><a class="abs-button abs-button-grey abs-button-small context-id" href="/list/cs.CV/2026-01" rel="nofollow">2026-01</a>
  </div><div class="abs-switch-cat">
    Change to browse by:
    <div class="switch context-change">
        <a href="/abs/2601.08151?context=cs" rel="nofollow">cs</a><br class="is-hidden-mobile">
        <a class="subclass" href="/abs/2601.08151?context=cs.MM" rel="nofollow">cs.MM</a><br class="is-hidden-mobile">
    </div>
  </div>

    </div>
      <div class="extra-ref-cite">
        <h3>References &amp; Citations</h3>
        <ul>
          <li><a  class="abs-button abs-button-small cite-ads" href="https://ui.adsabs.harvard.edu/abs/arXiv:2601.08151">NASA ADS</a></li><li><a  class="abs-button abs-button-small cite-google-scholar" href="https://scholar.google.com/scholar_lookup?arxiv_id=2601.08151" target="_blank" rel="noopener">Google Scholar</a></li>
          <li><a  class="abs-button abs-button-small cite-semantic-scholar" href="https://api.semanticscholar.org/arXiv:2601.08151" target="_blank" rel="noopener">Semantic Scholar</a></li>
        </ul>
        <div style="clear:both;"></div>
      </div>

<div class='extra-ref-cite'>
    <span id='bib-cite-trigger' class="bib-cite-button abs-button">export BibTeX citation</span>
    <span id='bib-cite-loading' hidden='true'>Loading...</span>
</div>

<div id='bib-cite-modal' class='bib-modal' hidden='true'>
    <div class='bib-modal-content'>
        <div class='bib-modal-title'>
            <h2>BibTeX formatted citation</h2>
            <span class='bib-modal-close' >&times;</span>
        </div>
        <div>
            <textarea id='bib-cite-target' class="bib-citation-content" aria-label="loading the citation">loading...</textarea>
        </div>
        <div>
            <span>Data provided by: </span>
            <a id='bib-cite-source-api'></a>
        </div>
    </div>
</div><div class="bookmarks">
  <div><h3>Bookmark</h3></div><a class="abs-button abs-button-grey abs-button-small" href="http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2601.08151&amp;description=Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"
     title="Bookmark on BibSonomy">
    <img src="/static/browse/0.3.4/images/icons/social/bibsonomy.png"
         alt="BibSonomy logo"/>
  </a>
  <a class="abs-button abs-button-grey abs-button-small" href="https://reddit.com/submit?url=https://arxiv.org/abs/2601.08151&amp;title=Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"
     title="Bookmark on Reddit">
    <img src="/static/browse/0.3.4/images/icons/social/reddit.png"
         alt="Reddit logo"/>
  </a>
</div>  </div>
  <!--end extra-services-->
<!-- LABS AREA -->
<div id="labstabs">
  <div class="labstabs"><input type="radio" name="tabs" id="tabone"checked="checked">
    <label for="tabone">Bibliographic Tools</label>
    <div class="tab labs-display-bib">
      <h1>Bibliographic and Citation Tools</h1>
      <div class="toggle">
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input id="bibex-toggle" type="checkbox" class="lab-toggle"
                     data-script-url="/static/browse/0.3.4/bibex/bibex.js?20241202">
              <span class="slider"></span>
              <span class="is-sr-only">Bibliographic Explorer Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-bibex">Bibliographic Explorer</span> <em>(<a href="https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer">What is the Explorer?</a>)</em>
          </div>
        </div>
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="connectedpapers-toggle"
                type="checkbox"
                class="lab-toggle"
                data-script-url="/static/browse/0.3.4/js/connectedpapers.js"
                aria-labelledby="label-for-connected-papers">
              <span class="slider"></span>
              <span class="is-sr-only">Connected Papers Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-connected-papers">Connected Papers</span> <em>(<a href="https://www.connectedpapers.com/about" target="_blank">What is Connected Papers?</a>)</em>
          </div>
        </div><div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="litmaps-toggle"
                type="checkbox"
                class="lab-toggle"
                data-script-url="/static/browse/0.3.4/js/litmaps.js?20210617"
                aria-labelledby="label-for-litmaps">
              <span class="slider"></span>
              <span class="is-sr-only">Litmaps Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-litmaps">Litmaps</span> <em>(<a href="https://www.litmaps.co/" target="_blank">What is Litmaps?</a>)</em>
          </div>
        </div>
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="scite-toggle"
                type="checkbox"
                class="lab-toggle"
                data-script-url="/static/browse/0.3.4/js/scite.js?20210617"
                aria-labelledby="label-for-scite">
              <span class="slider"></span>
              <span class="is-sr-only">scite.ai Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-scite">scite Smart Citations</span> <em>(<a href="https://www.scite.ai/" target="_blank">What are Smart Citations?</a>)</em>
          </div>
        </div>
      </div>
        <div class="labs-content-placeholder labs-display" style="display: none;"></div>
        <div style="min-height: 15px" id="connectedpapers-output"></div>
        <div style="min-height: 15px" id="litmaps-open-in"></div>
        <div style="min-height: 15px" id="scite-open-in"></div>
    </div>


    <input type="radio" name="tabs" id="tabtwo">
    <label for="tabtwo">Code, Data, Media</label>
    <div class="tab">
      <h1>Code, Data and Media Associated with this Article</h1>
      <div class="toggle">
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="alphaxiv-toggle"
                data-script-url="/static/browse/0.3.4/js/alphaxiv.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-alphaxiv">
              <span class="slider"></span>
              <span class="is-sr-only">alphaXiv Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-alphaxiv">alphaXiv</span> <em>(<a href="https://alphaxiv.org/" target="_blank">What is alphaXiv?</a>)</em>
          </div>
        </div>

        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input        
                id="catalyzex-toggle"
                data-script-url="/static/browse/0.3.4/js/catalyzex.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-cx">
              <span class="slider"></span>
              <span class="is-sr-only">Links to Code Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-cx">CatalyzeX Code Finder for Papers</span> <em>(<a href="https://www.catalyzex.com" target="_blank">What is CatalyzeX?</a>)</em>
          </div>
        </div>

        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="dagshub-toggle"
                data-script-url="/static/browse/0.3.4/js/dagshub.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-dagshub">
              <span class="slider"></span>
              <span class="is-sr-only">DagsHub Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-dagshub">DagsHub</span> <em>(<a href="https://dagshub.com/" target="_blank">What is DagsHub?</a>)</em>
          </div>
        </div>
  
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="gotitpub-toggle"
                data-script-url="/static/browse/0.3.4/js/gotitpub.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-gotitpub">
              <span class="slider"></span>
              <span class="is-sr-only">GotitPub Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-gotitpub">Gotit.pub</span> <em>(<a href="http://gotit.pub/faq" target="_blank">What is GotitPub?</a>)</em>
          </div>
        </div>

        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="huggingface-toggle"
                data-script-url="/static/browse/0.3.4/js/huggingface.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-huggingface">
              <span class="slider"></span>
              <span class="is-sr-only">Huggingface Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-huggingface">Hugging Face</span> <em>(<a href="https://huggingface.co/huggingface" target="_blank">What is Huggingface?</a>)</em>
          </div>
        </div>

        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="paperwithcode-toggle"
                data-script-url="/static/browse/0.3.4/js/paperswithcode.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-pwc">
              <span class="slider"></span>
              <span class="is-sr-only">Links to Code Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-pwc">Papers with Code</span> <em>(<a href="https://paperswithcode.com/" target="_blank">What is Papers with Code?</a>)</em>
          </div>
        </div>


        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="sciencecast-toggle"
                data-script-url="/static/browse/0.3.4/js/sciencecast.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-sciencecast">
              <span class="slider"></span>
              <span class="is-sr-only">ScienceCast Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-sciencecast">ScienceCast</span> <em>(<a href="https://sciencecast.org/welcome" target="_blank">What is ScienceCast?</a>)</em>
          </div>
        </div>
      </div>

      <div id="alphaxiv-output" style="display:none"></div>
      <div id="catalyzex-output" style="display:none"></div>
      <div id="dagshub-output" style="display:none"></div>
      <div id="gotitpub-output" style="display:none"></div>
      <div id="pwc-output" style="display:none"></div>
      <div id="pwc-data-output" style="display:none"></div>
      <div id="sciencecast-output" style="display:none"></div>
      <div id="huggingface-output" style="display:none"></div>
    </div>


      <input type="radio" name="tabs" id="labstabs-demos-input">
      <label for="labstabs-demos-input" id="labstabs-demos-label">Demos</label>
      <div class="tab">
        <h1>Demos</h1>
        <div class="toggle">
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input
                  id="replicate-toggle"
                  data-script-url="/static/browse/0.3.4/js/replicate.js"
                  type="checkbox" class="lab-toggle" aria-labelledby="label-for-replicate">
                <span class="slider"></span>
                <span class="is-sr-only">Replicate Toggle</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-replicate">Replicate</span> <em>(<a href="https://replicate.com/docs/arxiv/about" target="_blank">What is Replicate?</a>)</em>
            </div>
          </div>
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input
                  id="spaces-toggle"
                  data-script-url="/static/browse/0.3.4/js/spaces.js"
                  type="checkbox" class="lab-toggle" aria-labelledby="label-for-spaces">
                <span class="slider"></span>
                <span class="is-sr-only">Spaces Toggle</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-spaces">Hugging Face Spaces</span> <em>(<a href="https://huggingface.co/docs/hub/spaces" target="_blank">What is Spaces?</a>)</em>
            </div>
          </div>
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input
                  id="txyz-toggle"
                  data-script-url="/static/browse/0.3.4/js/txyz.js"
                  type="checkbox" class="lab-toggle" aria-labelledby="label-for-txyz">
                <span class="slider"></span>
                <span class="is-sr-only">Spaces Toggle</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-txyz">TXYZ.AI</span> <em>(<a href="https://txyz.ai" target="_blank">What is TXYZ.AI?</a>)</em>
            </div>
          </div>
        </div>
        <div id="replicate-output"></div>
        <div id="spaces-output"></div>
        <div id="txyz-output"></div>
      </div>
      <input type="radio" name="tabs" id="tabfour">
      <label for="tabfour">Related Papers</label>
      <div class="tab">
        <h1>Recommenders and Search Tools</h1>
        <div class="toggle">
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input id="influenceflower-toggle"
                data-script-url="/static/browse/0.3.4/js/influenceflower.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-influenceflower">
                <span class="slider"></span>
                <span class="is-sr-only">Link to Influence Flower</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-influenceflower">Influence Flower</span> <em>(<a href="https://influencemap.cmlab.dev/" target="_blank">What are Influence Flowers?</a>)</em>
            </div>
          </div>
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input id="core-recommender-toggle" type="checkbox" class="lab-toggle" aria-labelledby="label-for-core">
                <span class="slider"></span>
                <span class="is-sr-only">Core recommender toggle</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-core">CORE Recommender</span> <em>(<a href="https://core.ac.uk/services/recommender">What is CORE?</a>)</em>
            </div>
          </div></div>
        <div id="influenceflower-output"></div>
        <div id="influenceflower-output-graph" style="display:none">
          <ul class="flower-tabs">
            <li class="active"><a class="btn tab-btn" onclick="openTab(event, 'tab-author')">Author</a></li>
            <li><a class="btn tab-btn" onclick="openTab(event, 'tab-venue')">Venue</a></li>
            <li><a class="btn tab-btn" onclick="openTab(event, 'tab-inst')">Institution</a></li>
            <li><a class="btn tab-btn" onclick="openTab(event, 'tab-topic')">Topic</a></li>
          </ul>
          <div class="flower-tab-content">
            <div class="tab-flower active" id="tab-author"><svg id="flower-graph-author"></svg></div>
            <div class="tab-flower" id="tab-venue"><svg id="flower-graph-venue"></svg></div>
            <div class="tab-flower" id="tab-inst"><svg id="flower-graph-inst"></svg></div>
            <div class="tab-flower" id="tab-topic"><svg id="flower-graph-topic"></svg></div>
          </div>
        </div>
        <div id="coreRecommenderOutput"></div>
        <div id="iarxivOutput"></div>
      </div>

      <input type="radio" name="tabs" id="tabfive">
      <label for="tabfive">
        About arXivLabs
      </label>
      <div class="tab">
        <div class="columns">
          <div class="column">
            <h1>arXivLabs: experimental projects with community collaborators</h1>
            <p>arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.</p>
            <p>Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.</p>
            <p>Have an idea for a project that will add value for arXiv's community? <a href="https://info.arxiv.org/labs/index.html"><strong>Learn more about arXivLabs</strong></a>.</p>
          </div>
          <div class="column is-narrow is-full-mobile">
            <p class="icon-labs"><svg xmlns="http://www.w3.org/2000/svg" role="presentation" viewBox="0 0 635.572 811"><path d="M175.6 676v27h-27v-27zm-54 27v27h27v-27zm-27 27v27h27v-27zm396-54v27h-27v-27zm0 27v27h27v-27zm27 27v27h27v-27zm-27-414h27v27h-27zm27 0h27v-27h-27zm27-27h27v-27h-27zm-396 45h-27v-27h27zm-27-54h-27v27h27zm-27-27h-27v27h27z"/><path d="M94.6 730v27h-27v-27zm477 0v27h-27v-27zm-27-495h27v27h-27zm-450 18h-27v-27h27zm477 9h27v27h-27zm-54 495h27v27h-27zm-423 0h27v27h-27zm-54-504h27v27h-27z" fill="#666"/><path d="M67.6 730v27h-27v-27zm54 54v27h-27v-27zm0-108v27h27v-27zm-27 27v27h27v-27zm-81 0v27h27v-27zm585 27v27h-27v-27zm-108-54v27h27v-27zm27 27v27h27v-27zm81 0v27h27v-27zm-54-495h27v27h-27zm-54 108h27v-27h-27zm27-27h27v-27h-27zm0-81h27v-27h-27zm-423 18h-27v-27h27zm54 54h-27v27h27zm-27-27h-27v27h27zm0-81h-27v27h27zm423 612v27h-27v-27zm81-522v27h-27v-27zm-585-9v27h-27v-27z" fill="#999"/><path d="M94.6 784v27h-27v-27zm-27-27v27h27v-27zm-27-54v27h27v-27zm27 0v27h27v-27zm0-27v27h27v-27zm27 0v27h27v-27zm0-27v27h27v-27zm27 0v27h27v-27zm-108 81v27h27v-27zm558 54v27h-27v-27zm-27-27v27h27v-27zm27-54v27h27v-27zm-27 0v27h27v-27zm0-27v27h27v-27zm-27 0v27h27v-27zm0-27v27h27v-27zm-27 0v27h27v-27zm108 81v27h27v-27zm0-495h27v27h-27zm-27 27h27v-27h-27zm-54-27h27v-27h-27zm0 27h27v-27h-27zm-27 0h27v-27h-27zm0 27h27v-27h-27zm-27 0h27v-27h-27zm0 27h27v-27h-27zm81-108h27v-27h-27zm-504 45h-27v-27h27zm27-27h-27v27h27zm54-27h-27v27h27zm0 27h-27v27h27zm27 0h-27v27h27zm0 27h-27v27h27zm27 0h-27v27h27zm0 27h-27v27h27zm-81-108h-27v27h27z" fill="#ccc"/><path d="M598.6 665.1H41.5C-76.5 667 176 280.2 176 280.2h53a46.5 46.5 0 0162.8-56.3 29.2 29.2 0 1128.5 35.9h-1a46.5 46.5 0 01-1.5 20.3l142.5-.1s255.3 387 138.3 385.1zM291 181a29.3 29.3 0 10-29.2-29.3A29.3 29.3 0 00291 181zm65.4-66.8a22.4 22.4 0 10-22.5-22.4 22.4 22.4 0 0022.5 22.4z" fill="#fc0"/><path d="M245.5 172V10h153v162s324 495 198 495h-558c-126 0 207-495 207-495zm126 54h56m-13 72h56m-9 72h56m-20 72h56m-22 72h56m-29 72h56m-457-45c20.8 41.7 87.3 81 160.7 81 72.1 0 142.1-38.2 163.4-81" fill="none" stroke="#000" stroke-miterlimit="10" stroke-width="20"/><path d="M273.3 421.7c0 31-9.8 56.3-21.9 56.3s-21.8-25.2-21.8-56.3 9.8-56.3 21.8-56.3 21.9 25.2 21.9 56.3zm114.4-56.3c-12 0-21.8 25.2-21.8 56.3s9.7 56.3 21.8 56.3 21.9-25.2 21.9-56.3-9.8-56.3-21.9-56.3zM150.1 526.6c-18.2 6.7-27.5 22.9-23.2 30.2s14.8-5.5 33-12.2 37.4-4.9 33-12.2-24.5-12.6-42.8-5.8zm296 5.8c-4.2 7.3 14.9 5.5 33.1 12.2s28.7 19.5 33 12.2-5-23.5-23.2-30.2-38.5-1.5-42.8 5.8z"/></svg></p>
          </div>
        </div>
      </div>

    </div>
</div>
<!-- END LABS AREA -->
  <div class="endorsers">
    <a href="/auth/show-endorsers/2601.08151" class="endorser-who" rel="nofollow">Which authors of this paper are endorsers?</a> |
    <a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="https://info.arxiv.org/help/mathjax.html">What is MathJax?</a>)
    <span class="help" style="font-style: normal; float: right; margin-top: 0; margin-right: 1em;"></span>
  </div>
  <script type="text/javascript" language="javascript">mathjaxToggle();</script>
</div>
      </div>
    </main>

    <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://info.arxiv.org/about">About</a></li>
                <li><a href="https://info.arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
                <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
  </div>

  <script src="/static/base/1.0.1/js/member_acknowledgement.js"></script>

</body>

</html>

<!-- ===== END ABS ===== -->

<!-- ===== HTML URL: https://arxiv.org/html/2601.08151v1 | HTTP 200 ===== -->

<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention</title>
<!--Generated on Tue Jan 13 02:26:40 2026 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="/static/browse/0.3.4/css/arxiv-html-papers-20250916.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2601.08151v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S1" title="In Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S2" title="In Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S3" title="In Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Exploration of Visual Information Fusion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S3.SS1" title="In 3 Exploration of Visual Information Fusion ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Layer-wise Visual Information Masking</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S3.SS2" title="In 3 Exploration of Visual Information Fusion ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Analysis and Observations</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S3.SS2.SSS0.Px1" title="In 3.2 Analysis and Observations ‣ 3 Exploration of Visual Information Fusion ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_title">Layer-wise Differences in Visual-Language Fusion.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S3.SS2.SSS0.Px2" title="In 3.2 Analysis and Observations ‣ 3 Exploration of Visual Information Fusion ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_title">Identifying Critical Fusion Layers.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S4" title="In Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S4.SS1" title="In 4 Method ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Selecting Contrastive Layers</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S4.SS2" title="In 4 Method ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Contrastive Attention for Review</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5" title="In Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.SS1" title="In 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Datasets and Setting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.SS2" title="In 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Comparison with MLLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.SS3" title="In 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Comparison with Training-free Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.SS4" title="In 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Selection of post-integrated layer</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.SS4.SSS0.Px1" title="In 5.4 Selection of post-integrated layer ‣ 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_title">1. Where and how does contrastive attention function?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.SS4.SSS0.Px2" title="In 5.4 Selection of post-integrated layer ‣ 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_title">2. Why is layer 28 selected as the post-integrated layer?</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.SS5" title="In 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Selection of Pre-Integrated Layers</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.SS6" title="In 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6 </span>Masking Ratio Based on Contrastive Attention</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S6" title="In Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#Sx1.SS0.SSS0.Px1" title="In Reproducibility Checklist ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_title">1. General Paper Structure</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#Sx1.SS0.SSS0.Px2" title="In Reproducibility Checklist ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_title">2. Theoretical Contributions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#Sx1.SS0.SSS0.Px3" title="In Reproducibility Checklist ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_title">3. Dataset Usage</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#Sx1.SS0.SSS0.Px4" title="In Reproducibility Checklist ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_title">4. Computational Experiments</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Where Does Vision Meet Language? 
<br class="ltx_break"/>Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Shezheng Song, Shasha Li, Jie Yu
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Recent advances in Multimodal Large Language Models (MLLMs) have achieved impressive results in vision-language tasks. However, the internal mechanism of how visual information is integrated across layers remains poorly understood. In this work, we investigate the hierarchical process of visual integration in MLLMs through a series of layer-wise masking experiments.
Our findings reveal that vision-language fusion primarily occurs at several shallow layers.
We also discover a review-like behavior at a deep layer, where the model re-attends to the image before producing the final output.
We further analyze attention distributions and uncover a systematic bias: irrelevant image regions often receive consistently high attention across layers, resulting in noisy and suboptimal final predictions.
To address this issue, we propose a training-free method that leverages contrastive attention, defined as the difference in attention maps between a pre-integrated layer and a post-integrated layer. This captures the evolving focus of the model as it integrates visual information. We apply the contrastive attention at the review layer to selectively mask irrelevant image regions, guiding the model to attend more effectively to task-relevant content without requiring additional training.
Extensive experiments on multiple multimodal benchmarks demonstrate that our method significantly boosts performance, achieving new state-of-the-art results on the LLaVA series.
The code will be released.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In recent years, Multimodal Large Language Models (MLLMs) <cite class="ltx_cite ltx_citemacro_citep">(Liu<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib10" title="Visual instruction tuning">2023</a>; Bai<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib8" title="Qwen-vl: a versatile vision-language model for understanding, localization, text reading, and beyond">2023</a>)</cite> have shown strong performance on tasks such as visual question answering <cite class="ltx_cite ltx_citemacro_citep">(Goyal<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib16" title="Making the v in vqa matter: elevating the role of image understanding in visual question answering">2017</a>; Marino<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib18" title="Ok-vqa: a visual question answering benchmark requiring external knowledge">2019</a>)</cite>. However, how information flows between layers in MLLMs, especially how visual signals are gradually integrated and utilized across layers, has not been fully explored. This gap in understanding limits the development of effective training strategies and model architectures.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">To address this gap, we focus on the internal process of visual-textual information fusion: At which layers is visual information integrated, and how does integration in each layer affect task performance?
To explore this, we design a series of layer-wise masking experiments to systematically evaluate the role of each layer in cross-modal fusion. Specifically, we apply masking to visual feature at different layers and observe the resulting changes in model performance and inference speed. A significant performance drop after masking indicates that the corresponding layer plays a critical role in visual integration, whereas a minimal impact suggests a weaker dependence on visual information. This approach provides a functional perspective on the hierarchical dynamics of vision-language fusion in MLLMs.
Our experiments show that visual-text fusion mainly happens at several shallow layers. Masking visual inputs at these layers causes performance to drop nearly to zero, highlighting their role as <span class="ltx_text ltx_font_bold" id="S1.p2.1.1">fusion layers</span>. This effect is consistent across different datasets and MLLMs, showing stable layer-wise patterns. As depth increases, masking has less impact, suggesting that visual information has already been integrated. Notably, a sharp drop reappears at a late layer (e.g., layer 29), indicating the model briefly revisits the image before producing the final output. We call this a <span class="ltx_text ltx_font_bold" id="S1.p2.1.2">review mechanism</span>, and the corresponding layer the review layer.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In addition to analyzing the functional roles of each layer through masking, we further investigate how visual attention is distributed across layers. This analysis reveals a fundamental issue in current MLLMs:
<span class="ltx_text ltx_font_bold" id="S1.p3.1.1">Systematic bias in visual attention allocation</span>. As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">1</span></a>, the final layer often fails to accurately attend to regions in the image that are truly relevant to the question. Prior work such as <cite class="ltx_cite ltx_citemacro_citet">Zhang<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib1" title="MLLMs know where to look: training-free perception of small visual details with multimodal llms">2025</a>)</cite> has noted this limitation and attempted to address it by directly using attention maps from a fixed intermediate layer (e.g., layer 14 in LLaVA <cite class="ltx_cite ltx_citemacro_citep">(Liu<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib10" title="Visual instruction tuning">2023</a>)</cite>, layer 15 in InstructBLIP <cite class="ltx_cite ltx_citemacro_citep">(Dai<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib6" title="InstructBLIP: towards general-purpose vision-language models with instruction tuning">2023</a>)</cite>). However, relying on a single fixed layer is both rigid and suboptimal, as it overlooks the dynamic nature of visual reasoning across tasks and model architectures.
Furthermore, we observe that some irrelevant image regions consistently receive high attention across layers, a phenomenon we refer to as <span class="ltx_text ltx_font_bold" id="S1.p3.1.2">high-attention noise</span>. These regions tend to be activated in early layers and remain prominent in later ones. For example, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">1</span></a>, the yellow dashed region maintains high attention values from shallow to deep layers, despite its irrelevance to the question. This layer-to-layer consistency suggests that early attention biases may propagate through the network and contribute to the suboptimal final attention patterns.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="229" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Contrastive attention (pre-integrated <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S1.F1.2.m1" intent=":literal"><semantics><mo stretchy="false">→</mo><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math> last layer) showing focus shift and persistent high-attention noise.</figcaption>
</figure>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Recent work such as DoLa <cite class="ltx_cite ltx_citemacro_citep">(Chuang<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib2" title="Dola: decoding by contrasting layers improves factuality in large language models">2024</a>)</cite> has demonstrated that contrasting internal representations across layers can better reveal what the model truly learns. Specifically, DoLa computes the difference between the logits of early and later layers, producing <span class="ltx_text ltx_font_italic" id="S1.p4.1.1">contrastive logits</span> that better reflect the model’s factual knowledge compared to relying solely on the final layer output. Inspired by this idea, we extend the notion of inter-layer contrast from output space to the attention space.
In particular, we observe that as MLLMs process inputs layer by layer, their attention gradually shifts from task-irrelevant regions to more semantically meaningful areas. This shift reflects how the model integrates visual cues in a progressive manner. Shallow layers tend to contain noisy or indiscriminate attention, while deeper layers refine this focus toward task-relevant content. We refer to this evolving focus as <span class="ltx_text ltx_font_bold" id="S1.p4.1.2">contrastive attention</span>, which captures not the static attention at any single layer, but the transformation in attention patterns across layers that <span class="ltx_text ltx_font_italic" id="S1.p4.1.3">reveals what the model incrementally learns to attend to</span>.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To quantify the contrastive attention, we introduce two layer concepts:
(a) <span class="ltx_text ltx_font_italic" id="S1.p5.1.1">the post-integrated layer</span>, where the model has already combined visual and linguistic information and formed a representation capable of solving the task. We identify the post-integrated layer as the one immediately preceding the review layer (see <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.SS4" title="5.4 Selection of post-integrated layer ‣ 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">5.4</span></a> for detailed exploration).
(b) <span class="ltx_text ltx_font_italic" id="S1.p5.1.2">the pre-integrated layer</span>, which represents the initial perception of the image before substantial semantic fusion occurs. The pre-integrated layer is selected from the identified fusion layers as the one whose attention map exhibits the largest Hellinger distance from the post-integrated layer, indicating the point before major vision-language integration takes place (see <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.SS5" title="5.5 Selection of Pre-Integrated Layers ‣ 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">5.5</span></a>).</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Based on the above analysis, we introduce a training-free approach to enhance vision-language understanding of MLLMs.
Specifically, we compute the contrastive attention by comparing attention maps between the pre-integrated layer and the post-integrated layer. It captures the actual contribution of visual information as it flows through the model, revealing how visual semantics are progressively integrated. Moreover, it effectively suppresses spurious high-attention noise in later layers.
Then we apply contrastive attention at review layer to mask visual regions. This selective masking reduces the influence of irrelevant content and enables the model to concentrate more effectively on task-relevant areas without additional training.
Experiments show that our method significantly improves the performance of the widely used LLaVA series across six visual question answering benchmarks, achieving state-of-the-art results. Our contributions are as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We conduct a systematic investigation into how visual information is integrated across layers in MLLMs. By applying layer-wise visual masking, we identify key visual-textual fusion layers and a review-like mechanism.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We propose a training-free method that exploits contrastive attention, the divergence between pre-integrated and post-integrated attention maps, to refine visual focus.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Our method consistently outperforms existing techniques and achieves state-of-the-art results across multiple multimodal benchmarks, validating its generality and effectiveness.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Early efforts to understand how information flows within language models primarily focused on transformers in unimodal settings. For example, studies like <cite class="ltx_cite ltx_citemacro_citep">(Cao<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib35" title="Behind the scene: revealing the secrets of pre-trained vision-and-language models">2020</a>; Frank<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib36" title="Vision-and-language or vision-for-language? on cross-modal influence in multimodal transformers">2021</a>)</cite> analyzed how attention mechanisms propagate syntactic and semantic information in transformers, while others <cite class="ltx_cite ltx_citemacro_citep">(Aflalo<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib37" title="Vl-interpret: an interactive visualization tool for interpreting vision-language transformers">2022</a>; Chefer<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib38" title="Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers">2021</a>; Lyu<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib39" title="Dime: fine-grained interpretations of multimodal models via disentangled local explanations">2022</a>; Stan<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib40" title="LVLM-interpret: an interpretability tool for large vision-language models">2024</a>)</cite> explored the roles of feed-forward networks and memory representation in information transformation. These works provided foundational insights into token-level dependency and hierarchical abstraction but largely focused on language-only settings.
Building on this, recent studies examined how LLMs acquire knowledge across layers. <cite class="ltx_cite ltx_citemacro_citet">Lin<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib42" title="Multi-layer visual feature fusion in multimodal llms: methods, analysis, and best practices">2025</a>)</cite> investigates the impact of visual fusion positions on MLLM performance, comparing external and internal integration strategies for incorporating visual information. <cite class="ltx_cite ltx_citemacro_citet">Jin<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib3" title="Exploring concept depth: how large language models acquire knowledge and concept at different layers?">2024</a>)</cite> introduced the concept of “depth” in reasoning, and <cite class="ltx_cite ltx_citemacro_citet">Ju<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib41" title="How large language models encode context knowledge? a layer-wise probing study">2024</a>)</cite> showed context is unevenly distributed across layers.</p>
</div>
<figure class="ltx_figure" id="S2.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F2.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="507" id="S2.F2.1.g1" src="x2.png" width="969"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="515" id="S2.F2.2.g1" src="x3.png" width="968"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F2.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="515" id="S2.F2.3.g1" src="x4.png" width="968"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F2.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="507" id="S2.F2.4.g1" src="x5.png" width="969"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F2.5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="515" id="S2.F2.5.g1" src="x6.png" width="967"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F2.6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="515" id="S2.F2.6.g1" src="x7.png" width="968"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Layer-wise masking effects on accuracy and inference time across multimodal datasets using LLaVA-1.5. Acc is the performance after completely masking the visual information at the corresponding layer.</figcaption>
</figure>
<figure class="ltx_figure" id="S2.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F3.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="516" id="S2.F3.1.g1" src="x8.png" width="967"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F3.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="515" id="S2.F3.2.g1" src="x9.png" width="967"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F3.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="516" id="S2.F3.3.g1" src="x10.png" width="967"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F3.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="516" id="S2.F3.4.g1" src="x11.png" width="967"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F3.5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="525" id="S2.F3.5.g1" src="x12.png" width="969"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F3.6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="516" id="S2.F3.6.g1" src="x13.png" width="967"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Layer-wise masking effects on accuracy and inference time across multimodal datasets using LLaVA-1.6.</figcaption>
</figure>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">A growing body of work has recently turned to the interpretability of MLLMs. For example, <cite class="ltx_cite ltx_citemacro_citet">Palit<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib44" title="Towards vision-language mechanistic interpretability: a causal tracing tool for blip">2023</a>)</cite> extended causal tracing methods to MLLMs and showed that visual information progressively influences generation in later layers. <cite class="ltx_cite ltx_citemacro_citet">Zhao<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib43" title="The first to know: how token distributions reveal hidden knowledge in large vision-language models?">2024</a>)</cite> surveyed various interpretability techniques for MLLMs, categorizing insights at input, token, and attention levels. However, most of these efforts focus on saliency or input-output attribution and rarely dissect the internal fusion mechanism of visual features across layers.
Despite these efforts, what remains largely missing is a comprehensive understanding of how visual information is gradually injected and transformed across different layers within MLLMs. Existing works often rely on single-layer attention analysis or fixed fusion strategies, overlooking the progressive, multi-stage nature of visual integration.
A comprehensive understanding of how visual information is progressively injected across layers in MLLMs is still lacking. This gap limits both interpretability and usage of MLLMs in complex vision-language tasks.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Exploration of Visual Information Fusion</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Layer-wise Visual Information Masking</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">To better understand how multimodal information flows and is integrated across layers in MLLMs, we conduct a series of layer-wise image masking experiments on the LLaVA series.
Specifically, we follow the input format used in LLaVA and identify the position of image tokens based on the special marker <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p1.1.1">&lt;image&gt;</span>. In LLaVA, which uses a ViT-based image encoder, the number of image tokens is 576, allowing us to locate the image segment within the input sequence.
Thus, we apply a masking operation to the image tokens at each layer by directly setting their embeddings to zero, effectively removing visual information while preserving the input shape.
We then assess how each layer depends on image features by measuring performance changes across datasets. A significant drop in performance after masking indicates that the layer relies heavily on image for multimodal integration. The results are shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S2.F2" title="Figure 2 ‣ 2 Related Work ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">2</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S2.F3" title="Figure 3 ‣ 2 Related Work ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Analysis and Observations</h3>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Layer-wise Differences in Visual-Language Fusion.</h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.1">Our results show that masking visual information at shallow layers (layers 0–4) leads to a sharp performance drop across all datasets. This suggests that image features are not yet integrated into the multimodal representations at early layers. Removing image at these layers deprives the model of essential visual cues, severely affecting its output.
As the layer depth increases, the model begins to incorporate visual information gradually. The layer at which this integration completes varies by dataset:
(1) For simple tasks like VQAv2, image-text fusion appears to complete earlier, as performance stabilizes after around layer 16.
(2) For complex tasks such as OKVQA, the integration occurs at deeper layers, suggesting that more abstract reasoning requires longer propagation of visual context.
In summary, masking the image after layer 19 results in no substantial performance degradation, indicating that the visual semantics have already been sufficiently integrated.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Identifying Critical Fusion Layers.</h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p1.1">In the layer-wise masking experiments, we find that masking visual information at certain specific layers leads to a dramatic drop in model performance, sometimes approaching zero. Notably, these critical layers show strong consistency across different datasets and across various MLLMs, indicating that the same set of layers consistently exhibits such behavior under different settings.
Furthermore, we observe that masking at these layers not only degrades performance but also significantly increases inference time (measured as <math alttext="1/t" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p1.1.m1" intent=":literal"><semantics><mrow><mn>1</mn><mo>/</mo><mi>t</mi></mrow><annotation encoding="application/x-tex">1/t</annotation></semantics></math>). We hypothesize that this results from the disruption of efficient reasoning pathways within the model.
These critical layers could be categorized into two groups:</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">Early fusion layers (<math alttext="\mathcal{S}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.1.m1" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒮</mi><annotation encoding="application/x-tex">\mathcal{S}</annotation></semantics></math> = 2, 4, 8, 11, 12, 13): These critical layers are mostly located in the shallow stages of the model, where visual information has not yet been fully fused with textual input. Masking at these layers removes essential visual cues, leading to a sharp performance drop, indicating active fusion is still in progress.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">Review layer (29):
After the early fusion layers, model performance stabilizes and becomes largely insensitive to visual masking, suggesting that fusion is mostly complete. However, at layer 29, masking again causes a significant drop, resembling the behavior of early fusion layers.
We hypothesize that this layer performs a “<span class="ltx_text ltx_font_italic" id="S3.I1.i2.p1.1.1">final visual check</span>” before the model generates its output. This mirrors human behavior, where one might glance back at the image one last time before making a decision.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p2">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p2.1">The experiment reveals key phenomena: hierarchical differences in visual information fusion and the presence of critical fusion layer. Specifically, we observe:
<span class="ltx_text ltx_font_bold" id="S3.SS2.SSS0.Px2.p2.1.1">(1)</span> A set of shallow layers serve as essential <span class="ltx_text ltx_font_bold" id="S3.SS2.SSS0.Px2.p2.1.2">early fusion layers</span> for integrating multimodal information.
<span class="ltx_text ltx_font_bold" id="S3.SS2.SSS0.Px2.p2.1.3">(2)</span> At a deep layer (review layer), the LLM exhibits a <span class="ltx_text ltx_font_bold" id="S3.SS2.SSS0.Px2.p2.1.4">review-like</span> behavior, where visual information is revisited even after the integration process has been completed. This suggests that the model still relies on visual cues for final decision verification.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Method</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S4.F4" title="Figure 4 ‣ 4 Method ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">4</span></a>, we compute the <span class="ltx_text ltx_font_bold" id="S4.p1.1.1">contrastive attention</span> by selecting appropriate pre-integrated and post-integrated layers, in <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S4.SS1" title="4.1 Selecting Contrastive Layers ‣ 4 Method ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">4.1</span></a>. The contrastive attention is defined as the difference in attention distribution, capturing how attention shifts as visual information is fused.
Then, in <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S4.SS2" title="4.2 Contrastive Attention for Review ‣ 4 Method ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">4.2</span></a>, we leverage the contrastive attention to guide the <span class="ltx_text ltx_font_bold" id="S4.p1.1.2">review</span> process, helping suppress irrelevant information and focus on content that is more relevant to the task.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="1010" id="S4.F4.g1" src="x14.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>overview of contrastive attention and review-stage masking.</figcaption>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Selecting Contrastive Layers</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">To investigate how the attention shifts during the process of task understanding, we explore two contrastive layers: the post-integrated layer and the pre-integrated layer. The post-integrated layer represents the stage at which task semantics and visual information have been fully fused, while the pre-integrated layer captures the initial perception of the image. By comparing attention maps at these two layers, we aim to reveal which image regions become increasingly aligned with task objectives.
We begin by locating the post-integrated layer. Experiment shows that review behavior occurs at layer 29. Layer 28 masking has minimal impact on performance, indicating that the fusion of visual features is largely complete. Thus, we designate layer 28 as the post-integrated layer.
</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.2">MLLMs form an initial, task-agnostic perception of the image at shallow layers, which may lead to high-attention noise unrelated to task. To mitigate this, we propose contrastive attention, which captures the difference between the attention maps of the post-integrated layer and a pre-integrated layer. This difference is designed to suppress irrelevant high-attention regions and enhance task-relevant visual focus.
We compute the Hellinger distance between each layer’s attention map and that of the post-integrated layer, and select the one with the highest distance as the pre-integrated layer.
We explore multiple strategies to define the candidate set <math alttext="\mathcal{C}" class="ltx_Math" display="inline" id="S4.SS1.p2.1.m1" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒞</mi><annotation encoding="application/x-tex">\mathcal{C}</annotation></semantics></math> for selecting the pre-integrated layer: (a) without any constraint, (b) constrained to shallow layers (layers 1–16), (c) constrained to deep layers (layers 17–28), and (d) constrained to a predefined early fusion set <math alttext="\mathcal{S}" class="ltx_Math" display="inline" id="S4.SS1.p2.2.m2" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒮</mi><annotation encoding="application/x-tex">\mathcal{S}</annotation></semantics></math> identified through our analysis. Details are provided in Table <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.T3" title="Table 3 ‣ 5.3 Comparison with Training-free Methods ‣ 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">3</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.SS5" title="5.5 Selection of Pre-Integrated Layers ‣ 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">5.5</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Effectiveness of our method across multiple MLLM baselines.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" id="S4.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.1">MLLMs</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.1.2">GQA</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.1.3">VQAv2</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.1.4">OKVQA</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.1.5">VizWiz</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.1.6">TextVQA</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.1.7">DocVQA</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.1.8"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.8.1">Average</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.2.2.1">BLIP-2 <cite class="ltx_cite ltx_citemacro_citep">(Li<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib4" title="Blip-2: bootstrapping language-image pre-training with frozen image encoders and large language models">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.2.2">41.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.2.3">41.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.2.4">45.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.2.5">19.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.2.6">42.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.2.7">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.2.8">38.00</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.3.3.1">Flamingo-80B <cite class="ltx_cite ltx_citemacro_citep">(Alayrac<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib5" title="Flamingo: a visual language model for few-shot learning">2022</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.3.2">43.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.3.3">56.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.3.4">50.6</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.3.5">31.6</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.3.6">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.3.7">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.3.8">45.45</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.4.4.1">InstructBILP-8B <cite class="ltx_cite ltx_citemacro_citep">(Dai<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib6" title="InstructBLIP: towards general-purpose vision-language models with instruction tuning">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.4.2">49.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.4.3">51.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.4.4">49.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.4.5">34.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.4.6">50.1</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.4.7">9.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.4.8">40.80</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.5.5.1">IDEFICS <cite class="ltx_cite ltx_citemacro_citep">(Laurençon<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib7" title="Obelics: an open web-scale filtered dataset of interleaved image-text documents">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.5.2">38.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.5.3">50.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.5.4">38.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.5.5">35.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.5.6">25.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.5.7">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.5.8">37.82</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.6.6.1">LLaVA <cite class="ltx_cite ltx_citemacro_citep">(Liu<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib10" title="Visual instruction tuning">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.6.2">49.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.6.3">63.6</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.6.4">45.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.6.5">30.7</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.6.6">38.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.6.7">10.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.6.8">39.72</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.7.7.1">InternVL-MLP <cite class="ltx_cite ltx_citemacro_citep">(Chen<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib26" title="Internvl: scaling up vision foundation models and aligning for generic visual-linguistic tasks">2024</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.7.2">62.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.7.3">79.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.7.4">42.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.7.5">52.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.7.6">57.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.7.7">23.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.7.8">52.97</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.8.8.1">InternVL-QLLaMA <cite class="ltx_cite ltx_citemacro_citep">(Chen<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib26" title="Internvl: scaling up vision foundation models and aligning for generic visual-linguistic tasks">2024</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.8.2">57.7</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.8.3">72.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.8.4">51.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.8.5">44.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.8.6">42.1</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.8.7">25.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.8.8">48.77</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.9.9.1">Qwen-VL <cite class="ltx_cite ltx_citemacro_citep">(Bai<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib8" title="Qwen-vl: a versatile vision-language model for understanding, localization, text reading, and beyond">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.9.9.2">59.3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.9.9.3">78.8</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.9.9.4">46.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.9.9.5">35.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.9.9.6">57.2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.9.9.7">25.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.9.9.8">50.49</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.10.10.1">Qwen2-VL <cite class="ltx_cite ltx_citemacro_citep">(Wang<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib33" title="Qwen2-vl: enhancing vision-language model’s perception of the world at any resolution">2024</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.10.10.2">63.7</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.10.10.3">79.8</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.10.10.4">48.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.10.10.5">60.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.10.10.6">71.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.10.10.7">59.8</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.10.10.8">64.02</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.11.11.1">
<span class="ltx_text ltx_font_italic" id="S4.T1.1.11.11.1.1">LLaVA-v1.5 </span> <cite class="ltx_cite ltx_citemacro_citep">(Liu<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib10" title="Visual instruction tuning">2023</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.11.11.2">66.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.11.11.3">74.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.11.11.4">51.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.11.11.5">57.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.11.11.6">57.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.11.11.7">24.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.11.11.8">55.19</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.12.12.1"><span class="ltx_text ltx_font_italic" id="S4.T1.1.12.12.1.1">+ <span class="ltx_text ltx_font_bold" id="S4.T1.1.12.12.1.1.1">Ours</span></span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.12.12.2">69.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.12.12.3">77.6</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.12.12.4">55.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.12.12.5">60.4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.12.12.6">59.8</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.12.12.7">26.9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.12.12.8">58.25</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.13.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.13.13.1">
<span class="ltx_text ltx_font_italic" id="S4.T1.1.13.13.1.1">LLaVA-v1.6 </span> <cite class="ltx_cite ltx_citemacro_citep">(Liu<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib9" title="Improved baselines with visual instruction tuning">2024</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.13.13.2">69.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.13.13.3">79.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.13.13.4">52.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.13.13.5">59.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.13.13.6">72.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.13.13.7">65.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.13.13.8">66.52</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.14.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T1.1.14.14.1"><span class="ltx_text ltx_font_italic" id="S4.T1.1.14.14.1.1">+ <span class="ltx_text ltx_font_bold" id="S4.T1.1.14.14.1.1.1">Ours</span></span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.14.14.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.14.14.2.1">71.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.14.14.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.14.14.3.1">80.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.14.14.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.14.14.4.1">56.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.14.14.5"><span class="ltx_text ltx_font_bold" id="S4.T1.1.14.14.5.1">62.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.14.14.6"><span class="ltx_text ltx_font_bold" id="S4.T1.1.14.14.6.1">75.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.14.14.7"><span class="ltx_text ltx_font_bold" id="S4.T1.1.14.14.7.1">68.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.14.14.8"><span class="ltx_text ltx_font_bold" id="S4.T1.1.14.14.8.1">69.18</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.9">We formalize the selection process as follows: given a set of candidate attention matrices <math alttext="{A^{(i)}}" class="ltx_Math" display="inline" id="S4.SS1.p3.1.m1" intent=":literal"><semantics><msup><mi>A</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><annotation encoding="application/x-tex">{A^{(i)}}</annotation></semantics></math>, where each <math alttext="A^{(i)}\in\mathbb{R}^{d\times d}" class="ltx_Math" display="inline" id="S4.SS1.p3.2.m2" intent=":literal"><semantics><mrow><msup><mi>A</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>d</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">A^{(i)}\in\mathbb{R}^{d\times d}</annotation></semantics></math> represents the attention weights at layer <math alttext="i" class="ltx_Math" display="inline" id="S4.SS1.p3.3.m3" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>, we define the attention at the post-integrated layer (layer <math alttext="k=28" class="ltx_Math" display="inline" id="S4.SS1.p3.4.m4" intent=":literal"><semantics><mrow><mi>k</mi><mo>=</mo><mn>28</mn></mrow><annotation encoding="application/x-tex">k=28</annotation></semantics></math>) as <math alttext="A^{(k)}" class="ltx_Math" display="inline" id="S4.SS1.p3.5.m5" intent=":literal"><semantics><msup><mi>A</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup><annotation encoding="application/x-tex">A^{(k)}</annotation></semantics></math>.
To identify the attention matrix that deviates most significantly from <math alttext="A^{(k)}" class="ltx_Math" display="inline" id="S4.SS1.p3.6.m6" intent=":literal"><semantics><msup><mi>A</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup><annotation encoding="application/x-tex">A^{(k)}</annotation></semantics></math> in terms of distributional difference, we compute the Hellinger distance between each <math alttext="A^{(i)}" class="ltx_Math" display="inline" id="S4.SS1.p3.7.m7" intent=":literal"><semantics><msup><mi>A</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><annotation encoding="application/x-tex">A^{(i)}</annotation></semantics></math> and the reference <math alttext="A^{(k)}" class="ltx_Math" display="inline" id="S4.SS1.p3.8.m8" intent=":literal"><semantics><msup><mi>A</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup><annotation encoding="application/x-tex">A^{(k)}</annotation></semantics></math>, and select the one with the maximal distance from candidate layers <math alttext="\mathcal{C}" class="ltx_Math" display="inline" id="S4.SS1.p3.9.m9" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒞</mi><annotation encoding="application/x-tex">\mathcal{C}</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="i^{*}=\arg\max_{i\in\mathcal{C}}H\left(A^{(i)},A^{(k)}\right)" class="ltx_Math" display="block" id="S4.E1.m1" intent=":literal"><semantics><mrow><msup><mi>i</mi><mo>∗</mo></msup><mo>=</mo><mrow><mrow><mi>arg</mi><mo lspace="0.167em">⁡</mo><mrow><munder><mi>max</mi><mrow><mi>i</mi><mo>∈</mo><mi class="ltx_font_mathcaligraphic">𝒞</mi></mrow></munder><mo lspace="0.167em">⁡</mo><mi>H</mi></mrow></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><msup><mi>A</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>,</mo><msup><mi>A</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup><mo>)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">i^{*}=\arg\max_{i\in\mathcal{C}}H\left(A^{(i)},A^{(k)}\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS1.p3.12">where <math alttext="H(P,Q)" class="ltx_Math" display="inline" id="S4.SS1.p3.10.m1" intent=":literal"><semantics><mrow><mi>H</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>P</mi><mo>,</mo><mi>Q</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">H(P,Q)</annotation></semantics></math> denotes the Hellinger distance between two probability distributions <math alttext="P" class="ltx_Math" display="inline" id="S4.SS1.p3.11.m2" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> and <math alttext="Q" class="ltx_Math" display="inline" id="S4.SS1.p3.12.m3" intent=":literal"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math>, defined as:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="H(P,Q)=\frac{1}{\sqrt{2}}\sqrt{\sum_{j=1}^{d}\left(\sqrt{p_{j}}-\sqrt{q_{j}}\right)^{2}}" class="ltx_Math" display="block" id="S4.E2.m1" intent=":literal"><semantics><mrow><mrow><mi>H</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>P</mi><mo>,</mo><mi>Q</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mfrac><mn>1</mn><msqrt><mn>2</mn></msqrt></mfrac><mo lspace="0em" rspace="0em">​</mo><msqrt><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>d</mi></munderover><msup><mrow><mo lspace="0em">(</mo><mrow><msqrt><msub><mi>p</mi><mi>j</mi></msub></msqrt><mo>−</mo><msqrt><msub><mi>q</mi><mi>j</mi></msub></msqrt></mrow><mo>)</mo></mrow><mn>2</mn></msup></mrow></msqrt></mrow></mrow><annotation encoding="application/x-tex">H(P,Q)=\frac{1}{\sqrt{2}}\sqrt{\sum_{j=1}^{d}\left(\sqrt{p_{j}}-\sqrt{q_{j}}\right)^{2}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS1.p3.13">where <math alttext="d" class="ltx_Math" display="inline" id="S4.SS1.p3.13.m1" intent=":literal"><semantics><mi>d</mi><annotation encoding="application/x-tex">d</annotation></semantics></math> denotes the number of image tokens involved in the attention distribution.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Contrastive Attention for Review</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.2">As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S4.F4" title="Figure 4 ‣ 4 Method ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">4</span></a>, we leverage the observed layer-wise fusion phenomenon in MLLMs, particularly the review-like behavior, where the model reattends to the image even after the primary fusion appears to be completed. Based on this, we apply the contrastive attention to guide the masking of visual information during this review stage.
Specifically, the <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.2.1">contrastive attention</span> is computed as the difference between the attention at pre-integrated layer <math alttext="\mathbf{A}^{(i^{*})}" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1" intent=":literal"><semantics><msup><mi>𝐀</mi><mrow><mo stretchy="false">(</mo><msup><mi>i</mi><mo>∗</mo></msup><mo stretchy="false">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{A}^{(i^{*})}</annotation></semantics></math> and that at post-integrated layer <math alttext="\mathbf{A}^{(k)}" class="ltx_Math" display="inline" id="S4.SS2.p1.2.m2" intent=":literal"><semantics><msup><mi>𝐀</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup><annotation encoding="application/x-tex">\mathbf{A}^{(k)}</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<table class="ltx_equation ltx_eqn_table" id="S4.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{IA}=\left|\mathbf{A}^{(k)}-\mathbf{A}^{(i^{*})}\right|" class="ltx_Math" display="block" id="S4.E3.m1" intent=":literal"><semantics><mrow><mtext>IA</mtext><mo>=</mo><mrow><mo>|</mo><mrow><msup><mi>𝐀</mi><mrow><mo stretchy="false">(</mo><mi>k</mi><mo stretchy="false">)</mo></mrow></msup><mo>−</mo><msup><mi>𝐀</mi><mrow><mo stretchy="false">(</mo><msup><mi>i</mi><mo>∗</mo></msup><mo stretchy="false">)</mo></mrow></msup></mrow><mo>|</mo></mrow></mrow><annotation encoding="application/x-tex">\text{IA}=\left|\mathbf{A}^{(k)}-\mathbf{A}^{(i^{*})}\right|</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.4">The difference between the attention from the pre-integrated layer and the post-integrated layer captures how visual information shifts the attention during the visual integration and understanding.
We leverage this signal to refine the model’s attention at review layer through a soft masking strategy. Specifically, we identify visual tokens whose contrastive attention scores fall below the <math alttext="\rho" class="ltx_Math" display="inline" id="S4.SS2.p3.1.m1" intent=":literal"><semantics><mi>ρ</mi><annotation encoding="application/x-tex">\rho</annotation></semantics></math>-th percentile and softly suppress their influence by scaling down features. Formally, let <math alttext="Q_{\rho}(\text{IA})" class="ltx_Math" display="inline" id="S4.SS2.p3.2.m2" intent=":literal"><semantics><mrow><msub><mi>Q</mi><mi>ρ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mtext>IA</mtext><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">Q_{\rho}(\text{IA})</annotation></semantics></math> denote the <math alttext="\rho" class="ltx_Math" display="inline" id="S4.SS2.p3.3.m3" intent=":literal"><semantics><mi>ρ</mi><annotation encoding="application/x-tex">\rho</annotation></semantics></math>-th quantile of the contrastive attention scores <span class="ltx_text ltx_markedasmath" id="S4.SS2.p3.4.1">IA</span>. The masked visual features are computed as:</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<table class="ltx_equation ltx_eqn_table" id="S4.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{E}_{j}^{\text{masked}}=\lambda\cdot\mathbf{E}_{j},\quad\text{if }\text{IA}j&lt;Q_{\rho}(\text{IA})" class="ltx_Math" display="block" id="S4.E4.m1" intent=":literal"><semantics><mrow><mrow><msubsup><mi>𝐄</mi><mi>j</mi><mtext>masked</mtext></msubsup><mo>=</mo><mrow><mi>λ</mi><mo lspace="0.222em" rspace="0.222em">⋅</mo><msub><mi>𝐄</mi><mi>j</mi></msub></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mrow><mrow><mtext>if </mtext><mtext>IA</mtext></mrow><mo lspace="0em" rspace="0em">​</mo><mi>j</mi></mrow><mo>&lt;</mo><mrow><msub><mi>Q</mi><mi>ρ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mtext>IA</mtext><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\mathbf{E}_{j}^{\text{masked}}=\lambda\cdot\mathbf{E}_{j},\quad\text{if }\text{IA}j&lt;Q_{\rho}(\text{IA})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS2.p4.4">where <math alttext="\mathbf{E}_{j}" class="ltx_Math" display="inline" id="S4.SS2.p4.1.m1" intent=":literal"><semantics><msub><mi>𝐄</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\mathbf{E}_{j}</annotation></semantics></math> is the embedding of the <math alttext="j" class="ltx_Math" display="inline" id="S4.SS2.p4.2.m2" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>-th visual token, and <math alttext="\lambda\ll 1" class="ltx_Math" display="inline" id="S4.SS2.p4.3.m3" intent=":literal"><semantics><mrow><mi>λ</mi><mo>≪</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\lambda\ll 1</annotation></semantics></math> is a constant that softly downweights low-relevance regions without fully discarding them. We analyze the effect of varying the masking ratio <math alttext="\rho" class="ltx_Math" display="inline" id="S4.SS2.p4.4.m4" intent=":literal"><semantics><mi>ρ</mi><annotation encoding="application/x-tex">\rho</annotation></semantics></math> in <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.SS6" title="5.6 Masking Ratio Based on Contrastive Attention ‣ 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">5.6</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Effectiveness of contrastive attention compared to existing enhanced methods.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T2.1" style="width:433.6pt;height:56.7pt;vertical-align:-26.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-111.8pt,14.6pt) scale(0.659855967102078,0.659855967102078) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T2.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1.1.1">MLLMs</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T2.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1.2.1">Methods</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.1.3">GQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.1.4">VQAv2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.1.5">OKVQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.1.6">VizWiz</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.1.7">TextVQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.1.8">DocVQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.1.1.1.1.9"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1.9.1">Average</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.1.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T2.1.1.2.1.1" rowspan="3"><span class="ltx_text ltx_font_italic" id="S5.T2.1.1.2.1.1.1">LLaVA-v1.5</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.1.1.2.1.2">+ DoLA <cite class="ltx_cite ltx_citemacro_citep">(Chuang<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib2" title="Dola: decoding by contrasting layers improves factuality in large language models">2024</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.2.1.3">66.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.2.1.4">70.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.2.1.5">52.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.2.1.6">58.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.2.1.7">56.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.2.1.8">26.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.2.1.9">55.04</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.1.3.2.1">+ ViCrop <cite class="ltx_cite ltx_citemacro_citep">(Zhang<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib1" title="MLLMs know where to look: training-free perception of small visual details with multimodal llms">2025</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.3.2.2">67.0</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.3.2.3">76.0</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.3.2.4">54.8</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.3.2.5">59.6</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.3.2.6">56.6</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.3.2.7">25.1</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.3.2.8">56.52</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.1.4.3.1">+ <span class="ltx_text ltx_font_bold" id="S5.T2.1.1.4.3.1.1">Ours</span>
</th>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.4.3.2"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.4.3.2.1">69.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.4.3.3"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.4.3.3.1">77.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.4.3.4"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.4.3.4.1">55.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.4.3.5"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.4.3.5.1">60.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.4.3.6"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.4.3.6.1">59.8</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.4.3.7"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.4.3.7.1">26.9</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.4.3.8"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.4.3.8.1">58.25</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S5.T2.1.1.5.4.1" rowspan="3"><span class="ltx_text ltx_font_italic" id="S5.T2.1.1.5.4.1.1">LLaVA-v1.6</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.1.1.5.4.2">+ DoLA <cite class="ltx_cite ltx_citemacro_citep">(Chuang<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib2" title="Dola: decoding by contrasting layers improves factuality in large language models">2024</a>)</cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.5.4.3">71.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.5.4.4">75.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.5.4.5">53.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.5.4.6">60.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.5.4.7">72.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.5.4.8">67.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.1.5.4.9">66.69</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.1.6.5.1">+ ViCrop <cite class="ltx_cite ltx_citemacro_citep">(Zhang<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib1" title="MLLMs know where to look: training-free perception of small visual details with multimodal llms">2025</a>)</cite>
</th>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.6.5.2">70.2</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.6.5.3"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.6.5.3.1">81.4</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.6.5.4">54.1</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.6.5.5">61.3</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.6.5.6">73.4</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.6.5.7">65.6</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.6.5.8">67.00</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T2.1.1.7.6.1">+ <span class="ltx_text ltx_font_bold" id="S5.T2.1.1.7.6.1.1">Ours</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.1.7.6.2"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.7.6.2.1">71.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.1.7.6.3">80.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.1.7.6.4"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.7.6.4.1">56.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.1.7.6.5"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.7.6.5.1">62.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.1.7.6.6"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.7.6.6.1">75.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.1.7.6.7"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.7.6.7.1">68.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.1.7.6.8"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.7.6.8.1">69.18</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Datasets and Setting</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">The proposed method is evaluated on six widely-used multimodal datasets: VQAv2 <cite class="ltx_cite ltx_citemacro_citep">(Goyal<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib16" title="Making the v in vqa matter: elevating the role of image understanding in visual question answering">2017</a>)</cite>, GQA <cite class="ltx_cite ltx_citemacro_citep">(Hudson and Manning <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib17" title="Gqa: a new dataset for real-world visual reasoning and compositional question answering">2019</a>)</cite>, TextVQA <cite class="ltx_cite ltx_citemacro_citep">(Singh<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib20" title="Towards vqa models that can read">2019</a>)</cite>, OKVQA <cite class="ltx_cite ltx_citemacro_citep">(Marino<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib18" title="Ok-vqa: a visual question answering benchmark requiring external knowledge">2019</a>)</cite>, VizWiz <cite class="ltx_cite ltx_citemacro_citep">(Gurari<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib19" title="Vizwiz grand challenge: answering visual questions from blind people">2018</a>)</cite>, and DocVQA <cite class="ltx_cite ltx_citemacro_citep">(Mathew<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib34" title="Docvqa: a dataset for vqa on document images">2021</a>)</cite>. We report accuracy on each dataset as the evaluation metric. All experiments are conducted based on the open-source and SOTA LLaVA series <cite class="ltx_cite ltx_citemacro_citep">(Liu<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib10" title="Visual instruction tuning">2023</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib9" title="Improved baselines with visual instruction tuning">2024</a>)</cite>: LLaVA-1.5-7B, LLaVA-1.6-Vicuna-7B.
The experiments were conducted on an RTX A800 GPU using PyTorch 2.0, and the system was based on Ubuntu 20.04.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Comparison with MLLMs</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">The baseline MLLMs include BLIP-2 <cite class="ltx_cite ltx_citemacro_citep">(Li<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib4" title="Blip-2: bootstrapping language-image pre-training with frozen image encoders and large language models">2023</a>)</cite>, Flamingo <cite class="ltx_cite ltx_citemacro_citep">(Alayrac<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib5" title="Flamingo: a visual language model for few-shot learning">2022</a>)</cite>, InstructBLIP <cite class="ltx_cite ltx_citemacro_citep">(Dai<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib6" title="InstructBLIP: towards general-purpose vision-language models with instruction tuning">2023</a>)</cite>, IDEFICS <cite class="ltx_cite ltx_citemacro_citep">(Laurençon<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib7" title="Obelics: an open web-scale filtered dataset of interleaved image-text documents">2023</a>)</cite>, LLaVA <cite class="ltx_cite ltx_citemacro_citep">(Liu<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib10" title="Visual instruction tuning">2023</a>)</cite>, LLaVA-1.5 <cite class="ltx_cite ltx_citemacro_citep">(Liu<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib9" title="Improved baselines with visual instruction tuning">2024</a>)</cite>, InternVL-MLP <cite class="ltx_cite ltx_citemacro_citep">(Chen<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib26" title="Internvl: scaling up vision foundation models and aligning for generic visual-linguistic tasks">2024</a>)</cite>, InternVL-QLLaMA <cite class="ltx_cite ltx_citemacro_citep">(Chen<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib26" title="Internvl: scaling up vision foundation models and aligning for generic visual-linguistic tasks">2024</a>)</cite>, and Qwen-VL <cite class="ltx_cite ltx_citemacro_citep">(Bai<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib8" title="Qwen-vl: a versatile vision-language model for understanding, localization, text reading, and beyond">2023</a>)</cite>.
As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S4.T1" title="Table 1 ‣ 4.1 Selecting Contrastive Layers ‣ 4 Method ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">1</span></a>, integrating our method into LLaVA-1.5 yields state-of-the-art performance across six VQA benchmarks, achieving an average score of 58.17. This surpasses strong recent baselines such as InternVL-MLP (52.97) and Qwen-VL (50.49).</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Comparison with Training-free Methods</h3>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Pre-integrated layer selection on LLaVA-v1.5-7B.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.1.2.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S5.T3.1.2.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.1.2.1.2">GQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.1.2.1.3">VQAv2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.1.2.1.4">OKVQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.1.2.1.5">VizWiz</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.1.2.1.6">TextVQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.1.2.1.7">DocVQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T3.1.2.1.8"><span class="ltx_text ltx_font_bold" id="S5.T3.1.2.1.8.1">Average</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.1.3.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T3.1.3.1.1">all (0-28)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.3.1.2">65.40</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.3.1.3">72.97</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.3.1.4">51.34</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.3.1.5">57.77</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.3.1.6">52.50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.3.1.7">26.10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.1.3.1.8">54.35</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.4.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T3.1.4.2.1">deep (16-28)</th>
<td class="ltx_td ltx_align_center" id="S5.T3.1.4.2.2">60.30</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.4.2.3">70.11</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.4.2.4">52.80</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.4.2.5">57.40</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.4.2.6">56.10</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.4.2.7">26.00</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.4.2.8">53.79</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.5.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T3.1.5.3.1">shallow (0-15)</th>
<td class="ltx_td ltx_align_center" id="S5.T3.1.5.3.2">68.30</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.5.3.3">76.49</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.5.3.4">53.92</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.5.3.5">59.30</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.5.3.6">57.00</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.5.3.7">26.40</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.5.3.8">56.90</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S5.T3.1.1.1">fusion (<math alttext="\mathcal{S}" class="ltx_Math" display="inline" id="S5.T3.1.1.1.m1" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒮</mi><annotation encoding="application/x-tex">\mathcal{S}</annotation></semantics></math>)</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.2.1">69.40</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.3.1">77.59</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.4.1">55.40</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.1.5"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.5.1">60.40</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.1.6"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.6.1">59.80</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.1.7"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.7.1">26.90</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.1.1.8"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.8.1">58.25</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">To evaluate the effectiveness of our proposed training-free method, as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.T2" title="Table 2 ‣ 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">2</span></a>, we also compare it against two representative methods:
(1) <span class="ltx_text ltx_font_italic" id="S5.SS3.p1.1.1">DoLA</span> <cite class="ltx_cite ltx_citemacro_citep">(Chuang<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib2" title="Dola: decoding by contrasting layers improves factuality in large language models">2024</a>)</cite> proposes a training-free decoding strategy that contrasts the output logits between earlier and later transformer layers, amplifying deep-layer factual knowledge and reducing hallucination without requiring additional supervision.
(2) <span class="ltx_text ltx_font_italic" id="S5.SS3.p1.1.2">ViCrop</span> <cite class="ltx_cite ltx_citemacro_citep">(Zhang<span class="ltx_text ltx_bib_etal"> et al.</span> <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#bib.bib1" title="MLLMs know where to look: training-free perception of small visual details with multimodal llms">2025</a>)</cite> introduces a training-free adversarial framework that manipulates image inputs by strategically masking and cropping regions to challenge the visual reasoning of MLLMs, revealing their sensitivity to localized visual changes.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">As illustrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.T2" title="Table 2 ‣ 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">2</span></a>, our approach consistently outperforms existing representative methods, demonstrating improved stability and transferability during inference.
These results indicate that contrastive attention effectively identifies and enhances critical visual regions, providing a lightweight yet effective enhancement to the inference process without requiring any additional training.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Selection of post-integrated layer</h3>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="330" id="S5.F5.g1" src="x15.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Layer-wise Hellinger distance to the final layer.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">Before identifying the post-integrated layer for applying contrastive attention, we first clarify two key questions:</p>
</div>
<section class="ltx_paragraph" id="S5.SS4.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">1. Where and how does contrastive attention function?</h4>
<div class="ltx_para" id="S5.SS4.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS4.SSS0.Px1.p1.1">Contrastive attention functions by suppressing image regions that are irrelevant to the task. Our exploratory experiments show that image has little effect on deeper layers (e.g., beyond the 20th layer), where visual information has already been fully integrated. However, we observe that a secondary integration of visual and textual information occurs at layer 29, referred to as the “review” layer. Therefore, to influence the final reasoning, <span class="ltx_text ltx_font_bold" id="S5.SS4.SSS0.Px1.p1.1.1">contrastive attention should take effect at layer 29</span>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS4.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">2. Why is layer 28 selected as the post-integrated layer?</h4>
<div class="ltx_para" id="S5.SS4.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS4.SSS0.Px2.p1.1">While the final layer (layer 31) represents the model’s fully aggregated understanding, it cannot be directly used for contrastive attention. This is because contrastive attention must be computed before layer 29 in order to take effect at the point where visual-textual fusion reoccurs. Therefore, <span class="ltx_text ltx_font_bold" id="S5.SS4.SSS0.Px2.p1.1.1">the post-integrated layer needs to be selected before layer 29</span>.
To approximate the role of the final layer, we calculate the Hellinger Distance between the attention distributions of each layer and that of layer 31. The results show that both layer 28 and layer 30 exhibit minimal distance from the final layer, indicating that their attention patterns closely resemble the final attention. Thus, we select layer 28 as the post-integrated layer for computing contrastive attention.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Selection of Pre-Integrated Layers</h3>
<div class="ltx_para" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1">We systematically explored different strategies for selecting the pre-integrated layer, considering four selection scope from:
(a) <span class="ltx_text ltx_font_italic" id="S5.SS5.p1.1.1">all</span> layers (unconstrained setting, 0–28),
(b) <span class="ltx_text ltx_font_italic" id="S5.SS5.p1.1.2">deep</span> layers (16–28),
(c) <span class="ltx_text ltx_font_italic" id="S5.SS5.p1.1.3">shallow</span> layers (0–16), and
(d) the set of <span class="ltx_text ltx_font_italic" id="S5.SS5.p1.1.4">fusion</span> layers (2, 4, 8, 11, 12, 13) identified in <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S3.SS2" title="3.2 Analysis and Observations ‣ 3 Exploration of Visual Information Fusion ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS5.p2">
<p class="ltx_p" id="S5.SS5.p2.1">The pre-integrated layer refers to the stage where visual information is initially processed, before being influenced by textual context. Therefore, layers with already fused representations should be excluded. As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.T3" title="Table 3 ‣ 5.3 Comparison with Training-free Methods ‣ 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">3</span></a>, the <span class="ltx_text ltx_font_italic" id="S5.SS5.p2.1.1">deep</span> strategy performs poorly because these layers contain cross-modal information, which degrades the quality of contrastive attention. Similarly, the <span class="ltx_text ltx_font_italic" id="S5.SS5.p2.1.2">all</span> strategy includes deep layers, introducing noisy and biased attention.
In comparison, the <span class="ltx_text ltx_font_italic" id="S5.SS5.p2.1.3">shallow</span> strategy achieves better results. The best performance comes from the <span class="ltx_text ltx_font_italic" id="S5.SS5.p2.1.4">fusion</span> strategy, which limits selection to empirically identified fusion layers <math alttext="\mathcal{S}" class="ltx_Math" display="inline" id="S5.SS5.p2.1.m1" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒮</mi><annotation encoding="application/x-tex">\mathcal{S}</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S5.SS5.p3">
<p class="ltx_p" id="S5.SS5.p3.1">As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.F6" title="Figure 6 ‣ 5.5 Selection of Pre-Integrated Layers ‣ 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">6</span></a>, we further investigate the distribution of automatically selected pre-integrated layers when no constraints are imposed on the selection range. Specifically, we apply Equation (<a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S4.E1" title="Equation 1 ‣ 4.1 Selecting Contrastive Layers ‣ 4 Method ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">1</span></a>) to identify the layer that exhibits the largest attention divergence from the post-integrated layer.
As illustrated, layer 2 is selected with overwhelmingly high frequency across all datasets, indicating a strong concentration around this early layer. For instance, in DocVQA, over 86% of samples select layer 2 as the pre-integrated layer.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.F6" title="Figure 6 ‣ 5.5 Selection of Pre-Integrated Layers ‣ 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">6</span></a> shows that fusion layers <math alttext="\mathcal{S}" class="ltx_Math" display="inline" id="S5.SS5.p3.1.m1" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒮</mi><annotation encoding="application/x-tex">\mathcal{S}</annotation></semantics></math> are favored as pre-integrated layers, reinforcing our claim that they are well-suited for contrastive attention.</p>
</div>
<figure class="ltx_figure" id="S5.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F6.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="700" id="S5.F6.1.g1" src="x16.png" width="830"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F6.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="683" id="S5.F6.2.g1" src="x17.png" width="829"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F6.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="684" id="S5.F6.3.g1" src="x18.png" width="830"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F6.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="688" id="S5.F6.4.g1" src="x19.png" width="830"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F6.5"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="686" id="S5.F6.5.g1" src="x20.png" width="829"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F6.6"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="686" id="S5.F6.6.g1" src="x21.png" width="830"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Distribution of selected pre-integrated layers across tasks under unconstrained candidate setting.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.6 </span>Masking Ratio Based on Contrastive Attention</h3>
<figure class="ltx_figure" id="S5.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F7.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="770" id="S5.F7.1.g1" src="x22.png" width="829"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F7.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="770" id="S5.F7.2.g1" src="x23.png" width="829"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F7.3"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="770" id="S5.F7.3.g1" src="x24.png" width="830"/>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F7.4"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="770" id="S5.F7.4.g1" src="x25.png" width="829"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F7.5"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="770" id="S5.F7.5.g1" src="x26.png" width="829"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F7.6"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="793" id="S5.F7.6.g1" src="x27.png" width="829"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Effect of visual token masking ratio on accuracy and inference time at the review layer.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS6.p1">
<p class="ltx_p" id="S5.SS6.p1.1">Based on the observed review-like behavior, we applied contrastive attention at review layer of the MLLM and used a soft mask strategy to selectively suppress irrelevant visual information. In this section, we investigate how different masking ratios affect model performance and efficiency.
As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.F7" title="Figure 7 ‣ 5.6 Masking Ratio Based on Contrastive Attention ‣ 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">7</span></a>, we evaluated the accuracy and inference time across six benchmarks under varying mask ratios on LLaVA-1.5. We observe a consistent trend: accuracy peaks when 20% of the tokens are masked, but degrades as the masking ratio increases further. Meanwhile, inference time decreases steadily as the masking ratio increases.
When the masking ratio reaches 1.0—i.e., the model receives no visual input—the accuracy drops to zero on all datasets. This clearly confirms the critical role of visual signals at the review layer during final-stage reasoning.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we present a systematic analysis of visual information integration in MLLMs by conducting layer-wise masking experiments. Our findings reveal that visual-text fusion primarily occurs in a certain number of shallow layers, while a review-like re-attention behavior emerges just before the final output.
Building on these findings, we propose a training-free method that enhances vision-language reasoning using contrastive attention. This attention is computed by comparing the attention maps between a pre-integrated layer, which represents early visual perception, and an post-integrated layer, where vision and language are fully fused. The resulting contrastive signal reveals the model’s evolving visual focus and helps suppress irrelevant high-attention noise in later layers. When applied at the review layer, this approach guides the model to concentrate on task-relevant visual regions more effectively, without any additional training.
Extensive experiments across six VQA benchmarks demonstrate that our method consistently improves the performance of LLaVA series models, achieving new state-of-the-art results. Our work provides deeper insight into the internal mechanisms of MLLMs and offers a lightweight, effective strategy for enhancing multimodal understanding.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist" id="bib.L1">
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib37">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">E. Aflalo, M. Du, S. Tseng, Y. Liu, C. Wu, N. Duan, and V. Lal (2022)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Vl-interpret: an interactive visualization tool for interpreting vision-language transformers</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 21406–21415</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S2.p1.1" title="2 Related Work ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib5">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, <span class="ltx_text ltx_bib_etal">et al.</span> (2022)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Flamingo: a visual language model for few-shot learning</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Advances in neural information processing systems</span> <span class="ltx_text ltx_bib_volume">35</span>, <span class="ltx_text ltx_bib_pages"> pp. 23716–23736</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S4.T1.1.3.3.1" title="In 4.1 Selecting Contrastive Layers ‣ 4 Method ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">Table 1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.SS2.p1.1" title="5.2 Comparison with MLLMs ‣ 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">§5.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib8">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou (2023)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Qwen-vl: a versatile vision-language model for understanding, localization, text reading, and beyond</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2308.12966</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S1.p1.1" title="1 Introduction ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S4.T1.1.9.9.1" title="In 4.1 Selecting Contrastive Layers ‣ 4 Method ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">Table 1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.SS2.p1.1" title="5.2 Comparison with MLLMs ‣ 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">§5.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib35">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J. Cao, Z. Gan, Y. Cheng, L. Yu, Y. Chen, and J. Liu (2020)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Behind the scene: revealing the secrets of pre-trained vision-and-language models</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VI 16</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 565–580</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S2.p1.1" title="2 Related Work ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib38">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">H. Chefer, S. Gur, and L. Wolf (2021)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF international conference on computer vision</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 397–406</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S2.p1.1" title="2 Related Work ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib26">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, M. Zhong, Q. Zhang, X. Zhu, L. Lu, <span class="ltx_text ltx_bib_etal">et al.</span> (2024)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Internvl: scaling up vision foundation models and aligning for generic visual-linguistic tasks</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 24185–24198</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S4.T1.1.7.7.1" title="In 4.1 Selecting Contrastive Layers ‣ 4 Method ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">Table 1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S4.T1.1.8.8.1" title="In 4.1 Selecting Contrastive Layers ‣ 4 Method ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">Table 1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.SS2.p1.1" title="5.2 Comparison with MLLMs ‣ 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">§5.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib2">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Y. Chuang, Y. Xie, H. Luo, Y. Kim, J. Glass, and P. He (2024)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Dola: decoding by contrasting layers improves factuality in large language models</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2309.03883</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S1.p4.1" title="1 Introduction ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.SS3.p1.1" title="5.3 Comparison with Training-free Methods ‣ 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">§5.3</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.T2.1.1.2.1.2" title="In 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">Table 2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.T2.1.1.5.4.2" title="In 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">Table 2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib6">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi (2023)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">InstructBLIP: towards general-purpose vision-language models with instruction tuning</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">2305.06500</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S1.p3.1" title="1 Introduction ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S4.T1.1.4.4.1" title="In 4.1 Selecting Contrastive Layers ‣ 4 Method ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">Table 1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.SS2.p1.1" title="5.2 Comparison with MLLMs ‣ 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">§5.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib36">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">S. Frank, E. Bugliarello, and D. Elliott (2021)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Vision-and-language or vision-for-language? on cross-modal influence in multimodal transformers</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2109.04448</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S2.p1.1" title="2 Related Work ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib16">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh (2017)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Making the v in vqa matter: elevating the role of image understanding in visual question answering</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE conference on computer vision and pattern recognition</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 6904–6913</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S1.p1.1" title="1 Introduction ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.SS1.p1.1" title="5.1 Datasets and Setting ‣ 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">§5.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib19">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">D. Gurari, Q. Li, A. J. Stangl, A. Guo, C. Lin, K. Grauman, J. Luo, and J. P. Bigham (2018)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Vizwiz grand challenge: answering visual questions from blind people</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE conference on computer vision and pattern recognition</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 3608–3617</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.SS1.p1.1" title="5.1 Datasets and Setting ‣ 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">§5.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib17">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">D. A. Hudson and C. D. Manning (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Gqa: a new dataset for real-world visual reasoning and compositional question answering</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 6700–6709</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.SS1.p1.1" title="5.1 Datasets and Setting ‣ 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">§5.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib3">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">M. Jin, Q. Yu, J. Huang, Q. Zeng, Z. Wang, W. Hua, H. Zhao, K. Mei, Y. Meng, K. Ding, <span class="ltx_text ltx_bib_etal">et al.</span> (2024)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Exploring concept depth: how large language models acquire knowledge and concept at different layers?</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2404.07066</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S2.p1.1" title="2 Related Work ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib41">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">T. Ju, W. Sun, W. Du, X. Yuan, Z. Ren, and G. Liu (2024)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">How large language models encode context knowledge? a layer-wise probing study</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2402.16061</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S2.p1.1" title="2 Related Work ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib7">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">H. Laurençon, L. Saulnier, L. Tronchon, S. Bekman, A. Singh, A. Lozhkov, T. Wang, S. Karamcheti, A. Rush, D. Kiela, <span class="ltx_text ltx_bib_etal">et al.</span> (2023)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Obelics: an open web-scale filtered dataset of interleaved image-text documents</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Advances in Neural Information Processing Systems</span> <span class="ltx_text ltx_bib_volume">36</span>, <span class="ltx_text ltx_bib_pages"> pp. 71683–71702</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S4.T1.1.5.5.1" title="In 4.1 Selecting Contrastive Layers ‣ 4 Method ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">Table 1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.SS2.p1.1" title="5.2 Comparison with MLLMs ‣ 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">§5.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib4">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J. Li, D. Li, S. Savarese, and S. Hoi (2023)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Blip-2: bootstrapping language-image pre-training with frozen image encoders and large language models</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">International conference on machine learning</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 19730–19742</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S4.T1.1.2.2.1" title="In 4.1 Selecting Contrastive Layers ‣ 4 Method ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">Table 1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.SS2.p1.1" title="5.2 Comparison with MLLMs ‣ 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">§5.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib42">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J. Lin, H. Chen, Y. Fan, Y. Fan, X. Jin, H. Su, J. Fu, and X. Shen (2025)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Multi-layer visual feature fusion in multimodal llms: methods, analysis, and best practices</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2503.06063</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S2.p1.1" title="2 Related Work ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib9">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">H. Liu, C. Li, Y. Li, and Y. J. Lee (2024)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Improved baselines with visual instruction tuning</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">arXiv:2310.03744</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S4.T1.1.13.13.1" title="In 4.1 Selecting Contrastive Layers ‣ 4 Method ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">Table 1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.SS1.p1.1" title="5.1 Datasets and Setting ‣ 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">§5.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.SS2.p1.1" title="5.2 Comparison with MLLMs ‣ 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">§5.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib10">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">H. Liu, C. Li, Q. Wu, and Y. J. Lee (2023)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Visual instruction tuning</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">NeurIPS</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S1.p1.1" title="1 Introduction ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S1.p3.1" title="1 Introduction ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S4.T1.1.11.11.1" title="In 4.1 Selecting Contrastive Layers ‣ 4 Method ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">Table 1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S4.T1.1.6.6.1" title="In 4.1 Selecting Contrastive Layers ‣ 4 Method ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">Table 1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.SS1.p1.1" title="5.1 Datasets and Setting ‣ 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">§5.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.SS2.p1.1" title="5.2 Comparison with MLLMs ‣ 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">§5.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib39">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Y. Lyu, P. P. Liang, Z. Deng, R. Salakhutdinov, and L. Morency (2022)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Dime: fine-grained interpretations of multimodal models via disentangled local explanations</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 455–467</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S2.p1.1" title="2 Related Work ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib18">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">K. Marino, M. Rastegari, A. Farhadi, and R. Mottaghi (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Ok-vqa: a visual question answering benchmark requiring external knowledge</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/cvf conference on computer vision and pattern recognition</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 3195–3204</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S1.p1.1" title="1 Introduction ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.SS1.p1.1" title="5.1 Datasets and Setting ‣ 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">§5.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib34">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">M. Mathew, D. Karatzas, and C. Jawahar (2021)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Docvqa: a dataset for vqa on document images</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF winter conference on applications of computer vision</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 2200–2209</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.SS1.p1.1" title="5.1 Datasets and Setting ‣ 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">§5.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib44">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">V. Palit, R. Pandey, A. Arora, and P. P. Liang (2023)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Towards vision-language mechanistic interpretability: a causal tracing tool for blip</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 2856–2861</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S2.p2.1" title="2 Related Work ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib20">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Towards vqa models that can read</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 8317–8326</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.SS1.p1.1" title="5.1 Datasets and Setting ‣ 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">§5.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib40">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">G. B. M. Stan, E. Aflalo, R. Y. Rohekar, A. Bhiwandiwalla, S. Tseng, M. L. Olson, Y. Gurwicz, C. Wu, N. Duan, and V. Lal (2024)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">LVLM-interpret: an interpretability tool for large vision-language models</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2404.03118</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S2.p1.1" title="2 Related Work ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib33">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K. Chen, X. Liu, J. Wang, W. Ge, <span class="ltx_text ltx_bib_etal">et al.</span> (2024)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Qwen2-vl: enhancing vision-language model’s perception of the world at any resolution</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2409.12191</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S4.T1.1.10.10.1" title="In 4.1 Selecting Contrastive Layers ‣ 4 Method ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">Table 1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib1">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J. Zhang, M. Khayatkhoei, P. Chhikara, and F. Ilievski (2025)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">MLLMs know where to look: training-free perception of small visual details with multimodal llms</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2502.17422</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S1.p3.1" title="1 Introduction ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.SS3.p1.1" title="5.3 Comparison with Training-free Methods ‣ 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">§5.3</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.T2.1.1.3.2.1" title="In 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">Table 2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S5.T2.1.1.6.5.1" title="In 5 Experiments ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">Table 2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib43">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Q. Zhao, M. Xu, K. Gupta, A. Asthana, L. Zheng, and S. Gould (2024)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The first to know: how token distributions reveal hidden knowledge in large vision-language models?</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">European Conference on Computer Vision</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 127–142</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.08151v1#S2.p2.1" title="2 Related Work ‣ Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention"><span class="ltx_text ltx_ref_tag">§2</span></a>.
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Reproducibility Checklist</h2>
<span class="ltx_rule" style="width:100%;height:1px;--ltx-bg-color:black;display:inline-block;"> </span>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1"><span class="ltx_text ltx_font_bold" id="Sx1.p1.1.1">Instructions for Authors:</span></p>
</div>
<div class="ltx_para" id="Sx1.p2">
<p class="ltx_p" id="Sx1.p2.1">This document outlines key aspects for assessing reproducibility. Please provide your input by editing this <span class="ltx_text ltx_font_typewriter" id="Sx1.p2.1.1">.tex</span> file directly.</p>
</div>
<div class="ltx_para" id="Sx1.p3">
<p class="ltx_p" id="Sx1.p3.1">For each question (that applies), replace the “Type your response here” text with your answer.</p>
</div>
<div class="ltx_para ltx_noindent" id="Sx1.p4">
<p class="ltx_p" id="Sx1.p4.3"><span class="ltx_text ltx_font_bold" id="Sx1.p4.3.1">Example:</span> If a question appears as</p>
<div class="ltx_logical-block" id="Sx1.p4.1">
<div class="ltx_para ltx_noindent ltx_align_center" id="Sx1.p4.1.p1">
<p class="ltx_p ltx_minipage ltx_align_middle" id="Sx1.p4.1.p1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_typewriter" id="Sx1.p4.1.p1.1.1">\question{Proofs of all novel claims are included} {(yes/partial/no)} 
<br class="ltx_break"/>Type your response here</span></p>
</div>
</div>
<p class="ltx_p" id="Sx1.p4.4">you would change it to:</p>
<div class="ltx_logical-block" id="Sx1.p4.2">
<div class="ltx_para ltx_noindent ltx_align_center" id="Sx1.p4.2.p1">
<p class="ltx_p ltx_minipage ltx_align_middle" id="Sx1.p4.2.p1.1" style="width:390.3pt;"><span class="ltx_text ltx_font_typewriter" id="Sx1.p4.2.p1.1.1">\question{Proofs of all novel claims are included} {(yes/partial/no)} 
<br class="ltx_break"/>yes</span></p>
</div>
</div>
<p class="ltx_p" id="Sx1.p4.5">Please make sure to:</p>
<ul class="ltx_itemize" id="Sx1.I1">
<li class="ltx_item" id="Sx1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx1.I1.i1.p1">
<p class="ltx_p" id="Sx1.I1.i1.p1.1">Replace ONLY the “Type your response here” text and nothing else.</p>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i2" style="list-style-type:none;padding-top:1.0pt;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx1.I1.i2.p1">
<p class="ltx_p" id="Sx1.I1.i2.p1.1">Use one of the options listed for that question (e.g., <span class="ltx_text ltx_font_bold" id="Sx1.I1.i2.p1.1.1">yes</span>, <span class="ltx_text ltx_font_bold" id="Sx1.I1.i2.p1.1.2">no</span>, <span class="ltx_text ltx_font_bold" id="Sx1.I1.i2.p1.1.3">partial</span>, or <span class="ltx_text ltx_font_bold" id="Sx1.I1.i2.p1.1.4">NA</span>).</p>
</div>
</li>
<li class="ltx_item" id="Sx1.I1.i3" style="list-style-type:none;padding-top:1.0pt;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="Sx1.I1.i3.p1">
<p class="ltx_p" id="Sx1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="Sx1.I1.i3.p1.1.1">Not</span> modify any other part of the <span class="ltx_text ltx_font_typewriter" id="Sx1.I1.i3.p1.1.2">\question</span> command or any other lines in this document.
<br class="ltx_break"/></p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="Sx1.p5">
<p class="ltx_p" id="Sx1.p5.1">You can <span class="ltx_text ltx_font_typewriter" id="Sx1.p5.1.1">\input</span> this .tex file right before <span class="ltx_text ltx_font_typewriter" id="Sx1.p5.1.2">\end{document}</span> of your main file or compile it as a stand-alone document. Check the instructions on your conference’s website to see if you will be asked to provide this checklist with your paper or separately.</p>
</div>
<span class="ltx_rule" style="width:100%;height:1px;--ltx-bg-color:black;display:inline-block;"> </span>
<section class="ltx_paragraph" id="Sx1.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">1. General Paper Structure</h4>
<div class="ltx_para" id="Sx1.SS0.SSS0.Px1.p1">
<ul class="ltx_itemize" id="Sx1.I2">
<li class="ltx_item" id="Sx1.I2.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.1.</span>
<div class="ltx_para" id="Sx1.I2.ix1.p1">
<p class="ltx_p" id="Sx1.I2.ix1.p1.1">Includes a conceptual outline and/or pseudocode description of AI methods introduced (yes/partial/no/NA) <span class="ltx_text" id="Sx1.I2.ix1.p1.1.1" style="--ltx-fg-color:#0000FF;">
yes</span></p>
</div>
</li>
<li class="ltx_item" id="Sx1.I2.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.2.</span>
<div class="ltx_para" id="Sx1.I2.ix2.p1">
<p class="ltx_p" id="Sx1.I2.ix2.p1.1">Clearly delineates statements that are opinions, hypothesis, and speculation from objective facts and results (yes/no) <span class="ltx_text" id="Sx1.I2.ix2.p1.1.1" style="--ltx-fg-color:#0000FF;">
yes</span></p>
</div>
</li>
<li class="ltx_item" id="Sx1.I2.ix3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.3.</span>
<div class="ltx_para" id="Sx1.I2.ix3.p1">
<p class="ltx_p" id="Sx1.I2.ix3.p1.1">Provides well-marked pedagogical references for less-familiar readers to gain background necessary to replicate the paper (yes/no) <span class="ltx_text" id="Sx1.I2.ix3.p1.1.1" style="--ltx-fg-color:#0000FF;">
yes</span></p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_paragraph" id="Sx1.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">2. Theoretical Contributions</h4>
<div class="ltx_para" id="Sx1.SS0.SSS0.Px2.p1">
<ul class="ltx_itemize" id="Sx1.I3">
<li class="ltx_item" id="Sx1.I3.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.1.</span>
<div class="ltx_para" id="Sx1.I3.ix1.p1">
<p class="ltx_p" id="Sx1.I3.ix1.p1.1">Does this paper make theoretical contributions? (yes/no) <span class="ltx_text" id="Sx1.I3.ix1.p1.1.1" style="--ltx-fg-color:#0000FF;">
yes</span></p>
</div>
<div class="ltx_para" id="Sx1.I3.ix1.p2">
<p class="ltx_p" id="Sx1.I3.ix1.p2.1"><span class="ltx_text ltx_align_left ltx_inline-block" id="Sx1.I3.ix1.p2.1.1" style="width:0.0pt;--ltx-fg-color:#0000FF;"><span class="ltx_text" id="Sx1.I3.ix1.p2.1.1.1" style="--ltx-fg-color:#000000;">If yes, please address the following points:</span></span><span class="ltx_text" id="Sx1.I3.ix1.p2.1.2" style="--ltx-fg-color:#0000FF;"></span></p>
<ul class="ltx_itemize" id="Sx1.I3.ix1.I1">
<li class="ltx_item" id="Sx1.I3.ix1.I1.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.2.</span>
<div class="ltx_para" id="Sx1.I3.ix1.I1.ix1.p1">
<p class="ltx_p" id="Sx1.I3.ix1.I1.ix1.p1.1">All assumptions and restrictions are stated clearly and formally (yes/partial/no) <span class="ltx_text" id="Sx1.I3.ix1.I1.ix1.p1.1.1" style="--ltx-fg-color:#0000FF;">
yes</span></p>
</div>
</li>
<li class="ltx_item" id="Sx1.I3.ix1.I1.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.3.</span>
<div class="ltx_para" id="Sx1.I3.ix1.I1.ix2.p1">
<p class="ltx_p" id="Sx1.I3.ix1.I1.ix2.p1.1">All novel claims are stated formally (e.g., in theorem statements) (yes/partial/no) <span class="ltx_text" id="Sx1.I3.ix1.I1.ix2.p1.1.1" style="--ltx-fg-color:#0000FF;">
yes</span></p>
</div>
</li>
<li class="ltx_item" id="Sx1.I3.ix1.I1.ix3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.4.</span>
<div class="ltx_para" id="Sx1.I3.ix1.I1.ix3.p1">
<p class="ltx_p" id="Sx1.I3.ix1.I1.ix3.p1.1">Proofs of all novel claims are included (yes/partial/no) <span class="ltx_text" id="Sx1.I3.ix1.I1.ix3.p1.1.1" style="--ltx-fg-color:#0000FF;">
yes</span></p>
</div>
</li>
<li class="ltx_item" id="Sx1.I3.ix1.I1.ix4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.5.</span>
<div class="ltx_para" id="Sx1.I3.ix1.I1.ix4.p1">
<p class="ltx_p" id="Sx1.I3.ix1.I1.ix4.p1.1">Proof sketches or intuitions are given for complex and/or novel results (yes/partial/no) <span class="ltx_text" id="Sx1.I3.ix1.I1.ix4.p1.1.1" style="--ltx-fg-color:#0000FF;">
yes</span></p>
</div>
</li>
<li class="ltx_item" id="Sx1.I3.ix1.I1.ix5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.6.</span>
<div class="ltx_para" id="Sx1.I3.ix1.I1.ix5.p1">
<p class="ltx_p" id="Sx1.I3.ix1.I1.ix5.p1.1">Appropriate citations to theoretical tools used are given (yes/partial/no) <span class="ltx_text" id="Sx1.I3.ix1.I1.ix5.p1.1.1" style="--ltx-fg-color:#0000FF;">
yes</span></p>
</div>
</li>
<li class="ltx_item" id="Sx1.I3.ix1.I1.ix6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.7.</span>
<div class="ltx_para" id="Sx1.I3.ix1.I1.ix6.p1">
<p class="ltx_p" id="Sx1.I3.ix1.I1.ix6.p1.1">All theoretical claims are demonstrated empirically to hold (yes/partial/no/NA) <span class="ltx_text" id="Sx1.I3.ix1.I1.ix6.p1.1.1" style="--ltx-fg-color:#0000FF;">
yes</span></p>
</div>
</li>
<li class="ltx_item" id="Sx1.I3.ix1.I1.ix7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.8.</span>
<div class="ltx_para" id="Sx1.I3.ix1.I1.ix7.p1">
<p class="ltx_p" id="Sx1.I3.ix1.I1.ix7.p1.1">All experimental code used to eliminate or disprove claims is included (yes/no/NA) <span class="ltx_text" id="Sx1.I3.ix1.I1.ix7.p1.1.1" style="--ltx-fg-color:#0000FF;">
yes</span></p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_paragraph" id="Sx1.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">3. Dataset Usage</h4>
<div class="ltx_para" id="Sx1.SS0.SSS0.Px3.p1">
<ul class="ltx_itemize" id="Sx1.I4">
<li class="ltx_item" id="Sx1.I4.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.1.</span>
<div class="ltx_para" id="Sx1.I4.ix1.p1">
<p class="ltx_p" id="Sx1.I4.ix1.p1.1">Does this paper rely on one or more datasets? (yes/no) <span class="ltx_text" id="Sx1.I4.ix1.p1.1.1" style="--ltx-fg-color:#0000FF;">
yes</span></p>
</div>
<div class="ltx_para" id="Sx1.I4.ix1.p2">
<p class="ltx_p" id="Sx1.I4.ix1.p2.1"><span class="ltx_text ltx_align_left ltx_inline-block" id="Sx1.I4.ix1.p2.1.1" style="width:0.0pt;--ltx-fg-color:#0000FF;"><span class="ltx_text" id="Sx1.I4.ix1.p2.1.1.1" style="--ltx-fg-color:#000000;">If yes, please address the following points:</span></span><span class="ltx_text" id="Sx1.I4.ix1.p2.1.2" style="--ltx-fg-color:#0000FF;"></span></p>
<ul class="ltx_itemize" id="Sx1.I4.ix1.I1">
<li class="ltx_item" id="Sx1.I4.ix1.I1.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.2.</span>
<div class="ltx_para" id="Sx1.I4.ix1.I1.ix1.p1">
<p class="ltx_p" id="Sx1.I4.ix1.I1.ix1.p1.1">A motivation is given for why the experiments are conducted on the selected datasets (yes/partial/no/NA) <span class="ltx_text" id="Sx1.I4.ix1.I1.ix1.p1.1.1" style="--ltx-fg-color:#0000FF;">
yes</span></p>
</div>
</li>
<li class="ltx_item" id="Sx1.I4.ix1.I1.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.3.</span>
<div class="ltx_para" id="Sx1.I4.ix1.I1.ix2.p1">
<p class="ltx_p" id="Sx1.I4.ix1.I1.ix2.p1.1">All novel datasets introduced in this paper are included in a data appendix (yes/partial/no/NA) <span class="ltx_text" id="Sx1.I4.ix1.I1.ix2.p1.1.1" style="--ltx-fg-color:#0000FF;">
NA</span></p>
</div>
</li>
<li class="ltx_item" id="Sx1.I4.ix1.I1.ix3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.4.</span>
<div class="ltx_para" id="Sx1.I4.ix1.I1.ix3.p1">
<p class="ltx_p" id="Sx1.I4.ix1.I1.ix3.p1.1">All novel datasets introduced in this paper will be made publicly available upon publication of the paper with a license that allows free usage for research purposes (yes/partial/no/NA) <span class="ltx_text" id="Sx1.I4.ix1.I1.ix3.p1.1.1" style="--ltx-fg-color:#0000FF;">
NA</span></p>
</div>
</li>
<li class="ltx_item" id="Sx1.I4.ix1.I1.ix4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.5.</span>
<div class="ltx_para" id="Sx1.I4.ix1.I1.ix4.p1">
<p class="ltx_p" id="Sx1.I4.ix1.I1.ix4.p1.1">All datasets drawn from the existing literature (potentially including authors’ own previously published work) are accompanied by appropriate citations (yes/no/NA) <span class="ltx_text" id="Sx1.I4.ix1.I1.ix4.p1.1.1" style="--ltx-fg-color:#0000FF;">
yes</span></p>
</div>
</li>
<li class="ltx_item" id="Sx1.I4.ix1.I1.ix5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.6.</span>
<div class="ltx_para" id="Sx1.I4.ix1.I1.ix5.p1">
<p class="ltx_p" id="Sx1.I4.ix1.I1.ix5.p1.1">All datasets drawn from the existing literature (potentially including authors’ own previously published work) are publicly available (yes/partial/no/NA) <span class="ltx_text" id="Sx1.I4.ix1.I1.ix5.p1.1.1" style="--ltx-fg-color:#0000FF;">
yes</span></p>
</div>
</li>
<li class="ltx_item" id="Sx1.I4.ix1.I1.ix6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.7.</span>
<div class="ltx_para" id="Sx1.I4.ix1.I1.ix6.p1">
<p class="ltx_p" id="Sx1.I4.ix1.I1.ix6.p1.1">All datasets that are not publicly available are described in detail, with explanation why publicly available alternatives are not scientifically satisficing (yes/partial/no/NA) <span class="ltx_text" id="Sx1.I4.ix1.I1.ix6.p1.1.1" style="--ltx-fg-color:#0000FF;">
NA</span></p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_paragraph" id="Sx1.SS0.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">4. Computational Experiments</h4>
<div class="ltx_para" id="Sx1.SS0.SSS0.Px4.p1">
<ul class="ltx_itemize" id="Sx1.I5">
<li class="ltx_item" id="Sx1.I5.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.1.</span>
<div class="ltx_para" id="Sx1.I5.ix1.p1">
<p class="ltx_p" id="Sx1.I5.ix1.p1.1">Does this paper include computational experiments? (yes/no) <span class="ltx_text" id="Sx1.I5.ix1.p1.1.1" style="--ltx-fg-color:#0000FF;">
yes</span></p>
</div>
<div class="ltx_para" id="Sx1.I5.ix1.p2">
<p class="ltx_p" id="Sx1.I5.ix1.p2.1"><span class="ltx_text ltx_align_left ltx_inline-block" id="Sx1.I5.ix1.p2.1.1" style="width:0.0pt;--ltx-fg-color:#0000FF;"><span class="ltx_text" id="Sx1.I5.ix1.p2.1.1.1" style="--ltx-fg-color:#000000;">If yes, please address the following points:</span></span><span class="ltx_text" id="Sx1.I5.ix1.p2.1.2" style="--ltx-fg-color:#0000FF;"></span></p>
<ul class="ltx_itemize" id="Sx1.I5.ix1.I1">
<li class="ltx_item" id="Sx1.I5.ix1.I1.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.2.</span>
<div class="ltx_para" id="Sx1.I5.ix1.I1.ix1.p1">
<p class="ltx_p" id="Sx1.I5.ix1.I1.ix1.p1.1">This paper states the number and range of values tried per (hyper-) parameter during development of the paper, along with the criterion used for selecting the final parameter setting (yes/partial/no/NA) <span class="ltx_text" id="Sx1.I5.ix1.I1.ix1.p1.1.1" style="--ltx-fg-color:#0000FF;">
yes</span></p>
</div>
</li>
<li class="ltx_item" id="Sx1.I5.ix1.I1.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.3.</span>
<div class="ltx_para" id="Sx1.I5.ix1.I1.ix2.p1">
<p class="ltx_p" id="Sx1.I5.ix1.I1.ix2.p1.1">Any code required for pre-processing data is included in the appendix (yes/partial/no) <span class="ltx_text" id="Sx1.I5.ix1.I1.ix2.p1.1.1" style="--ltx-fg-color:#0000FF;">
yes</span></p>
</div>
</li>
<li class="ltx_item" id="Sx1.I5.ix1.I1.ix3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.4.</span>
<div class="ltx_para" id="Sx1.I5.ix1.I1.ix3.p1">
<p class="ltx_p" id="Sx1.I5.ix1.I1.ix3.p1.1">All source code required for conducting and analyzing the experiments is included in a code appendix (yes/partial/no) <span class="ltx_text" id="Sx1.I5.ix1.I1.ix3.p1.1.1" style="--ltx-fg-color:#0000FF;">
yes</span></p>
</div>
</li>
<li class="ltx_item" id="Sx1.I5.ix1.I1.ix4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.5.</span>
<div class="ltx_para" id="Sx1.I5.ix1.I1.ix4.p1">
<p class="ltx_p" id="Sx1.I5.ix1.I1.ix4.p1.1">All source code required for conducting and analyzing the experiments will be made publicly available upon publication of the paper with a license that allows free usage for research purposes (yes/partial/no) <span class="ltx_text" id="Sx1.I5.ix1.I1.ix4.p1.1.1" style="--ltx-fg-color:#0000FF;">
yes</span></p>
</div>
</li>
<li class="ltx_item" id="Sx1.I5.ix1.I1.ix5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.6.</span>
<div class="ltx_para" id="Sx1.I5.ix1.I1.ix5.p1">
<p class="ltx_p" id="Sx1.I5.ix1.I1.ix5.p1.1">All source code implementing new methods have comments detailing the implementation, with references to the paper where each step comes from (yes/partial/no) <span class="ltx_text" id="Sx1.I5.ix1.I1.ix5.p1.1.1" style="--ltx-fg-color:#0000FF;">
yes</span></p>
</div>
</li>
<li class="ltx_item" id="Sx1.I5.ix1.I1.ix6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.7.</span>
<div class="ltx_para" id="Sx1.I5.ix1.I1.ix6.p1">
<p class="ltx_p" id="Sx1.I5.ix1.I1.ix6.p1.1">If an algorithm depends on randomness, then the method used for setting seeds is described in a way sufficient to allow replication of results (yes/partial/no/NA) <span class="ltx_text" id="Sx1.I5.ix1.I1.ix6.p1.1.1" style="--ltx-fg-color:#0000FF;">
yes</span></p>
</div>
</li>
<li class="ltx_item" id="Sx1.I5.ix1.I1.ix7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.8.</span>
<div class="ltx_para" id="Sx1.I5.ix1.I1.ix7.p1">
<p class="ltx_p" id="Sx1.I5.ix1.I1.ix7.p1.1">This paper specifies the computing infrastructure used for running experiments (hardware and software), including GPU/CPU models; amount of memory; operating system; names and versions of relevant software libraries and frameworks (yes/partial/no) <span class="ltx_text" id="Sx1.I5.ix1.I1.ix7.p1.1.1" style="--ltx-fg-color:#0000FF;">
yes</span></p>
</div>
</li>
<li class="ltx_item" id="Sx1.I5.ix1.I1.ix8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.9.</span>
<div class="ltx_para" id="Sx1.I5.ix1.I1.ix8.p1">
<p class="ltx_p" id="Sx1.I5.ix1.I1.ix8.p1.1">This paper formally describes evaluation metrics used and explains the motivation for choosing these metrics (yes/partial/no) <span class="ltx_text" id="Sx1.I5.ix1.I1.ix8.p1.1.1" style="--ltx-fg-color:#0000FF;">
yes</span></p>
</div>
</li>
<li class="ltx_item" id="Sx1.I5.ix1.I1.ix9" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.10.</span>
<div class="ltx_para" id="Sx1.I5.ix1.I1.ix9.p1">
<p class="ltx_p" id="Sx1.I5.ix1.I1.ix9.p1.1">This paper states the number of algorithm runs used to compute each reported result (yes/no) <span class="ltx_text" id="Sx1.I5.ix1.I1.ix9.p1.1.1" style="--ltx-fg-color:#0000FF;">
yes</span></p>
</div>
</li>
<li class="ltx_item" id="Sx1.I5.ix1.I1.ix10" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.11.</span>
<div class="ltx_para" id="Sx1.I5.ix1.I1.ix10.p1">
<p class="ltx_p" id="Sx1.I5.ix1.I1.ix10.p1.1">Analysis of experiments goes beyond single-dimensional summaries of performance (e.g., average; median) to include measures of variation, confidence, or other distributional information (yes/no) <span class="ltx_text" id="Sx1.I5.ix1.I1.ix10.p1.1.1" style="--ltx-fg-color:#0000FF;">
no</span></p>
</div>
</li>
<li class="ltx_item" id="Sx1.I5.ix1.I1.ix11" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.12.</span>
<div class="ltx_para" id="Sx1.I5.ix1.I1.ix11.p1">
<p class="ltx_p" id="Sx1.I5.ix1.I1.ix11.p1.1">The significance of any improvement or decrease in performance is judged using appropriate statistical tests (e.g., Wilcoxon signed-rank) (yes/partial/no) <span class="ltx_text" id="Sx1.I5.ix1.I1.ix11.p1.1.1" style="--ltx-fg-color:#0000FF;">
no</span></p>
</div>
</li>
<li class="ltx_item" id="Sx1.I5.ix1.I1.ix12" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.13.</span>
<div class="ltx_para" id="Sx1.I5.ix1.I1.ix12.p1">
<p class="ltx_p" id="Sx1.I5.ix1.I1.ix12.p1.1">This paper lists all final (hyper-)parameters used for each model/algorithm in the paper’s experiments (yes/partial/no/NA) <span class="ltx_text" id="Sx1.I5.ix1.I1.ix12.p1.1.1" style="--ltx-fg-color:#0000FF;">
yes</span></p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jan 13 02:26:40 2026 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>


<!-- ===== END HTML ===== -->
