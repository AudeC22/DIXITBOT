{
  "ok": true,
  "query": "multimodal transformer",
  "count": 5,
  "max_results": 5,
  "hit_limit_100": false,
  "message_if_limit": "",
  "items": [
    {
      "arxiv_id": "",
      "title": "BenchSeg: A Large-Scale Dataset and Benchmark for Multi-View Food Video Segmentation",
      "authors": [
        "Ahmad AlMughrabi",
        "Guillermo Rivo",
        "Carlos Jiménez-Farfán",
        "Umair Haroon",
        "Farid Al-Areqi",
        "Hyunjun Jung",
        "Benjamin Busam",
        "Ricardo Marques",
        "Petia Radeva"
      ],
      "abstract": "Food image segmentation is a critical task for dietary analysis, enabling accurate estimation of food volume and nutrients. However, current methods suffer from limited multi-view data and poor generalization to new viewpoints. We introduce BenchSeg, a novel multi-view food video segmentation dataset and benchmark. BenchSeg aggregates 55 dish scenes (from Nutrition5k, Vegetables & Fruits, MetaFood3D, and FoodKit) with 25,284 meticulously annotated frames, capturing each dish under free 360° camera motion. We evaluate a diverse set of 20 state-of-the-art segmentation models (e.g., SAM-based, transformer , CNN, and large multimodal ) on the existing FoodSeg103 dataset and evaluate them (alone and combined with video-memory modules) on BenchSeg. Quantitative and qualitative results demonstrate that while standard image segmenters degrade sharply under novel viewpoints, memory-augmented methods maintain temporal consistency across frames. Our best model based on a combination of SeTR-MLA+XMem2 outperforms prior work (e.g., improving over FoodMem by ~2.63% mAP), offering new insights into food segmentation and tracking for dietary analysis. We release BenchSeg to foster future research. The project page including the dataset annotations and the food segmentation models can be found at https://amughrabi.github.io/benchseg.",
      "abs_url": "",
      "pdf_url": "",
      "source": "arxiv_search_cs"
    },
    {
      "arxiv_id": "",
      "title": "Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions",
      "authors": [
        "Yongqi Li",
        "Hao Lang",
        "Tieyun Qian",
        "Yongbin Li"
      ],
      "abstract": "Vision-language models are increasingly employed as multimodal conversational agents (MCAs) for diverse conversational tasks. Recently, reinforcement learning (RL) has been widely explored for adapting MCAs to various human-AI interaction scenarios. Despite showing great enhancement in generalization performance, fine-tuning MCAs via RL still faces challenges in handling the extremely large text token space. To address this, we learn a compact latent action space for RL fine-tuning instead. Specifically, we adopt the learning from observation mechanism to construct the codebook for the latent action space, where future observations are leveraged to estimate current latent actions that could further be used to reconstruct future observations. However, the scarcity of paired image-text data hinders learning a codebook with sufficient coverage. Thus, we leverage both paired image-text data and text-only data to construct the latent action space, using a cross-modal projector for transforming text embeddings into image-text embeddings. We initialize the cross-modal projector on paired image-text data, and further train it on massive text-only data with a novel cycle consistency loss to enhance its robustness. We show that our latent action based method outperforms competitive baselines on two conversation tasks across various RL algorithms.",
      "abs_url": "",
      "pdf_url": "",
      "source": "arxiv_search_cs"
    },
    {
      "arxiv_id": "",
      "title": "Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges",
      "authors": [
        "Agnivo Gosai",
        "Shuvodeep De",
        "Karun Thankachan"
      ],
      "abstract": "This paper presents a comprehensive survey of sentiment analysis methods for movie reviews, a benchmark task that has played a central role in advancing natural language processing. We review the evolution of techniques from early lexicon-based and classical machine learning approaches to modern deep learning architectures and large language models, covering widely used datasets such as IMDb, Rotten Tomatoes, and SST-2, and models ranging from Naive Bayes and support vector machines to LSTM networks, BERT, and attention-based transformers . Beyond summarizing prior work, this survey differentiates itself by offering a comparative, challenge-driven analysis of how these modeling paradigms address domain-specific issues such as sarcasm, negation, contextual ambiguity, and domain shift, which remain open problems in existing literature. Unlike earlier reviews that focus primarily on text-only pipelines, we also synthesize recent advances in multimodal sentiment analysis that integrate textual, audio, and visual cues from movie trailers and clips. In addition, we examine emerging concerns related to interpretability, fairness, and robustness that are often underexplored in prior surveys, and we outline future research directions including zero-shot and few-shot learning, hybrid symbolic--neural models, and real-time deployment considerations. Overall, this abstract provides a domain-focused roadmap that highlights both established solutions and unresolved challenges toward building more accurate, generalizable, and explainable sentiment analysis systems for movie review data.",
      "abs_url": "",
      "pdf_url": "",
      "source": "arxiv_search_cs"
    },
    {
      "arxiv_id": "",
      "title": "MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation",
      "authors": [
        "Changli Wu",
        "Haodong Wang",
        "Jiayi Ji",
        "Yutian Yao",
        "Chunsai Du",
        "Jihua Kang",
        "Yanwei Fu",
        "Liujuan Cao"
      ],
      "abstract": "Most existing 3D referring expression segmentation (3DRES) methods rely on dense, high-quality point clouds, while real-world agents such as robots and mobile phones operate with only a few sparse RGB views and strict latency constraints. We introduce Multi-view 3D Referring Expression Segmentation (MV-3DRES), where the model must recover scene structure and segment the referred object directly from sparse multi-view images. Traditional two-stage pipelines, which first reconstruct a point cloud and then perform segmentation, often yield low-quality geometry, produce coarse or degraded target regions, and run slowly. We propose the Multimodal Visual Geometry Grounded Transformer (MVGGT), an efficient end-to-end framework that integrates language information into sparse-view geometric reasoning through a dual-branch design. Training in this setting exposes a critical optimization barrier, termed Foreground Gradient Dilution (FGD), where sparse 3D signals lead to weak supervision. To resolve this, we introduce Per-view No-target Suppression Optimization (PVSO), which provides stronger and more balanced gradients across views, enabling stable and efficient learning. To support consistent evaluation, we build MVRefer, a benchmark that defines standardized settings and metrics for MV-3DRES. Experiments show that MVGGT establishes the first strong baseline and achieves both high accuracy and fast inference, outperforming existing alternatives. Code and models are publicly available at https://mvggt.github.io.",
      "abs_url": "",
      "pdf_url": "",
      "source": "arxiv_search_cs"
    },
    {
      "arxiv_id": "",
      "title": "MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data",
      "authors": [
        "Mengmeng Zhang",
        "Xiaoping Wu",
        "Hao Luo",
        "Fan Wang",
        "Yisheng Lv"
      ],
      "abstract": "Vision-Language Models (VLMs) can generate convincing clinical narratives, yet frequently struggle to visually ground their statements. We posit this limitation arises from the scarcity of high-quality, large-scale clinical referring-localization pairs. To address this, we introduce MedGround, an automated pipeline that transforms segmentation resources into high-quality medical referring grounding data. Leveraging expert masks as spatial anchors, MedGround precisely derives localization targets, extracts shape and spatial cues, and guides VLMs to synthesize natural, clinically grounded queries that reflect morphology and location. To ensure data rigor, a multi-stage verification system integrates strict formatting checks, geometry- and medical-prior rules, and image-based visual judging to filter out ambiguous or visually unsupported samples. Finally, we present MedGround-35K, a novel multimodal medical dataset. Extensive experiments demonstrate that VLMs trained with MedGround-35K consistently achieve improved referring grounding performance, enhance multi-object semantic disambiguation, and exhibit strong generalization to unseen grounding settings. This work highlights MedGround as a scalable, data-driven approach to anchor medical language to verifiable visual evidence. Dataset and code will be released publicly upon acceptance.",
      "abs_url": "",
      "pdf_url": "",
      "source": "arxiv_search_cs"
    }
  ],
  "raw_html_files": [
    "data_lake\\raw\\arxiv_search_20260113_103039_start_0.html"
  ],
  "source_url_example": "https://arxiv.org/search/cs?query=multimodal%20transformer&searchtype=all&abstracts=show&order=-announced_date_first&size=50&start=0"
}