{
  "ok": true,
  "query": "multimodal transformer",
  "sort": "relevance",
  "count": 5,
  "max_results": 5,
  "hit_limit_100": false,
  "message_if_limit": "",
  "items": [
    {
      "arxiv_id": "2601.08457",
      "title": "An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English",
      "authors": [
        "Sargam Yadav",
        "Abhishek Kaushik",
        "Kevin Mc Daid"
      ],
      "abstract": "Digital platforms have an ever-expanding user base, and act as a hub for communication, business, and connectivity. However, this has also allowed for the spread of hate speech and misogyny. Artificial intelligence models have emerged as an effective solution for countering online hate speech but are under explored for low resource and code-mixed languages and suffer from a lack of interpretability. Explainable Artificial Intelligence (XAI) can enhance transparency in the decisions of deep learning models, which is crucial for a sensitive domain such as hate speech detection. In this paper, we present a multi-modal and explainable web application for detecting misogyny in text and memes in code-mixed Hindi and English. The system leverages state-of-the-art transformer -based models that support multilingual and multimodal settings. For text-based misogyny identification, the system utilizes XLM-RoBERTa (XLM-R) and multilingual Bidirectional Encoder Representations from Transformers (mBERT) on a dataset of approximately 4,193 comments. For multimodal misogyny identification from memes, the system utilizes mBERT + EfficientNet, and mBERT + ResNET trained on a dataset of approximately 4,218 memes. It also provides feature importance scores using explainability techniques including Shapley Additive Values (SHAP) and Local Interpretable Model Agnostic Explanations (LIME). The application aims to serve as a tool for both researchers and content moderators, to promote further research in the field, combat gender based digital violence, and ensure a safe digital space. The system has been evaluated using human evaluators who provided their responses on Chatbot Usability Questionnaire (CUQ) and User Experience Questionnaire (UEQ) to determine overall usability.",
      "submitted_date": "13 January, 2026",
      "abs_url": "https://arxiv.org/abs/2601.08457",
      "pdf_url": "https://arxiv.org/pdf/2601.08457",
      "html_url": "https://arxiv.org/html/2601.08457",
      "published_date": "",
      "doi": "",
      "license": "",
      "references": [],
      "references_dois": [],
      "errors": [
        "html_http_404"
      ],
      "missing_fields": [
        "published_date",
        "doi",
        "license",
        "references",
        "references_dois"
      ],
      "url_hint_if_missing": "Page HTML indisponible (404). Vérifie l'abs: https://arxiv.org/abs/2601.08457"
    },
    {
      "arxiv_id": "2601.08179",
      "title": "Instruction-Driven 3D Facial Expression Generation and Transition",
      "authors": [
        "Anh H. Vo",
        "Tae-Seok Kim",
        "Hulin Jin",
        "Soo-Mi Choi",
        "Yong-Guk Kim"
      ],
      "abstract": "A 3D avatar typically has one of six cardinal facial expressions. To simulate realistic emotional variation, we should be able to render a facial transition between two arbitrary expressions. This study presents a new framework for instruction-driven facial expression generation that produces a 3D face and, starting from an image of the face, transforms the facial expression from one designated facial expression to another. The Instruction-driven Facial Expression Decomposer (IFED) module is introduced to facilitate multimodal data learning and capture the correlation between textual descriptions and facial expression features. Subsequently, we propose the Instruction to Facial Expression Transition (I2FET) method, which leverages IFED and a vertex reconstruction loss function to refine the semantic comprehension of latent vectors, thus generating a facial expression sequence according to the given instruction. Lastly, we present the Facial Expression Transition model to generate smooth transitions between facial expressions. Extensive evaluation suggests that the proposed model outperforms state-of-the-art methods on the CK+ and CelebV-HQ datasets. The results show that our framework can generate facial expression trajectories according to text instruction. Considering that text prompts allow us to make diverse descriptions of human emotional states, the repertoire of facial expressions and the transitions between them can be expanded greatly. We expect our framework to find various practical applications More information about our project can be found at https://vohoanganh.github.io/tg3dfet/",
      "submitted_date": "12 January, 2026",
      "abs_url": "https://arxiv.org/abs/2601.08179",
      "pdf_url": "https://arxiv.org/pdf/2601.08179",
      "html_url": "https://arxiv.org/html/2601.08179v1",
      "published_date": "",
      "doi": "https://dx.doi.org/10.1109/CVPR.2016.90",
      "license": "",
      "references": [
        {
          "raw_text": "[1] S. Aneja, J. Thies, A. Dai, and M. Nießner (2023) ClipFace: text-guided editing of textured 3d morphable models . In SIGGRAPH ’23 Conference Proceedings , Cited by: §I , §I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p2.1",
            "https://arxiv.org/html/2601.08179v1#S1.p3.1"
          ],
          "dois": []
        },
        {
          "raw_text": "[2] M. C. Buehler, A. Meka, G. Li, T. Beeler, and O. Hilliges (2021) VariTex: variational neural face textures . In Proceedings of the IEEE/CVF International Conference on Computer Vision , Cited by: §I , § II-B .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p2.1",
            "https://arxiv.org/html/2601.08179v1#S2.SS2.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "[3] C. Chen, Q. Fan, and R. Panda (2021) CrossViT: cross-attention multi-scale vision transformer for image classification . 2021 IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 347–356 . Cited by: § III-B 1 .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S3.SS2.SSS1.p2.7"
          ],
          "dois": []
        },
        {
          "raw_text": "[4] Y. Chen and S. Xiong (2023) Rethinking one-shot face reenactment: a spatial–temporal reconstruction view . Knowledge Based Systems 277 ( C ). Cited by: §I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "[5] Y. Cui, M. Jia, T. Lin, Y. Song, and S. J. Belongie (2019) Class-balanced loss based on effective number of samples . 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 9260–9269 . Cited by: § IV-C , § IV-G 1 .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S4.SS3.p2.2",
            "https://arxiv.org/html/2601.08179v1#S4.SS7.SSS1.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "[6] R. Daněček, K. Chhatre, S. Tripathi, Y. Wen, M. Black, and T. Bolkart (2023) Emotional speech-driven animation with content-emotion disentanglement . In SIGGRAPH Asia 2023 Conference Papers , Cited by: § II-A , § II-A , § III-B 1 .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S2.SS1.p1.1",
            "https://arxiv.org/html/2601.08179v1#S2.SS1.p2.1",
            "https://arxiv.org/html/2601.08179v1#S3.SS2.SSS1.p2.7"
          ],
          "dois": []
        },
        {
          "raw_text": "[7] H. Ding, K. Sricharan, and R. Chellappa (2018) Exprgan: facial expression editing with controllable expression intensity . AAAI . Cited by: §I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "[8] M. C. Doukas, S. Zafeiriou, and V. Sharmanska (2021-10) HeadGAN: one-shot neural head synthesis and editing . In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 14398–14407 . Cited by: §I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p2.1"
          ],
          "dois": []
        },
        {
          "raw_text": "[9] Y. Feng, H. Feng, M. J. Black, and T. Bolkart (2021) Learning an animatable detailed 3D face model from in-the-wild images . In ACM Transactions on Graphics, (Proc. SIGGRAPH) , Vol. 40 . Cited by: § III-B 1 , § III-B 1 , §V .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S3.SS2.SSS1.p7.8",
            "https://arxiv.org/html/2601.08179v1#S3.SS2.SSS1.p8.12",
            "https://arxiv.org/html/2601.08179v1#S5.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "[10] P. Grassal, M. Prinzler, T. Leistner, C. Rother, M. Nießner, and J. Thies (2022) Neural head avatars from monocular rgb videos . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 18653–18664 . Cited by: §I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p2.1"
          ],
          "dois": []
        },
        {
          "raw_text": "[11] X. Gu, C. Wen, W. Ye, J. Song, and Y. Gao (2023) Seer: language instructed video prediction with latent diffusion models . ICLR 2024 . Cited by: §I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p3.1"
          ],
          "dois": []
        },
        {
          "raw_text": "[12] J. Guo, H. Manukyan, C. Yang, C. Wang, L. Khachatryan, S. Navasardyan, S. Song, H. Shi, and G. Huang (2023) FaceCLIP: facial image-to-video translation via a brief text description . IEEE Transactions on Circuits and Systems for Video Technology , pp. 1–1 . Cited by: §I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "[13] T. Hang, H. Yang, B. Liu, J. Fu, X. Geng, and B. Guo (2023) Language-guided face animation by recurrent stylegan-based generator . IEEE Transactions on Multimedia 25 ( ), pp. 1–12 . Cited by: §I , §I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p1.1",
            "https://arxiv.org/html/2601.08179v1#S1.p3.1"
          ],
          "dois": []
        },
        {
          "raw_text": "[14] K. He, X. Zhang, S. Ren, and J. Sun (2016) Deep Residual Learning for Image Recognition . In Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition , CVPR ’16 , pp. 770–778 . External Links: Document , ISSN 1063-6919 Cited by: § IV-C , § IV-G 1 , TABLE X , TABLE X .",
          "urls": [
            "https://dx.doi.org/10.1109/CVPR.2016.90",
            "https://arxiv.org/html/2601.08179v1#S4.SS3.p2.2",
            "https://arxiv.org/html/2601.08179v1#S4.SS7.SSS1.p1.1",
            "https://arxiv.org/html/2601.08179v1#S4.T10.21.21.2.2",
            "https://arxiv.org/html/2601.08179v1#S4.T10.21.24.5.2"
          ],
          "dois": [
            "https://dx.doi.org/10.1109/CVPR.2016.90"
          ]
        },
        {
          "raw_text": "[15] Z. Huang, K. C.K. Chan, Y. Jiang, and Z. Liu (2023) Collaborative diffusion for multi-modal face generation and editing . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , Cited by: §I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "[16] S. Hwang, J. Hyung, D. Kim, M. Kim, and J. Choo (2023-10) FaceCLIPNeRF: text-driven 3d face manipulation using deformable neural radiance fields . In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 3469–3479 . Cited by: §I , §I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p2.1",
            "https://arxiv.org/html/2601.08179v1#S1.p3.1"
          ],
          "dois": []
        },
        {
          "raw_text": "[17] Y. Jiang, Z. Huang, X. Pan, C. C. Loy, and Z. Liu (2021) Talk-to-edit: fine-grained facial editing via dialog . In Proceedings of the IEEE/CVF International Conference on Computer Vision , Cited by: §I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "[18] T. Khakhulin, V. Sklyarova, V. Lempitsky, and E. Zakharov (2022) Realistic one-shot mesh-based head avatars . In European Conference of Computer vision (ECCV) , Cited by: §I , § II-B , § III-B 2 , § III-B 2 , Figure 11 , Figure 11 , § IV-B , § IV-D 1 , §V .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p1.1",
            "https://arxiv.org/html/2601.08179v1#S2.SS2.p1.1",
            "https://arxiv.org/html/2601.08179v1#S3.SS2.SSS2.p2.3",
            "https://arxiv.org/html/2601.08179v1#S3.SS2.SSS2.p3.4",
            "https://arxiv.org/html/2601.08179v1#S4.F11",
            "https://arxiv.org/html/2601.08179v1#S4.F11.3.2",
            "https://arxiv.org/html/2601.08179v1#S4.SS2.p1.1",
            "https://arxiv.org/html/2601.08179v1#S4.SS4.SSS1.p4.1",
            "https://arxiv.org/html/2601.08179v1#S5.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "[19] M. Kim, F. Liu, A. Jain, and X. Liu (2023-06) DCFace: synthetic face generation with dual condition diffusion model . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 12715–12725 . Cited by: §I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "[20] D. P. Kingma and J. Ba (2014) Adam: a method for stochastic optimization . ICLR abs/1412.6980 . Cited by: § IV-B .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S4.SS2.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "[21] Y. Lee, T. Choi, H. Go, H. Lee, S. Cho, and J. Kim (2023) Exp-gan: 3d-aware facial image generation with expression control . In Computer Vision – ACCV 2022: 16th Asian Conference on Computer Vision , pp. 151–167 . External Links: ISBN 978-3-031-26292-0 Cited by: § II-B .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S2.SS2.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "[22] T. Li, T. Bolkart, M. J. Black, H. Li, and J. Romero (2017) Learning a model of facial shape and expression from 4d scans . ACM Transactions on Graphics (TOG) 36 , pp. 1 – 17 . Cited by: §I , § II-A , § III-B 1 .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p3.1",
            "https://arxiv.org/html/2601.08179v1#S2.SS1.p1.1",
            "https://arxiv.org/html/2601.08179v1#S3.SS2.SSS1.p7.8"
          ],
          "dois": []
        },
        {
          "raw_text": "[23] X. Li, D. Zhang, M. Li, and D. Lee (2023) Accurate head pose estimation using image rectification and a lightweight convolutional neural network . IEEE Transactions on Multimedia 25 ( ), pp. 2239–2251 . Cited by: § IV-A 2 .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S4.SS1.SSS2.p1.2"
          ],
          "dois": []
        },
        {
          "raw_text": "[24] K. Lin, A. Trevithick, K. Cheng, M. Sarkis, M. Ghafoorian, N. Bi, G. Reitmayr, and R. Ramamoorthi (2023) PVP: Personalized Video Prior for Editable Dynamic Portraits using StyleGAN . Computer Graphics Forum 42 ( 4 ), pp. 13 pages . Cited by: § III-B 1 .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S3.SS2.SSS1.p2.7"
          ],
          "dois": []
        },
        {
          "raw_text": "[25] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and I. Matthews (2010) The extended cohn-kanade dataset (ck+): a complete dataset for action unit and emotion-specified expression . In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops , Vol. , pp. 94–101 . Cited by: § IV-A 1 .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S4.SS1.SSS1.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "[26] H. Ma, T. Zhang, S. Sun, X. Yan, K. Han, and X. Xie (2024) CVTHead: one-shot controllable head avatar with vertex-feature transformer . IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) . Cited by: §I , § II-B , § III-B 2 , Figure 11 , Figure 11 , § IV-B , § IV-D 1 , §V .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p1.1",
            "https://arxiv.org/html/2601.08179v1#S2.SS2.p1.1",
            "https://arxiv.org/html/2601.08179v1#S3.SS2.SSS2.p3.4",
            "https://arxiv.org/html/2601.08179v1#S4.F11",
            "https://arxiv.org/html/2601.08179v1#S4.F11.3.2",
            "https://arxiv.org/html/2601.08179v1#S4.SS2.p1.1",
            "https://arxiv.org/html/2601.08179v1#S4.SS4.SSS1.p4.1",
            "https://arxiv.org/html/2601.08179v1#S5.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "[27] N. Otberdout, C. Ferrari, M. Daoudi, S. Berretti, and A. Bimbo (2023) Generating complex 4d expression transitions by learning face landmark trajectories . IEEE Transactions on Affective Computing . Cited by: §I , § II-A , § II-A , § IV-C .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p2.1",
            "https://arxiv.org/html/2601.08179v1#S2.SS1.p1.1",
            "https://arxiv.org/html/2601.08179v1#S2.SS1.p2.1",
            "https://arxiv.org/html/2601.08179v1#S4.SS3.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "[28] N. Otberdout, C. Ferrari, M. Daoudi, S. Berritti, and A. Del Bimbo (2022-06) Sparse to dense dynamic 3d facial expression generation . In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , Cited by: § II-A .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S2.SS1.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "[29] F. Paraperas Papantoniou, P. P. Filntisis, P. Maragos, and A. Roussos (2022) Neural emotion director: speech-preserving semantic control of facial expressions in ”in-the-wild” videos . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , Cited by: §I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p2.1"
          ],
          "dois": []
        },
        {
          "raw_text": "[30] O. Patashnik, Z. Wu, E. Shechtman, D. Cohen-Or, and D. Lischinski (2021-10) StyleCLIP: text-driven manipulation of stylegan imagery . In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 2085–2094 . Cited by: §I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p3.1"
          ],
          "dois": []
        },
        {
          "raw_text": "[31] B. Peng, H. Fan, W. Wang, J. Dong, and S. Lyu (2022) A unified framework for high fidelity face swap and expression reenactment . IEEE Transactions on Circuits and Systems for Video Technology 32 ( 6 ), pp. 3673–3684 . Cited by: §I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "[32] S. Qian, T. Kirschstein, L. Schoneveld, D. Davoli, S. Giebenhain, and M. Nießner (2024) GaussianAvatars: photorealistic head avatars with rigged 3d gaussians . In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , Cited by: §I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p2.1"
          ],
          "dois": []
        },
        {
          "raw_text": "[33] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. (2021) Learning transferable visual models from natural language supervision . In International conference on machine learning , pp. 8748–8763 . Cited by: § III-B 1 .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S3.SS2.SSS1.p2.7"
          ],
          "dois": []
        },
        {
          "raw_text": "[34] J. Russell and J. M. F. dez Dols (1997) Reading emotions from and into faces: resurrecting dimensional-contextual perspective . The psychology of facial expression , pp. 295–320 . Note: Cambridge University Press Cited by: § IV-F .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S4.SS6.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "[35] K. R. Scherer (2001) Appraisal considered as a process of multilevel sequential checking. . Oxford University Press , pp. 92 – 120 . Cited by: § IV-F .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S4.SS6.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "[36] S. Su, J. Zhu, L. Gao, and J. Song (2024) Utilizing greedy nature for multimodal conditional image synthesis in transformers . IEEE Transactions on Multimedia 26 ( ), pp. 2354–2366 . Cited by: §I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "[37] T. Sutter, I. Daunhawer, and J. Vogt (2020) Multimodal generative learning utilizing jensen-shannon-divergence . Vol. 33 , pp. 6100–6110 . Cited by: §I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p3.1"
          ],
          "dois": []
        },
        {
          "raw_text": "[38] M. Suzuki and Y. Matsuo (2022) A survey of multimodal deep generative models . Advanced Robotics 36 , pp. 261 – 278 . Cited by: §I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p3.1"
          ],
          "dois": []
        },
        {
          "raw_text": "[39] G. Tevet, B. Gordon, A. Hertz, A. H. Bermano, and D. Cohen-Or (2022) Motionclip: exposing human motion generation to clip space . In Computer Vision–ECCV 2022: 17th European Conference , pp. 358–374 . Cited by: § II-A , § IV-B .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S2.SS1.p1.1",
            "https://arxiv.org/html/2601.08179v1#S4.SS2.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "[40] Y. Wu, S. Xu, J. Xiang, F. Wei, Q. Chen, J. Yang, and X. Tong (2023) AniPortraitGAN: animatable 3d portrait generation from 2d image collections . In SIGGRAPH Asia 2023 Conference Proceedings , Cited by: §I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p2.1"
          ],
          "dois": []
        },
        {
          "raw_text": "[41] W. Xia, Y. Yang, J. Xue, and B. Wu (2021) TediGAN: text-guided diverse face image generation and manipulation . In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , Cited by: §I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "[42] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang (2018) The unreasonable effectiveness of deep features as a perceptual metric . In CVPR , Cited by: § IV-D 1 .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S4.SS4.SSS1.p4.1"
          ],
          "dois": []
        },
        {
          "raw_text": "[43] Y. Zhang, Y. Li, lixiong Qin, X. Liu, and W. Deng (2023) Leave no stone unturned: mine extra knowledge for imbalanced facial expression recognition . In Thirty-seventh Conference on Neural Information Processing Systems , Cited by: § IV-C , § IV-G 1 , TABLE X , TABLE X .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S4.SS3.p2.2",
            "https://arxiv.org/html/2601.08179v1#S4.SS7.SSS1.p1.1",
            "https://arxiv.org/html/2601.08179v1#S4.T10.21.20.1.2",
            "https://arxiv.org/html/2601.08179v1#S4.T10.21.23.4.2"
          ],
          "dois": []
        },
        {
          "raw_text": "[44] H. Zhu, W. Wu, W. Zhu, L. Jiang, S. Tang, L. Zhang, Z. Liu, and C. C. Loy (2022) CelebV-HQ: a large-scale video facial attributes dataset . In ECCV , Cited by: § IV-A 2 .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S4.SS1.SSS2.p1.2"
          ],
          "dois": []
        },
        {
          "raw_text": "[45] W. Zielonka, T. Bolkart, and J. Thies (2023-06) Instant volumetric head avatars . In IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) , External Links: Document Cited by: §I .",
          "urls": [
            "https://dx.doi.org/",
            "https://arxiv.org/html/2601.08179v1#S1.p1.1"
          ],
          "dois": [
            "https://dx.doi.org/"
          ]
        },
        {
          "raw_text": "[46] K. Zou, S. Faisan, B. Yu, S. Valette, and H. Seo (2023) 4D facial expression diffusion model . External Links: 2303.16611 Cited by: §I , § II-A , § II-A , § IV-C .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p2.1",
            "https://arxiv.org/html/2601.08179v1#S2.SS1.p1.1",
            "https://arxiv.org/html/2601.08179v1#S2.SS1.p2.1",
            "https://arxiv.org/html/2601.08179v1#S4.SS3.p1.1"
          ],
          "dois": []
        }
      ],
      "references_dois": [
        "https://dx.doi.org/10.1109/CVPR.2016.90",
        "https://dx.doi.org/"
      ],
      "errors": [],
      "missing_fields": [
        "published_date",
        "license"
      ],
      "url_hint_if_missing": "Champs manquants: published_date, license. Vérifie HTML: https://arxiv.org/html/2601.08179v1 | ABS: https://arxiv.org/abs/2601.08179 | PDF: https://arxiv.org/pdf/2601.08179"
    },
    {
      "arxiv_id": "2601.08151",
      "title": "Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention",
      "authors": [
        "Shezheng Song",
        "Shasha Li",
        "Jie Yu"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable progress in vision-language understanding, yet how they internally integrate visual and textual information remains poorly understood. To bridge this gap, we perform a systematic layer-wise masking analysis across multiple architectures, revealing how visual-text fusion evolves within MLLMs. The results show that fusion emerges at several specific layers rather than being uniformly distributed across the network, and certain models exhibit a late-stage \"review\" phenomenon where visual signals are reactivated before output generation. Besides, we further analyze layer-wise attention evolution and observe persistent high-attention noise on irrelevant regions, along with gradually increasing attention on text-aligned areas. Guided by these insights, we introduce a training-free contrastive attention framework that models the transformation between early fusion and final layers to highlight meaningful attention shifts. Extensive experiments across various MLLMs and benchmarks validate our analysis and demonstrate that the proposed approach improves multimodal reasoning performance. Code will be released.",
      "submitted_date": "12 January, 2026",
      "abs_url": "https://arxiv.org/abs/2601.08151",
      "pdf_url": "https://arxiv.org/pdf/2601.08151",
      "html_url": "https://arxiv.org/html/2601.08151v1",
      "published_date": "",
      "doi": "",
      "license": "",
      "references": [
        {
          "raw_text": "E. Aflalo, M. Du, S. Tseng, Y. Liu, C. Wu, N. Duan, and V. Lal (2022) Vl-interpret: an interactive visualization tool for interpreting vision-language transformers . In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition , pp. 21406–21415 . Cited by: §2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S2.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "J. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. (2022) Flamingo: a visual language model for few-shot learning . Advances in neural information processing systems 35 , pp. 23716–23736 . Cited by: Table 1 , §5.2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S4.T1.1.3.3.1",
            "https://arxiv.org/html/2601.08151v1#S5.SS2.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou (2023) Qwen-vl: a versatile vision-language model for understanding, localization, text reading, and beyond . arXiv preprint arXiv:2308.12966 . Cited by: §1 , Table 1 , §5.2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S1.p1.1",
            "https://arxiv.org/html/2601.08151v1#S4.T1.1.9.9.1",
            "https://arxiv.org/html/2601.08151v1#S5.SS2.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "J. Cao, Z. Gan, Y. Cheng, L. Yu, Y. Chen, and J. Liu (2020) Behind the scene: revealing the secrets of pre-trained vision-and-language models . In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VI 16 , pp. 565–580 . Cited by: §2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S2.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "H. Chefer, S. Gur, and L. Wolf (2021) Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers . In Proceedings of the IEEE/CVF international conference on computer vision , pp. 397–406 . Cited by: §2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S2.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, M. Zhong, Q. Zhang, X. Zhu, L. Lu, et al. (2024) Internvl: scaling up vision foundation models and aligning for generic visual-linguistic tasks . In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 24185–24198 . Cited by: Table 1 , Table 1 , §5.2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S4.T1.1.7.7.1",
            "https://arxiv.org/html/2601.08151v1#S4.T1.1.8.8.1",
            "https://arxiv.org/html/2601.08151v1#S5.SS2.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "Y. Chuang, Y. Xie, H. Luo, Y. Kim, J. Glass, and P. He (2024) Dola: decoding by contrasting layers improves factuality in large language models . arXiv preprint arXiv:2309.03883 . Cited by: §1 , §5.3 , Table 2 , Table 2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S1.p4.1",
            "https://arxiv.org/html/2601.08151v1#S5.SS3.p1.1",
            "https://arxiv.org/html/2601.08151v1#S5.T2.1.1.2.1.2",
            "https://arxiv.org/html/2601.08151v1#S5.T2.1.1.5.4.2"
          ],
          "dois": []
        },
        {
          "raw_text": "W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi (2023) InstructBLIP: towards general-purpose vision-language models with instruction tuning . External Links: 2305.06500 Cited by: §1 , Table 1 , §5.2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S1.p3.1",
            "https://arxiv.org/html/2601.08151v1#S4.T1.1.4.4.1",
            "https://arxiv.org/html/2601.08151v1#S5.SS2.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "S. Frank, E. Bugliarello, and D. Elliott (2021) Vision-and-language or vision-for-language? on cross-modal influence in multimodal transformers . arXiv preprint arXiv:2109.04448 . Cited by: §2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S2.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh (2017) Making the v in vqa matter: elevating the role of image understanding in visual question answering . In Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 6904–6913 . Cited by: §1 , §5.1 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S1.p1.1",
            "https://arxiv.org/html/2601.08151v1#S5.SS1.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "D. Gurari, Q. Li, A. J. Stangl, A. Guo, C. Lin, K. Grauman, J. Luo, and J. P. Bigham (2018) Vizwiz grand challenge: answering visual questions from blind people . In Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 3608–3617 . Cited by: §5.1 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S5.SS1.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "D. A. Hudson and C. D. Manning (2019) Gqa: a new dataset for real-world visual reasoning and compositional question answering . In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 6700–6709 . Cited by: §5.1 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S5.SS1.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "M. Jin, Q. Yu, J. Huang, Q. Zeng, Z. Wang, W. Hua, H. Zhao, K. Mei, Y. Meng, K. Ding, et al. (2024) Exploring concept depth: how large language models acquire knowledge and concept at different layers? . arXiv preprint arXiv:2404.07066 . Cited by: §2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S2.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "T. Ju, W. Sun, W. Du, X. Yuan, Z. Ren, and G. Liu (2024) How large language models encode context knowledge? a layer-wise probing study . arXiv preprint arXiv:2402.16061 . Cited by: §2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S2.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "H. Laurençon, L. Saulnier, L. Tronchon, S. Bekman, A. Singh, A. Lozhkov, T. Wang, S. Karamcheti, A. Rush, D. Kiela, et al. (2023) Obelics: an open web-scale filtered dataset of interleaved image-text documents . Advances in Neural Information Processing Systems 36 , pp. 71683–71702 . Cited by: Table 1 , §5.2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S4.T1.1.5.5.1",
            "https://arxiv.org/html/2601.08151v1#S5.SS2.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "J. Li, D. Li, S. Savarese, and S. Hoi (2023) Blip-2: bootstrapping language-image pre-training with frozen image encoders and large language models . In International conference on machine learning , pp. 19730–19742 . Cited by: Table 1 , §5.2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S4.T1.1.2.2.1",
            "https://arxiv.org/html/2601.08151v1#S5.SS2.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "J. Lin, H. Chen, Y. Fan, Y. Fan, X. Jin, H. Su, J. Fu, and X. Shen (2025) Multi-layer visual feature fusion in multimodal llms: methods, analysis, and best practices . arXiv preprint arXiv:2503.06063 . Cited by: §2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S2.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "H. Liu, C. Li, Y. Li, and Y. J. Lee (2024) Improved baselines with visual instruction tuning . arXiv:2310.03744 . Cited by: Table 1 , §5.1 , §5.2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S4.T1.1.13.13.1",
            "https://arxiv.org/html/2601.08151v1#S5.SS1.p1.1",
            "https://arxiv.org/html/2601.08151v1#S5.SS2.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "H. Liu, C. Li, Q. Wu, and Y. J. Lee (2023) Visual instruction tuning . NeurIPS . Cited by: §1 , §1 , Table 1 , Table 1 , §5.1 , §5.2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S1.p1.1",
            "https://arxiv.org/html/2601.08151v1#S1.p3.1",
            "https://arxiv.org/html/2601.08151v1#S4.T1.1.11.11.1",
            "https://arxiv.org/html/2601.08151v1#S4.T1.1.6.6.1",
            "https://arxiv.org/html/2601.08151v1#S5.SS1.p1.1",
            "https://arxiv.org/html/2601.08151v1#S5.SS2.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "Y. Lyu, P. P. Liang, Z. Deng, R. Salakhutdinov, and L. Morency (2022) Dime: fine-grained interpretations of multimodal models via disentangled local explanations . In Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society , pp. 455–467 . Cited by: §2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S2.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "K. Marino, M. Rastegari, A. Farhadi, and R. Mottaghi (2019) Ok-vqa: a visual question answering benchmark requiring external knowledge . In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition , pp. 3195–3204 . Cited by: §1 , §5.1 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S1.p1.1",
            "https://arxiv.org/html/2601.08151v1#S5.SS1.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "M. Mathew, D. Karatzas, and C. Jawahar (2021) Docvqa: a dataset for vqa on document images . In Proceedings of the IEEE/CVF winter conference on applications of computer vision , pp. 2200–2209 . Cited by: §5.1 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S5.SS1.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "V. Palit, R. Pandey, A. Arora, and P. P. Liang (2023) Towards vision-language mechanistic interpretability: a causal tracing tool for blip . In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 2856–2861 . Cited by: §2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S2.p2.1"
          ],
          "dois": []
        },
        {
          "raw_text": "A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach (2019) Towards vqa models that can read . In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 8317–8326 . Cited by: §5.1 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S5.SS1.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "G. B. M. Stan, E. Aflalo, R. Y. Rohekar, A. Bhiwandiwalla, S. Tseng, M. L. Olson, Y. Gurwicz, C. Wu, N. Duan, and V. Lal (2024) LVLM-interpret: an interpretability tool for large vision-language models . arXiv preprint arXiv:2404.03118 . Cited by: §2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S2.p1.1"
          ],
          "dois": []
        },
        {
          "raw_text": "P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K. Chen, X. Liu, J. Wang, W. Ge, et al. (2024) Qwen2-vl: enhancing vision-language model’s perception of the world at any resolution . arXiv preprint arXiv:2409.12191 . Cited by: Table 1 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S4.T1.1.10.10.1"
          ],
          "dois": []
        },
        {
          "raw_text": "J. Zhang, M. Khayatkhoei, P. Chhikara, and F. Ilievski (2025) MLLMs know where to look: training-free perception of small visual details with multimodal llms . arXiv preprint arXiv:2502.17422 . Cited by: §1 , §5.3 , Table 2 , Table 2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S1.p3.1",
            "https://arxiv.org/html/2601.08151v1#S5.SS3.p1.1",
            "https://arxiv.org/html/2601.08151v1#S5.T2.1.1.3.2.1",
            "https://arxiv.org/html/2601.08151v1#S5.T2.1.1.6.5.1"
          ],
          "dois": []
        },
        {
          "raw_text": "Q. Zhao, M. Xu, K. Gupta, A. Asthana, L. Zheng, and S. Gould (2024) The first to know: how token distributions reveal hidden knowledge in large vision-language models? . In European Conference on Computer Vision , pp. 127–142 . Cited by: §2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S2.p2.1"
          ],
          "dois": []
        }
      ],
      "references_dois": [],
      "errors": [],
      "missing_fields": [
        "published_date",
        "doi",
        "license",
        "references_dois"
      ],
      "url_hint_if_missing": "Champs manquants: published_date, doi, license, references_dois. Vérifie HTML: https://arxiv.org/html/2601.08151v1 | ABS: https://arxiv.org/abs/2601.08151 | PDF: https://arxiv.org/pdf/2601.08151"
    },
    {
      "arxiv_id": "2601.07581",
      "title": "BenchSeg: A Large-Scale Dataset and Benchmark for Multi-View Food Video Segmentation",
      "authors": [
        "Ahmad AlMughrabi",
        "Guillermo Rivo",
        "Carlos Jiménez-Farfán",
        "Umair Haroon",
        "Farid Al-Areqi",
        "Hyunjun Jung",
        "Benjamin Busam",
        "Ricardo Marques",
        "Petia Radeva"
      ],
      "abstract": "Food image segmentation is a critical task for dietary analysis, enabling accurate estimation of food volume and nutrients. However, current methods suffer from limited multi-view data and poor generalization to new viewpoints. We introduce BenchSeg, a novel multi-view food video segmentation dataset and benchmark. BenchSeg aggregates 55 dish scenes (from Nutrition5k, Vegetables & Fruits, MetaFood3D, and FoodKit) with 25,284 meticulously annotated frames, capturing each dish under free 360° camera motion. We evaluate a diverse set of 20 state-of-the-art segmentation models (e.g., SAM-based, transformer , CNN, and large multimodal ) on the existing FoodSeg103 dataset and evaluate them (alone and combined with video-memory modules) on BenchSeg. Quantitative and qualitative results demonstrate that while standard image segmenters degrade sharply under novel viewpoints, memory-augmented methods maintain temporal consistency across frames. Our best model based on a combination of SeTR-MLA+XMem2 outperforms prior work (e.g., improving over FoodMem by ~2.63% mAP), offering new insights into food segmentation and tracking for dietary analysis. We release BenchSeg to foster future research. The project page including the dataset annotations and the food segmentation models can be found at https://amughrabi.github.io/benchseg.",
      "submitted_date": "12 January, 2026",
      "abs_url": "https://arxiv.org/abs/2601.07581",
      "pdf_url": "https://arxiv.org/pdf/2601.07581",
      "html_url": "https://arxiv.org/html/2601.07581",
      "published_date": "",
      "doi": "",
      "license": "",
      "references": [],
      "references_dois": [],
      "errors": [
        "html_http_404"
      ],
      "missing_fields": [
        "published_date",
        "doi",
        "license",
        "references",
        "references_dois"
      ],
      "url_hint_if_missing": "Page HTML indisponible (404). Vérifie l'abs: https://arxiv.org/abs/2601.07581"
    },
    {
      "arxiv_id": "2601.07516",
      "title": "Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions",
      "authors": [
        "Yongqi Li",
        "Hao Lang",
        "Tieyun Qian",
        "Yongbin Li"
      ],
      "abstract": "Vision-language models are increasingly employed as multimodal conversational agents (MCAs) for diverse conversational tasks. Recently, reinforcement learning (RL) has been widely explored for adapting MCAs to various human-AI interaction scenarios. Despite showing great enhancement in generalization performance, fine-tuning MCAs via RL still faces challenges in handling the extremely large text token space. To address this, we learn a compact latent action space for RL fine-tuning instead. Specifically, we adopt the learning from observation mechanism to construct the codebook for the latent action space, where future observations are leveraged to estimate current latent actions that could further be used to reconstruct future observations. However, the scarcity of paired image-text data hinders learning a codebook with sufficient coverage. Thus, we leverage both paired image-text data and text-only data to construct the latent action space, using a cross-modal projector for transforming text embeddings into image-text embeddings. We initialize the cross-modal projector on paired image-text data, and further train it on massive text-only data with a novel cycle consistency loss to enhance its robustness. We show that our latent action based method outperforms competitive baselines on two conversation tasks across various RL algorithms.",
      "submitted_date": "12 January, 2026",
      "abs_url": "https://arxiv.org/abs/2601.07516",
      "pdf_url": "https://arxiv.org/pdf/2601.07516",
      "html_url": "https://arxiv.org/html/2601.07516v1",
      "published_date": "",
      "doi": "",
      "license": "",
      "references": [],
      "references_dois": [],
      "errors": [],
      "missing_fields": [
        "published_date",
        "doi",
        "license",
        "references",
        "references_dois"
      ],
      "url_hint_if_missing": "Champs manquants: published_date, doi, license, references, references_dois. Vérifie HTML: https://arxiv.org/html/2601.07516v1 | ABS: https://arxiv.org/abs/2601.07516 | PDF: https://arxiv.org/pdf/2601.07516"
    }
  ],
  "bundle_html_file": "data_lake\\raw\\arxiv_bundle_20260114_094058.html",
  "supported_fields": [
    "arxiv_id",
    "title",
    "authors",
    "abstract",
    "submitted_date",
    "abs_url",
    "pdf_url",
    "html_url",
    "published_date",
    "doi",
    "license",
    "references",
    "references_dois",
    "missing_fields",
    "errors",
    "url_hint_if_missing"
  ]
}