{
  "ok": true,
  "query": "multimodal transformer",
  "sort": "relevance",
  "count": 3,
  "max_results": 3,
  "hit_limit_100": false,
  "message_if_limit": "",
  "items": [
    {
      "arxiv_id": "2601.08457",
      "title": "An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English",
      "authors": [
        "Sargam Yadav",
        "Abhishek Kaushik",
        "Kevin Mc Daid"
      ],
      "abstract": "Digital platforms have an ever-expanding user base, and act as a hub for communication, business, and connectivity. However, this has also allowed for the spread of hate speech and misogyny. Artificial intelligence models have emerged as an effective solution for countering online hate speech but are under explored for low resource and code-mixed languages and suffer from a lack of interpretability. Explainable Artificial Intelligence (XAI) can enhance transparency in the decisions of deep learning models, which is crucial for a sensitive domain such as hate speech detection. In this paper, we present a multi-modal and explainable web application for detecting misogyny in text and memes in code-mixed Hindi and English. The system leverages state-of-the-art transformer -based models that support multilingual and multimodal settings. For text-based misogyny identification, the system utilizes XLM-RoBERTa (XLM-R) and multilingual Bidirectional Encoder Representations from Transformers (mBERT) on a dataset of approximately 4,193 comments. For multimodal misogyny identification from memes, the system utilizes mBERT + EfficientNet, and mBERT + ResNET trained on a dataset of approximately 4,218 memes. It also provides feature importance scores using explainability techniques including Shapley Additive Values (SHAP) and Local Interpretable Model Agnostic Explanations (LIME). The application aims to serve as a tool for both researchers and content moderators, to promote further research in the field, combat gender based digital violence, and ensure a safe digital space. The system has been evaluated using human evaluators who provided their responses on Chatbot Usability Questionnaire (CUQ) and User Experience Questionnaire (UEQ) to determine overall usability.",
      "submitted_date": "13 January, 2026",
      "abs_url": "https://arxiv.org/abs/2601.08457",
      "pdf_url": "https://arxiv.org/pdf/2601.08457",
      "doi": "",
      "versions": [],
      "last_updated_raw": "",
      "html_url": "https://arxiv.org/html/2601.08457",
      "published_date": "",
      "license": "",
      "sections": [],
      "content_text": "",
      "references": [],
      "references_dois": [],
      "fallback_urls": [
        "https://arxiv.org/html/2601.08457"
      ],
      "errors": [
        "html_http_404"
      ],
      "missing_fields": [
        "doi",
        "versions",
        "last_updated_raw",
        "published_date",
        "license",
        "sections",
        "content_text",
        "references",
        "references_dois"
      ],
      "url_hint_if_missing": "Champs manquants: doi, versions, last_updated_raw, published_date, license, sections, content_text, references, references_dois. Tu peux vÃ©rifier ici: abs=https://arxiv.org/abs/2601.08457 | html=https://arxiv.org/html/2601.08457 | pdf=https://arxiv.org/pdf/2601.08457"
    },
    {
      "arxiv_id": "2601.08179",
      "title": "Instruction-Driven 3D Facial Expression Generation and Transition",
      "authors": [
        "Anh H. Vo",
        "Tae-Seok Kim",
        "Hulin Jin",
        "Soo-Mi Choi",
        "Yong-Guk Kim"
      ],
      "abstract": "A 3D avatar typically has one of six cardinal facial expressions. To simulate realistic emotional variation, we should be able to render a facial transition between two arbitrary expressions. This study presents a new framework for instruction-driven facial expression generation that produces a 3D face and, starting from an image of the face, transforms the facial expression from one designated facial expression to another. The Instruction-driven Facial Expression Decomposer (IFED) module is introduced to facilitate multimodal data learning and capture the correlation between textual descriptions and facial expression features. Subsequently, we propose the Instruction to Facial Expression Transition (I2FET) method, which leverages IFED and a vertex reconstruction loss function to refine the semantic comprehension of latent vectors, thus generating a facial expression sequence according to the given instruction. Lastly, we present the Facial Expression Transition model to generate smooth transitions between facial expressions. Extensive evaluation suggests that the proposed model outperforms state-of-the-art methods on the CK+ and CelebV-HQ datasets. The results show that our framework can generate facial expression trajectories according to text instruction. Considering that text prompts allow us to make diverse descriptions of human emotional states, the repertoire of facial expressions and the transitions between them can be expanded greatly. We expect our framework to find various practical applications More information about our project can be found at https://vohoanganh.github.io/tg3dfet/",
      "submitted_date": "12 January, 2026",
      "abs_url": "https://arxiv.org/abs/2601.08179",
      "pdf_url": "https://arxiv.org/pdf/2601.08179",
      "doi": "https://doi.org/10.1109/TMM.2025.3565929",
      "versions": [],
      "last_updated_raw": "",
      "html_url": "https://arxiv.org/html/2601.08179v1",
      "published_date": "",
      "license": "",
      "sections": [
        {
          "section_index": 1,
          "heading_level": "h1",
          "heading": "Instruction-Driven 3D Facial Expression Generation and Transition",
          "text": "Anh H. Vo, Tae-Seok Kim, Hulin Jin, Soo-Mi Choi, and Yong-Guk Kim A H Vo, T-S Kim, S-M Choi, and Y-G Kim are with the Department of Computer Engineering, Sejong University, Seoul, Republic of Korea.H Jin is with the School of Computer Science and Technology, Anhui University, Hefei, China.* Corresponding Author: ykim@sejong.ac.kr. This is the authorâ€™s accepted manuscript. The final version is published in IEEE Transactions on Multimedia, 2025 Abstract A 3D avatar typically has one of six cardinal facial expressions. To simulate realistic emotional variation, we should be able to render a facial transition between two arbitrary expressions. This study presents a new framework for instruction-driven facial expression generation that produces a 3D face and, starting from an image of the face, transforms the facial expression from one designated facial expression to another. The Instruction-driven Facial Expression Decomposer (IFED) module is introduced to facilitate multimodal data learning and capture the correlation between textual descriptions and facial expression features. Subsequently, we propose the Instruction to Facial Expression Transition (I2FET) method, which leverages IFED and a vertex reconstruction loss function to refine the semantic comprehension of latent vectors, thus generating a facial expression sequence according to the given instruction. Lastly, we present the Facial Expression Transition model to generate smooth transitions between facial expressions. Extensive evaluation suggests that the proposed model outperforms state-of-the-art methods on the CK+ and CelebV-HQ datasets. The results show that our framework can generate facial expression trajectories according to text instruction. Considering that text prompts allow us to make diverse descriptions of human emotional states, the repertoire of facial expressions and the transitions between them can be expanded greatly. We expect our framework to find various practical applications. More information about our project can be found at https://vohoanganh.github.io/tg3dfet/ Index Terms: Instruction-Driven, Facial Expression and Transition, Controllable Avatar, CK+ and CelebV-HQ datasets. I Introduction Face and facial expression generation have attracted much attention in recent years due to their widespread application in many fields such as augmented reality/virtual reality, the gaming industry, movie production, or human-computer interfaces. Face generation can be divided into two categories depending on whether the face is two or three-dimensional (2D or 3D faces). For the 2D case, several methods [ 15 , 41 , 17 , 19 , 36 ] have been proposed to generate facial images using multimodal input, such as text, sketches, and segmentation. Previous studies [ 7 , 41 , 12 , 13 ] have attempted to solve facial expression generation in response to input provided as an emotion label or text prompts. However, these methods have limited ability to control the geometrical information of the output image and are effective only for specific applications. In the case of a 3D face, several studies [ 18 , 45 , 26 , 31 , 4 ] have demonstrated methods for generating 3D faces from 2D ones. The models generate the output image combining geometrical information from the source image, such as the pose, shape, and texture. Figure 1 : Illustration of our framework wherein it accepts a text instruction with a face image (source) as input and generates a 3D face from it. Then, it transforms the facial expression, from disgust (expression 1) to happiness (expression 2) via a neutral expression. Other researchers [ 16 , 1 ] generate a specific 3D facial expression based on text prompts. They utilize CLIP (Contrastive Language-Image Pretraining) or NeRF (Neural Radiance Field) to learn the latent code for manipulating facial expressions. Yet, NeRF requires substantial computational resources and time. For instance, [ 1 ] learns facial parameters and generates facial texture and expression based on text prompts. Although these methods can generate specific facial expressions, they do not address facial expression transitions based on instruction provided. Meanwhile, other studies [ 8 , 2 , 10 , 29 , 32 ] proposed to control facial expressions from an RGB video or generate portrait images [ 40 ] with controllable facial expression, head pose, and shoulder movements. Alternatively, [ 27 , 46 ] utilized the facial landmark trajectories to generate multiple 4D expression transitions on a meshed face. These studies led us to propose a new framework that enables a 3D face to be generated from a source image. Then, the facial expression undergoes transition in response to prompt text provided by the user to request a facial transition between two arbitrary expressions. Recent studies have used text-driven methods to generate a deformed face via NeRF or create an animated face via StyleGAN [ 13 , 16 ] . They generate sequence of facial expressions by specifying a certain emotion, such as â€™the person is happyâ€™. In contrast, our study utilizes a text instruction to control the variation in the facial expression, such as â€™Turn this face from disgust to happinessâ€™, as illustrated in Fig. 1 . Yet, understanding the target expressions specified by text instructions presents a challenge when relying solely on global text embedding. Indeed, even though CLIP is a notable method for the language-conditioning module in the text-to-image task [ 13 , 16 , 11 , 1 ] , it still faces challenges related to bias in understanding the text despite its large-scale training or the provision of global instruction, which may overlook fine-grained sub-instructions. Moreover, a text-only approach could be sensitive in generative models because changes in the structure of instructions can significantly impact their performance. The additional challenge lies therein to ensure that the facial motion described in the instruction is aligned with the semantics of the language. A straightforward approach to generating the facial motion would entail interpolating between the source code and the edited code from StyleClip [ 30 ] . Nevertheless, the additional challenge of controlling the latent code to adjust the head pose of the source image while generating the facial animation makes it difficult to achieve the intended goal. To make the connection between the text and facial expressions, we supplement a text instruction for making a transition between facial expressions into two standard datasets containing facial images, i.e., CelebV-HQ (High-Quality Celebrity Video) and CK+ (Extended Cohn-Kanade). To control the facial model in detail, FLAME (Faces Learned with an Articulated Model and Expressions) [ 22 ] is adopted and then fine-tuned with basic facial expressions and corresponding poses to capture the movement of the head, eyes, and mouth. Furthermore, integrating feature representations from different modalities minimizes ambiguity in redundant resources and effectively handles missing modalities, particularly in generative models, by improving inter-modality relations [ 37 , 38 ] . The proposed approach allows our model, i.e. Instruction to Facial Expression Transition (I2FET), to enhance the relationships between facial expression features and textual descriptions, thereby improving the latent representation for accurately generating target facial expressions and reducing the reliance solely on instructional text. Then, facial expression trajectories are created using the target facial expressions, which are generated by the facial parameters of the source image using I2FET. Using this framework, one can generate a 3D facial expression avatar from a photograph of a face and create any facial transition based on a text prompt instruction that contains the initial and final facial expressions. The main contributions of our study are given as follows: â€¢ We present an instruction-driven 3D facial expression generation and transition framework to generate transitions in facial expressions based on instructions, starting from a photograph of a face. â€¢ We propose an Instruction-Driven Facial Expression Decomposer (IFED), designed to learn multimodal data and capture the correlation between facial expression features and textual descriptions. â€¢ We propose an Instruction to Facial Expression Transition method (I2FET), which utilizes conditional variational autoencoders, integrated with IFED and a vertex reconstruction loss function, to enhance the semantic comprehension of the latent vectors. â€¢ Extensive evaluation of the proposed model on the CK+ and CelebV-HQ datasets suggests that it outperforms SOTA methods."
        },
        {
          "section_index": 2,
          "heading_level": "h6",
          "heading": "Abstract",
          "text": "A 3D avatar typically has one of six cardinal facial expressions. To simulate realistic emotional variation, we should be able to render a facial transition between two arbitrary expressions. This study presents a new framework for instruction-driven facial expression generation that produces a 3D face and, starting from an image of the face, transforms the facial expression from one designated facial expression to another. The Instruction-driven Facial Expression Decomposer (IFED) module is introduced to facilitate multimodal data learning and capture the correlation between textual descriptions and facial expression features. Subsequently, we propose the Instruction to Facial Expression Transition (I2FET) method, which leverages IFED and a vertex reconstruction loss function to refine the semantic comprehension of latent vectors, thus generating a facial expression sequence according to the given instruction. Lastly, we present the Facial Expression Transition model to generate smooth transitions between facial expressions. Extensive evaluation suggests that the proposed model outperforms state-of-the-art methods on the CK+ and CelebV-HQ datasets. The results show that our framework can generate facial expression trajectories according to text instruction. Considering that text prompts allow us to make diverse descriptions of human emotional states, the repertoire of facial expressions and the transitions between them can be expanded greatly. We expect our framework to find various practical applications. More information about our project can be found at https://vohoanganh.github.io/tg3dfet/"
        },
        {
          "section_index": 4,
          "heading_level": "h2",
          "heading": "I Introduction",
          "text": "Face and facial expression generation have attracted much attention in recent years due to their widespread application in many fields such as augmented reality/virtual reality, the gaming industry, movie production, or human-computer interfaces. Face generation can be divided into two categories depending on whether the face is two or three-dimensional (2D or 3D faces). For the 2D case, several methods [ 15 , 41 , 17 , 19 , 36 ] have been proposed to generate facial images using multimodal input, such as text, sketches, and segmentation. Previous studies [ 7 , 41 , 12 , 13 ] have attempted to solve facial expression generation in response to input provided as an emotion label or text prompts. However, these methods have limited ability to control the geometrical information of the output image and are effective only for specific applications. In the case of a 3D face, several studies [ 18 , 45 , 26 , 31 , 4 ] have demonstrated methods for generating 3D faces from 2D ones. The models generate the output image combining geometrical information from the source image, such as the pose, shape, and texture. Figure 1 : Illustration of our framework wherein it accepts a text instruction with a face image (source) as input and generates a 3D face from it. Then, it transforms the facial expression, from disgust (expression 1) to happiness (expression 2) via a neutral expression. Other researchers [ 16 , 1 ] generate a specific 3D facial expression based on text prompts. They utilize CLIP (Contrastive Language-Image Pretraining) or NeRF (Neural Radiance Field) to learn the latent code for manipulating facial expressions. Yet, NeRF requires substantial computational resources and time. For instance, [ 1 ] learns facial parameters and generates facial texture and expression based on text prompts. Although these methods can generate specific facial expressions, they do not address facial expression transitions based on instruction provided. Meanwhile, other studies [ 8 , 2 , 10 , 29 , 32 ] proposed to control facial expressions from an RGB video or generate portrait images [ 40 ] with controllable facial expression, head pose, and shoulder movements. Alternatively, [ 27 , 46 ] utilized the facial landmark trajectories to generate multiple 4D expression transitions on a meshed face. These studies led us to propose a new framework that enables a 3D face to be generated from a source image. Then, the facial expression undergoes transition in response to prompt text provided by the user to request a facial transition between two arbitrary expressions. Recent studies have used text-driven methods to generate a deformed face via NeRF or create an animated face via StyleGAN [ 13 , 16 ] . They generate sequence of facial expressions by specifying a certain emotion, such as â€™the person is happyâ€™. In contrast, our study utilizes a text instruction to control the variation in the facial expression, such as â€™Turn this face from disgust to happinessâ€™, as illustrated in Fig. 1 . Yet, understanding the target expressions specified by text instructions presents a challenge when relying solely on global text embedding. Indeed, even though CLIP is a notable method for the language-conditioning module in the text-to-image task [ 13 , 16 , 11 , 1 ] , it still faces challenges related to bias in understanding the text despite its large-scale training or the provision of global instruction, which may overlook fine-grained sub-instructions. Moreover, a text-only approach could be sensitive in generative models because changes in the structure of instructions can significantly impact their performance. The additional challenge lies therein to ensure that the facial motion described in the instruction is aligned with the semantics of the language. A straightforward approach to generating the facial motion would entail interpolating between the source code and the edited code from StyleClip [ 30 ] . Nevertheless, the additional challenge of controlling the latent code to adjust the head pose of the source image while generating the facial animation makes it difficult to achieve the intended goal. To make the connection between the text and facial expressions, we supplement a text instruction for making a transition between facial expressions into two standard datasets containing facial images, i.e., CelebV-HQ (High-Quality Celebrity Video) and CK+ (Extended Cohn-Kanade). To control the facial model in detail, FLAME (Faces Learned with an Articulated Model and Expressions) [ 22 ] is adopted and then fine-tuned with basic facial expressions and corresponding poses to capture the movement of the head, eyes, and mouth. Furthermore, integrating feature representations from different modalities minimizes ambiguity in redundant resources and effectively handles missing modalities, particularly in generative models, by improving inter-modality relations [ 37 , 38 ] . The proposed approach allows our model, i.e. Instruction to Facial Expression Transition (I2FET), to enhance the relationships between facial expression features and textual descriptions, thereby improving the latent representation for accurately generating target facial expressions and reducing the reliance solely on instructional text. Then, facial expression trajectories are created using the target facial expressions, which are generated by the facial parameters of the source image using I2FET. Using this framework, one can generate a 3D facial expression avatar from a photograph of a face and create any facial transition based on a text prompt instruction that contains the initial and final facial expressions. The main contributions of our study are given as follows: â€¢ We present an instruction-driven 3D facial expression generation and transition framework to generate transitions in facial expressions based on instructions, starting from a photograph of a face. â€¢ We propose an Instruction-Driven Facial Expression Decomposer (IFED), designed to learn multimodal data and capture the correlation between facial expression features and textual descriptions. â€¢ We propose an Instruction to Facial Expression Transition method (I2FET), which utilizes conditional variational autoencoders, integrated with IFED and a vertex reconstruction loss function, to enhance the semantic comprehension of the latent vectors. â€¢ Extensive evaluation of the proposed model on the CK+ and CelebV-HQ datasets suggests that it outperforms SOTA methods."
        },
        {
          "section_index": 5,
          "heading_level": "h2",
          "heading": "II Related Work",
          "text": "II-A Facial Expression Transition Generative Adversarial Networks (GANs) have been favored for generating facial expressions. For instance, Motion3DGAN [ 27 ] , an extension of MotionGAN [ 28 ] , addresses the dynamics of 3D landmarks, whereas WGAN learns the distribution of 3D expression dynamics across the hypersphere space, sampled with a condition to generate landmark sequences. In this case, a neutral 3D face mesh, frame by frame, is deformed by a mesh decoder using the generated landmark sequences although the disadvantage of this work is that the start and end expressions have to be provided to generate a concatenated expression transition. On the other hand, a diffusion model is adopted for facial expression generation [ 46 ] by modeling the input distribution. The deformation of the 3D mesh is guided by a specific landmark, and the original shape of the input facial mesh is considered to apply displacement to each vertex. In this work, MotionClip [ 39 ] , which was originally designed for human motion synthesis, is compared with the task of generating the expression transition conditions given by the text. EMOTE [ 6 ] utilizes emotion labels and various inputs to generate a talking-head avatar that maintains lip-sync from speech as well as facial expression. In particular, the facial motion prior, called FLINT (FLAME IN TIME), uses the expression and pose parameters of FLAME [ 22 ] to represent facial motion sequences. Although several studies have demonstrated methods to produce transitions between facial expressions under diverse conditions, the generated faces are typically conditioned on the front face by neglecting the head pose variations. Inspired by previous works [ 27 , 46 , 6 ] , the present study proposes a model for facial expression transition that learns the expression and pose parameters. The additional merit of our model would be that it can easily be plugged into the models that generate the facial appearance. II-B Face Rendering In the face rendering area, 3D Morphable Model (3DMM) has been widely used for rendering realistic faces [ 2 , 21 , 18 , 26 ] . Among these methods, VariTex [ 2 ] employs a neural texture obtained by a face texture decoder to sample with a UV map. Subsequently, this neural face feature image is merged with an additive feature image to serve as the input for a U-Net to generate the desired image. Exp-GAN [ 21 ] employs a neural volume generator and StyleGAN2 to generate images at various resolutions. This process combines different feature components, such as face features, depth images, and feature volumes. The facial feature is also obtained by sampling the neural texture using the UV map. However, this work is computationally intensive and time-consuming. ROME [ 18 ] can synthesize realistic face images by estimating a specific head mesh and neural texture from an input image, whereupon a neural rendering technique is adapted to render the rigged mesh. Next, the U-net-like neural rendering network, which accepts inputs such as neural texture and surface normals, is utilized to acquire both the target image and its corresponding mask. Similarly, CVTHead [ 26 ] also estimates a neural texture from an input image, and yet, in this case, the neural texture is flattened to a feature vector form before being passed to transformers along with vertex tokens to output a vertex descriptor. Specifically, each of the vertices is considered a query token, and a transformer is employed to learn the canonical vertex feature from the source image. To generate the desired image and its mask, the feature descriptor of each vertex and depth are projected into image space before being processed by a U-Net. In the current study, we employ ROME and CVTHead to generate facial appearances by leveraging DECA for the extraction of facial expression coefficients. This streamlines the integration into a facial expression transition model and enhances the applicability of the proposed framework to practical scenarios."
        },
        {
          "section_index": 6,
          "heading_level": "h3",
          "heading": "II-A Facial Expression Transition",
          "text": "Generative Adversarial Networks (GANs) have been favored for generating facial expressions. For instance, Motion3DGAN [ 27 ] , an extension of MotionGAN [ 28 ] , addresses the dynamics of 3D landmarks, whereas WGAN learns the distribution of 3D expression dynamics across the hypersphere space, sampled with a condition to generate landmark sequences. In this case, a neutral 3D face mesh, frame by frame, is deformed by a mesh decoder using the generated landmark sequences although the disadvantage of this work is that the start and end expressions have to be provided to generate a concatenated expression transition. On the other hand, a diffusion model is adopted for facial expression generation [ 46 ] by modeling the input distribution. The deformation of the 3D mesh is guided by a specific landmark, and the original shape of the input facial mesh is considered to apply displacement to each vertex. In this work, MotionClip [ 39 ] , which was originally designed for human motion synthesis, is compared with the task of generating the expression transition conditions given by the text. EMOTE [ 6 ] utilizes emotion labels and various inputs to generate a talking-head avatar that maintains lip-sync from speech as well as facial expression. In particular, the facial motion prior, called FLINT (FLAME IN TIME), uses the expression and pose parameters of FLAME [ 22 ] to represent facial motion sequences. Although several studies have demonstrated methods to produce transitions between facial expressions under diverse conditions, the generated faces are typically conditioned on the front face by neglecting the head pose variations. Inspired by previous works [ 27 , 46 , 6 ] , the present study proposes a model for facial expression transition that learns the expression and pose parameters. The additional merit of our model would be that it can easily be plugged into the models that generate the facial appearance."
        },
        {
          "section_index": 7,
          "heading_level": "h3",
          "heading": "II-B Face Rendering",
          "text": "In the face rendering area, 3D Morphable Model (3DMM) has been widely used for rendering realistic faces [ 2 , 21 , 18 , 26 ] . Among these methods, VariTex [ 2 ] employs a neural texture obtained by a face texture decoder to sample with a UV map. Subsequently, this neural face feature image is merged with an additive feature image to serve as the input for a U-Net to generate the desired image. Exp-GAN [ 21 ] employs a neural volume generator and StyleGAN2 to generate images at various resolutions. This process combines different feature components, such as face features, depth images, and feature volumes. The facial feature is also obtained by sampling the neural texture using the UV map. However, this work is computationally intensive and time-consuming. ROME [ 18 ] can synthesize realistic face images by estimating a specific head mesh and neural texture from an input image, whereupon a neural rendering technique is adapted to render the rigged mesh. Next, the U-net-like neural rendering network, which accepts inputs such as neural texture and surface normals, is utilized to acquire both the target image and its corresponding mask. Similarly, CVTHead [ 26 ] also estimates a neural texture from an input image, and yet, in this case, the neural texture is flattened to a feature vector form before being passed to transformers along with vertex tokens to output a vertex descriptor. Specifically, each of the vertices is considered a query token, and a transformer is employed to learn the canonical vertex feature from the source image. To generate the desired image and its mask, the feature descriptor of each vertex and depth are projected into image space before being processed by a U-Net. In the current study, we employ ROME and CVTHead to generate facial appearances by leveraging DECA for the extraction of facial expression coefficients. This streamlines the integration into a facial expression transition model and enhances the applicability of the proposed framework to practical scenarios."
        },
        {
          "section_index": 8,
          "heading_level": "h2",
          "heading": "III Method",
          "text": "III-A Problem Statement We aim to develop a model to generate 3D facial expression with two inputs: a face image I s I_{s} and a text instruction t t . The facial expression transition model ğ’¯ w â€‹ ( t , I s ) \\mathcal{T}_{w}(t,I_{s}) , having weights w w , is trained with a training set ğ’³ = { ( t , ( e 0 , Î¸ 0 ) , ( e 1 , Î¸ 1 ) ) ( j ) } j = 1 n \\mathcal{X}=\\{(t,(e_{0},\\theta_{0}),(e_{1},\\theta_{1}))^{(j)}\\}_{j=1}^{n} , where e 0 , e 1 âˆˆ R 1 Ã— 50 e_{0},e_{1}\\in R^{1\\times 50} denote expression vectors with 50 dimensions and Î¸ 0 , Î¸ 1 âˆˆ R 1 Ã— 6 \\theta_{0},\\theta_{1}\\in R^{1\\times 6} represent pose vectors with 6 dimensions corresponding to the first and next expressions in the facial expression sequence specified in instruction t t , respectively. n n is the number of samples in the training dataset. Our network learns ğ’¯ w ( . ) \\mathcal{T}_{w}(.) that approximates the data distribution p â€‹ ( ğ’³ ) p(\\mathcal{X}) and then generates a facial expression coefficient sequence { ( e 0 , Î¸ 0 ) , ( e 1 , Î¸ 1 ) } \\{(e_{0},\\theta_{0}),(e_{1},\\theta_{1})\\} with a given instruction t t . Next, a face rendering module ğ’¢ â€‹ ( I s , ğ’® ) \\mathcal{G}(I_{s},\\mathcal{S}) is used to render a sequence of facial appearances ğ’´ = { y s ( 1 ) , y ( i ) , y e 0 ( k ) , y ( j ) , y e 1 ( T ) } \\mathcal{Y}=\\{y_{s}^{(1)},y^{(i)},y_{e_{0}}^{(k)},y^{(j)},y_{e_{1}}^{(T)}\\} , with inputs, consisting of the texture of source image I s I_{s} and facial expression trajectories S = { s s ( 1 ) , s ( i ) , s e 0 ( k ) , s ( j ) , s e 1 ( T ) } S=\\{s_{s}^{(1)},s^{(i)},s_{e_{0}}^{(k)},s^{(j)},s_{e_{1}}^{(T)}\\} , where { s s ( 1 ) , s e 0 ( k ) , s e 1 ( T ) } \\{s_{s}^{(1)},s_{e_{0}}^{(k)},s_{e_{1}}^{(T)}\\} are anchor facial expression sequences corresponding to the source image, and specific facial expressions, generated by ğ’¯ w ( . ) \\mathcal{T}_{w}(.) . Then, facial expression sequences s ( i ) s^{(i)} and s ( j ) s^{(j)} transitioning between these anchor facial expressions can be inferred by a linear interpolation function to ensure the smoothness of the sequence of facial expressions. III-B The Proposed Method Figure 2 provides an overview of the proposed framework whereby one can control 3D facial expressions of the monocular image I s I_{s} following an input instruction t t . The framework consists of two major modules: (1) Facial Expression Transition (FET), ğ’¯ w â€‹ ( t , I s ) \\mathcal{T}_{w}(t,I_{s}) , for generating diverse expression trajectories; (2) Face Rendering (FR), ğ’¢ â€‹ ( I s , S ) \\mathcal{G}(I_{s},S) , for rendering facial appearances based on the texture of the source image I s I_{s} and the expression trajectories S S generated by the FET module. Figure 2 : Overview of the proposed framework consisting of two major modules: Facial Expression Transition (FET) and Face Rendering (FR). First, textual vector x t x_{t} is obtained from the pre-trained CLIP encoder for the FET module. Simultaneously, latent representations z p z_{p} and z e z_{e} are drawn from ğ’© â€‹ ( 0 , I ) \\mathcal{N}(0,I) . Then, the latent vectors are processed and concatenated to obtain x ^ f \\hat{x}^{f} . Afterward, the IFED module utilizes x t x^{t} and x ^ f \\hat{x}^{f} as inputs to create conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} , and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} for the pose and expression decoders. Subsequently, for smoothness of the facial expression sequence, the facial expression in the source image ( e s , Î¸ s ) (e_{s},\\theta_{s}) is subjected to linear interpolation and the specific facial expressions ( e 0 , Î¸ 0 ) , ( e 1 , Î¸ 1 ) (e_{0},\\theta_{0}),(e_{1},\\theta_{1}) , generated by the I2FET decoders. These sequences are then combined with the shape Ï• s \\phi_{s} and camera c s c_{s} parameters of the source image, obtained from DECA, to form facial expression trajectories { s s ( 1 ) , s ( i ) , s e 0 ( k ) , s ( j ) , s e 1 ( T ) } \\{s_{s}^{(1)},s^{(i)},s^{(k)}_{e_{0}},s^{(j)},s_{e_{1}}^{(T)}\\} . For FR, head mesh reconstruction produces a flame mesh sequence { m s ( 1 ) , m ( i ) , m e 0 ( k ) , m ( j ) , m e 1 ( T ) } \\{m_{s}^{(1)},m^{(i)},m^{(k)}_{e_{0}},m^{(j)},m_{e_{1}}^{(T)}\\} aligned with the facial expression trajectories and the texture encoder extracts the facial appearance from the source image. Finally, the rendering module generates the facial appearances { y s ( 1 ) , y ( i ) , y e 0 ( k ) , y ( j ) , y e 1 ( T ) } \\{y_{s}^{(1)},y^{(i)},y^{(k)}_{e_{0}},y^{(j)},y_{e_{1}}^{(T)}\\} within the facial expression trajectories using the flame mesh sequence and the extracted facial appearance. III-B 1 Facial Expression Transition This module is designed to generate diverse expression trajectories that correspond to the facial parameters of the source face image I s I_{s} and the parameters of specific facial expressions, described in an instruction t t . The module has two primary branches: the first receives an instruction to generate the corresponding sequence of facial expression based on instruction t t , whereas the second takes a 2D face image to encode its facial coefficients and create the facial expression trajectories. Figure 3 : Flow diagram of IFED (Instruction-Driven Facial Expression Decomposer). It consists of CAFT and the decomposition of parameters. The major function of CAFT is to introduce cross-attention between the facial parameters and text instruction. The two outputs from CAFT pass through layer normalization and are combined to create the fused feature vector x f â€‹ u â€‹ s â€‹ e â€‹ d x^{fused} . Then, pose projection function ğ’« p \\mathcal{P}_{p} and expression projection function ğ’« e \\mathcal{P}_{e} are used to decompose x f â€‹ u â€‹ s â€‹ e â€‹ d x^{fused} into two conditional vectors for facial expression ( x e â€‹ m â€‹ b e x_{emb}^{e} ) and pose ( x e â€‹ m â€‹ b p x_{emb}^{p} ). Figure 4 : Detailed architecture of the I2FET (Instruction to Facial Expression Transition) method. Given an instruction and a sequence of facial parameters, including pose and expression, the IFED module utilizes the facial parameters and textual description extracted by CLIP to produce the corresponding conditional feature vectors for pose x e â€‹ m â€‹ b p x_{emb}^{p} and expression x e â€‹ m â€‹ b e x_{emb}^{e} , respectively. The pose and expression encoders then map this sequence of facial parameters and conditional feature vectors to latent representations corresponding to pose and expression. These latent vectors, along with the textual description, are passed through the IFED module to generate the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} . Following this, the expression and pose decoders reconstruct expression e ^ \\hat{e} and pose Î¸ ^ \\hat{\\theta} parameters using these latent representations and the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} , respectively. Finally, the pre-trained FLAME decoder reconstructs the vertex coordinates v ^ \\hat{v} based on e ^ \\hat{e} and Î¸ ^ \\hat{\\theta} . In the former, the pre-trained CLIP encoder [ 33 ] extracts text description x t âˆˆ R 77 Ã— 768 x^{t}\\in R^{77\\times 768} according to the given instruction t t . Then, the I2FET model generates a sequence of expression and pose coefficients for the facial expression trajectories. Instruction-driven Facial Expression Decomposer : A dual-branch vision transformer [ 3 ] is adopted for extracting multi-scale features and fusing cross-attention information at different scales because it is efficient for the image classification task. Here, the Cross-Attention for Facial Expression Parameter and Text (CAFT) is used to learn the relationship between instruction and facial expression parameters by enhancing the understanding of multimodal data and refining the latent representation for the Instruction to Facial Expression Transition model. The proposed IFED module is illustrated in Fig. 3 . In particular, we directly incorporate facial expression parameters and textual descriptions into CAFT instead of adding class tokens as in [ 3 ] . With respect to the transformer encoders, the first branch learns the correlation between the facial expression parameters, while the second branch focuses on learning the textual descriptions. To begin with, each vector representing facial expression parameters comprises two elements: expression and pose parameters, denoted as x f = { e , Î¸ } âˆˆ R m Ã— 56 x^{f}=\\{e,\\theta\\}\\in R^{m\\times 56} , where m m denotes the number of expressions in a given instruction. As [ 24 , 6 ] , we only utilize jaw pose Î¸ j â€‹ a â€‹ w \\theta^{jaw} and expression e e to convey facial expression information. Therefore, we rewrite x f = { e , Î¸ j â€‹ a â€‹ w } âˆˆ R m Ã— 53 x^{f}=\\{e,\\theta^{jaw}\\}\\in R^{m\\times 53} as the input of the first branch. Then, the transformer encoder is utilized as y f = x f + M â€‹ S â€‹ A â€‹ ( L â€‹ N â€‹ ( x f ) ) \\displaystyle y^{f}=x^{f}+MSA(LN(x^{f})) (1) x ^ f = y f + F â€‹ F â€‹ N â€‹ ( L â€‹ N â€‹ ( y f ) ) \\displaystyle\\hat{x}^{f}=y^{f}+FFN(LN(y^{f})) where x ^ f \\hat{x}^{f} denotes the output of the transformer encoder in the facial expression parameters branch â„° P \\mathcal{E}_{P} . L N ( . ) LN(.) , M S A ( . ) MSA(.) , and F F N ( . ) FFN(.) are the layer norm, multi-head self-attention, and feed-forward network, respectively. Likewise, in the text branch, we replicate x t x^{t} to create a vector { x } i = { 0 , 1 } t \\{x\\}^{t}_{i=\\{0,1\\}} for computational purposes. Subsequently, the input for the transformer encoder is generated using a text projection function P t ( . ) P_{t}(.) on { x } i = { 0 , 1 } t \\{x\\}^{t}_{i=\\{0,1\\}} . As a result, the textual description x ^ t \\hat{x}^{t} is obtained through the transformer encoder â„° t \\mathcal{E}_{t} . Both branches are then fused by cross-attention to capture the relationship between the facial expression parameters and textual descriptions. The fusion process can be expressed as y c f = h f â€‹ 2 â€‹ t â€‹ ( x ^ 0 f ) , y c t = h t â€‹ 2 â€‹ f â€‹ ( x ^ 0 t ) , \\displaystyle y_{c}^{f}=h^{f2t}(\\hat{x}_{0}^{f}),\\quad y_{c}^{t}=h^{t2f}(\\hat{x}_{0}^{t}), (2) x o f = g t â€‹ 2 â€‹ f â€‹ ( [ C â€‹ A â€‹ ( y c f âŠ— x 1 t ) + y c f ] ) âŠ— x ^ 1 f \\displaystyle x_{o}^{f}=g^{t2f}([CA(y^{f}_{c}\\otimes x_{1}^{t})+y_{c}^{f}])\\otimes\\hat{x}_{1}^{f} x o t = g f â€‹ 2 â€‹ t â€‹ ( [ C â€‹ A â€‹ ( y c t âŠ— x 1 f ) + y c t ] ) âŠ— x ^ 1 t \\displaystyle x_{o}^{t}=g^{f2t}([CA(y_{c}^{t}\\otimes x_{1}^{f})+y_{c}^{t}])\\otimes\\hat{x}_{1}^{t} where h t â€‹ 2 â€‹ f , h f â€‹ 2 â€‹ t , g t â€‹ 2 â€‹ f h^{t2f},h^{f2t},g^{t2f} , and g f â€‹ 2 â€‹ t g^{f2t} are the projection function and back-projection function between the vectors that represent the text and facial expression parameters, x o f âˆˆ R m Ã— 53 x_{o}^{f}\\in R^{m\\times 53} and x o t âˆˆ R m Ã— 768 x_{o}^{t}\\in R^{m\\times 768} are the outputs of CAFT, C â€‹ A CA denotes cross-attention and âŠ— \\otimes denotes the concatenation operator. Next, the fused feature vector is obtained by concatenating x o f x_{o}^{f} and x o t x_{o}^{t} , after passing through L N ( . ) LN(.) , as follows: x f â€‹ u â€‹ s â€‹ e â€‹ d = L â€‹ N â€‹ ( x o t ) âŠ— L â€‹ N â€‹ ( x o f ) x^{fused}=LN(x_{o}^{t})\\otimes LN(x_{o}^{f}) (3) Finally, the projection functions for pose ğ’« p ( . ) \\mathcal{P}_{p}(.) and expression ğ’« e ( . ) \\mathcal{P}_{e}(.) are applied to the fused feature vector as follows: x e â€‹ m â€‹ b e = ğ’« e â€‹ ( x f â€‹ u â€‹ s â€‹ e â€‹ d ) \\displaystyle x^{e}_{emb}=\\mathcal{P}_{e}(x^{fused}) (4) x e â€‹ m â€‹ b p = ğ’« p â€‹ ( x f â€‹ u â€‹ s â€‹ e â€‹ d ) \\displaystyle x^{p}_{emb}=\\mathcal{P}_{p}(x^{fused}) where x e â€‹ m â€‹ b e âˆˆ R m Ã— 50 x^{e}_{emb}\\in R^{m\\times 50} and x e â€‹ m â€‹ b p âˆˆ R m Ã— 6 x^{p}_{emb}\\in R^{m\\times 6} are the conditional feature vectors for pose and expression, respectively. In this work, ğ’« p ( . ) \\mathcal{P}_{p}(.) and ğ’« e ( . ) \\mathcal{P}_{e}(.) are designed as linear transformation functions to map the feature x f â€‹ u â€‹ s â€‹ e â€‹ d x^{fused} to the corresponding pose and expression feature vectors. Instruction to Facial Expression Transition : The proposed I2FET method is based on a conditional variational autoencoder architecture. The method includes encoders â„° e t ( . ) \\mathcal{E}_{e}^{t}(.) and â„° p t ( . ) \\mathcal{E}_{p}^{t}(.) , as well as decoders ğ’Ÿ e t ( . ) \\mathcal{D}^{t}_{e}(.) and ğ’Ÿ p t ( . ) \\mathcal{D}^{t}_{p}(.) , which are defined using three Multi-Layer Perceptrons (MLPs). These components are conditioned on feature vectors that represent expression and pose information, respectively. An overview of the proposed I2FET architecture is presented in Fig. 4 . Here, the encoders are defined as follows: â„° e t â€‹ ( e âŠ— x e â€‹ m â€‹ b e ) âŸ¶ ( Î¼ e , Ïƒ e ) \\mathcal{E}_{e}^{t}(e\\otimes x^{e}_{emb})\\longrightarrow(\\mu_{e},\\sigma_{e}) (5) and â„° p t â€‹ ( Î¸ âŠ— x e â€‹ m â€‹ b p ) âŸ¶ ( Î¼ p , Ïƒ p ) \\mathcal{E}_{p}^{t}(\\theta\\otimes x^{p}_{emb})\\longrightarrow(\\mu_{p},\\sigma_{p}) (6) where âŠ— \\otimes denotes the concatenation operator and x e â€‹ m â€‹ b e x^{e}_{emb} and x e â€‹ m â€‹ b p x^{p}_{emb} are conditional feature vectors for expression and pose, respectively. For the encoder stage, x e â€‹ m â€‹ b e x^{e}_{emb} and x e â€‹ m â€‹ b p x^{p}_{emb} are created by textual description x t x_{t} and the facial expression parameters ( e , Î¸ ) (e,\\theta) . Î¼ e \\mu_{e} and Î¼ p \\mu_{p} denote the mean and standard deviation of the expression distribution, whereas Ïƒ e \\sigma_{e} , and Ïƒ p \\sigma_{p} are the mean and standard deviation of the pose distribution. In this study, the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}^{e}_{emb} and x ^ e â€‹ m â€‹ b p \\hat{x}^{p}_{emb} in the decoder are computed by textual description x t x_{t} and the latent vectors z ^ p \\hat{z}_{p} and z ^ e \\hat{z}_{e} representing pose and expression, respectively. These latent vectors are calculated as follows: z ~ p = Ïƒ p âˆ— z p + Î¼ p \\displaystyle\\tilde{z}_{p}=\\sigma_{p}*z_{p}+\\mu_{p} (7) z ^ p = ğ’¯ p â€‹ ( z ~ p ) \\displaystyle\\hat{z}_{p}=\\mathcal{T}_{p}(\\tilde{z}_{p}) z ~ e = Ïƒ e âˆ— z e + Î¼ e \\displaystyle\\tilde{z}_{e}=\\sigma_{e}*z_{e}+\\mu_{e} z ^ e = ğ’¯ e â€‹ ( z ~ e ) \\displaystyle\\hat{z}_{e}=\\mathcal{T}_{e}(\\tilde{z}_{e}) where z p z_{p} and z e z_{e} are the latent vectors sampled from ğ’© â€‹ ( 0 , I ) \\mathcal{N}(0,I) . In this case, we utilize linear transformation functions ğ’¯ p ( . ) \\mathcal{T}_{p}(.) and ğ’¯ e ( . ) \\mathcal{T}_{e}(.) to transform z ~ p \\tilde{z}_{p} and z ~ e âˆˆ R m Ã— 16 \\tilde{z}_{e}\\in R^{m\\times 16} into z ^ p âˆˆ R m Ã— 6 \\hat{z}_{p}\\in R^{m\\times 6} and z ^ e âˆˆ R m Ã— 50 \\hat{z}_{e}\\in R^{m\\times 50} , respectively, similar to the expression and pose dimensions. Thus, the latent vectors z ~ e \\tilde{z}_{e} and z ~ p \\tilde{z}_{p} can be used as input for the IFED module. Then, the latent vectors z ^ e \\hat{z}_{e} and z ^ p \\hat{z}_{p} are concatenated together, and along with the text vector x t x^{t} , they are passed through the IFED module to obtain the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} as x ^ e â€‹ m â€‹ b e , x ^ e â€‹ m â€‹ b p = I â€‹ F â€‹ E â€‹ D â€‹ ( z ^ p âŠ— z ^ e , x t ) \\displaystyle\\hat{x}_{emb}^{e},\\hat{x}_{emb}^{p}=IFED(\\hat{z}_{p}\\otimes\\hat{z}_{e},x^{t}) (8) This approach enhances the semantic representation of latent vectors by refining the conditional feature vectors for decoders based on the learned latent vectors throughout the training process. The decoders are defined as follows ğ’Ÿ p t â€‹ ( z ~ p âŠ— x ^ e â€‹ m â€‹ b p ) âŸ¶ ( Î¸ ^ 0 , Î¸ ^ 1 ) \\mathcal{D}^{t}_{p}(\\tilde{z}_{p}\\otimes\\hat{x}^{p}_{emb})\\longrightarrow(\\hat{\\theta}_{0},\\hat{\\theta}_{1}) (9) and ğ’Ÿ e t â€‹ ( z ~ e âŠ— x ^ e â€‹ m â€‹ b e ) âŸ¶ ( e ^ 0 , e ^ 1 ) \\mathcal{D}^{t}_{e}(\\tilde{z}_{e}\\otimes\\hat{x}^{e}_{emb})\\longrightarrow(\\hat{e}_{0},\\hat{e}_{1}) (10) The parameters of the I2FET model are optimized by minimizing the reconstruction loss function â„’ M â€‹ S â€‹ E ( . , . ) \\mathcal{L}_{MSE}(.,.) between the ground truth coefficients e e , Î¸ \\theta , and the predicted coefficients e ^ , Î¸ ^ \\hat{e},\\hat{\\theta} . In addition, the Kullback-Leibler divergence loss function is utilized to minimize the distance between the probability distribution of the ground truth and that of the predicted values. â„’ e = â„’ M â€‹ S â€‹ E â€‹ ( e , e ^ ) \\displaystyle\\mathcal{L}_{e}=\\mathcal{L}_{MSE}(e,\\hat{e}) (11) + 0.5 â€‹ [ âˆ’ âˆ‘ i ( l â€‹ o â€‹ g â€‹ Ïƒ i 2 + 1 ) + âˆ‘ i Ïƒ i 2 + âˆ‘ i Î¼ i 2 ] \\displaystyle+5\\left[-\\sum_{i}(log\\>\\sigma_{i}^{2}+1)+\\sum_{i}\\sigma^{2}_{i}+\\sum_{i}\\mu_{i}^{2}\\right] â„’ p = â„’ M â€‹ S â€‹ E â€‹ ( Î¸ , Î¸ ^ ) \\displaystyle\\mathcal{L}_{p}=\\mathcal{L}_{MSE}(\\theta,\\hat{\\theta}) (12) + 0.5 â€‹ [ âˆ’ âˆ‘ i ( l â€‹ o â€‹ g â€‹ Ïƒ i 2 + 1 ) + âˆ‘ i Ïƒ i 2 + âˆ‘ i Î¼ i 2 ] \\displaystyle+5\\left[-\\sum_{i}(log\\>\\sigma_{i}^{2}+1)+\\sum_{i}\\sigma^{2}_{i}+\\sum_{i}\\mu_{i}^{2}\\right] where â„’ e \\mathcal{L}_{e} and â„’ p \\mathcal{L}_{p} denote the expression and pose loss function, respectively. Furthermore, we utilize the pretrained FLAME [ 22 ] , a parametric model of a 3D head that is integrated into DECA [ 9 ] , which has N = 5023 N=5023 base vertices V b âˆˆ â„› N Ã— 3 V_{b}\\in\\mathcal{R}^{N\\times 3} and two sets of M M and K K basis vectors that encode shape blendshapes ğ’® âˆˆ R N Ã— 3 Ã— M \\mathcal{S}\\in R^{N\\times 3\\times M} , and expression blendshapes B âˆˆ R N Ã— 3 Ã— K B\\in R^{N\\times 3\\times K} . The basic vectors are first blended and then Linear Blend Skinning (LBS) ğ’² ( . ) \\mathcal{W}(.) is applied to rotate the vertices following the pose Î¸ \\theta . The final reconstruction in world coordinates can be computed as follows ğ’± â€‹ ( Ï• , e , Î¸ ) = ğ’² â€‹ ( V b + ğ’® â€‹ Ï• + â„¬ â€‹ e , Î¸ ) \\mathcal{V}(\\phi,e,\\theta)=\\mathcal{W}(V_{b}+\\mathcal{S}\\phi+\\mathcal{B}e,\\theta) (13) where Ï• \\phi , e e , and Î¸ \\theta denote the identity shape, facial expression, and head pose parameters, respectively. In this case, FLAME is used to reconstruct vertices to improve the performance of I2FET. â„’ v = â„’ M â€‹ S â€‹ E â€‹ ( v , v ^ ) \\mathcal{L}_{v}=\\mathcal{L}_{MSE}(v,\\hat{v}) (14) where the vertex coordinates v v and v ^ \\hat{v} are computed by feeding the ground truth { Î¸ , e , Ï• } \\{\\theta,e,\\phi\\} and the reconstructed { Î¸ ^ , e ^ , Ï• } \\{\\hat{\\theta},\\hat{e},\\phi\\} through FLAME. Finally, the total loss function is given as â„’ t â€‹ o â€‹ t â€‹ a â€‹ l = â„’ e + â„’ p + â„’ v \\mathcal{L}_{total}=\\mathcal{L}_{e}+\\mathcal{L}_{p}+\\mathcal{L}_{v} (15) For the second branch, the pre-trained DECA [ 9 ] is used to encode the facial coefficients of the source face I s I_{s} including shape Ï• s âˆˆ R 1 Ã— 100 \\phi_{s}\\in R^{1\\times 100} , expression e s âˆˆ R 1 Ã— 50 e_{s}\\in R^{1\\times 50} , pose Î¸ s âˆˆ R 1 Ã— 6 \\theta_{s}\\in R^{1\\times 6} and camera c s âˆˆ R 1 Ã— 3 c_{s}\\in R^{1\\times 3} . Following this step, { Ï• s , e s , Î¸ s , c s } \\{\\phi_{s},e_{s},\\theta_{s},c_{s}\\} is integrated with the sequence of facial expression coefficients { ( e 0 , Î¸ 0 ) , ( e 1 , Î¸ 1 ) } \\{(e_{0},\\theta_{0}),(e_{1},\\theta_{1})\\} , generated by the first branch to generate the anchor facial expressions { s s ( 1 ) , s e 0 ( k ) , s e 1 ( T ) } \\{s^{(1)}_{s},s^{(k)}_{e_{0}},s^{(T)}_{e_{1}}\\} . Then, a linear interpolation function Î¨ ( . ) \\Psi(.) is applied to smoothen the transition between the two anchored facial expressions. The Î¨ â€‹ ( s ( l ) , s ( n ) ) \\Psi(s^{(l)},s^{(n)}) function for interpolation between facial expression sequences s ( l ) s^{(l)} and s ( n ) s^{(n)} can be defined as e ( k ) = Î´ âˆ— e ( l ) + ( 1 âˆ’ Î´ ) âˆ— e ( n ) \\displaystyle e^{(k)}=\\delta*e^{(l)}+(1-\\delta)*e^{(n)} (16) Î¸ ( k ) = Î´ âˆ— Î¸ ( l ) + ( 1 âˆ’ Î´ ) âˆ— Î¸ ( n ) \\displaystyle\\theta^{(k)}=\\delta*\\theta^{(l)}+(1-\\delta)*\\theta^{(n)} where 0 â‰¤ Î´ â‰¤ 1 0\\leq\\delta\\leq 1 is a linear coefficient and { e ( l ) , Î¸ ( l ) } âˆˆ s ( l ) \\{e^{(l)},\\theta^{(l)}\\}\\in s^{(l)} . This allows our framework to ensure temporal consistency when the motion changes are sampled uniformly. III-B 2 Face Rendering This module renders the facial appearance for the facial expression trajectories, generated by FET. In this work, the face rendering approach is based on the facial parameters of FLAME for integration with FET. Head Mesh Reconstruction: It generates the vertex coordinates corresponding to the facial expressions using Eq. 13 by using the sequence of facial expression transitions as the input. Then, the target vertex coordinates are obtained by combining these expressions with the deformation of hair and shoulder regions f H ( . ) f_{H}(.) from I s I_{s} as follows â„³ t = ğ’± â€‹ ( Ï• , e , Î¸ ) + f H â€‹ ( I s ) \\mathcal{M}^{t}=\\mathcal{V}(\\phi,e,\\theta)+f_{H}(I_{s}) (17) where f H ( . ) f_{H}(.) is the pre-trained linear deformation model [ 18 ] that is used to refine the vertices locations. Rendering : Following the head mesh reconstruction step, facial appearances can be rendered from the reconstructed head vertices â„³ = { m s ( 1 ) , m ( i ) , m e 0 ( k ) , m ( j ) , m e 1 ( T ) } \\mathcal{M}=\\{m_{s}^{(1)},m^{(i)},m_{e_{0}}^{(k)},m^{(j)},m_{e_{1}}^{(T)}\\} and the extracted neutral texture of the source image I s I_{s} by the texture decoder â„° t â€‹ e â€‹ x â€‹ ( I s ) \\mathcal{E}_{tex}(I_{s}) . The present work explores a face-rendering model suitable for facial expression transition. Therefore, we utilize pre-trained facial rendering models, known as state-of-the-art models [ 18 , 26 ] , capable of rendering facial appearances based on the facial parameters calculated with FLAME. Subsequently, the sequence of facial appearances ğ’´ = { y s ( 1 ) , y ( i ) , y e 0 ( k ) , y ( j ) , y e 1 ( T ) } \\mathcal{Y}=\\{y_{s}^{(1)},y^{(i)},y_{e_{0}}^{(k)},y^{(j)},y_{e_{1}}^{(T)}\\} is generated as the output of the proposed framework. (a) (b) (c) Figure 5 : Statistics of the facial expressions utilized in this study for the CK+ (a) and CelebV-HQ (b) datasets, respectively, and a collection of sampled words extracted from the text instructions (c)."
        },
        {
          "section_index": 9,
          "heading_level": "h3",
          "heading": "III-A Problem Statement",
          "text": "We aim to develop a model to generate 3D facial expression with two inputs: a face image I s I_{s} and a text instruction t t . The facial expression transition model ğ’¯ w â€‹ ( t , I s ) \\mathcal{T}_{w}(t,I_{s}) , having weights w w , is trained with a training set ğ’³ = { ( t , ( e 0 , Î¸ 0 ) , ( e 1 , Î¸ 1 ) ) ( j ) } j = 1 n \\mathcal{X}=\\{(t,(e_{0},\\theta_{0}),(e_{1},\\theta_{1}))^{(j)}\\}_{j=1}^{n} , where e 0 , e 1 âˆˆ R 1 Ã— 50 e_{0},e_{1}\\in R^{1\\times 50} denote expression vectors with 50 dimensions and Î¸ 0 , Î¸ 1 âˆˆ R 1 Ã— 6 \\theta_{0},\\theta_{1}\\in R^{1\\times 6} represent pose vectors with 6 dimensions corresponding to the first and next expressions in the facial expression sequence specified in instruction t t , respectively. n n is the number of samples in the training dataset. Our network learns ğ’¯ w ( . ) \\mathcal{T}_{w}(.) that approximates the data distribution p â€‹ ( ğ’³ ) p(\\mathcal{X}) and then generates a facial expression coefficient sequence { ( e 0 , Î¸ 0 ) , ( e 1 , Î¸ 1 ) } \\{(e_{0},\\theta_{0}),(e_{1},\\theta_{1})\\} with a given instruction t t . Next, a face rendering module ğ’¢ â€‹ ( I s , ğ’® ) \\mathcal{G}(I_{s},\\mathcal{S}) is used to render a sequence of facial appearances ğ’´ = { y s ( 1 ) , y ( i ) , y e 0 ( k ) , y ( j ) , y e 1 ( T ) } \\mathcal{Y}=\\{y_{s}^{(1)},y^{(i)},y_{e_{0}}^{(k)},y^{(j)},y_{e_{1}}^{(T)}\\} , with inputs, consisting of the texture of source image I s I_{s} and facial expression trajectories S = { s s ( 1 ) , s ( i ) , s e 0 ( k ) , s ( j ) , s e 1 ( T ) } S=\\{s_{s}^{(1)},s^{(i)},s_{e_{0}}^{(k)},s^{(j)},s_{e_{1}}^{(T)}\\} , where { s s ( 1 ) , s e 0 ( k ) , s e 1 ( T ) } \\{s_{s}^{(1)},s_{e_{0}}^{(k)},s_{e_{1}}^{(T)}\\} are anchor facial expression sequences corresponding to the source image, and specific facial expressions, generated by ğ’¯ w ( . ) \\mathcal{T}_{w}(.) . Then, facial expression sequences s ( i ) s^{(i)} and s ( j ) s^{(j)} transitioning between these anchor facial expressions can be inferred by a linear interpolation function to ensure the smoothness of the sequence of facial expressions."
        },
        {
          "section_index": 10,
          "heading_level": "h3",
          "heading": "III-B The Proposed Method",
          "text": "Figure 2 provides an overview of the proposed framework whereby one can control 3D facial expressions of the monocular image I s I_{s} following an input instruction t t . The framework consists of two major modules: (1) Facial Expression Transition (FET), ğ’¯ w â€‹ ( t , I s ) \\mathcal{T}_{w}(t,I_{s}) , for generating diverse expression trajectories; (2) Face Rendering (FR), ğ’¢ â€‹ ( I s , S ) \\mathcal{G}(I_{s},S) , for rendering facial appearances based on the texture of the source image I s I_{s} and the expression trajectories S S generated by the FET module. Figure 2 : Overview of the proposed framework consisting of two major modules: Facial Expression Transition (FET) and Face Rendering (FR). First, textual vector x t x_{t} is obtained from the pre-trained CLIP encoder for the FET module. Simultaneously, latent representations z p z_{p} and z e z_{e} are drawn from ğ’© â€‹ ( 0 , I ) \\mathcal{N}(0,I) . Then, the latent vectors are processed and concatenated to obtain x ^ f \\hat{x}^{f} . Afterward, the IFED module utilizes x t x^{t} and x ^ f \\hat{x}^{f} as inputs to create conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} , and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} for the pose and expression decoders. Subsequently, for smoothness of the facial expression sequence, the facial expression in the source image ( e s , Î¸ s ) (e_{s},\\theta_{s}) is subjected to linear interpolation and the specific facial expressions ( e 0 , Î¸ 0 ) , ( e 1 , Î¸ 1 ) (e_{0},\\theta_{0}),(e_{1},\\theta_{1}) , generated by the I2FET decoders. These sequences are then combined with the shape Ï• s \\phi_{s} and camera c s c_{s} parameters of the source image, obtained from DECA, to form facial expression trajectories { s s ( 1 ) , s ( i ) , s e 0 ( k ) , s ( j ) , s e 1 ( T ) } \\{s_{s}^{(1)},s^{(i)},s^{(k)}_{e_{0}},s^{(j)},s_{e_{1}}^{(T)}\\} . For FR, head mesh reconstruction produces a flame mesh sequence { m s ( 1 ) , m ( i ) , m e 0 ( k ) , m ( j ) , m e 1 ( T ) } \\{m_{s}^{(1)},m^{(i)},m^{(k)}_{e_{0}},m^{(j)},m_{e_{1}}^{(T)}\\} aligned with the facial expression trajectories and the texture encoder extracts the facial appearance from the source image. Finally, the rendering module generates the facial appearances { y s ( 1 ) , y ( i ) , y e 0 ( k ) , y ( j ) , y e 1 ( T ) } \\{y_{s}^{(1)},y^{(i)},y^{(k)}_{e_{0}},y^{(j)},y_{e_{1}}^{(T)}\\} within the facial expression trajectories using the flame mesh sequence and the extracted facial appearance. III-B 1 Facial Expression Transition This module is designed to generate diverse expression trajectories that correspond to the facial parameters of the source face image I s I_{s} and the parameters of specific facial expressions, described in an instruction t t . The module has two primary branches: the first receives an instruction to generate the corresponding sequence of facial expression based on instruction t t , whereas the second takes a 2D face image to encode its facial coefficients and create the facial expression trajectories. Figure 3 : Flow diagram of IFED (Instruction-Driven Facial Expression Decomposer). It consists of CAFT and the decomposition of parameters. The major function of CAFT is to introduce cross-attention between the facial parameters and text instruction. The two outputs from CAFT pass through layer normalization and are combined to create the fused feature vector x f â€‹ u â€‹ s â€‹ e â€‹ d x^{fused} . Then, pose projection function ğ’« p \\mathcal{P}_{p} and expression projection function ğ’« e \\mathcal{P}_{e} are used to decompose x f â€‹ u â€‹ s â€‹ e â€‹ d x^{fused} into two conditional vectors for facial expression ( x e â€‹ m â€‹ b e x_{emb}^{e} ) and pose ( x e â€‹ m â€‹ b p x_{emb}^{p} ). Figure 4 : Detailed architecture of the I2FET (Instruction to Facial Expression Transition) method. Given an instruction and a sequence of facial parameters, including pose and expression, the IFED module utilizes the facial parameters and textual description extracted by CLIP to produce the corresponding conditional feature vectors for pose x e â€‹ m â€‹ b p x_{emb}^{p} and expression x e â€‹ m â€‹ b e x_{emb}^{e} , respectively. The pose and expression encoders then map this sequence of facial parameters and conditional feature vectors to latent representations corresponding to pose and expression. These latent vectors, along with the textual description, are passed through the IFED module to generate the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} . Following this, the expression and pose decoders reconstruct expression e ^ \\hat{e} and pose Î¸ ^ \\hat{\\theta} parameters using these latent representations and the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} , respectively. Finally, the pre-trained FLAME decoder reconstructs the vertex coordinates v ^ \\hat{v} based on e ^ \\hat{e} and Î¸ ^ \\hat{\\theta} . In the former, the pre-trained CLIP encoder [ 33 ] extracts text description x t âˆˆ R 77 Ã— 768 x^{t}\\in R^{77\\times 768} according to the given instruction t t . Then, the I2FET model generates a sequence of expression and pose coefficients for the facial expression trajectories. Instruction-driven Facial Expression Decomposer : A dual-branch vision transformer [ 3 ] is adopted for extracting multi-scale features and fusing cross-attention information at different scales because it is efficient for the image classification task. Here, the Cross-Attention for Facial Expression Parameter and Text (CAFT) is used to learn the relationship between instruction and facial expression parameters by enhancing the understanding of multimodal data and refining the latent representation for the Instruction to Facial Expression Transition model. The proposed IFED module is illustrated in Fig. 3 . In particular, we directly incorporate facial expression parameters and textual descriptions into CAFT instead of adding class tokens as in [ 3 ] . With respect to the transformer encoders, the first branch learns the correlation between the facial expression parameters, while the second branch focuses on learning the textual descriptions. To begin with, each vector representing facial expression parameters comprises two elements: expression and pose parameters, denoted as x f = { e , Î¸ } âˆˆ R m Ã— 56 x^{f}=\\{e,\\theta\\}\\in R^{m\\times 56} , where m m denotes the number of expressions in a given instruction. As [ 24 , 6 ] , we only utilize jaw pose Î¸ j â€‹ a â€‹ w \\theta^{jaw} and expression e e to convey facial expression information. Therefore, we rewrite x f = { e , Î¸ j â€‹ a â€‹ w } âˆˆ R m Ã— 53 x^{f}=\\{e,\\theta^{jaw}\\}\\in R^{m\\times 53} as the input of the first branch. Then, the transformer encoder is utilized as y f = x f + M â€‹ S â€‹ A â€‹ ( L â€‹ N â€‹ ( x f ) ) \\displaystyle y^{f}=x^{f}+MSA(LN(x^{f})) (1) x ^ f = y f + F â€‹ F â€‹ N â€‹ ( L â€‹ N â€‹ ( y f ) ) \\displaystyle\\hat{x}^{f}=y^{f}+FFN(LN(y^{f})) where x ^ f \\hat{x}^{f} denotes the output of the transformer encoder in the facial expression parameters branch â„° P \\mathcal{E}_{P} . L N ( . ) LN(.) , M S A ( . ) MSA(.) , and F F N ( . ) FFN(.) are the layer norm, multi-head self-attention, and feed-forward network, respectively. Likewise, in the text branch, we replicate x t x^{t} to create a vector { x } i = { 0 , 1 } t \\{x\\}^{t}_{i=\\{0,1\\}} for computational purposes. Subsequently, the input for the transformer encoder is generated using a text projection function P t ( . ) P_{t}(.) on { x } i = { 0 , 1 } t \\{x\\}^{t}_{i=\\{0,1\\}} . As a result, the textual description x ^ t \\hat{x}^{t} is obtained through the transformer encoder â„° t \\mathcal{E}_{t} . Both branches are then fused by cross-attention to capture the relationship between the facial expression parameters and textual descriptions. The fusion process can be expressed as y c f = h f â€‹ 2 â€‹ t â€‹ ( x ^ 0 f ) , y c t = h t â€‹ 2 â€‹ f â€‹ ( x ^ 0 t ) , \\displaystyle y_{c}^{f}=h^{f2t}(\\hat{x}_{0}^{f}),\\quad y_{c}^{t}=h^{t2f}(\\hat{x}_{0}^{t}), (2) x o f = g t â€‹ 2 â€‹ f â€‹ ( [ C â€‹ A â€‹ ( y c f âŠ— x 1 t ) + y c f ] ) âŠ— x ^ 1 f \\displaystyle x_{o}^{f}=g^{t2f}([CA(y^{f}_{c}\\otimes x_{1}^{t})+y_{c}^{f}])\\otimes\\hat{x}_{1}^{f} x o t = g f â€‹ 2 â€‹ t â€‹ ( [ C â€‹ A â€‹ ( y c t âŠ— x 1 f ) + y c t ] ) âŠ— x ^ 1 t \\displaystyle x_{o}^{t}=g^{f2t}([CA(y_{c}^{t}\\otimes x_{1}^{f})+y_{c}^{t}])\\otimes\\hat{x}_{1}^{t} where h t â€‹ 2 â€‹ f , h f â€‹ 2 â€‹ t , g t â€‹ 2 â€‹ f h^{t2f},h^{f2t},g^{t2f} , and g f â€‹ 2 â€‹ t g^{f2t} are the projection function and back-projection function between the vectors that represent the text and facial expression parameters, x o f âˆˆ R m Ã— 53 x_{o}^{f}\\in R^{m\\times 53} and x o t âˆˆ R m Ã— 768 x_{o}^{t}\\in R^{m\\times 768} are the outputs of CAFT, C â€‹ A CA denotes cross-attention and âŠ— \\otimes denotes the concatenation operator. Next, the fused feature vector is obtained by concatenating x o f x_{o}^{f} and x o t x_{o}^{t} , after passing through L N ( . ) LN(.) , as follows: x f â€‹ u â€‹ s â€‹ e â€‹ d = L â€‹ N â€‹ ( x o t ) âŠ— L â€‹ N â€‹ ( x o f ) x^{fused}=LN(x_{o}^{t})\\otimes LN(x_{o}^{f}) (3) Finally, the projection functions for pose ğ’« p ( . ) \\mathcal{P}_{p}(.) and expression ğ’« e ( . ) \\mathcal{P}_{e}(.) are applied to the fused feature vector as follows: x e â€‹ m â€‹ b e = ğ’« e â€‹ ( x f â€‹ u â€‹ s â€‹ e â€‹ d ) \\displaystyle x^{e}_{emb}=\\mathcal{P}_{e}(x^{fused}) (4) x e â€‹ m â€‹ b p = ğ’« p â€‹ ( x f â€‹ u â€‹ s â€‹ e â€‹ d ) \\displaystyle x^{p}_{emb}=\\mathcal{P}_{p}(x^{fused}) where x e â€‹ m â€‹ b e âˆˆ R m Ã— 50 x^{e}_{emb}\\in R^{m\\times 50} and x e â€‹ m â€‹ b p âˆˆ R m Ã— 6 x^{p}_{emb}\\in R^{m\\times 6} are the conditional feature vectors for pose and expression, respectively. In this work, ğ’« p ( . ) \\mathcal{P}_{p}(.) and ğ’« e ( . ) \\mathcal{P}_{e}(.) are designed as linear transformation functions to map the feature x f â€‹ u â€‹ s â€‹ e â€‹ d x^{fused} to the corresponding pose and expression feature vectors. Instruction to Facial Expression Transition : The proposed I2FET method is based on a conditional variational autoencoder architecture. The method includes encoders â„° e t ( . ) \\mathcal{E}_{e}^{t}(.) and â„° p t ( . ) \\mathcal{E}_{p}^{t}(.) , as well as decoders ğ’Ÿ e t ( . ) \\mathcal{D}^{t}_{e}(.) and ğ’Ÿ p t ( . ) \\mathcal{D}^{t}_{p}(.) , which are defined using three Multi-Layer Perceptrons (MLPs). These components are conditioned on feature vectors that represent expression and pose information, respectively. An overview of the proposed I2FET architecture is presented in Fig. 4 . Here, the encoders are defined as follows: â„° e t â€‹ ( e âŠ— x e â€‹ m â€‹ b e ) âŸ¶ ( Î¼ e , Ïƒ e ) \\mathcal{E}_{e}^{t}(e\\otimes x^{e}_{emb})\\longrightarrow(\\mu_{e},\\sigma_{e}) (5) and â„° p t â€‹ ( Î¸ âŠ— x e â€‹ m â€‹ b p ) âŸ¶ ( Î¼ p , Ïƒ p ) \\mathcal{E}_{p}^{t}(\\theta\\otimes x^{p}_{emb})\\longrightarrow(\\mu_{p},\\sigma_{p}) (6) where âŠ— \\otimes denotes the concatenation operator and x e â€‹ m â€‹ b e x^{e}_{emb} and x e â€‹ m â€‹ b p x^{p}_{emb} are conditional feature vectors for expression and pose, respectively. For the encoder stage, x e â€‹ m â€‹ b e x^{e}_{emb} and x e â€‹ m â€‹ b p x^{p}_{emb} are created by textual description x t x_{t} and the facial expression parameters ( e , Î¸ ) (e,\\theta) . Î¼ e \\mu_{e} and Î¼ p \\mu_{p} denote the mean and standard deviation of the expression distribution, whereas Ïƒ e \\sigma_{e} , and Ïƒ p \\sigma_{p} are the mean and standard deviation of the pose distribution. In this study, the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}^{e}_{emb} and x ^ e â€‹ m â€‹ b p \\hat{x}^{p}_{emb} in the decoder are computed by textual description x t x_{t} and the latent vectors z ^ p \\hat{z}_{p} and z ^ e \\hat{z}_{e} representing pose and expression, respectively. These latent vectors are calculated as follows: z ~ p = Ïƒ p âˆ— z p + Î¼ p \\displaystyle\\tilde{z}_{p}=\\sigma_{p}*z_{p}+\\mu_{p} (7) z ^ p = ğ’¯ p â€‹ ( z ~ p ) \\displaystyle\\hat{z}_{p}=\\mathcal{T}_{p}(\\tilde{z}_{p}) z ~ e = Ïƒ e âˆ— z e + Î¼ e \\displaystyle\\tilde{z}_{e}=\\sigma_{e}*z_{e}+\\mu_{e} z ^ e = ğ’¯ e â€‹ ( z ~ e ) \\displaystyle\\hat{z}_{e}=\\mathcal{T}_{e}(\\tilde{z}_{e}) where z p z_{p} and z e z_{e} are the latent vectors sampled from ğ’© â€‹ ( 0 , I ) \\mathcal{N}(0,I) . In this case, we utilize linear transformation functions ğ’¯ p ( . ) \\mathcal{T}_{p}(.) and ğ’¯ e ( . ) \\mathcal{T}_{e}(.) to transform z ~ p \\tilde{z}_{p} and z ~ e âˆˆ R m Ã— 16 \\tilde{z}_{e}\\in R^{m\\times 16} into z ^ p âˆˆ R m Ã— 6 \\hat{z}_{p}\\in R^{m\\times 6} and z ^ e âˆˆ R m Ã— 50 \\hat{z}_{e}\\in R^{m\\times 50} , respectively, similar to the expression and pose dimensions. Thus, the latent vectors z ~ e \\tilde{z}_{e} and z ~ p \\tilde{z}_{p} can be used as input for the IFED module. Then, the latent vectors z ^ e \\hat{z}_{e} and z ^ p \\hat{z}_{p} are concatenated together, and along with the text vector x t x^{t} , they are passed through the IFED module to obtain the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} as x ^ e â€‹ m â€‹ b e , x ^ e â€‹ m â€‹ b p = I â€‹ F â€‹ E â€‹ D â€‹ ( z ^ p âŠ— z ^ e , x t ) \\displaystyle\\hat{x}_{emb}^{e},\\hat{x}_{emb}^{p}=IFED(\\hat{z}_{p}\\otimes\\hat{z}_{e},x^{t}) (8) This approach enhances the semantic representation of latent vectors by refining the conditional feature vectors for decoders based on the learned latent vectors throughout the training process. The decoders are defined as follows ğ’Ÿ p t â€‹ ( z ~ p âŠ— x ^ e â€‹ m â€‹ b p ) âŸ¶ ( Î¸ ^ 0 , Î¸ ^ 1 ) \\mathcal{D}^{t}_{p}(\\tilde{z}_{p}\\otimes\\hat{x}^{p}_{emb})\\longrightarrow(\\hat{\\theta}_{0},\\hat{\\theta}_{1}) (9) and ğ’Ÿ e t â€‹ ( z ~ e âŠ— x ^ e â€‹ m â€‹ b e ) âŸ¶ ( e ^ 0 , e ^ 1 ) \\mathcal{D}^{t}_{e}(\\tilde{z}_{e}\\otimes\\hat{x}^{e}_{emb})\\longrightarrow(\\hat{e}_{0},\\hat{e}_{1}) (10) The parameters of the I2FET model are optimized by minimizing the reconstruction loss function â„’ M â€‹ S â€‹ E ( . , . ) \\mathcal{L}_{MSE}(.,.) between the ground truth coefficients e e , Î¸ \\theta , and the predicted coefficients e ^ , Î¸ ^ \\hat{e},\\hat{\\theta} . In addition, the Kullback-Leibler divergence loss function is utilized to minimize the distance between the probability distribution of the ground truth and that of the predicted values. â„’ e = â„’ M â€‹ S â€‹ E â€‹ ( e , e ^ ) \\displaystyle\\mathcal{L}_{e}=\\mathcal{L}_{MSE}(e,\\hat{e}) (11) + 0.5 â€‹ [ âˆ’ âˆ‘ i ( l â€‹ o â€‹ g â€‹ Ïƒ i 2 + 1 ) + âˆ‘ i Ïƒ i 2 + âˆ‘ i Î¼ i 2 ] \\displaystyle+5\\left[-\\sum_{i}(log\\>\\sigma_{i}^{2}+1)+\\sum_{i}\\sigma^{2}_{i}+\\sum_{i}\\mu_{i}^{2}\\right] â„’ p = â„’ M â€‹ S â€‹ E â€‹ ( Î¸ , Î¸ ^ ) \\displaystyle\\mathcal{L}_{p}=\\mathcal{L}_{MSE}(\\theta,\\hat{\\theta}) (12) + 0.5 â€‹ [ âˆ’ âˆ‘ i ( l â€‹ o â€‹ g â€‹ Ïƒ i 2 + 1 ) + âˆ‘ i Ïƒ i 2 + âˆ‘ i Î¼ i 2 ] \\displaystyle+5\\left[-\\sum_{i}(log\\>\\sigma_{i}^{2}+1)+\\sum_{i}\\sigma^{2}_{i}+\\sum_{i}\\mu_{i}^{2}\\right] where â„’ e \\mathcal{L}_{e} and â„’ p \\mathcal{L}_{p} denote the expression and pose loss function, respectively. Furthermore, we utilize the pretrained FLAME [ 22 ] , a parametric model of a 3D head that is integrated into DECA [ 9 ] , which has N = 5023 N=5023 base vertices V b âˆˆ â„› N Ã— 3 V_{b}\\in\\mathcal{R}^{N\\times 3} and two sets of M M and K K basis vectors that encode shape blendshapes ğ’® âˆˆ R N Ã— 3 Ã— M \\mathcal{S}\\in R^{N\\times 3\\times M} , and expression blendshapes B âˆˆ R N Ã— 3 Ã— K B\\in R^{N\\times 3\\times K} . The basic vectors are first blended and then Linear Blend Skinning (LBS) ğ’² ( . ) \\mathcal{W}(.) is applied to rotate the vertices following the pose Î¸ \\theta . The final reconstruction in world coordinates can be computed as follows ğ’± â€‹ ( Ï• , e , Î¸ ) = ğ’² â€‹ ( V b + ğ’® â€‹ Ï• + â„¬ â€‹ e , Î¸ ) \\mathcal{V}(\\phi,e,\\theta)=\\mathcal{W}(V_{b}+\\mathcal{S}\\phi+\\mathcal{B}e,\\theta) (13) where Ï• \\phi , e e , and Î¸ \\theta denote the identity shape, facial expression, and head pose parameters, respectively. In this case, FLAME is used to reconstruct vertices to improve the performance of I2FET. â„’ v = â„’ M â€‹ S â€‹ E â€‹ ( v , v ^ ) \\mathcal{L}_{v}=\\mathcal{L}_{MSE}(v,\\hat{v}) (14) where the vertex coordinates v v and v ^ \\hat{v} are computed by feeding the ground truth { Î¸ , e , Ï• } \\{\\theta,e,\\phi\\} and the reconstructed { Î¸ ^ , e ^ , Ï• } \\{\\hat{\\theta},\\hat{e},\\phi\\} through FLAME. Finally, the total loss function is given as â„’ t â€‹ o â€‹ t â€‹ a â€‹ l = â„’ e + â„’ p + â„’ v \\mathcal{L}_{total}=\\mathcal{L}_{e}+\\mathcal{L}_{p}+\\mathcal{L}_{v} (15) For the second branch, the pre-trained DECA [ 9 ] is used to encode the facial coefficients of the source face I s I_{s} including shape Ï• s âˆˆ R 1 Ã— 100 \\phi_{s}\\in R^{1\\times 100} , expression e s âˆˆ R 1 Ã— 50 e_{s}\\in R^{1\\times 50} , pose Î¸ s âˆˆ R 1 Ã— 6 \\theta_{s}\\in R^{1\\times 6} and camera c s âˆˆ R 1 Ã— 3 c_{s}\\in R^{1\\times 3} . Following this step, { Ï• s , e s , Î¸ s , c s } \\{\\phi_{s},e_{s},\\theta_{s},c_{s}\\} is integrated with the sequence of facial expression coefficients { ( e 0 , Î¸ 0 ) , ( e 1 , Î¸ 1 ) } \\{(e_{0},\\theta_{0}),(e_{1},\\theta_{1})\\} , generated by the first branch to generate the anchor facial expressions { s s ( 1 ) , s e 0 ( k ) , s e 1 ( T ) } \\{s^{(1)}_{s},s^{(k)}_{e_{0}},s^{(T)}_{e_{1}}\\} . Then, a linear interpolation function Î¨ ( . ) \\Psi(.) is applied to smoothen the transition between the two anchored facial expressions. The Î¨ â€‹ ( s ( l ) , s ( n ) ) \\Psi(s^{(l)},s^{(n)}) function for interpolation between facial expression sequences s ( l ) s^{(l)} and s ( n ) s^{(n)} can be defined as e ( k ) = Î´ âˆ— e ( l ) + ( 1 âˆ’ Î´ ) âˆ— e ( n ) \\displaystyle e^{(k)}=\\delta*e^{(l)}+(1-\\delta)*e^{(n)} (16) Î¸ ( k ) = Î´ âˆ— Î¸ ( l ) + ( 1 âˆ’ Î´ ) âˆ— Î¸ ( n ) \\displaystyle\\theta^{(k)}=\\delta*\\theta^{(l)}+(1-\\delta)*\\theta^{(n)} where 0 â‰¤ Î´ â‰¤ 1 0\\leq\\delta\\leq 1 is a linear coefficient and { e ( l ) , Î¸ ( l ) } âˆˆ s ( l ) \\{e^{(l)},\\theta^{(l)}\\}\\in s^{(l)} . This allows our framework to ensure temporal consistency when the motion changes are sampled uniformly."
        },
        {
          "section_index": 11,
          "heading_level": "h4",
          "heading": "III-B 1 Facial Expression Transition",
          "text": "This module is designed to generate diverse expression trajectories that correspond to the facial parameters of the source face image I s I_{s} and the parameters of specific facial expressions, described in an instruction t t . The module has two primary branches: the first receives an instruction to generate the corresponding sequence of facial expression based on instruction t t , whereas the second takes a 2D face image to encode its facial coefficients and create the facial expression trajectories. Figure 3 : Flow diagram of IFED (Instruction-Driven Facial Expression Decomposer). It consists of CAFT and the decomposition of parameters. The major function of CAFT is to introduce cross-attention between the facial parameters and text instruction. The two outputs from CAFT pass through layer normalization and are combined to create the fused feature vector x f â€‹ u â€‹ s â€‹ e â€‹ d x^{fused} . Then, pose projection function ğ’« p \\mathcal{P}_{p} and expression projection function ğ’« e \\mathcal{P}_{e} are used to decompose x f â€‹ u â€‹ s â€‹ e â€‹ d x^{fused} into two conditional vectors for facial expression ( x e â€‹ m â€‹ b e x_{emb}^{e} ) and pose ( x e â€‹ m â€‹ b p x_{emb}^{p} ). Figure 4 : Detailed architecture of the I2FET (Instruction to Facial Expression Transition) method. Given an instruction and a sequence of facial parameters, including pose and expression, the IFED module utilizes the facial parameters and textual description extracted by CLIP to produce the corresponding conditional feature vectors for pose x e â€‹ m â€‹ b p x_{emb}^{p} and expression x e â€‹ m â€‹ b e x_{emb}^{e} , respectively. The pose and expression encoders then map this sequence of facial parameters and conditional feature vectors to latent representations corresponding to pose and expression. These latent vectors, along with the textual description, are passed through the IFED module to generate the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} . Following this, the expression and pose decoders reconstruct expression e ^ \\hat{e} and pose Î¸ ^ \\hat{\\theta} parameters using these latent representations and the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} , respectively. Finally, the pre-trained FLAME decoder reconstructs the vertex coordinates v ^ \\hat{v} based on e ^ \\hat{e} and Î¸ ^ \\hat{\\theta} . In the former, the pre-trained CLIP encoder [ 33 ] extracts text description x t âˆˆ R 77 Ã— 768 x^{t}\\in R^{77\\times 768} according to the given instruction t t . Then, the I2FET model generates a sequence of expression and pose coefficients for the facial expression trajectories. Instruction-driven Facial Expression Decomposer : A dual-branch vision transformer [ 3 ] is adopted for extracting multi-scale features and fusing cross-attention information at different scales because it is efficient for the image classification task. Here, the Cross-Attention for Facial Expression Parameter and Text (CAFT) is used to learn the relationship between instruction and facial expression parameters by enhancing the understanding of multimodal data and refining the latent representation for the Instruction to Facial Expression Transition model. The proposed IFED module is illustrated in Fig. 3 . In particular, we directly incorporate facial expression parameters and textual descriptions into CAFT instead of adding class tokens as in [ 3 ] . With respect to the transformer encoders, the first branch learns the correlation between the facial expression parameters, while the second branch focuses on learning the textual descriptions. To begin with, each vector representing facial expression parameters comprises two elements: expression and pose parameters, denoted as x f = { e , Î¸ } âˆˆ R m Ã— 56 x^{f}=\\{e,\\theta\\}\\in R^{m\\times 56} , where m m denotes the number of expressions in a given instruction. As [ 24 , 6 ] , we only utilize jaw pose Î¸ j â€‹ a â€‹ w \\theta^{jaw} and expression e e to convey facial expression information. Therefore, we rewrite x f = { e , Î¸ j â€‹ a â€‹ w } âˆˆ R m Ã— 53 x^{f}=\\{e,\\theta^{jaw}\\}\\in R^{m\\times 53} as the input of the first branch. Then, the transformer encoder is utilized as y f = x f + M â€‹ S â€‹ A â€‹ ( L â€‹ N â€‹ ( x f ) ) \\displaystyle y^{f}=x^{f}+MSA(LN(x^{f})) (1) x ^ f = y f + F â€‹ F â€‹ N â€‹ ( L â€‹ N â€‹ ( y f ) ) \\displaystyle\\hat{x}^{f}=y^{f}+FFN(LN(y^{f})) where x ^ f \\hat{x}^{f} denotes the output of the transformer encoder in the facial expression parameters branch â„° P \\mathcal{E}_{P} . L N ( . ) LN(.) , M S A ( . ) MSA(.) , and F F N ( . ) FFN(.) are the layer norm, multi-head self-attention, and feed-forward network, respectively. Likewise, in the text branch, we replicate x t x^{t} to create a vector { x } i = { 0 , 1 } t \\{x\\}^{t}_{i=\\{0,1\\}} for computational purposes. Subsequently, the input for the transformer encoder is generated using a text projection function P t ( . ) P_{t}(.) on { x } i = { 0 , 1 } t \\{x\\}^{t}_{i=\\{0,1\\}} . As a result, the textual description x ^ t \\hat{x}^{t} is obtained through the transformer encoder â„° t \\mathcal{E}_{t} . Both branches are then fused by cross-attention to capture the relationship between the facial expression parameters and textual descriptions. The fusion process can be expressed as y c f = h f â€‹ 2 â€‹ t â€‹ ( x ^ 0 f ) , y c t = h t â€‹ 2 â€‹ f â€‹ ( x ^ 0 t ) , \\displaystyle y_{c}^{f}=h^{f2t}(\\hat{x}_{0}^{f}),\\quad y_{c}^{t}=h^{t2f}(\\hat{x}_{0}^{t}), (2) x o f = g t â€‹ 2 â€‹ f â€‹ ( [ C â€‹ A â€‹ ( y c f âŠ— x 1 t ) + y c f ] ) âŠ— x ^ 1 f \\displaystyle x_{o}^{f}=g^{t2f}([CA(y^{f}_{c}\\otimes x_{1}^{t})+y_{c}^{f}])\\otimes\\hat{x}_{1}^{f} x o t = g f â€‹ 2 â€‹ t â€‹ ( [ C â€‹ A â€‹ ( y c t âŠ— x 1 f ) + y c t ] ) âŠ— x ^ 1 t \\displaystyle x_{o}^{t}=g^{f2t}([CA(y_{c}^{t}\\otimes x_{1}^{f})+y_{c}^{t}])\\otimes\\hat{x}_{1}^{t} where h t â€‹ 2 â€‹ f , h f â€‹ 2 â€‹ t , g t â€‹ 2 â€‹ f h^{t2f},h^{f2t},g^{t2f} , and g f â€‹ 2 â€‹ t g^{f2t} are the projection function and back-projection function between the vectors that represent the text and facial expression parameters, x o f âˆˆ R m Ã— 53 x_{o}^{f}\\in R^{m\\times 53} and x o t âˆˆ R m Ã— 768 x_{o}^{t}\\in R^{m\\times 768} are the outputs of CAFT, C â€‹ A CA denotes cross-attention and âŠ— \\otimes denotes the concatenation operator. Next, the fused feature vector is obtained by concatenating x o f x_{o}^{f} and x o t x_{o}^{t} , after passing through L N ( . ) LN(.) , as follows: x f â€‹ u â€‹ s â€‹ e â€‹ d = L â€‹ N â€‹ ( x o t ) âŠ— L â€‹ N â€‹ ( x o f ) x^{fused}=LN(x_{o}^{t})\\otimes LN(x_{o}^{f}) (3) Finally, the projection functions for pose ğ’« p ( . ) \\mathcal{P}_{p}(.) and expression ğ’« e ( . ) \\mathcal{P}_{e}(.) are applied to the fused feature vector as follows: x e â€‹ m â€‹ b e = ğ’« e â€‹ ( x f â€‹ u â€‹ s â€‹ e â€‹ d ) \\displaystyle x^{e}_{emb}=\\mathcal{P}_{e}(x^{fused}) (4) x e â€‹ m â€‹ b p = ğ’« p â€‹ ( x f â€‹ u â€‹ s â€‹ e â€‹ d ) \\displaystyle x^{p}_{emb}=\\mathcal{P}_{p}(x^{fused}) where x e â€‹ m â€‹ b e âˆˆ R m Ã— 50 x^{e}_{emb}\\in R^{m\\times 50} and x e â€‹ m â€‹ b p âˆˆ R m Ã— 6 x^{p}_{emb}\\in R^{m\\times 6} are the conditional feature vectors for pose and expression, respectively. In this work, ğ’« p ( . ) \\mathcal{P}_{p}(.) and ğ’« e ( . ) \\mathcal{P}_{e}(.) are designed as linear transformation functions to map the feature x f â€‹ u â€‹ s â€‹ e â€‹ d x^{fused} to the corresponding pose and expression feature vectors. Instruction to Facial Expression Transition : The proposed I2FET method is based on a conditional variational autoencoder architecture. The method includes encoders â„° e t ( . ) \\mathcal{E}_{e}^{t}(.) and â„° p t ( . ) \\mathcal{E}_{p}^{t}(.) , as well as decoders ğ’Ÿ e t ( . ) \\mathcal{D}^{t}_{e}(.) and ğ’Ÿ p t ( . ) \\mathcal{D}^{t}_{p}(.) , which are defined using three Multi-Layer Perceptrons (MLPs). These components are conditioned on feature vectors that represent expression and pose information, respectively. An overview of the proposed I2FET architecture is presented in Fig. 4 . Here, the encoders are defined as follows: â„° e t â€‹ ( e âŠ— x e â€‹ m â€‹ b e ) âŸ¶ ( Î¼ e , Ïƒ e ) \\mathcal{E}_{e}^{t}(e\\otimes x^{e}_{emb})\\longrightarrow(\\mu_{e},\\sigma_{e}) (5) and â„° p t â€‹ ( Î¸ âŠ— x e â€‹ m â€‹ b p ) âŸ¶ ( Î¼ p , Ïƒ p ) \\mathcal{E}_{p}^{t}(\\theta\\otimes x^{p}_{emb})\\longrightarrow(\\mu_{p},\\sigma_{p}) (6) where âŠ— \\otimes denotes the concatenation operator and x e â€‹ m â€‹ b e x^{e}_{emb} and x e â€‹ m â€‹ b p x^{p}_{emb} are conditional feature vectors for expression and pose, respectively. For the encoder stage, x e â€‹ m â€‹ b e x^{e}_{emb} and x e â€‹ m â€‹ b p x^{p}_{emb} are created by textual description x t x_{t} and the facial expression parameters ( e , Î¸ ) (e,\\theta) . Î¼ e \\mu_{e} and Î¼ p \\mu_{p} denote the mean and standard deviation of the expression distribution, whereas Ïƒ e \\sigma_{e} , and Ïƒ p \\sigma_{p} are the mean and standard deviation of the pose distribution. In this study, the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}^{e}_{emb} and x ^ e â€‹ m â€‹ b p \\hat{x}^{p}_{emb} in the decoder are computed by textual description x t x_{t} and the latent vectors z ^ p \\hat{z}_{p} and z ^ e \\hat{z}_{e} representing pose and expression, respectively. These latent vectors are calculated as follows: z ~ p = Ïƒ p âˆ— z p + Î¼ p \\displaystyle\\tilde{z}_{p}=\\sigma_{p}*z_{p}+\\mu_{p} (7) z ^ p = ğ’¯ p â€‹ ( z ~ p ) \\displaystyle\\hat{z}_{p}=\\mathcal{T}_{p}(\\tilde{z}_{p}) z ~ e = Ïƒ e âˆ— z e + Î¼ e \\displaystyle\\tilde{z}_{e}=\\sigma_{e}*z_{e}+\\mu_{e} z ^ e = ğ’¯ e â€‹ ( z ~ e ) \\displaystyle\\hat{z}_{e}=\\mathcal{T}_{e}(\\tilde{z}_{e}) where z p z_{p} and z e z_{e} are the latent vectors sampled from ğ’© â€‹ ( 0 , I ) \\mathcal{N}(0,I) . In this case, we utilize linear transformation functions ğ’¯ p ( . ) \\mathcal{T}_{p}(.) and ğ’¯ e ( . ) \\mathcal{T}_{e}(.) to transform z ~ p \\tilde{z}_{p} and z ~ e âˆˆ R m Ã— 16 \\tilde{z}_{e}\\in R^{m\\times 16} into z ^ p âˆˆ R m Ã— 6 \\hat{z}_{p}\\in R^{m\\times 6} and z ^ e âˆˆ R m Ã— 50 \\hat{z}_{e}\\in R^{m\\times 50} , respectively, similar to the expression and pose dimensions. Thus, the latent vectors z ~ e \\tilde{z}_{e} and z ~ p \\tilde{z}_{p} can be used as input for the IFED module. Then, the latent vectors z ^ e \\hat{z}_{e} and z ^ p \\hat{z}_{p} are concatenated together, and along with the text vector x t x^{t} , they are passed through the IFED module to obtain the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} as x ^ e â€‹ m â€‹ b e , x ^ e â€‹ m â€‹ b p = I â€‹ F â€‹ E â€‹ D â€‹ ( z ^ p âŠ— z ^ e , x t ) \\displaystyle\\hat{x}_{emb}^{e},\\hat{x}_{emb}^{p}=IFED(\\hat{z}_{p}\\otimes\\hat{z}_{e},x^{t}) (8)"
        },
        {
          "section_index": 12,
          "heading_level": "h4",
          "heading": "III-B 2 Face Rendering",
          "text": "This module renders the facial appearance for the facial expression trajectories, generated by FET. In this work, the face rendering approach is based on the facial parameters of FLAME for integration with FET. Head Mesh Reconstruction: It generates the vertex coordinates corresponding to the facial expressions using Eq. 13 by using the sequence of facial expression transitions as the input. Then, the target vertex coordinates are obtained by combining these expressions with the deformation of hair and shoulder regions f H ( . ) f_{H}(.) from I s I_{s} as follows â„³ t = ğ’± â€‹ ( Ï• , e , Î¸ ) + f H â€‹ ( I s ) \\mathcal{M}^{t}=\\mathcal{V}(\\phi,e,\\theta)+f_{H}(I_{s}) (17) where f H ( . ) f_{H}(.) is the pre-trained linear deformation model [ 18 ] that is used to refine the vertices locations. Rendering : Following the head mesh reconstruction step, facial appearances can be rendered from the reconstructed head vertices â„³ = { m s ( 1 ) , m ( i ) , m e 0 ( k ) , m ( j ) , m e 1 ( T ) } \\mathcal{M}=\\{m_{s}^{(1)},m^{(i)},m_{e_{0}}^{(k)},m^{(j)},m_{e_{1}}^{(T)}\\} and the extracted neutral texture of the source image I s I_{s} by the texture decoder â„° t â€‹ e â€‹ x â€‹ ( I s ) \\mathcal{E}_{tex}(I_{s}) . The present work explores a face-rendering model suitable for facial expression transition. Therefore, we utilize pre-trained facial rendering models, known as state-of-the-art models [ 18 , 26 ] , capable of rendering facial appearances based on the facial parameters calculated with FLAME. Subsequently, the sequence of facial appearances ğ’´ = { y s ( 1 ) , y ( i ) , y e 0 ( k ) , y ( j ) , y e 1 ( T ) } \\mathcal{Y}=\\{y_{s}^{(1)},y^{(i)},y_{e_{0}}^{(k)},y^{(j)},y_{e_{1}}^{(T)}\\} is generated as the output of the proposed framework. (a) (b) (c) Figure 5 : Statistics of the facial expressions utilized in this study for the CK+ (a) and CelebV-HQ (b) datasets, respectively, and a collection of sampled words extracted from the text instructions (c)."
        },
        {
          "section_index": 13,
          "heading_level": "h2",
          "heading": "IV Experiments",
          "text": "IV-A Dataset TABLE I : N 1 , N 2 âˆˆ { N_{1},N_{2}\\in\\{ HAPPINESS, DISGUST, ANGER, FEAR, SURPRISE, CONTEMPT, SADNESS } \\} IN THE CK+ DATASET AND N 1 , N 2 âˆˆ { N_{1},N_{2}\\in\\{ HAPPINESS, DISGUST, ANGER, FEAR, SURPRISE, CONTEMPT, SADNESS, NEUTRAL } \\} IN THE CELEBV-HQ DATASET. No Instruction/ Prompts 1 Turn this face from [ N 1 N_{1} ] to [ N 2 N_{2} ]. 2 Change this face from [ N 1 N_{1} ] to [ N 2 N_{2} ]. 3 Transform this face from [ N 1 N_{1} ] to [ N 2 N_{2} ]. 4 Modify this face, changing it from [ N 1 N_{1} ] to [ N 2 N_{2} ]. 5 Replace this face from [ N 1 N_{1} to [ N 2 N_{2} ]. IV-A 1 CK+ dataset The CK+ dataset was supplemented with text instructions for facial expression generation and transition. The original CK+ dataset [ 25 ] includes 117 subjects and 7 emotions, such as happiness, disgust, anger, fear, surprise, contempt, and sadness . The instructions are created to describe the transitions between these facial expressions. To extract the facial parameters and the shape image corresponding to the facial expression in the image, we utilize the pre-trained DECA model. For each sample, a tuple is created with an instruction, the target shape image, and facial parameters that correspond to the same subject. The CK+ dataset, in this study, contains a total of 26,352 samples, and the composition of this dataset in terms of the seven facial expressions is shown in Fig. 5(a) . IV-A 2 CelebV-HQ dataset Similar to the CK+ case, text instructions were added to the CelebV-HQ dataset for our experiments [ 44 ] . The original CelebV-HQ dataset included 15,653 subjects and 8 expressions of neutral, happiness, disgust, anger, fear, surprise, contempt, and sadness . The images in the CelebV-HQ dataset were spontaneously collected and presented many challenges such as highly non-frontal head poses, blur, and brightness. This problem was overcome by using a head pose algorithm [ 23 ] to estimate parameters such as the yaw, roll, and pitch , of the head pose ranging from -15 to 15 degrees. Given that the subjects were chosen indiscriminately, we select those with more than two facial expressions. Then, the pre-trained DECA was applied to extract the corresponding shape images and facial parameters. The CelebV-HQ dataset adopted in this study includes 614 subjects and 28,335 samples, and Fig. 5(b) shows a statistical breakdown of the eight facial expressions. Table I lists the template sentences that are used to generate guided instructions for facial expression transitions. We generated five instructions for each sample and depicted the transition from expression N 1 N_{1} to expression N 2 N_{2} . Subsequently, one presentation sentence is randomly chosen from these instructions. Fig. 5(c) illustrates some sampled words used in the text instructions. Figure 6 : The shapes of facial expression are generated using images from the CK+ dataset. The top and bottom rows illustrate surprised and angry expressions with various head poses. IV-B Implementation The I2FET model was trained for 200 epochs with a batch size of 128 and a learning rate of 8 â€‹ e âˆ’ 4 8e^{-4} . The Adam optimizer [ 20 ] was adopted to optimize the model. For the experiments, 10% of the dataset was allocated to the testing set, whereas the remaining portion was used for training. Then, a validation set was drawn from 10% of the training set. In this work, we repeated each experiment ten times and used the average results to evaluate the overall performance of the proposed I2FET. Because our task is new and a specific method that would allow a fair comparison is not available, we chose MotionClip [ 39 ] as a baseline. For the face rendering experiment, the configuration by [ 18 , 26 ] was used. IV-C Classification of Facial Expressions In this study, it was important to employ an appropriate classifier to evaluate the method that was used to generate facial expressions as prior studies [ 27 , 46 ] . Given that the CK+ and CelebV-HQ datasets had skewed distributions, the performance for minor classes could be affected by the facial expression classifiers trained on these imbalanced datasets, in which the results would be biased towards the major class. Moreover, the effectiveness of the classification model could also be affected by head movements, which were commonly encountered as shown in Fig. 6 . These problems were addressed by conducting an experiment to identify an appropriate classifier that can handle not only imbalanced datasets but also invariant rotation transformations. ResNet-101 [ 14 ] , MEK [ 43 ] , and ResNet-101 integrated with Reweighted Focal Loss (RFL) [ 5 ] were utilized to evaluate the performance of facial expression classification. For this purpose, augmented datasets were created by rotating the original CK+ and CelebV-HQ datasets at angles of Â± 10 o , Â± 15 o \\pm 10^{o},\\pm 15^{o} , and Â± 30 o \\pm 30^{o} . Then, the three networks are trained using the original datasets and evaluated on all datasets. The Accuracy was employed to assess whether samples had been correctly classified across all classes, while G-mean was used to evaluate the ability of the model to effectively recognize minor classes that are predicted by the facial expression classification model. The supplementary data in the ablation study in Section IV-E suggests that MEK can address imbalanced datasets but is sensitive to rotational transformations. ResNet101 demonstrates positive results with rotation transformations but performs worse on datasets that are highly imbalanced. ResNet101 integrated with RFL outperforms ResNet101 and MEK in both scenarios. As a result, ResNet101 with RFL was selected as our classifier for evaluating the ability of our models to generate facial expressions. IV-D Results (a) (b) (c) Figure 7 : Visualization of the learned latent space for seven facial expressions by (a) MotionClip, (b) I2FET without IFED, and (c) I2FET with IFED, respectively, using t-SNE. (a) (b) Figure 8 : Confusion matrices for seven facial expressions by (a) MotionClip and (b) our method on CK+. (a) (b) Figure 9 : Confusion matrices for eight facial expressions by (a) MotionClip and (b) our method on CelebV-HQ. TABLE II : COMPARISON OF ACCURACY BETWEEN OUR METHOD AND MOTIONCLIP ON THE CK+ DATASET Method Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow Ground-truth 99.69 Â± \\pm 0.0001 99.39 Â± \\pm 0.0002 99.69 Â± \\pm 9.3e -5 MotionClip 52.00 Â± \\pm 0.0058 20.00 Â± \\pm 0.0082 40.48 Â± \\pm 0.0060 Ours 91.44 Â± \\pm 0.0024 84.03 Â± \\pm 0.0034 80.30 Â± \\pm 0.0064 TABLE III : COMPARISON OF ACCURACY BETWEEN OUR METHOD AND MOTIONCLIP ON THE CELEBV-HQ DATASET Method Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow Ground-truth 93.67 Â± \\pm 0.0000 87.89 Â± \\pm 0.0000 95.04 Â± \\pm 1.1e -16 MotionClip 40.00 Â± \\pm 0.0039 13.73 Â± \\pm 0.0056 34.42 Â± \\pm 0.0069 Ours 58.24 Â± \\pm 0.0028 33.45 Â± \\pm 0.0052 46.47 Â± \\pm 0.0065 TABLE IV : RESULT OBTAINED BY COMBINING OUR METHOD WITH ROME OR CVTHEAD FOR TEXTURE GENERATION ON THE CK+ AND CELEBV-HQ DATASETS, RESPECTIVELY. NOTE THAT â†“ \\downarrow INDICATES THAT LOWER IS BETTER, WHEREAS â†‘ \\uparrow DENOTES THAT THE HIGHER IS BETTER. Dataset Method L â†“ 1 {}_{1}\\downarrow PSNR â†‘ {\\uparrow} LPIPS â†“ {\\downarrow} MS_SSIM â†‘ {\\uparrow} CK+ Ours+ROME 0.199 9.939 0.490 0.347 Ours+CVTHead 0.005 33.74 0.021 0.978 CelebV-HQ Ours+ROME 0.168 11.316 0.505 0.418 Ours+CVTHead 0.003 37.08 0.010 0.990 Figure 10 : Qualitative comparison between 3D expressive heads that were generated by MotionClip and our method, respectively. The input, consisting of a shape image and instruction, was used for generating expressions 1 and 2 using either MotionClip (b) or Ours (c) on CK+ and CelebV-HQ (d, e), respectively. Figure 11 : Qualitative comparison between generated 3D expressive faces when our method is combined with texture generation [ 18 , 26 ] to generate the desired expressions. The input (a), consisting of the source image and instruction, was used for generating expressions 1 and 2 using either Ours+ROME (b) or Ours+CVTHead (c) on CK+ and CelebV-HQ (d, e), respectively. Figure 12 : Successive snapshots of the generated 3D expressive faces that include a neutral expression between two facial expressions instructed by text prompts. As the present study aims to generate an avatar with a 3D facial expression from a 2D image of a face and transform the facial emotion according to the given instructions, two major challenges arise (1) facial expression transition based on expression information given by the instruction and (2) rendering the facial appearance such that it corresponds with the desired facial expression. Two experiments were prepared to evaluate the results obtained with the proposed framework both quantitatively and qualitatively. IV-D 1 Quantitative Comparison Two tasks are prepared for quantitative comparison. Facial Expression Transition The DECA encoder extracts the shape, facial expression, and pose from the source image. At the same time, the CLIP encoder combined with IFED and I2FET was used to interpret the text instruction: the initial facial expression, the final facial expression, and pose variation. To compare our approach with MotionClip, the first accuracy metric (Acc 1 ) was used to count the correctly classified samples for all expression labels in the instructions. The other one (Acc 2 ) was used to measure the correctly classified samples corresponding to both expression labels presented in each input instruction. The result suggests that the accuracy of our model is higher than that of MotionClip on both datasets as summarized in Table II . For example, our model reaches 91.44 % accuracy for generating an independent expression and 84.03 % for generating expression sequences on the CK+ dataset. The Gmean of our model is 80.30%. For the CelebV-HQ dataset, which is the real-world dataset, our model scores 58.24 %, 33.45 %, and 46.47 % for Acc 1 , Acc 2 , and Gmean, respectively, as indicated in Table III . This result suggests that our model outperforms MotionClip for learning the facial expression parameters. Visualization with t-SNE : Fig. 7 shows three learned latent representations, i.e. with MotionClip, I2FET, and I2FET with IFED, for seven facial expressions. These visual representations indicate that segregation between facial expressions is highly distinctive when IFED is combined with I2FET, compared with either MotionClip or I2FET alone. Confusion Matrices Analysis : Fig. 8 and Fig. 9 show that IFED enhances the accuracy across most classes compared to MotionClip on both datasets. Yet, a few case of misclassification occurred between certain classes of expressions, such as anger and sadness, or contempt and anger with sadness on CK+. In real-world datasets such as CelebV-HQ, some misclassification occurs between classes, particularly those with challenging expressions such as fear and surprise, or contempt and neutral. Face Rendering : For the face-rendering task, FET was combined with state-of-the-art models [ 18 , 26 ] for rendering facial appearances that correspond to specific expressions requested by instruction. This was accomplished by integrating whether ROME [ 18 ] or CVTHead [ 26 ] with facial expression transition. In this case, DECA extracts facial parameters to render geometrical information for a specific face. Instead of shapes, the RGB images and instructions are used to evaluate the performance. The L 1 loss, Peak Signal to Noise Ratio (PSNR), Learned Perceptual Image Patch Similarity (LPIPS) [ 42 ] , and Multi-Scale Structured Similarity (MS-SSIM) were used to measure the quality of reconstruction between the target image and synthesized image in the generated expression. As indicated by Table IV , our model with CVTHead produces the best result. IV-D 2 Qualitative Comparison In a similar vein, two tasks were prepared for qualitative comparisons. Facial Expression Transition : Fig. 10 compares sequences of facial expressions, generated by Motionclip and our method on CK+ and CelebV-HQ, respectively. These results clearly show that our method can generate facial expressions that convey more accurate expressions than MotionClip. Face Rendering : The resulting expressions can be qualitatively compared by viewing and deciding which expression looks better and more natural than the other. Fig. 11 shows the generated facial expressions based on learning the latent space of facial parameters on both datasets. The viewer may notice the difference between (b) and (c) for the CK+ dataset, and between (d) and (e) for the CelebV-HQ dataset, respectively, in terms of facial expressions and facial texture. For instance, the facial transition from surprise to happiness seen in the 2nd row is clearer in (c) than in (b), and the facial texture seen in the 4th row is much smoother in (c) than in (b). A similar observation can be made between (d) and (e), suggesting that Ours+CVTHead is superior to than Ours+ROME. For instance, in some cases, those enclosed within dashed boxes in Fig. 11 , a subtle distortion arose either when the facial region of the source image was occluded or when the instructions describe expressions with significant pose changes, such as transitioning from surprise to happiness or from fear to surprise. This led to inconsistencies, and missing details in the generated facial regions compared to Ours+CVTHead. A comparison between the two datasets, such as between (b) and (d) or between (c) and (e), reveals that the visual quality of CK+ is better than that of CelebV-HQ. These results confirm that our model has outstanding ability to transform facial expressions, learned on CK+ or CelebV-HQ, to a specific facial image. In addition, a user study was carried out by requesting 24 subjects to view 60 randomly paired images of various identities selected from either CVTHead + MotionClip or CVTHead + Ours. Subjects were asked to choose the expression closest to the expressions specified in the instructions or indicate whether none of them matched the provided expressions. Naturally, they were unaware as to which algorithm was used to the presented images. Four subjects viewed each video and voted for the one that more closely resembled the expression in the instructions. In cases in which the votes were tied, an additional subject was asked to make the final decision. Based on these outcomes, CVTHead + Ours achieved 73% accuracy, whereas CVTHead + MotionClip yielded 58% accuracy for CK+. For CelebV-HQ, CVTHead + Ours and CVTHead + MotionClip achieved 77% and 70.5% accuracy, respectively. IV-D 3 Inference Time During inference, CVTHead + MotionClip takes 3.97 seconds to generate a video, whereas CVTHead + Ours only takes 3.92 seconds on a single NVIDIA RTX A6000, suggesting that our model is a slightly faster than MotionClip."
        },
        {
          "section_index": 14,
          "heading_level": "h3",
          "heading": "IV-A Dataset",
          "text": "TABLE I : N 1 , N 2 âˆˆ { N_{1},N_{2}\\in\\{ HAPPINESS, DISGUST, ANGER, FEAR, SURPRISE, CONTEMPT, SADNESS } \\} IN THE CK+ DATASET AND N 1 , N 2 âˆˆ { N_{1},N_{2}\\in\\{ HAPPINESS, DISGUST, ANGER, FEAR, SURPRISE, CONTEMPT, SADNESS, NEUTRAL } \\} IN THE CELEBV-HQ DATASET. No Instruction/ Prompts 1 Turn this face from [ N 1 N_{1} ] to [ N 2 N_{2} ]. 2 Change this face from [ N 1 N_{1} ] to [ N 2 N_{2} ]. 3 Transform this face from [ N 1 N_{1} ] to [ N 2 N_{2} ]. 4 Modify this face, changing it from [ N 1 N_{1} ] to [ N 2 N_{2} ]. 5 Replace this face from [ N 1 N_{1} to [ N 2 N_{2} ]. IV-A 1 CK+ dataset The CK+ dataset was supplemented with text instructions for facial expression generation and transition. The original CK+ dataset [ 25 ] includes 117 subjects and 7 emotions, such as happiness, disgust, anger, fear, surprise, contempt, and sadness . The instructions are created to describe the transitions between these facial expressions. To extract the facial parameters and the shape image corresponding to the facial expression in the image, we utilize the pre-trained DECA model. For each sample, a tuple is created with an instruction, the target shape image, and facial parameters that correspond to the same subject. The CK+ dataset, in this study, contains a total of 26,352 samples, and the composition of this dataset in terms of the seven facial expressions is shown in Fig. 5(a) . IV-A 2 CelebV-HQ dataset Similar to the CK+ case, text instructions were added to the CelebV-HQ dataset for our experiments [ 44 ] . The original CelebV-HQ dataset included 15,653 subjects and 8 expressions of neutral, happiness, disgust, anger, fear, surprise, contempt, and sadness . The images in the CelebV-HQ dataset were spontaneously collected and presented many challenges such as highly non-frontal head poses, blur, and brightness. This problem was overcome by using a head pose algorithm [ 23 ] to estimate parameters such as the yaw, roll, and pitch , of the head pose ranging from -15 to 15 degrees. Given that the subjects were chosen indiscriminately, we select those with more than two facial expressions. Then, the pre-trained DECA was applied to extract the corresponding shape images and facial parameters. The CelebV-HQ dataset adopted in this study includes 614 subjects and 28,335 samples, and Fig. 5(b) shows a statistical breakdown of the eight facial expressions. Table I lists the template sentences that are used to generate guided instructions for facial expression transitions. We generated five instructions for each sample and depicted the transition from expression N 1 N_{1} to expression N 2 N_{2} . Subsequently, one presentation sentence is randomly chosen from these instructions. Fig. 5(c) illustrates some sampled words used in the text instructions. Figure 6 : The shapes of facial expression are generated using images from the CK+ dataset. The top and bottom rows illustrate surprised and angry expressions with various head poses."
        },
        {
          "section_index": 15,
          "heading_level": "h4",
          "heading": "IV-A 1 CK+ dataset",
          "text": "The CK+ dataset was supplemented with text instructions for facial expression generation and transition. The original CK+ dataset [ 25 ] includes 117 subjects and 7 emotions, such as happiness, disgust, anger, fear, surprise, contempt, and sadness . The instructions are created to describe the transitions between these facial expressions. To extract the facial parameters and the shape image corresponding to the facial expression in the image, we utilize the pre-trained DECA model. For each sample, a tuple is created with an instruction, the target shape image, and facial parameters that correspond to the same subject. The CK+ dataset, in this study, contains a total of 26,352 samples, and the composition of this dataset in terms of the seven facial expressions is shown in Fig. 5(a) ."
        },
        {
          "section_index": 16,
          "heading_level": "h4",
          "heading": "IV-A 2 CelebV-HQ dataset",
          "text": "Similar to the CK+ case, text instructions were added to the CelebV-HQ dataset for our experiments [ 44 ] . The original CelebV-HQ dataset included 15,653 subjects and 8 expressions of neutral, happiness, disgust, anger, fear, surprise, contempt, and sadness . The images in the CelebV-HQ dataset were spontaneously collected and presented many challenges such as highly non-frontal head poses, blur, and brightness. This problem was overcome by using a head pose algorithm [ 23 ] to estimate parameters such as the yaw, roll, and pitch , of the head pose ranging from -15 to 15 degrees. Given that the subjects were chosen indiscriminately, we select those with more than two facial expressions. Then, the pre-trained DECA was applied to extract the corresponding shape images and facial parameters. The CelebV-HQ dataset adopted in this study includes 614 subjects and 28,335 samples, and Fig. 5(b) shows a statistical breakdown of the eight facial expressions. Table I lists the template sentences that are used to generate guided instructions for facial expression transitions. We generated five instructions for each sample and depicted the transition from expression N 1 N_{1} to expression N 2 N_{2} . Subsequently, one presentation sentence is randomly chosen from these instructions. Fig. 5(c) illustrates some sampled words used in the text instructions. Figure 6 : The shapes of facial expression are generated using images from the CK+ dataset. The top and bottom rows illustrate surprised and angry expressions with various head poses."
        },
        {
          "section_index": 17,
          "heading_level": "h3",
          "heading": "IV-B Implementation",
          "text": "The I2FET model was trained for 200 epochs with a batch size of 128 and a learning rate of 8 â€‹ e âˆ’ 4 8e^{-4} . The Adam optimizer [ 20 ] was adopted to optimize the model. For the experiments, 10% of the dataset was allocated to the testing set, whereas the remaining portion was used for training. Then, a validation set was drawn from 10% of the training set. In this work, we repeated each experiment ten times and used the average results to evaluate the overall performance of the proposed I2FET. Because our task is new and a specific method that would allow a fair comparison is not available, we chose MotionClip [ 39 ] as a baseline. For the face rendering experiment, the configuration by [ 18 , 26 ] was used."
        },
        {
          "section_index": 18,
          "heading_level": "h3",
          "heading": "IV-C Classification of Facial Expressions",
          "text": "In this study, it was important to employ an appropriate classifier to evaluate the method that was used to generate facial expressions as prior studies [ 27 , 46 ] . Given that the CK+ and CelebV-HQ datasets had skewed distributions, the performance for minor classes could be affected by the facial expression classifiers trained on these imbalanced datasets, in which the results would be biased towards the major class. Moreover, the effectiveness of the classification model could also be affected by head movements, which were commonly encountered as shown in Fig. 6 . These problems were addressed by conducting an experiment to identify an appropriate classifier that can handle not only imbalanced datasets but also invariant rotation transformations. ResNet-101 [ 14 ] , MEK [ 43 ] , and ResNet-101 integrated with Reweighted Focal Loss (RFL) [ 5 ] were utilized to evaluate the performance of facial expression classification. For this purpose, augmented datasets were created by rotating the original CK+ and CelebV-HQ datasets at angles of Â± 10 o , Â± 15 o \\pm 10^{o},\\pm 15^{o} , and Â± 30 o \\pm 30^{o} . Then, the three networks are trained using the original datasets and evaluated on all datasets. The Accuracy was employed to assess whether samples had been correctly classified across all classes, while G-mean was used to evaluate the ability of the model to effectively recognize minor classes that are predicted by the facial expression classification model. The supplementary data in the ablation study in Section IV-E suggests that MEK can address imbalanced datasets but is sensitive to rotational transformations. ResNet101 demonstrates positive results with rotation transformations but performs worse on datasets that are highly imbalanced. ResNet101 integrated with RFL outperforms ResNet101 and MEK in both scenarios. As a result, ResNet101 with RFL was selected as our classifier for evaluating the ability of our models to generate facial expressions."
        },
        {
          "section_index": 19,
          "heading_level": "h3",
          "heading": "IV-D Results",
          "text": "(a) (b) (c) Figure 7 : Visualization of the learned latent space for seven facial expressions by (a) MotionClip, (b) I2FET without IFED, and (c) I2FET with IFED, respectively, using t-SNE. (a) (b) Figure 8 : Confusion matrices for seven facial expressions by (a) MotionClip and (b) our method on CK+. (a) (b) Figure 9 : Confusion matrices for eight facial expressions by (a) MotionClip and (b) our method on CelebV-HQ. TABLE II : COMPARISON OF ACCURACY BETWEEN OUR METHOD AND MOTIONCLIP ON THE CK+ DATASET Method Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow Ground-truth 99.69 Â± \\pm 0.0001 99.39 Â± \\pm 0.0002 99.69 Â± \\pm 9.3e -5 MotionClip 52.00 Â± \\pm 0.0058 20.00 Â± \\pm 0.0082 40.48 Â± \\pm 0.0060 Ours 91.44 Â± \\pm 0.0024 84.03 Â± \\pm 0.0034 80.30 Â± \\pm 0.0064 TABLE III : COMPARISON OF ACCURACY BETWEEN OUR METHOD AND MOTIONCLIP ON THE CELEBV-HQ DATASET Method Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow Ground-truth 93.67 Â± \\pm 0.0000 87.89 Â± \\pm 0.0000 95.04 Â± \\pm 1.1e -16 MotionClip 40.00 Â± \\pm 0.0039 13.73 Â± \\pm 0.0056 34.42 Â± \\pm 0.0069 Ours 58.24 Â± \\pm 0.0028 33.45 Â± \\pm 0.0052 46.47 Â± \\pm 0.0065 TABLE IV : RESULT OBTAINED BY COMBINING OUR METHOD WITH ROME OR CVTHEAD FOR TEXTURE GENERATION ON THE CK+ AND CELEBV-HQ DATASETS, RESPECTIVELY. NOTE THAT â†“ \\downarrow INDICATES THAT LOWER IS BETTER, WHEREAS â†‘ \\uparrow DENOTES THAT THE HIGHER IS BETTER. Dataset Method L â†“ 1 {}_{1}\\downarrow PSNR â†‘ {\\uparrow} LPIPS â†“ {\\downarrow} MS_SSIM â†‘ {\\uparrow} CK+ Ours+ROME 0.199 9.939 0.490 0.347 Ours+CVTHead 0.005 33.74 0.021 0.978 CelebV-HQ Ours+ROME 0.168 11.316 0.505 0.418 Ours+CVTHead 0.003 37.08 0.010 0.990 Figure 10 : Qualitative comparison between 3D expressive heads that were generated by MotionClip and our method, respectively. The input, consisting of a shape image and instruction, was used for generating expressions 1 and 2 using either MotionClip (b) or Ours (c) on CK+ and CelebV-HQ (d, e), respectively. Figure 11 : Qualitative comparison between generated 3D expressive faces when our method is combined with texture generation [ 18 , 26 ] to generate the desired expressions. The input (a), consisting of the source image and instruction, was used for generating expressions 1 and 2 using either Ours+ROME (b) or Ours+CVTHead (c) on CK+ and CelebV-HQ (d, e), respectively. Figure 12 : Successive snapshots of the generated 3D expressive faces that include a neutral expression between two facial expressions instructed by text prompts. As the present study aims to generate an avatar with a 3D facial expression from a 2D image of a face and transform the facial emotion according to the given instructions, two major challenges arise (1) facial expression transition based on expression information given by the instruction and (2) rendering the facial appearance such that it corresponds with the desired facial expression. Two experiments were prepared to evaluate the results obtained with the proposed framework both quantitatively and qualitatively. IV-D 1 Quantitative Comparison Two tasks are prepared for quantitative comparison. Facial Expression Transition The DECA encoder extracts the shape, facial expression, and pose from the source image. At the same time, the CLIP encoder combined with IFED and I2FET was used to interpret the text instruction: the initial facial expression, the final facial expression, and pose variation. To compare our approach with MotionClip, the first accuracy metric (Acc 1 ) was used to count the correctly classified samples for all expression labels in the instructions. The other one (Acc 2 ) was used to measure the correctly classified samples corresponding to both expression labels presented in each input instruction. The result suggests that the accuracy of our model is higher than that of MotionClip on both datasets as summarized in Table II . For example, our model reaches 91.44 % accuracy for generating an independent expression and 84.03 % for generating expression sequences on the CK+ dataset. The Gmean of our model is 80.30%. For the CelebV-HQ dataset, which is the real-world dataset, our model scores 58.24 %, 33.45 %, and 46.47 % for Acc 1 , Acc 2 , and Gmean, respectively, as indicated in Table III . This result suggests that our model outperforms MotionClip for learning the facial expression parameters. Visualization with t-SNE : Fig. 7 shows three learned latent representations, i.e. with MotionClip, I2FET, and I2FET with IFED, for seven facial expressions. These visual representations indicate that segregation between facial expressions is highly distinctive when IFED is combined with I2FET, compared with either MotionClip or I2FET alone. Confusion Matrices Analysis : Fig. 8 and Fig. 9 show that IFED enhances the accuracy across most classes compared to MotionClip on both datasets. Yet, a few case of misclassification occurred between certain classes of expressions, such as anger and sadness, or contempt and anger with sadness on CK+. In real-world datasets such as CelebV-HQ, some misclassification occurs between classes, particularly those with challenging expressions such as fear and surprise, or contempt and neutral. Face Rendering : For the face-rendering task, FET was combined with state-of-the-art models [ 18 , 26 ] for rendering facial appearances that correspond to specific expressions requested by instruction. This was accomplished by integrating whether ROME [ 18 ] or CVTHead [ 26 ] with facial expression transition. In this case, DECA extracts facial parameters to render geometrical information for a specific face. Instead of shapes, the RGB images and instructions are used to evaluate the performance. The L 1 loss, Peak Signal to Noise Ratio (PSNR), Learned Perceptual Image Patch Similarity (LPIPS) [ 42 ] , and Multi-Scale Structured Similarity (MS-SSIM) were used to measure the quality of reconstruction between the target image and synthesized image in the generated expression. As indicated by Table IV , our model with CVTHead produces the best result. IV-D 2 Qualitative Comparison In a similar vein, two tasks were prepared for qualitative comparisons. Facial Expression Transition : Fig. 10 compares sequences of facial expressions, generated by Motionclip and our method on CK+ and CelebV-HQ, respectively. These results clearly show that our method can generate facial expressions that convey more accurate expressions than MotionClip. Face Rendering : The resulting expressions can be qualitatively compared by viewing and deciding which expression looks better and more natural than the other. Fig. 11 shows the generated facial expressions based on learning the latent space of facial parameters on both datasets. The viewer may notice the difference between (b) and (c) for the CK+ dataset, and between (d) and (e) for the CelebV-HQ dataset, respectively, in terms of facial expressions and facial texture. For instance, the facial transition from surprise to happiness seen in the 2nd row is clearer in (c) than in (b), and the facial texture seen in the 4th row is much smoother in (c) than in (b). A similar observation can be made between (d) and (e), suggesting that Ours+CVTHead is superior to than Ours+ROME. For instance, in some cases, those enclosed within dashed boxes in Fig. 11 , a subtle distortion arose either when the facial region of the source image was occluded or when the instructions describe expressions with significant pose changes, such as transitioning from surprise to happiness or from fear to surprise. This led to inconsistencies, and missing details in the generated facial regions compared to Ours+CVTHead. A comparison between the two datasets, such as between (b) and (d) or between (c) and (e), reveals that the visual quality of CK+ is better than that of CelebV-HQ. These results confirm that our model has outstanding ability to transform facial expressions, learned on CK+ or CelebV-HQ, to a specific facial image. In addition, a user study was carried out by requesting 24 subjects to view 60 randomly paired images of various identities selected from either CVTHead + MotionClip or CVTHead + Ours. Subjects were asked to choose the expression closest to the expressions specified in the instructions or indicate whether none of them matched the provided expressions. Naturally, they were unaware as to which algorithm was used to the presented images. Four subjects viewed each video and voted for the one that more closely resembled the expression in the instructions. In cases in which the votes were tied, an additional subject was asked to make the final decision. Based on these outcomes, CVTHead + Ours achieved 73% accuracy, whereas CVTHead + MotionClip yielded 58% accuracy for CK+. For CelebV-HQ, CVTHead + Ours and CVTHead + MotionClip achieved 77% and 70.5% accuracy, respectively."
        },
        {
          "section_index": 20,
          "heading_level": "h4",
          "heading": "IV-D 1 Quantitative Comparison",
          "text": "Two tasks are prepared for quantitative comparison. Facial Expression Transition The DECA encoder extracts the shape, facial expression, and pose from the source image. At the same time, the CLIP encoder combined with IFED and I2FET was used to interpret the text instruction: the initial facial expression, the final facial expression, and pose variation. To compare our approach with MotionClip, the first accuracy metric (Acc 1 ) was used to count the correctly classified samples for all expression labels in the instructions. The other one (Acc 2 ) was used to measure the correctly classified samples corresponding to both expression labels presented in each input instruction. The result suggests that the accuracy of our model is higher than that of MotionClip on both datasets as summarized in Table II . For example, our model reaches 91.44 % accuracy for generating an independent expression and 84.03 % for generating expression sequences on the CK+ dataset. The Gmean of our model is 80.30%. For the CelebV-HQ dataset, which is the real-world dataset, our model scores 58.24 %, 33.45 %, and 46.47 % for Acc 1 , Acc 2 , and Gmean, respectively, as indicated in Table III . This result suggests that our model outperforms MotionClip for learning the facial expression parameters. Visualization with t-SNE : Fig. 7 shows three learned latent representations, i.e. with MotionClip, I2FET, and I2FET with IFED, for seven facial expressions. These visual representations indicate that segregation between facial expressions is highly distinctive when IFED is combined with I2FET, compared with either MotionClip or I2FET alone. Confusion Matrices Analysis : Fig. 8 and Fig. 9 show that IFED enhances the accuracy across most classes compared to MotionClip on both datasets. Yet, a few case of misclassification occurred between certain classes of expressions, such as anger and sadness, or contempt and anger with sadness on CK+. In real-world datasets such as CelebV-HQ, some misclassification occurs between classes, particularly those with challenging expressions such as fear and surprise, or contempt and neutral. Face Rendering : For the face-rendering task, FET was combined with state-of-the-art models [ 18 , 26 ] for rendering facial appearances that correspond to specific expressions requested by instruction. This was accomplished by integrating whether ROME [ 18 ] or CVTHead [ 26 ] with facial expression transition. In this case, DECA extracts facial parameters to render geometrical information for a specific face. Instead of shapes, the RGB images and instructions are used to evaluate the performance. The L 1 loss, Peak Signal to Noise Ratio (PSNR), Learned Perceptual Image Patch Similarity (LPIPS) [ 42 ] , and Multi-Scale Structured Similarity (MS-SSIM) were used to measure the quality of reconstruction between the target image and synthesized image in the generated expression. As indicated by Table IV , our model with CVTHead produces the best result."
        },
        {
          "section_index": 21,
          "heading_level": "h4",
          "heading": "IV-D 2 Qualitative Comparison",
          "text": "In a similar vein, two tasks were prepared for qualitative comparisons. Facial Expression Transition : Fig. 10 compares sequences of facial expressions, generated by Motionclip and our method on CK+ and CelebV-HQ, respectively. These results clearly show that our method can generate facial expressions that convey more accurate expressions than MotionClip. Face Rendering : The resulting expressions can be qualitatively compared by viewing and deciding which expression looks better and more natural than the other. Fig. 11 shows the generated facial expressions based on learning the latent space of facial parameters on both datasets. The viewer may notice the difference between (b) and (c) for the CK+ dataset, and between (d) and (e) for the CelebV-HQ dataset, respectively, in terms of facial expressions and facial texture. For instance, the facial transition from surprise to happiness seen in the 2nd row is clearer in (c) than in (b), and the facial texture seen in the 4th row is much smoother in (c) than in (b). A similar observation can be made between (d) and (e), suggesting that Ours+CVTHead is superior to than Ours+ROME. For instance, in some cases, those enclosed within dashed boxes in Fig. 11 , a subtle distortion arose either when the facial region of the source image was occluded or when the instructions describe expressions with significant pose changes, such as transitioning from surprise to happiness or from fear to surprise. This led to inconsistencies, and missing details in the generated facial regions compared to Ours+CVTHead. A comparison between the two datasets, such as between (b) and (d) or between (c) and (e), reveals that the visual quality of CK+ is better than that of CelebV-HQ. These results confirm that our model has outstanding ability to transform facial expressions, learned on CK+ or CelebV-HQ, to a specific facial image. In addition, a user study was carried out by requesting 24 subjects to view 60 randomly paired images of various identities selected from either CVTHead + MotionClip or CVTHead + Ours. Subjects were asked to choose the expression closest to the expressions specified in the instructions or indicate whether none of them matched the provided expressions. Naturally, they were unaware as to which algorithm was used to the presented images. Four subjects viewed each video and voted for the one that more closely resembled the expression in the instructions. In cases in which the votes were tied, an additional subject was asked to make the final decision. Based on these outcomes, CVTHead + Ours achieved 73% accuracy, whereas CVTHead + MotionClip yielded 58% accuracy for CK+. For CelebV-HQ, CVTHead + Ours and CVTHead + MotionClip achieved 77% and 70.5% accuracy, respectively."
        },
        {
          "section_index": 22,
          "heading_level": "h4",
          "heading": "IV-D 3 Inference Time",
          "text": "During inference, CVTHead + MotionClip takes 3.97 seconds to generate a video, whereas CVTHead + Ours only takes 3.92 seconds on a single NVIDIA RTX A6000, suggesting that our model is a slightly faster than MotionClip."
        },
        {
          "section_index": 23,
          "heading_level": "h3",
          "heading": "IV-E Ablation Study",
          "text": "This section presents a series of ablation studies that were carried out to evaluate the performance of our model. IV-E 1 Effectiveness of the Proposed Components To demonstrate the effectiveness of each proposed component, we conducted a comparative experiment, as shown in Table V . It can be observed that the IFED module significantly improves the performance of I2FET compared to I2FET without IFED on both CK+ and CelebV-HQ datasets. Additionally, integration of the vertex loss function with IFED increases Acc 1 and Gmean on both datasets. Despite a slight decrease in Acc 2 on CK+, it still performs well on CelebV-HQ. TABLE V : COMPARISON OF THE ACCURACY BETWEEN THE PROPOSED DIFFERENT COMPONENTS FOR OUR METHOD (I2FET) ON THE TWO DATASETS Dataset Method Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow w/o IFED 76.89 Â± \\pm 0.0029 63.23 Â± \\pm 0.0047 63.59 Â± \\pm 0.0059 w/ IFED 91.30 Â± \\pm 0.0017 84.06 Â± \\pm 0.0026 79.41 Â± \\pm 0.0061 CK+ w/ IFED 91.44 Â± \\pm 0.0024 84.03 Â± \\pm 0.0034 80.30 Â± \\pm 0.0064 + w/ â„’ v \\mathcal{L}_{v} w/o IFED 48.35 Â± \\pm 0.0037 22.1 Â± \\pm 0.0040 32.88 Â± \\pm 0.0058 w/ IFED 57.80 Â± \\pm 0.0020 32.60 Â± \\pm 0.0037 46.04 Â± \\pm 0.0062 CelebV-HQ w/ IFED 58.24 Â± \\pm 0.0028 33.45 Â± \\pm 0.0052 46.47 Â± \\pm 0.0065 + w/ â„’ v \\mathcal{L}_{v} IV-E 2 Impact of Three Loss Functions To analyze the impact of each loss function in I2FET, an experiment was carried out to compare the performance of the model in various scenarios. Specifically, IFED was integrated with the I2FET model and each loss function was incrementally added to determine its impact on the model performance of the model. The results in Table VI indicate that the performance improved when the pose loss function integrated with the expression loss function, whereas the vertex loss function yielded a slight improvement when used alongside both the pose and expression loss functions. In this study, the vertex loss was integrated with other loss functions to capture the geometry of the face, to enhance the accuracy of facial expression recognition. Fig 13 and 14 are the confusion matrix of our model when the total loss function integrated with â„’ v \\mathcal{L}_{v} or without â„’ v \\mathcal{L}_{v} on the CK+ and CelebV-HQ datasets. Note that the fear accuracies for both datasets rather decrease slightly when the vertex loss is applied. It is known that the fear expression often overlaps with other expressions (e.g., surprise and happiness), thereby making it challenging to distinguish. Moreover, since the fear portion of CK+ is much larger than that of CelebV-HQ in Fig. 5, it appears that confusion during the facial expression transition has a reverse impact on Acc 2 as shown in Table VI (a) (b) Figure 13 : Confusion matrices for seven facial expressions by I2FET (a) without â„’ v \\mathcal{L}_{v} and (b) with â„’ v \\mathcal{L}_{v} on CK+. (a) (b) Figure 14 : Confusion matrices for eight facial expressions by I2FET (a) without â„’ v \\mathcal{L}_{v} and (b) with â„’ v \\mathcal{L}_{v} on CelebV-HQ. TABLE VI : COMPARISON OF THE IMPACT OF LOSS FUNCTIONS ON THE PERFORMANCE OF OUR METHOD (I2FET) ACROSS THE TWO DATASETS Dataset Method Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow â„’ e \\mathcal{L}_{e} 66.80 Â± \\pm 0.0056 45.96 Â± \\pm 0.0112 52.42 Â± \\pm 0.0730 â„’ e \\mathcal{L}_{e} + â„’ p \\mathcal{L}_{p} 91.30 Â± \\pm 0.0017 84.06 Â± \\pm 0.0026 79.41 Â± \\pm 0.0061 CK+ â„’ e \\mathcal{L}_{e} + â„’ p \\mathcal{L}_{p} + â„’ v \\mathcal{L}_{v} 91.44 Â± \\pm 0.0024 84.03 Â± \\pm 0.0034 80.30 Â± \\pm 0.0064 â„’ e \\mathcal{L}_{e} 37.57 Â± \\pm 0.0052 11.99 Â± \\pm 0.0326 25.82 Â± \\pm 0.0114 â„’ e \\mathcal{L}_{e} + â„’ p \\mathcal{L}_{p} 57.80 Â± \\pm 0.0020 32.60 Â± \\pm 0.0037 46.04 Â± \\pm 0.0062 CelebV-HQ â„’ e \\mathcal{L}_{e} + â„’ p \\mathcal{L}_{p} + â„’ v \\mathcal{L}_{v} 58.24 Â± \\pm 0.0028 33.45 Â± \\pm 0.0052 46.47 Â± \\pm 0.0065 IV-E 3 Impact of IFED on the Training Process This experiment was conducted to compare the two loss functions with and without incorporating the IFED module in I2FET. Fig. 15(a) and Fig. 15(b) show the results for CK+ and CelebV-HQ, respectively, confirming that I2FED plays an important role. IV-E 4 Effect of IFED on Intensity of Facial Expression Fig. 16 demonstrates that IFED helps I2FED to generate high-intensity facial expressions. (a) (b) Figure 15 : Comparison of the validation loss during the training process of I2FET w/ and w/o IFED, respectively, on (a) CK+ and (b) CelebV-HQ. The green curves represent I2FET without IFED, whereas the blue curves are the results for I2FET with IFED. Figure 16 : Qualitative comparison between 3D expressive faces generated (a) w/o and (b) w/ IFED. IV-E 5 Impact of Quantity of CAFT Layer The experiments were conducted to evaluate the results of the proposed I2FET model by designing the IFED module with varying numbers of CAFT layers. We fixed the number of transformer layers to 1 in both branches and adjusted the number of CAFT layers. The results in Table VII indicate that increasing the quantity of CAFT layers does not improve the performance of the proposed I2FET. IV-E 6 Impact of Quantity of Transformer Encoder in Facial Parameter Branch and Text Branch In this experiment, we utilized different configurations by adjusting the number of transformer encoder layers of IFED. The findings suggest that the I2FET model performs better when N = 2 and M = 1 compared to other configurations, particularly in terms of Acc 1 and G-mean, as shown in Table VIII . TABLE VII : COMPARISON OF THE ACCURACY OF THE I2FET METHOD ON BOTH DATASETS BY VARYING THE NUMBER OF CAFT LAYERS IN I2FED Dataset #Layers Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow 1 91.36 Â± \\pm 0.0023 84.13 Â± \\pm 0.0046 79.65 Â± \\pm 0.0080 CK+ 2 88.95 Â± \\pm 0.0019 80.95 Â± \\pm 0.0038 78.72 Â± \\pm 0.0071 1 58.25 Â± \\pm 0.0034 33.03 Â± \\pm 0.0056 46.46 Â± \\pm 0.0066 CelebV-HQ 2 55.68 Â± \\pm 0.0034 29.82 Â± \\pm 0.0066 42.59 Â± \\pm 0.0085 TABLE VIII : COMPARISON OF THE ACCURACY OF THE PROPOSED METHOD WITH THE NUMBER OF TRANSFORMER LAYERS FOR FACIAL PARAMETERS BRANCH ( N ) AND TEXT BRANCH ( M ) IN TFED ON CK+ Mode N M Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow A 1 1 91.36 Â± \\pm 0.0023 84.13 Â± \\pm 0.0046 79.67 Â± \\pm 0.008 B 2 1 91.44 Â± \\pm 0.0024 84.03 Â± \\pm 0.0034 80.30 Â± \\pm 0.0064 C 1 2 91.27 Â± \\pm 0.0019 83.84 Â± \\pm 0.0037 79.69 Â± \\pm 0.0052 D 2 2 89.07 Â± \\pm 0.0021 81.13 Â± \\pm 0.0034 78.71 Â± \\pm 0.0070"
        },
        {
          "section_index": 24,
          "heading_level": "h4",
          "heading": "IV-E 1 Effectiveness of the Proposed Components",
          "text": "To demonstrate the effectiveness of each proposed component, we conducted a comparative experiment, as shown in Table V . It can be observed that the IFED module significantly improves the performance of I2FET compared to I2FET without IFED on both CK+ and CelebV-HQ datasets. Additionally, integration of the vertex loss function with IFED increases Acc 1 and Gmean on both datasets. Despite a slight decrease in Acc 2 on CK+, it still performs well on CelebV-HQ. TABLE V : COMPARISON OF THE ACCURACY BETWEEN THE PROPOSED DIFFERENT COMPONENTS FOR OUR METHOD (I2FET) ON THE TWO DATASETS Dataset Method Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow w/o IFED 76.89 Â± \\pm 0.0029 63.23 Â± \\pm 0.0047 63.59 Â± \\pm 0.0059 w/ IFED 91.30 Â± \\pm 0.0017 84.06 Â± \\pm 0.0026 79.41 Â± \\pm 0.0061 CK+ w/ IFED 91.44 Â± \\pm 0.0024 84.03 Â± \\pm 0.0034 80.30 Â± \\pm 0.0064 + w/ â„’ v \\mathcal{L}_{v} w/o IFED 48.35 Â± \\pm 0.0037 22.1 Â± \\pm 0.0040 32.88 Â± \\pm 0.0058 w/ IFED 57.80 Â± \\pm 0.0020 32.60 Â± \\pm 0.0037 46.04 Â± \\pm 0.0062 CelebV-HQ w/ IFED 58.24 Â± \\pm 0.0028 33.45 Â± \\pm 0.0052 46.47 Â± \\pm 0.0065 + w/ â„’ v \\mathcal{L}_{v}"
        },
        {
          "section_index": 25,
          "heading_level": "h4",
          "heading": "IV-E 2 Impact of Three Loss Functions",
          "text": "To analyze the impact of each loss function in I2FET, an experiment was carried out to compare the performance of the model in various scenarios. Specifically, IFED was integrated with the I2FET model and each loss function was incrementally added to determine its impact on the model performance of the model. The results in Table VI indicate that the performance improved when the pose loss function integrated with the expression loss function, whereas the vertex loss function yielded a slight improvement when used alongside both the pose and expression loss functions. In this study, the vertex loss was integrated with other loss functions to capture the geometry of the face, to enhance the accuracy of facial expression recognition. Fig 13 and 14 are the confusion matrix of our model when the total loss function integrated with â„’ v \\mathcal{L}_{v} or without â„’ v \\mathcal{L}_{v} on the CK+ and CelebV-HQ datasets. Note that the fear accuracies for both datasets rather decrease slightly when the vertex loss is applied. It is known that the fear expression often overlaps with other expressions (e.g., surprise and happiness), thereby making it challenging to distinguish. Moreover, since the fear portion of CK+ is much larger than that of CelebV-HQ in Fig. 5, it appears that confusion during the facial expression transition has a reverse impact on Acc 2 as shown in Table VI (a) (b) Figure 13 : Confusion matrices for seven facial expressions by I2FET (a) without â„’ v \\mathcal{L}_{v} and (b) with â„’ v \\mathcal{L}_{v} on CK+. (a) (b) Figure 14 : Confusion matrices for eight facial expressions by I2FET (a) without â„’ v \\mathcal{L}_{v} and (b) with â„’ v \\mathcal{L}_{v} on CelebV-HQ. TABLE VI : COMPARISON OF THE IMPACT OF LOSS FUNCTIONS ON THE PERFORMANCE OF OUR METHOD (I2FET) ACROSS THE TWO DATASETS Dataset Method Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow â„’ e \\mathcal{L}_{e} 66.80 Â± \\pm 0.0056 45.96 Â± \\pm 0.0112 52.42 Â± \\pm 0.0730 â„’ e \\mathcal{L}_{e} + â„’ p \\mathcal{L}_{p} 91.30 Â± \\pm 0.0017 84.06 Â± \\pm 0.0026 79.41 Â± \\pm 0.0061 CK+ â„’ e \\mathcal{L}_{e} + â„’ p \\mathcal{L}_{p} + â„’ v \\mathcal{L}_{v} 91.44 Â± \\pm 0.0024 84.03 Â± \\pm 0.0034 80.30 Â± \\pm 0.0064 â„’ e \\mathcal{L}_{e} 37.57 Â± \\pm 0.0052 11.99 Â± \\pm 0.0326 25.82 Â± \\pm 0.0114 â„’ e \\mathcal{L}_{e} + â„’ p \\mathcal{L}_{p} 57.80 Â± \\pm 0.0020 32.60 Â± \\pm 0.0037 46.04 Â± \\pm 0.0062 CelebV-HQ â„’ e \\mathcal{L}_{e} + â„’ p \\mathcal{L}_{p} + â„’ v \\mathcal{L}_{v} 58.24 Â± \\pm 0.0028 33.45 Â± \\pm 0.0052 46.47 Â± \\pm 0.0065"
        },
        {
          "section_index": 26,
          "heading_level": "h4",
          "heading": "IV-E 3 Impact of IFED on the Training Process",
          "text": "This experiment was conducted to compare the two loss functions with and without incorporating the IFED module in I2FET. Fig. 15(a) and Fig. 15(b) show the results for CK+ and CelebV-HQ, respectively, confirming that I2FED plays an important role."
        },
        {
          "section_index": 27,
          "heading_level": "h4",
          "heading": "IV-E 4 Effect of IFED on Intensity of Facial Expression",
          "text": "Fig. 16 demonstrates that IFED helps I2FED to generate high-intensity facial expressions. (a) (b) Figure 15 : Comparison of the validation loss during the training process of I2FET w/ and w/o IFED, respectively, on (a) CK+ and (b) CelebV-HQ. The green curves represent I2FET without IFED, whereas the blue curves are the results for I2FET with IFED. Figure 16 : Qualitative comparison between 3D expressive faces generated (a) w/o and (b) w/ IFED."
        },
        {
          "section_index": 28,
          "heading_level": "h4",
          "heading": "IV-E 5 Impact of Quantity of CAFT Layer",
          "text": "The experiments were conducted to evaluate the results of the proposed I2FET model by designing the IFED module with varying numbers of CAFT layers. We fixed the number of transformer layers to 1 in both branches and adjusted the number of CAFT layers. The results in Table VII indicate that increasing the quantity of CAFT layers does not improve the performance of the proposed I2FET."
        },
        {
          "section_index": 29,
          "heading_level": "h4",
          "heading": "IV-E 6 Impact of Quantity of Transformer Encoder in Facial Parameter Branch and Text Branch",
          "text": "In this experiment, we utilized different configurations by adjusting the number of transformer encoder layers of IFED. The findings suggest that the I2FET model performs better when N = 2 and M = 1 compared to other configurations, particularly in terms of Acc 1 and G-mean, as shown in Table VIII . TABLE VII : COMPARISON OF THE ACCURACY OF THE I2FET METHOD ON BOTH DATASETS BY VARYING THE NUMBER OF CAFT LAYERS IN I2FED Dataset #Layers Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow 1 91.36 Â± \\pm 0.0023 84.13 Â± \\pm 0.0046 79.65 Â± \\pm 0.0080 CK+ 2 88.95 Â± \\pm 0.0019 80.95 Â± \\pm 0.0038 78.72 Â± \\pm 0.0071 1 58.25 Â± \\pm 0.0034 33.03 Â± \\pm 0.0056 46.46 Â± \\pm 0.0066 CelebV-HQ 2 55.68 Â± \\pm 0.0034 29.82 Â± \\pm 0.0066 42.59 Â± \\pm 0.0085 TABLE VIII : COMPARISON OF THE ACCURACY OF THE PROPOSED METHOD WITH THE NUMBER OF TRANSFORMER LAYERS FOR FACIAL PARAMETERS BRANCH ( N ) AND TEXT BRANCH ( M ) IN TFED ON CK+ Mode N M Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow A 1 1 91.36 Â± \\pm 0.0023 84.13 Â± \\pm 0.0046 79.67 Â± \\pm 0.008 B 2 1 91.44 Â± \\pm 0.0024 84.03 Â± \\pm 0.0034 80.30 Â± \\pm 0.0064 C 1 2 91.27 Â± \\pm 0.0019 83.84 Â± \\pm 0.0037 79.69 Â± \\pm 0.0052 D 2 2 89.07 Â± \\pm 0.0021 81.13 Â± \\pm 0.0034 78.71 Â± \\pm 0.0070"
        },
        {
          "section_index": 30,
          "heading_level": "h3",
          "heading": "IV-F Longer Facial Expression Generation with Neutral Expressions",
          "text": "According to appraisal theory [ 35 ] , which is one of the major emotion theories, the dynamic of emotion is described as the sequential check theory of emotion differentiation. In other words, the differentiation of emotional states of humans is the result of a sequence of specific stimulus evaluation appraisal checks, rather than an action of hopping from one emotional state to another in a discrete manner. In our framework, the transition of the facial expression from A to B is similar to the differentiation of emotional states in a sequence: starting from A facial expression with strong emotion a neutral expression and then moving to strong facial expression B. In addition to appraisal theory, the present approach is suitable for investigating facial behaviors within the pleasure-arousal (P-A) space [ 34 ] , where the neutral expression serves as a transitional expression between two specific expressions. For this purpose, a neutral encoder-decoder (NED) was adopted to generate neutral faces. The aim thereof was to improve the performance of our framework when a neutral expression is not available in the instruction to increase its flexibility for extending facial expression sequences. IV-F 1 Implementation Details In this scenario, two datasets containing the parameters of neutral expressions and poses were constructed the pretrained DECA to extract the data from the CelebV-HQ and CK+ datasets. As the CK+ dataset does not contain neutral expressions, the first frame of each video was extracted and considered as the representative neutral case. Afterward, NED was trained on these two neutral datasets. IV-F 2 Network Architecture The neutral encoder-decoder model was constructed with fully connected layers. The encoder consisted of four fully connected layers, whereas the decoder was structured with four fully connected layers dedicated to pose and expression, respectively. IV-F 3 Analysis Similarly, we also conducted experiments to evaluate neutral expression generation for both quantitative and qualitative comparisons. Quantitative comparison : Specifically, the accuracy of NED on the CK+ and CelebV-HQ datasets was 49.5% and 51.33% for generating the neutral expression, respectively. These results are presented in Table IX , which confirms that NED + CVTHead outperforms NED + ROME. Qualitative comparison : Fig. 17 visualizes neutral expressions generated by NED + CVTHead on CK+ and CelebV-HQ. Moreover, we also illustrate neutral expressions generated by FLAME with the expression parameter set to zero, as shown in Fig. 17 c. The results show that the facial units differ, depending on whether the environment is controllable or non-controllable, or whether FLAME was used. This indicates that neutral expressions are more complex in real-world scenarios, leading us to prefer using a neural encoder-decoder instead of the expression parameter settings provided by FLAME Figure 17 : Samples of facial images with a neutral expression generated by NED+CVTHead. The input images are shown in the leftmost column (Source). The images generated using CK+ dataset (controllable environment) are shown in (a), whereas those using CelebV-HQ dataset (non-controllable environment) are shown in (b). On the other hand, the generated 3D face with FLAME is depicted by setting the expression parameter to zero in (c). Note that subtle differences exist in the image quality between the three generated cases. TABLE IX : RESULT OBTAINED WHEN NED IS COMBINED WITH ROME OR CVTHEAD FOR TEXTURE GENERATION ON CK+ AND CELEBV-HQ, RESPECTIVELY. Dataset Method L â†“ 1 {}_{1}\\downarrow PSNR â†‘ {\\uparrow} LPIPS â†“ {\\downarrow} MS_SSIM â†‘ {\\uparrow} CK+ NED + ROME 0.193 10.207 0.516 0.348 NED + CVTHead 0.004 35.158 0.014 0.987 CelebV-HQ NED + ROME 0.183 10.818 0.498 0.407 NED + CVTHead 0.004 36.347 0.0117 0.989 In a user study similar to that described in Section IV-D 2 , we asked subjects to determine whether the generated images display neutral expressions. Most subjects succeeded in identifying the neutral expressions produced by our model on CK+, and they had difficulties with only 4% of the samples generated by the learned model on CelebV-HQ. TABLE X : COMPARISON OF FACIAL EXPRESSION CLASSIFIERS ON CK+ AND CELEBV-HQ WITH SAMPLES ROTATED AT VARIOUS ANGLES { 0 o , Â± 10 o , Â± 15 o , Â± 30 o } \\{0^{o},\\pm 10^{o},\\pm 15^{o},\\pm 30^{o}\\} 0 o Â± \\pm 10 o Â± \\pm 15 o Â± \\pm 30 o Dataset Method Acc 1 â†‘ \\uparrow G-mean â†‘ \\uparrow Acc 1 â†‘ \\uparrow G-mean â†‘ \\uparrow Acc 1 â†‘ \\uparrow G-mean â†‘ \\uparrow Acc 1 â†‘ \\uparrow G-mean â†‘ \\uparrow MEK [ 43 ] 100 100 86.41 76.20 76.01 52.16 32.56 0.0 CK+ ResNet-101 [ 14 ] 99.56 99.4 84.40 73.28 79.50 67.6 75.29 33.39 ResNet-101 with RFL 99.69 99.69 87.18 75.85 83.66 66.13 77.48 31.5 MEK [ 43 ] 99.75 99.83 60.49 55.07 49.08 33.84 32.05 9.64 CelebV-HQ ResNet-101 [ 14 ] 37 12.77 35.77 13.39 34.97 0.0 28.92 0.0 ResNet-101 with RFL 93.67 95.04 58.54 49.92 51.53 41.22 40.71 26.80"
        },
        {
          "section_index": 31,
          "heading_level": "h4",
          "heading": "IV-F 1 Implementation Details",
          "text": "In this scenario, two datasets containing the parameters of neutral expressions and poses were constructed the pretrained DECA to extract the data from the CelebV-HQ and CK+ datasets. As the CK+ dataset does not contain neutral expressions, the first frame of each video was extracted and considered as the representative neutral case. Afterward, NED was trained on these two neutral datasets."
        },
        {
          "section_index": 32,
          "heading_level": "h4",
          "heading": "IV-F 2 Network Architecture",
          "text": "The neutral encoder-decoder model was constructed with fully connected layers. The encoder consisted of four fully connected layers, whereas the decoder was structured with four fully connected layers dedicated to pose and expression, respectively."
        },
        {
          "section_index": 33,
          "heading_level": "h4",
          "heading": "IV-F 3 Analysis",
          "text": "Similarly, we also conducted experiments to evaluate neutral expression generation for both quantitative and qualitative comparisons. Quantitative comparison : Specifically, the accuracy of NED on the CK+ and CelebV-HQ datasets was 49.5% and 51.33% for generating the neutral expression, respectively. These results are presented in Table IX , which confirms that NED + CVTHead outperforms NED + ROME. Qualitative comparison : Fig. 17 visualizes neutral expressions generated by NED + CVTHead on CK+ and CelebV-HQ. Moreover, we also illustrate neutral expressions generated by FLAME with the expression parameter set to zero, as shown in Fig. 17 c. The results show that the facial units differ, depending on whether the environment is controllable or non-controllable, or whether FLAME was used. This indicates that neutral expressions are more complex in real-world scenarios, leading us to prefer using a neural encoder-decoder instead of the expression parameter settings provided by FLAME Figure 17 : Samples of facial images with a neutral expression generated by NED+CVTHead. The input images are shown in the leftmost column (Source). The images generated using CK+ dataset (controllable environment) are shown in (a), whereas those using CelebV-HQ dataset (non-controllable environment) are shown in (b). On the other hand, the generated 3D face with FLAME is depicted by setting the expression parameter to zero in (c). Note that subtle differences exist in the image quality between the three generated cases. TABLE IX : RESULT OBTAINED WHEN NED IS COMBINED WITH ROME OR CVTHEAD FOR TEXTURE GENERATION ON CK+ AND CELEBV-HQ, RESPECTIVELY. Dataset Method L â†“ 1 {}_{1}\\downarrow PSNR â†‘ {\\uparrow} LPIPS â†“ {\\downarrow} MS_SSIM â†‘ {\\uparrow} CK+ NED + ROME 0.193 10.207 0.516 0.348 NED + CVTHead 0.004 35.158 0.014 0.987 CelebV-HQ NED + ROME 0.183 10.818 0.498 0.407 NED + CVTHead 0.004 36.347 0.0117 0.989 In a user study similar to that described in Section IV-D 2 , we asked subjects to determine whether the generated images display neutral expressions. Most subjects succeeded in identifying the neutral expressions produced by our model on CK+, and they had difficulties with only 4% of the samples generated by the learned model on CelebV-HQ. TABLE X : COMPARISON OF FACIAL EXPRESSION CLASSIFIERS ON CK+ AND CELEBV-HQ WITH SAMPLES ROTATED AT VARIOUS ANGLES { 0 o , Â± 10 o , Â± 15 o , Â± 30 o } \\{0^{o},\\pm 10^{o},\\pm 15^{o},\\pm 30^{o}\\} 0 o Â± \\pm 10 o Â± \\pm 15 o Â± \\pm 30 o Dataset Method Acc 1 â†‘ \\uparrow G-mean â†‘ \\uparrow Acc 1 â†‘ \\uparrow G-mean â†‘ \\uparrow Acc 1 â†‘ \\uparrow G-mean â†‘ \\uparrow Acc 1 â†‘ \\uparrow G-mean â†‘ \\uparrow MEK [ 43 ] 100 100 86.41 76.20 76.01 52.16 32.56 0.0 CK+ ResNet-101 [ 14 ] 99.56 99.4 84.40 73.28 79.50 67.6 75.29 33.39 ResNet-101 with RFL 99.69 99.69 87.18 75.85 83.66 66.13 77.48 31.5 MEK [ 43 ] 99.75 99.83 60.49 55.07 49.08 33.84 32.05 9.64 CelebV-HQ ResNet-101 [ 14 ] 37 12.77 35.77 13.39 34.97 0.0 28.92 0.0 ResNet-101 with RFL 93.67 95.04 58.54 49.92 51.53 41.22 40.71 26.80"
        },
        {
          "section_index": 34,
          "heading_level": "h3",
          "heading": "IV-G Facial Expression Classification",
          "text": "IV-G 1 Implementation For ResNet-101 [ 14 ] and ResNet-101 with RFL [ 5 ] , the learning rate was 1 â€‹ e âˆ’ 4 1e^{-4} and the batch size was 64. Adam was used to optimize the models with 50 epochs per cycle. For the MEK model, the configuration in [ 43 ] was used to optimize the training process for both the CK+ and CelebV-HQ datasets. IV-G 2 Analysis Based on the results in Table X , ResNet-101 with RFL was more stable compared to ResNet-101 or MEK in terms of their classification performance, suggesting that ResNet-101 with RFL can handle both imbalanced datasets and rotational transformations fairly well. In particular, MEK achieved the best result on the original images although its performance was affected by rotated images. ResNet-101 with RFL demonstrated more stable performance than ResNet-101 and MEK on CK+. For real-world scenarios such as those in the CelebV-HQ dataset, we obtained similar results. These findings indicate that the performance of facial expression classifiers deteriorates for faces that are rotated by Â± 15 \\pm 15 degrees or more. In addition, MEK performs better for the original datasets, but worse for the augmented datasets. The performance of ResNet-101 is high on datasets with low imbalance rates, whereas it is low on datasets with high imbalance rates. Meanwhile, ResNet-101 with RFL delivers stable performance on imbalanced and rotated datasets. Based on this result, we adopted ResNet-101 by default during evaluation. (a) (b) Figure 18 : Generated 3D faces (a) by changing the template sentences and (b) a few unsuccessful cases."
        },
        {
          "section_index": 35,
          "heading_level": "h4",
          "heading": "IV-G 1 Implementation",
          "text": "For ResNet-101 [ 14 ] and ResNet-101 with RFL [ 5 ] , the learning rate was 1 â€‹ e âˆ’ 4 1e^{-4} and the batch size was 64. Adam was used to optimize the models with 50 epochs per cycle. For the MEK model, the configuration in [ 43 ] was used to optimize the training process for both the CK+ and CelebV-HQ datasets."
        },
        {
          "section_index": 36,
          "heading_level": "h4",
          "heading": "IV-G 2 Analysis",
          "text": "Based on the results in Table X , ResNet-101 with RFL was more stable compared to ResNet-101 or MEK in terms of their classification performance, suggesting that ResNet-101 with RFL can handle both imbalanced datasets and rotational transformations fairly well. In particular, MEK achieved the best result on the original images although its performance was affected by rotated images. ResNet-101 with RFL demonstrated more stable performance than ResNet-101 and MEK on CK+. For real-world scenarios such as those in the CelebV-HQ dataset, we obtained similar results. These findings indicate that the performance of facial expression classifiers deteriorates for faces that are rotated by Â± 15 \\pm 15 degrees or more. In addition, MEK performs better for the original datasets, but worse for the augmented datasets. The performance of ResNet-101 is high on datasets with low imbalance rates, whereas it is low on datasets with high imbalance rates. Meanwhile, ResNet-101 with RFL delivers stable performance on imbalanced and rotated datasets. Based on this result, we adopted ResNet-101 by default during evaluation. (a) (b) Figure 18 : Generated 3D faces (a) by changing the template sentences and (b) a few unsuccessful cases."
        },
        {
          "section_index": 37,
          "heading_level": "h3",
          "heading": "IV-H Discussion",
          "text": "IV-H 1 Diversity of Facial Expression Sequences Our framework is capable of animating various facial expressions guided by instruction and facial images with pose variation, as illustrated in Fig 12 . Moreover, it can generate facial expressions corresponding to both controlled and real-world environments, enabling the generation of a diverse range of expressions for creating a long video. IV-H 2 Results on Instructions with Changes in Template Sentences Fig. 18(a) shows the outcomes obtained by changing the template sentences. The result suggests that our model can successfully generate facial expressions that closely resemble specific emotions described in the instruction. IV-H 3 Failure Cases Although our model performs well in many cases, the model failed in certain instances as exemplified in Fig 18(b) . In the top row, significant pose variation in the input image leads to regions of the shoulders being missing from the resulting image due to constraints within the pre-trained face-rendering model. Likewise, images in which regions of the head and those containing hair were missing from the source image also led to the lack of information in the face rendering process because of this constraint, as shown in the bottom row. In the second row, the generated result is restricted by the size of the expression vocabulary, causing a scared face to be transformed into an angry face."
        },
        {
          "section_index": 38,
          "heading_level": "h4",
          "heading": "IV-H 1 Diversity of Facial Expression Sequences",
          "text": "Our framework is capable of animating various facial expressions guided by instruction and facial images with pose variation, as illustrated in Fig 12 . Moreover, it can generate facial expressions corresponding to both controlled and real-world environments, enabling the generation of a diverse range of expressions for creating a long video."
        },
        {
          "section_index": 39,
          "heading_level": "h4",
          "heading": "IV-H 2 Results on Instructions with Changes in Template Sentences",
          "text": "Fig. 18(a) shows the outcomes obtained by changing the template sentences. The result suggests that our model can successfully generate facial expressions that closely resemble specific emotions described in the instruction."
        },
        {
          "section_index": 40,
          "heading_level": "h4",
          "heading": "IV-H 3 Failure Cases",
          "text": "Although our model performs well in many cases, the model failed in certain instances as exemplified in Fig 18(b) . In the top row, significant pose variation in the input image leads to regions of the shoulders being missing from the resulting image due to constraints within the pre-trained face-rendering model. Likewise, images in which regions of the head and those containing hair were missing from the source image also led to the lack of information in the face rendering process because of this constraint, as shown in the bottom row. In the second row, the generated result is restricted by the size of the expression vocabulary, causing a scared face to be transformed into an angry face."
        },
        {
          "section_index": 41,
          "heading_level": "h2",
          "heading": "V Conclusion",
          "text": "In this study, we present a novel framework for generating 3D facial expressions from an RGB image and animating facial transitions between two expressions as specified by entering text instructions. First, FET was introduced to produce a sequence of facial expressions. The Instruction-Driven Facial Expression Decomposer was designed to learn from multimodal data and capture correlations between textual descriptions and facial expression features. We proposed an Instruction to Facial Expression Transition model to generate target facial expressions guided by a single instruction. Our framework integrates FET and a pre-trained face rendering model to generate facial appearances aligned with the expected expression sequence. Furthermore, our framework can be expanded to include the generation of a neutral expression, thereby enabling the creation of diverse expressions in a video, thus closely simulating facial expression behaviors in real-world scenarios. Extensive experiments were conducted on the CK+ and CelebV-HQ datasets to demonstrate the effectiveness of our framework. Although our proposed approach demonstrates positive outcomes in terms of controlling the facial expression of an RGB image according to the provided instructions, it does have certain limitations. Similar to previous studies [ 18 , 26 ] in which facial parameters were used, the performance of our model relies on the precision of 3D face coefficients, especially using DECA [ 9 ] within our configurations since it may encounter difficulty, often seen in challenging scenarios, in disentangling facial factors. Also, even though linear interpolation can maintain temporal consistency, it often compromises the realism of the synthesized videos. Although the present work uses either basic facial expressions or two designated expressions, the vocabulary number can be easily expanded by combining a Large Language Model (LLM) with our model. Given that text prompting is a powerful tool by which various emotional states can be expressed on a 3D avatar via our framework, we expect it to find various applications in the future."
        },
        {
          "section_index": 42,
          "heading_level": "h2",
          "heading": "Acknowledgment",
          "text": "This work was supported by the Information Technology Research Center (ITRC) support program (IITP-2022-RS-2022-00156354) and a Korean government grant (MSIT) (No.RS-2019-II190231) from the Institute of Information & Communications Technology Planning & Evaluation (IITP) as well as by the Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (2020R1A6A1A03038540)."
        },
        {
          "section_index": 43,
          "heading_level": "h2",
          "heading": "References",
          "text": "[1] S. Aneja, J. Thies, A. Dai, and M. NieÃŸner (2023) ClipFace: text-guided editing of textured 3d morphable models . In SIGGRAPH â€™23 Conference Proceedings , Cited by: Â§I , Â§I . [2] M. C. Buehler, A. Meka, G. Li, T. Beeler, and O. Hilliges (2021) VariTex: variational neural face textures . In Proceedings of the IEEE/CVF International Conference on Computer Vision , Cited by: Â§I , Â§ II-B . [3] C. Chen, Q. Fan, and R. Panda (2021) CrossViT: cross-attention multi-scale vision transformer for image classification . 2021 IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 347â€“356 . Cited by: Â§ III-B 1 . [4] Y. Chen and S. Xiong (2023) Rethinking one-shot face reenactment: a spatialâ€“temporal reconstruction view . Knowledge Based Systems 277 ( C ). Cited by: Â§I . [5] Y. Cui, M. Jia, T. Lin, Y. Song, and S. J. Belongie (2019) Class-balanced loss based on effective number of samples . 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 9260â€“9269 . Cited by: Â§ IV-C , Â§ IV-G 1 . [6] R. DanÄ›Äek, K. Chhatre, S. Tripathi, Y. Wen, M. Black, and T. Bolkart (2023) Emotional speech-driven animation with content-emotion disentanglement . In SIGGRAPH Asia 2023 Conference Papers , Cited by: Â§ II-A , Â§ II-A , Â§ III-B 1 . [7] H. Ding, K. Sricharan, and R. Chellappa (2018) Exprgan: facial expression editing with controllable expression intensity . AAAI . Cited by: Â§I . [8] M. C. Doukas, S. Zafeiriou, and V. Sharmanska (2021-10) HeadGAN: one-shot neural head synthesis and editing . In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 14398â€“14407 . Cited by: Â§I . [9] Y. Feng, H. Feng, M. J. Black, and T. Bolkart (2021) Learning an animatable detailed 3D face model from in-the-wild images . In ACM Transactions on Graphics, (Proc. SIGGRAPH) , Vol. 40 . Cited by: Â§ III-B 1 , Â§ III-B 1 , Â§V . [10] P. Grassal, M. Prinzler, T. Leistner, C. Rother, M. NieÃŸner, and J. Thies (2022) Neural head avatars from monocular rgb videos . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 18653â€“18664 . Cited by: Â§I . [11] X. Gu, C. Wen, W. Ye, J. Song, and Y. Gao (2023) Seer: language instructed video prediction with latent diffusion models . ICLR 2024 . Cited by: Â§I . [12] J. Guo, H. Manukyan, C. Yang, C. Wang, L. Khachatryan, S. Navasardyan, S. Song, H. Shi, and G. Huang (2023) FaceCLIP: facial image-to-video translation via a brief text description . IEEE Transactions on Circuits and Systems for Video Technology , pp. 1â€“1 . Cited by: Â§I . [13] T. Hang, H. Yang, B. Liu, J. Fu, X. Geng, and B. Guo (2023) Language-guided face animation by recurrent stylegan-based generator . IEEE Transactions on Multimedia 25 ( ), pp. 1â€“12 . Cited by: Â§I , Â§I . [14] K. He, X. Zhang, S. Ren, and J. Sun (2016) Deep Residual Learning for Image Recognition . In Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition , CVPR â€™16 , pp. 770â€“778 . External Links: Document , ISSN 1063-6919 Cited by: Â§ IV-C , Â§ IV-G 1 , TABLE X , TABLE X . [15] Z. Huang, K. C.K. Chan, Y. Jiang, and Z. Liu (2023) Collaborative diffusion for multi-modal face generation and editing . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , Cited by: Â§I . [16] S. Hwang, J. Hyung, D. Kim, M. Kim, and J. Choo (2023-10) FaceCLIPNeRF: text-driven 3d face manipulation using deformable neural radiance fields . In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 3469â€“3479 . Cited by: Â§I , Â§I . [17] Y. Jiang, Z. Huang, X. Pan, C. C. Loy, and Z. Liu (2021) Talk-to-edit: fine-grained facial editing via dialog . In Proceedings of the IEEE/CVF International Conference on Computer Vision , Cited by: Â§I . [18] T. Khakhulin, V. Sklyarova, V. Lempitsky, and E. Zakharov (2022) Realistic one-shot mesh-based head avatars . In European Conference of Computer vision (ECCV) , Cited by: Â§I , Â§ II-B , Â§ III-B 2 , Â§ III-B 2 , Figure 11 , Figure 11 , Â§ IV-B , Â§ IV-D 1 , Â§V . [19] M. Kim, F. Liu, A. Jain, and X. Liu (2023-06) DCFace: synthetic face generation with dual condition diffusion model . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 12715â€“12725 . Cited by: Â§I . [20] D. P. Kingma and J. Ba (2014) Adam: a method for stochastic optimization . ICLR abs/1412.6980 . Cited by: Â§ IV-B . [21] Y. Lee, T. Choi, H. Go, H. Lee, S. Cho, and J. Kim (2023) Exp-gan: 3d-aware facial image generation with expression control . In Computer Vision â€“ ACCV 2022: 16th Asian Conference on Computer Vision , pp. 151â€“167 . External Links: ISBN 978-3-031-26292-0 Cited by: Â§ II-B . [22] T. Li, T. Bolkart, M. J. Black, H. Li, and J. Romero (2017) Learning a model of facial shape and expression from 4d scans . ACM Transactions on Graphics (TOG) 36 , pp. 1 â€“ 17 . Cited by: Â§I , Â§ II-A , Â§ III-B 1 . [23] X. Li, D. Zhang, M. Li, and D. Lee (2023) Accurate head pose estimation using image rectification and a lightweight convolutional neural network . IEEE Transactions on Multimedia 25 ( ), pp. 2239â€“2251 . Cited by: Â§ IV-A 2 . [24] K. Lin, A. Trevithick, K. Cheng, M. Sarkis, M. Ghafoorian, N. Bi, G. Reitmayr, and R. Ramamoorthi (2023) PVP: Personalized Video Prior for Editable Dynamic Portraits using StyleGAN . Computer Graphics Forum 42 ( 4 ), pp. 13 pages . Cited by: Â§ III-B 1 . [25] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and I. Matthews (2010) The extended cohn-kanade dataset (ck+): a complete dataset for action unit and emotion-specified expression . In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops , Vol. , pp. 94â€“101 . Cited by: Â§ IV-A 1 . [26] H. Ma, T. Zhang, S. Sun, X. Yan, K. Han, and X. Xie (2024) CVTHead: one-shot controllable head avatar with vertex-feature transformer . IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) . Cited by: Â§I , Â§ II-B , Â§ III-B 2 , Figure 11 , Figure 11 , Â§ IV-B , Â§ IV-D 1 , Â§V . [27] N. Otberdout, C. Ferrari, M. Daoudi, S. Berretti, and A. Bimbo (2023) Generating complex 4d expression transitions by learning face landmark trajectories . IEEE Transactions on Affective Computing . Cited by: Â§I , Â§ II-A , Â§ II-A , Â§ IV-C . [28] N. Otberdout, C. Ferrari, M. Daoudi, S. Berritti, and A. Del Bimbo (2022-06) Sparse to dense dynamic 3d facial expression generation . In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , Cited by: Â§ II-A . [29] F. Paraperas Papantoniou, P. P. Filntisis, P. Maragos, and A. Roussos (2022) Neural emotion director: speech-preserving semantic control of facial expressions in â€in-the-wildâ€ videos . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , Cited by: Â§I . [30] O. Patashnik, Z. Wu, E. Shechtman, D. Cohen-Or, and D. Lischinski (2021-10) StyleCLIP: text-driven manipulation of stylegan imagery . In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 2085â€“2094 . Cited by: Â§I . [31] B. Peng, H. Fan, W. Wang, J. Dong, and S. Lyu (2022) A unified framework for high fidelity face swap and expression reenactment . IEEE Transactions on Circuits and Systems for Video Technology 32 ( 6 ), pp. 3673â€“3684 . Cited by: Â§I . [32] S. Qian, T. Kirschstein, L. Schoneveld, D. Davoli, S. Giebenhain, and M. NieÃŸner (2024) GaussianAvatars: photorealistic head avatars with rigged 3d gaussians . In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , Cited by: Â§I . [33] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. (2021) Learning transferable visual models from natural language supervision . In International conference on machine learning , pp. 8748â€“8763 . Cited by: Â§ III-B 1 . [34] J. Russell and J. M. F. dez Dols (1997) Reading emotions from and into faces: resurrecting dimensional-contextual perspective . The psychology of facial expression , pp. 295â€“320 . Note: Cambridge University Press Cited by: Â§ IV-F . [35] K. R. Scherer (2001) Appraisal considered as a process of multilevel sequential checking. . Oxford University Press , pp. 92 â€“ 120 . Cited by: Â§ IV-F . [36] S. Su, J. Zhu, L. Gao, and J. Song (2024) Utilizing greedy nature for multimodal conditional image synthesis in transformers . IEEE Transactions on Multimedia 26 ( ), pp. 2354â€“2366 . Cited by: Â§I . [37] T. Sutter, I. Daunhawer, and J. Vogt (2020) Multimodal generative learning utilizing jensen-shannon-divergence . Vol. 33 , pp. 6100â€“6110 . Cited by: Â§I . [38] M. Suzuki and Y. Matsuo (2022) A survey of multimodal deep generative models . Advanced Robotics 36 , pp. 261 â€“ 278 . Cited by: Â§I . [39] G. Tevet, B. Gordon, A. Hertz, A. H. Bermano, and D. Cohen-Or (2022) Motionclip: exposing human motion generation to clip space . In Computer Visionâ€“ECCV 2022: 17th European Conference , pp. 358â€“374 . Cited by: Â§ II-A , Â§ IV-B . [40] Y. Wu, S. Xu, J. Xiang, F. Wei, Q. Chen, J. Yang, and X. Tong (2023) AniPortraitGAN: animatable 3d portrait generation from 2d image collections . In SIGGRAPH Asia 2023 Conference Proceedings , Cited by: Â§I . [41] W. Xia, Y. Yang, J. Xue, and B. Wu (2021) TediGAN: text-guided diverse face image generation and manipulation . In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , Cited by: Â§I . [42] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang (2018) The unreasonable effectiveness of deep features as a perceptual metric . In CVPR , Cited by: Â§ IV-D 1 . [43] Y. Zhang, Y. Li, lixiong Qin, X. Liu, and W. Deng (2023) Leave no stone unturned: mine extra knowledge for imbalanced facial expression recognition . In Thirty-seventh Conference on Neural Information Processing Systems , Cited by: Â§ IV-C , Â§ IV-G 1 , TABLE X , TABLE X . [44] H. Zhu, W. Wu, W. Zhu, L. Jiang, S. Tang, L. Zhang, Z. Liu, and C. C. Loy (2022) CelebV-HQ: a large-scale video facial attributes dataset . In ECCV , Cited by: Â§ IV-A 2 . [45] W. Zielonka, T. Bolkart, and J. Thies (2023-06) Instant volumetric head avatars . In IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) , External Links: Document Cited by: Â§I . [46] K. Zou, S. Faisan, B. Yu, S. Valette, and H. Seo (2023) 4D facial expression diffusion model . External Links: 2303.16611 Cited by: Â§I , Â§ II-A , Â§ II-A , Â§ IV-C ."
        }
      ],
      "content_text": "Instruction-Driven 3D Facial Expression Generation and Transition\nAnh H. Vo, Tae-Seok Kim, Hulin Jin, Soo-Mi Choi, and Yong-Guk Kim A H Vo, T-S Kim, S-M Choi, and Y-G Kim are with the Department of Computer Engineering, Sejong University, Seoul, Republic of Korea.H Jin is with the School of Computer Science and Technology, Anhui University, Hefei, China.* Corresponding Author: ykim@sejong.ac.kr. This is the authorâ€™s accepted manuscript. The final version is published in IEEE Transactions on Multimedia, 2025 Abstract A 3D avatar typically has one of six cardinal facial expressions. To simulate realistic emotional variation, we should be able to render a facial transition between two arbitrary expressions. This study presents a new framework for instruction-driven facial expression generation that produces a 3D face and, starting from an image of the face, transforms the facial expression from one designated facial expression to another. The Instruction-driven Facial Expression Decomposer (IFED) module is introduced to facilitate multimodal data learning and capture the correlation between textual descriptions and facial expression features. Subsequently, we propose the Instruction to Facial Expression Transition (I2FET) method, which leverages IFED and a vertex reconstruction loss function to refine the semantic comprehension of latent vectors, thus generating a facial expression sequence according to the given instruction. Lastly, we present the Facial Expression Transition model to generate smooth transitions between facial expressions. Extensive evaluation suggests that the proposed model outperforms state-of-the-art methods on the CK+ and CelebV-HQ datasets. The results show that our framework can generate facial expression trajectories according to text instruction. Considering that text prompts allow us to make diverse descriptions of human emotional states, the repertoire of facial expressions and the transitions between them can be expanded greatly. We expect our framework to find various practical applications. More information about our project can be found at https://vohoanganh.github.io/tg3dfet/ Index Terms: Instruction-Driven, Facial Expression and Transition, Controllable Avatar, CK+ and CelebV-HQ datasets. I Introduction Face and facial expression generation have attracted much attention in recent years due to their widespread application in many fields such as augmented reality/virtual reality, the gaming industry, movie production, or human-computer interfaces. Face generation can be divided into two categories depending on whether the face is two or three-dimensional (2D or 3D faces). For the 2D case, several methods [ 15 , 41 , 17 , 19 , 36 ] have been proposed to generate facial images using multimodal input, such as text, sketches, and segmentation. Previous studies [ 7 , 41 , 12 , 13 ] have attempted to solve facial expression generation in response to input provided as an emotion label or text prompts. However, these methods have limited ability to control the geometrical information of the output image and are effective only for specific applications. In the case of a 3D face, several studies [ 18 , 45 , 26 , 31 , 4 ] have demonstrated methods for generating 3D faces from 2D ones. The models generate the output image combining geometrical information from the source image, such as the pose, shape, and texture. Figure 1 : Illustration of our framework wherein it accepts a text instruction with a face image (source) as input and generates a 3D face from it. Then, it transforms the facial expression, from disgust (expression 1) to happiness (expression 2) via a neutral expression. Other researchers [ 16 , 1 ] generate a specific 3D facial expression based on text prompts. They utilize CLIP (Contrastive Language-Image Pretraining) or NeRF (Neural Radiance Field) to learn the latent code for manipulating facial expressions. Yet, NeRF requires substantial computational resources and time. For instance, [ 1 ] learns facial parameters and generates facial texture and expression based on text prompts. Although these methods can generate specific facial expressions, they do not address facial expression transitions based on instruction provided. Meanwhile, other studies [ 8 , 2 , 10 , 29 , 32 ] proposed to control facial expressions from an RGB video or generate portrait images [ 40 ] with controllable facial expression, head pose, and shoulder movements. Alternatively, [ 27 , 46 ] utilized the facial landmark trajectories to generate multiple 4D expression transitions on a meshed face. These studies led us to propose a new framework that enables a 3D face to be generated from a source image. Then, the facial expression undergoes transition in response to prompt text provided by the user to request a facial transition between two arbitrary expressions. Recent studies have used text-driven methods to generate a deformed face via NeRF or create an animated face via StyleGAN [ 13 , 16 ] . They generate sequence of facial expressions by specifying a certain emotion, such as â€™the person is happyâ€™. In contrast, our study utilizes a text instruction to control the variation in the facial expression, such as â€™Turn this face from disgust to happinessâ€™, as illustrated in Fig. 1 . Yet, understanding the target expressions specified by text instructions presents a challenge when relying solely on global text embedding. Indeed, even though CLIP is a notable method for the language-conditioning module in the text-to-image task [ 13 , 16 , 11 , 1 ] , it still faces challenges related to bias in understanding the text despite its large-scale training or the provision of global instruction, which may overlook fine-grained sub-instructions. Moreover, a text-only approach could be sensitive in generative models because changes in the structure of instructions can significantly impact their performance. The additional challenge lies therein to ensure that the facial motion described in the instruction is aligned with the semantics of the language. A straightforward approach to generating the facial motion would entail interpolating between the source code and the edited code from StyleClip [ 30 ] . Nevertheless, the additional challenge of controlling the latent code to adjust the head pose of the source image while generating the facial animation makes it difficult to achieve the intended goal. To make the connection between the text and facial expressions, we supplement a text instruction for making a transition between facial expressions into two standard datasets containing facial images, i.e., CelebV-HQ (High-Quality Celebrity Video) and CK+ (Extended Cohn-Kanade). To control the facial model in detail, FLAME (Faces Learned with an Articulated Model and Expressions) [ 22 ] is adopted and then fine-tuned with basic facial expressions and corresponding poses to capture the movement of the head, eyes, and mouth. Furthermore, integrating feature representations from different modalities minimizes ambiguity in redundant resources and effectively handles missing modalities, particularly in generative models, by improving inter-modality relations [ 37 , 38 ] . The proposed approach allows our model, i.e. Instruction to Facial Expression Transition (I2FET), to enhance the relationships between facial expression features and textual descriptions, thereby improving the latent representation for accurately generating target facial expressions and reducing the reliance solely on instructional text. Then, facial expression trajectories are created using the target facial expressions, which are generated by the facial parameters of the source image using I2FET. Using this framework, one can generate a 3D facial expression avatar from a photograph of a face and create any facial transition based on a text prompt instruction that contains the initial and final facial expressions. The main contributions of our study are given as follows: â€¢ We present an instruction-driven 3D facial expression generation and transition framework to generate transitions in facial expressions based on instructions, starting from a photograph of a face. â€¢ We propose an Instruction-Driven Facial Expression Decomposer (IFED), designed to learn multimodal data and capture the correlation between facial expression features and textual descriptions. â€¢ We propose an Instruction to Facial Expression Transition method (I2FET), which utilizes conditional variational autoencoders, integrated with IFED and a vertex reconstruction loss function, to enhance the semantic comprehension of the latent vectors. â€¢ Extensive evaluation of the proposed model on the CK+ and CelebV-HQ datasets suggests that it outperforms SOTA methods.\n\nAbstract\nA 3D avatar typically has one of six cardinal facial expressions. To simulate realistic emotional variation, we should be able to render a facial transition between two arbitrary expressions. This study presents a new framework for instruction-driven facial expression generation that produces a 3D face and, starting from an image of the face, transforms the facial expression from one designated facial expression to another. The Instruction-driven Facial Expression Decomposer (IFED) module is introduced to facilitate multimodal data learning and capture the correlation between textual descriptions and facial expression features. Subsequently, we propose the Instruction to Facial Expression Transition (I2FET) method, which leverages IFED and a vertex reconstruction loss function to refine the semantic comprehension of latent vectors, thus generating a facial expression sequence according to the given instruction. Lastly, we present the Facial Expression Transition model to generate smooth transitions between facial expressions. Extensive evaluation suggests that the proposed model outperforms state-of-the-art methods on the CK+ and CelebV-HQ datasets. The results show that our framework can generate facial expression trajectories according to text instruction. Considering that text prompts allow us to make diverse descriptions of human emotional states, the repertoire of facial expressions and the transitions between them can be expanded greatly. We expect our framework to find various practical applications. More information about our project can be found at https://vohoanganh.github.io/tg3dfet/\n\nI Introduction\nFace and facial expression generation have attracted much attention in recent years due to their widespread application in many fields such as augmented reality/virtual reality, the gaming industry, movie production, or human-computer interfaces. Face generation can be divided into two categories depending on whether the face is two or three-dimensional (2D or 3D faces). For the 2D case, several methods [ 15 , 41 , 17 , 19 , 36 ] have been proposed to generate facial images using multimodal input, such as text, sketches, and segmentation. Previous studies [ 7 , 41 , 12 , 13 ] have attempted to solve facial expression generation in response to input provided as an emotion label or text prompts. However, these methods have limited ability to control the geometrical information of the output image and are effective only for specific applications. In the case of a 3D face, several studies [ 18 , 45 , 26 , 31 , 4 ] have demonstrated methods for generating 3D faces from 2D ones. The models generate the output image combining geometrical information from the source image, such as the pose, shape, and texture. Figure 1 : Illustration of our framework wherein it accepts a text instruction with a face image (source) as input and generates a 3D face from it. Then, it transforms the facial expression, from disgust (expression 1) to happiness (expression 2) via a neutral expression. Other researchers [ 16 , 1 ] generate a specific 3D facial expression based on text prompts. They utilize CLIP (Contrastive Language-Image Pretraining) or NeRF (Neural Radiance Field) to learn the latent code for manipulating facial expressions. Yet, NeRF requires substantial computational resources and time. For instance, [ 1 ] learns facial parameters and generates facial texture and expression based on text prompts. Although these methods can generate specific facial expressions, they do not address facial expression transitions based on instruction provided. Meanwhile, other studies [ 8 , 2 , 10 , 29 , 32 ] proposed to control facial expressions from an RGB video or generate portrait images [ 40 ] with controllable facial expression, head pose, and shoulder movements. Alternatively, [ 27 , 46 ] utilized the facial landmark trajectories to generate multiple 4D expression transitions on a meshed face. These studies led us to propose a new framework that enables a 3D face to be generated from a source image. Then, the facial expression undergoes transition in response to prompt text provided by the user to request a facial transition between two arbitrary expressions. Recent studies have used text-driven methods to generate a deformed face via NeRF or create an animated face via StyleGAN [ 13 , 16 ] . They generate sequence of facial expressions by specifying a certain emotion, such as â€™the person is happyâ€™. In contrast, our study utilizes a text instruction to control the variation in the facial expression, such as â€™Turn this face from disgust to happinessâ€™, as illustrated in Fig. 1 . Yet, understanding the target expressions specified by text instructions presents a challenge when relying solely on global text embedding. Indeed, even though CLIP is a notable method for the language-conditioning module in the text-to-image task [ 13 , 16 , 11 , 1 ] , it still faces challenges related to bias in understanding the text despite its large-scale training or the provision of global instruction, which may overlook fine-grained sub-instructions. Moreover, a text-only approach could be sensitive in generative models because changes in the structure of instructions can significantly impact their performance. The additional challenge lies therein to ensure that the facial motion described in the instruction is aligned with the semantics of the language. A straightforward approach to generating the facial motion would entail interpolating between the source code and the edited code from StyleClip [ 30 ] . Nevertheless, the additional challenge of controlling the latent code to adjust the head pose of the source image while generating the facial animation makes it difficult to achieve the intended goal. To make the connection between the text and facial expressions, we supplement a text instruction for making a transition between facial expressions into two standard datasets containing facial images, i.e., CelebV-HQ (High-Quality Celebrity Video) and CK+ (Extended Cohn-Kanade). To control the facial model in detail, FLAME (Faces Learned with an Articulated Model and Expressions) [ 22 ] is adopted and then fine-tuned with basic facial expressions and corresponding poses to capture the movement of the head, eyes, and mouth. Furthermore, integrating feature representations from different modalities minimizes ambiguity in redundant resources and effectively handles missing modalities, particularly in generative models, by improving inter-modality relations [ 37 , 38 ] . The proposed approach allows our model, i.e. Instruction to Facial Expression Transition (I2FET), to enhance the relationships between facial expression features and textual descriptions, thereby improving the latent representation for accurately generating target facial expressions and reducing the reliance solely on instructional text. Then, facial expression trajectories are created using the target facial expressions, which are generated by the facial parameters of the source image using I2FET. Using this framework, one can generate a 3D facial expression avatar from a photograph of a face and create any facial transition based on a text prompt instruction that contains the initial and final facial expressions. The main contributions of our study are given as follows: â€¢ We present an instruction-driven 3D facial expression generation and transition framework to generate transitions in facial expressions based on instructions, starting from a photograph of a face. â€¢ We propose an Instruction-Driven Facial Expression Decomposer (IFED), designed to learn multimodal data and capture the correlation between facial expression features and textual descriptions. â€¢ We propose an Instruction to Facial Expression Transition method (I2FET), which utilizes conditional variational autoencoders, integrated with IFED and a vertex reconstruction loss function, to enhance the semantic comprehension of the latent vectors. â€¢ Extensive evaluation of the proposed model on the CK+ and CelebV-HQ datasets suggests that it outperforms SOTA methods.\n\nII Related Work\nII-A Facial Expression Transition Generative Adversarial Networks (GANs) have been favored for generating facial expressions. For instance, Motion3DGAN [ 27 ] , an extension of MotionGAN [ 28 ] , addresses the dynamics of 3D landmarks, whereas WGAN learns the distribution of 3D expression dynamics across the hypersphere space, sampled with a condition to generate landmark sequences. In this case, a neutral 3D face mesh, frame by frame, is deformed by a mesh decoder using the generated landmark sequences although the disadvantage of this work is that the start and end expressions have to be provided to generate a concatenated expression transition. On the other hand, a diffusion model is adopted for facial expression generation [ 46 ] by modeling the input distribution. The deformation of the 3D mesh is guided by a specific landmark, and the original shape of the input facial mesh is considered to apply displacement to each vertex. In this work, MotionClip [ 39 ] , which was originally designed for human motion synthesis, is compared with the task of generating the expression transition conditions given by the text. EMOTE [ 6 ] utilizes emotion labels and various inputs to generate a talking-head avatar that maintains lip-sync from speech as well as facial expression. In particular, the facial motion prior, called FLINT (FLAME IN TIME), uses the expression and pose parameters of FLAME [ 22 ] to represent facial motion sequences. Although several studies have demonstrated methods to produce transitions between facial expressions under diverse conditions, the generated faces are typically conditioned on the front face by neglecting the head pose variations. Inspired by previous works [ 27 , 46 , 6 ] , the present study proposes a model for facial expression transition that learns the expression and pose parameters. The additional merit of our model would be that it can easily be plugged into the models that generate the facial appearance. II-B Face Rendering In the face rendering area, 3D Morphable Model (3DMM) has been widely used for rendering realistic faces [ 2 , 21 , 18 , 26 ] . Among these methods, VariTex [ 2 ] employs a neural texture obtained by a face texture decoder to sample with a UV map. Subsequently, this neural face feature image is merged with an additive feature image to serve as the input for a U-Net to generate the desired image. Exp-GAN [ 21 ] employs a neural volume generator and StyleGAN2 to generate images at various resolutions. This process combines different feature components, such as face features, depth images, and feature volumes. The facial feature is also obtained by sampling the neural texture using the UV map. However, this work is computationally intensive and time-consuming. ROME [ 18 ] can synthesize realistic face images by estimating a specific head mesh and neural texture from an input image, whereupon a neural rendering technique is adapted to render the rigged mesh. Next, the U-net-like neural rendering network, which accepts inputs such as neural texture and surface normals, is utilized to acquire both the target image and its corresponding mask. Similarly, CVTHead [ 26 ] also estimates a neural texture from an input image, and yet, in this case, the neural texture is flattened to a feature vector form before being passed to transformers along with vertex tokens to output a vertex descriptor. Specifically, each of the vertices is considered a query token, and a transformer is employed to learn the canonical vertex feature from the source image. To generate the desired image and its mask, the feature descriptor of each vertex and depth are projected into image space before being processed by a U-Net. In the current study, we employ ROME and CVTHead to generate facial appearances by leveraging DECA for the extraction of facial expression coefficients. This streamlines the integration into a facial expression transition model and enhances the applicability of the proposed framework to practical scenarios.\n\nII-A Facial Expression Transition\nGenerative Adversarial Networks (GANs) have been favored for generating facial expressions. For instance, Motion3DGAN [ 27 ] , an extension of MotionGAN [ 28 ] , addresses the dynamics of 3D landmarks, whereas WGAN learns the distribution of 3D expression dynamics across the hypersphere space, sampled with a condition to generate landmark sequences. In this case, a neutral 3D face mesh, frame by frame, is deformed by a mesh decoder using the generated landmark sequences although the disadvantage of this work is that the start and end expressions have to be provided to generate a concatenated expression transition. On the other hand, a diffusion model is adopted for facial expression generation [ 46 ] by modeling the input distribution. The deformation of the 3D mesh is guided by a specific landmark, and the original shape of the input facial mesh is considered to apply displacement to each vertex. In this work, MotionClip [ 39 ] , which was originally designed for human motion synthesis, is compared with the task of generating the expression transition conditions given by the text. EMOTE [ 6 ] utilizes emotion labels and various inputs to generate a talking-head avatar that maintains lip-sync from speech as well as facial expression. In particular, the facial motion prior, called FLINT (FLAME IN TIME), uses the expression and pose parameters of FLAME [ 22 ] to represent facial motion sequences. Although several studies have demonstrated methods to produce transitions between facial expressions under diverse conditions, the generated faces are typically conditioned on the front face by neglecting the head pose variations. Inspired by previous works [ 27 , 46 , 6 ] , the present study proposes a model for facial expression transition that learns the expression and pose parameters. The additional merit of our model would be that it can easily be plugged into the models that generate the facial appearance.\n\nII-B Face Rendering\nIn the face rendering area, 3D Morphable Model (3DMM) has been widely used for rendering realistic faces [ 2 , 21 , 18 , 26 ] . Among these methods, VariTex [ 2 ] employs a neural texture obtained by a face texture decoder to sample with a UV map. Subsequently, this neural face feature image is merged with an additive feature image to serve as the input for a U-Net to generate the desired image. Exp-GAN [ 21 ] employs a neural volume generator and StyleGAN2 to generate images at various resolutions. This process combines different feature components, such as face features, depth images, and feature volumes. The facial feature is also obtained by sampling the neural texture using the UV map. However, this work is computationally intensive and time-consuming. ROME [ 18 ] can synthesize realistic face images by estimating a specific head mesh and neural texture from an input image, whereupon a neural rendering technique is adapted to render the rigged mesh. Next, the U-net-like neural rendering network, which accepts inputs such as neural texture and surface normals, is utilized to acquire both the target image and its corresponding mask. Similarly, CVTHead [ 26 ] also estimates a neural texture from an input image, and yet, in this case, the neural texture is flattened to a feature vector form before being passed to transformers along with vertex tokens to output a vertex descriptor. Specifically, each of the vertices is considered a query token, and a transformer is employed to learn the canonical vertex feature from the source image. To generate the desired image and its mask, the feature descriptor of each vertex and depth are projected into image space before being processed by a U-Net. In the current study, we employ ROME and CVTHead to generate facial appearances by leveraging DECA for the extraction of facial expression coefficients. This streamlines the integration into a facial expression transition model and enhances the applicability of the proposed framework to practical scenarios.\n\nIII Method\nIII-A Problem Statement We aim to develop a model to generate 3D facial expression with two inputs: a face image I s I_{s} and a text instruction t t . The facial expression transition model ğ’¯ w â€‹ ( t , I s ) \\mathcal{T}_{w}(t,I_{s}) , having weights w w , is trained with a training set ğ’³ = { ( t , ( e 0 , Î¸ 0 ) , ( e 1 , Î¸ 1 ) ) ( j ) } j = 1 n \\mathcal{X}=\\{(t,(e_{0},\\theta_{0}),(e_{1},\\theta_{1}))^{(j)}\\}_{j=1}^{n} , where e 0 , e 1 âˆˆ R 1 Ã— 50 e_{0},e_{1}\\in R^{1\\times 50} denote expression vectors with 50 dimensions and Î¸ 0 , Î¸ 1 âˆˆ R 1 Ã— 6 \\theta_{0},\\theta_{1}\\in R^{1\\times 6} represent pose vectors with 6 dimensions corresponding to the first and next expressions in the facial expression sequence specified in instruction t t , respectively. n n is the number of samples in the training dataset. Our network learns ğ’¯ w ( . ) \\mathcal{T}_{w}(.) that approximates the data distribution p â€‹ ( ğ’³ ) p(\\mathcal{X}) and then generates a facial expression coefficient sequence { ( e 0 , Î¸ 0 ) , ( e 1 , Î¸ 1 ) } \\{(e_{0},\\theta_{0}),(e_{1},\\theta_{1})\\} with a given instruction t t . Next, a face rendering module ğ’¢ â€‹ ( I s , ğ’® ) \\mathcal{G}(I_{s},\\mathcal{S}) is used to render a sequence of facial appearances ğ’´ = { y s ( 1 ) , y ( i ) , y e 0 ( k ) , y ( j ) , y e 1 ( T ) } \\mathcal{Y}=\\{y_{s}^{(1)},y^{(i)},y_{e_{0}}^{(k)},y^{(j)},y_{e_{1}}^{(T)}\\} , with inputs, consisting of the texture of source image I s I_{s} and facial expression trajectories S = { s s ( 1 ) , s ( i ) , s e 0 ( k ) , s ( j ) , s e 1 ( T ) } S=\\{s_{s}^{(1)},s^{(i)},s_{e_{0}}^{(k)},s^{(j)},s_{e_{1}}^{(T)}\\} , where { s s ( 1 ) , s e 0 ( k ) , s e 1 ( T ) } \\{s_{s}^{(1)},s_{e_{0}}^{(k)},s_{e_{1}}^{(T)}\\} are anchor facial expression sequences corresponding to the source image, and specific facial expressions, generated by ğ’¯ w ( . ) \\mathcal{T}_{w}(.) . Then, facial expression sequences s ( i ) s^{(i)} and s ( j ) s^{(j)} transitioning between these anchor facial expressions can be inferred by a linear interpolation function to ensure the smoothness of the sequence of facial expressions. III-B The Proposed Method Figure 2 provides an overview of the proposed framework whereby one can control 3D facial expressions of the monocular image I s I_{s} following an input instruction t t . The framework consists of two major modules: (1) Facial Expression Transition (FET), ğ’¯ w â€‹ ( t , I s ) \\mathcal{T}_{w}(t,I_{s}) , for generating diverse expression trajectories; (2) Face Rendering (FR), ğ’¢ â€‹ ( I s , S ) \\mathcal{G}(I_{s},S) , for rendering facial appearances based on the texture of the source image I s I_{s} and the expression trajectories S S generated by the FET module. Figure 2 : Overview of the proposed framework consisting of two major modules: Facial Expression Transition (FET) and Face Rendering (FR). First, textual vector x t x_{t} is obtained from the pre-trained CLIP encoder for the FET module. Simultaneously, latent representations z p z_{p} and z e z_{e} are drawn from ğ’© â€‹ ( 0 , I ) \\mathcal{N}(0,I) . Then, the latent vectors are processed and concatenated to obtain x ^ f \\hat{x}^{f} . Afterward, the IFED module utilizes x t x^{t} and x ^ f \\hat{x}^{f} as inputs to create conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} , and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} for the pose and expression decoders. Subsequently, for smoothness of the facial expression sequence, the facial expression in the source image ( e s , Î¸ s ) (e_{s},\\theta_{s}) is subjected to linear interpolation and the specific facial expressions ( e 0 , Î¸ 0 ) , ( e 1 , Î¸ 1 ) (e_{0},\\theta_{0}),(e_{1},\\theta_{1}) , generated by the I2FET decoders. These sequences are then combined with the shape Ï• s \\phi_{s} and camera c s c_{s} parameters of the source image, obtained from DECA, to form facial expression trajectories { s s ( 1 ) , s ( i ) , s e 0 ( k ) , s ( j ) , s e 1 ( T ) } \\{s_{s}^{(1)},s^{(i)},s^{(k)}_{e_{0}},s^{(j)},s_{e_{1}}^{(T)}\\} . For FR, head mesh reconstruction produces a flame mesh sequence { m s ( 1 ) , m ( i ) , m e 0 ( k ) , m ( j ) , m e 1 ( T ) } \\{m_{s}^{(1)},m^{(i)},m^{(k)}_{e_{0}},m^{(j)},m_{e_{1}}^{(T)}\\} aligned with the facial expression trajectories and the texture encoder extracts the facial appearance from the source image. Finally, the rendering module generates the facial appearances { y s ( 1 ) , y ( i ) , y e 0 ( k ) , y ( j ) , y e 1 ( T ) } \\{y_{s}^{(1)},y^{(i)},y^{(k)}_{e_{0}},y^{(j)},y_{e_{1}}^{(T)}\\} within the facial expression trajectories using the flame mesh sequence and the extracted facial appearance. III-B 1 Facial Expression Transition This module is designed to generate diverse expression trajectories that correspond to the facial parameters of the source face image I s I_{s} and the parameters of specific facial expressions, described in an instruction t t . The module has two primary branches: the first receives an instruction to generate the corresponding sequence of facial expression based on instruction t t , whereas the second takes a 2D face image to encode its facial coefficients and create the facial expression trajectories. Figure 3 : Flow diagram of IFED (Instruction-Driven Facial Expression Decomposer). It consists of CAFT and the decomposition of parameters. The major function of CAFT is to introduce cross-attention between the facial parameters and text instruction. The two outputs from CAFT pass through layer normalization and are combined to create the fused feature vector x f â€‹ u â€‹ s â€‹ e â€‹ d x^{fused} . Then, pose projection function ğ’« p \\mathcal{P}_{p} and expression projection function ğ’« e \\mathcal{P}_{e} are used to decompose x f â€‹ u â€‹ s â€‹ e â€‹ d x^{fused} into two conditional vectors for facial expression ( x e â€‹ m â€‹ b e x_{emb}^{e} ) and pose ( x e â€‹ m â€‹ b p x_{emb}^{p} ). Figure 4 : Detailed architecture of the I2FET (Instruction to Facial Expression Transition) method. Given an instruction and a sequence of facial parameters, including pose and expression, the IFED module utilizes the facial parameters and textual description extracted by CLIP to produce the corresponding conditional feature vectors for pose x e â€‹ m â€‹ b p x_{emb}^{p} and expression x e â€‹ m â€‹ b e x_{emb}^{e} , respectively. The pose and expression encoders then map this sequence of facial parameters and conditional feature vectors to latent representations corresponding to pose and expression. These latent vectors, along with the textual description, are passed through the IFED module to generate the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} . Following this, the expression and pose decoders reconstruct expression e ^ \\hat{e} and pose Î¸ ^ \\hat{\\theta} parameters using these latent representations and the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} , respectively. Finally, the pre-trained FLAME decoder reconstructs the vertex coordinates v ^ \\hat{v} based on e ^ \\hat{e} and Î¸ ^ \\hat{\\theta} . In the former, the pre-trained CLIP encoder [ 33 ] extracts text description x t âˆˆ R 77 Ã— 768 x^{t}\\in R^{77\\times 768} according to the given instruction t t . Then, the I2FET model generates a sequence of expression and pose coefficients for the facial expression trajectories. Instruction-driven Facial Expression Decomposer : A dual-branch vision transformer [ 3 ] is adopted for extracting multi-scale features and fusing cross-attention information at different scales because it is efficient for the image classification task. Here, the Cross-Attention for Facial Expression Parameter and Text (CAFT) is used to learn the relationship between instruction and facial expression parameters by enhancing the understanding of multimodal data and refining the latent representation for the Instruction to Facial Expression Transition model. The proposed IFED module is illustrated in Fig. 3 . In particular, we directly incorporate facial expression parameters and textual descriptions into CAFT instead of adding class tokens as in [ 3 ] . With respect to the transformer encoders, the first branch learns the correlation between the facial expression parameters, while the second branch focuses on learning the textual descriptions. To begin with, each vector representing facial expression parameters comprises two elements: expression and pose parameters, denoted as x f = { e , Î¸ } âˆˆ R m Ã— 56 x^{f}=\\{e,\\theta\\}\\in R^{m\\times 56} , where m m denotes the number of expressions in a given instruction. As [ 24 , 6 ] , we only utilize jaw pose Î¸ j â€‹ a â€‹ w \\theta^{jaw} and expression e e to convey facial expression information. Therefore, we rewrite x f = { e , Î¸ j â€‹ a â€‹ w } âˆˆ R m Ã— 53 x^{f}=\\{e,\\theta^{jaw}\\}\\in R^{m\\times 53} as the input of the first branch. Then, the transformer encoder is utilized as y f = x f + M â€‹ S â€‹ A â€‹ ( L â€‹ N â€‹ ( x f ) ) \\displaystyle y^{f}=x^{f}+MSA(LN(x^{f})) (1) x ^ f = y f + F â€‹ F â€‹ N â€‹ ( L â€‹ N â€‹ ( y f ) ) \\displaystyle\\hat{x}^{f}=y^{f}+FFN(LN(y^{f})) where x ^ f \\hat{x}^{f} denotes the output of the transformer encoder in the facial expression parameters branch â„° P \\mathcal{E}_{P} . L N ( . ) LN(.) , M S A ( . ) MSA(.) , and F F N ( . ) FFN(.) are the layer norm, multi-head self-attention, and feed-forward network, respectively. Likewise, in the text branch, we replicate x t x^{t} to create a vector { x } i = { 0 , 1 } t \\{x\\}^{t}_{i=\\{0,1\\}} for computational purposes. Subsequently, the input for the transformer encoder is generated using a text projection function P t ( . ) P_{t}(.) on { x } i = { 0 , 1 } t \\{x\\}^{t}_{i=\\{0,1\\}} . As a result, the textual description x ^ t \\hat{x}^{t} is obtained through the transformer encoder â„° t \\mathcal{E}_{t} . Both branches are then fused by cross-attention to capture the relationship between the facial expression parameters and textual descriptions. The fusion process can be expressed as y c f = h f â€‹ 2 â€‹ t â€‹ ( x ^ 0 f ) , y c t = h t â€‹ 2 â€‹ f â€‹ ( x ^ 0 t ) , \\displaystyle y_{c}^{f}=h^{f2t}(\\hat{x}_{0}^{f}),\\quad y_{c}^{t}=h^{t2f}(\\hat{x}_{0}^{t}), (2) x o f = g t â€‹ 2 â€‹ f â€‹ ( [ C â€‹ A â€‹ ( y c f âŠ— x 1 t ) + y c f ] ) âŠ— x ^ 1 f \\displaystyle x_{o}^{f}=g^{t2f}([CA(y^{f}_{c}\\otimes x_{1}^{t})+y_{c}^{f}])\\otimes\\hat{x}_{1}^{f} x o t = g f â€‹ 2 â€‹ t â€‹ ( [ C â€‹ A â€‹ ( y c t âŠ— x 1 f ) + y c t ] ) âŠ— x ^ 1 t \\displaystyle x_{o}^{t}=g^{f2t}([CA(y_{c}^{t}\\otimes x_{1}^{f})+y_{c}^{t}])\\otimes\\hat{x}_{1}^{t} where h t â€‹ 2 â€‹ f , h f â€‹ 2 â€‹ t , g t â€‹ 2 â€‹ f h^{t2f},h^{f2t},g^{t2f} , and g f â€‹ 2 â€‹ t g^{f2t} are the projection function and back-projection function between the vectors that represent the text and facial expression parameters, x o f âˆˆ R m Ã— 53 x_{o}^{f}\\in R^{m\\times 53} and x o t âˆˆ R m Ã— 768 x_{o}^{t}\\in R^{m\\times 768} are the outputs of CAFT, C â€‹ A CA denotes cross-attention and âŠ— \\otimes denotes the concatenation operator. Next, the fused feature vector is obtained by concatenating x o f x_{o}^{f} and x o t x_{o}^{t} , after passing through L N ( . ) LN(.) , as follows: x f â€‹ u â€‹ s â€‹ e â€‹ d = L â€‹ N â€‹ ( x o t ) âŠ— L â€‹ N â€‹ ( x o f ) x^{fused}=LN(x_{o}^{t})\\otimes LN(x_{o}^{f}) (3) Finally, the projection functions for pose ğ’« p ( . ) \\mathcal{P}_{p}(.) and expression ğ’« e ( . ) \\mathcal{P}_{e}(.) are applied to the fused feature vector as follows: x e â€‹ m â€‹ b e = ğ’« e â€‹ ( x f â€‹ u â€‹ s â€‹ e â€‹ d ) \\displaystyle x^{e}_{emb}=\\mathcal{P}_{e}(x^{fused}) (4) x e â€‹ m â€‹ b p = ğ’« p â€‹ ( x f â€‹ u â€‹ s â€‹ e â€‹ d ) \\displaystyle x^{p}_{emb}=\\mathcal{P}_{p}(x^{fused}) where x e â€‹ m â€‹ b e âˆˆ R m Ã— 50 x^{e}_{emb}\\in R^{m\\times 50} and x e â€‹ m â€‹ b p âˆˆ R m Ã— 6 x^{p}_{emb}\\in R^{m\\times 6} are the conditional feature vectors for pose and expression, respectively. In this work, ğ’« p ( . ) \\mathcal{P}_{p}(.) and ğ’« e ( . ) \\mathcal{P}_{e}(.) are designed as linear transformation functions to map the feature x f â€‹ u â€‹ s â€‹ e â€‹ d x^{fused} to the corresponding pose and expression feature vectors. Instruction to Facial Expression Transition : The proposed I2FET method is based on a conditional variational autoencoder architecture. The method includes encoders â„° e t ( . ) \\mathcal{E}_{e}^{t}(.) and â„° p t ( . ) \\mathcal{E}_{p}^{t}(.) , as well as decoders ğ’Ÿ e t ( . ) \\mathcal{D}^{t}_{e}(.) and ğ’Ÿ p t ( . ) \\mathcal{D}^{t}_{p}(.) , which are defined using three Multi-Layer Perceptrons (MLPs). These components are conditioned on feature vectors that represent expression and pose information, respectively. An overview of the proposed I2FET architecture is presented in Fig. 4 . Here, the encoders are defined as follows: â„° e t â€‹ ( e âŠ— x e â€‹ m â€‹ b e ) âŸ¶ ( Î¼ e , Ïƒ e ) \\mathcal{E}_{e}^{t}(e\\otimes x^{e}_{emb})\\longrightarrow(\\mu_{e},\\sigma_{e}) (5) and â„° p t â€‹ ( Î¸ âŠ— x e â€‹ m â€‹ b p ) âŸ¶ ( Î¼ p , Ïƒ p ) \\mathcal{E}_{p}^{t}(\\theta\\otimes x^{p}_{emb})\\longrightarrow(\\mu_{p},\\sigma_{p}) (6) where âŠ— \\otimes denotes the concatenation operator and x e â€‹ m â€‹ b e x^{e}_{emb} and x e â€‹ m â€‹ b p x^{p}_{emb} are conditional feature vectors for expression and pose, respectively. For the encoder stage, x e â€‹ m â€‹ b e x^{e}_{emb} and x e â€‹ m â€‹ b p x^{p}_{emb} are created by textual description x t x_{t} and the facial expression parameters ( e , Î¸ ) (e,\\theta) . Î¼ e \\mu_{e} and Î¼ p \\mu_{p} denote the mean and standard deviation of the expression distribution, whereas Ïƒ e \\sigma_{e} , and Ïƒ p \\sigma_{p} are the mean and standard deviation of the pose distribution. In this study, the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}^{e}_{emb} and x ^ e â€‹ m â€‹ b p \\hat{x}^{p}_{emb} in the decoder are computed by textual description x t x_{t} and the latent vectors z ^ p \\hat{z}_{p} and z ^ e \\hat{z}_{e} representing pose and expression, respectively. These latent vectors are calculated as follows: z ~ p = Ïƒ p âˆ— z p + Î¼ p \\displaystyle\\tilde{z}_{p}=\\sigma_{p}*z_{p}+\\mu_{p} (7) z ^ p = ğ’¯ p â€‹ ( z ~ p ) \\displaystyle\\hat{z}_{p}=\\mathcal{T}_{p}(\\tilde{z}_{p}) z ~ e = Ïƒ e âˆ— z e + Î¼ e \\displaystyle\\tilde{z}_{e}=\\sigma_{e}*z_{e}+\\mu_{e} z ^ e = ğ’¯ e â€‹ ( z ~ e ) \\displaystyle\\hat{z}_{e}=\\mathcal{T}_{e}(\\tilde{z}_{e}) where z p z_{p} and z e z_{e} are the latent vectors sampled from ğ’© â€‹ ( 0 , I ) \\mathcal{N}(0,I) . In this case, we utilize linear transformation functions ğ’¯ p ( . ) \\mathcal{T}_{p}(.) and ğ’¯ e ( . ) \\mathcal{T}_{e}(.) to transform z ~ p \\tilde{z}_{p} and z ~ e âˆˆ R m Ã— 16 \\tilde{z}_{e}\\in R^{m\\times 16} into z ^ p âˆˆ R m Ã— 6 \\hat{z}_{p}\\in R^{m\\times 6} and z ^ e âˆˆ R m Ã— 50 \\hat{z}_{e}\\in R^{m\\times 50} , respectively, similar to the expression and pose dimensions. Thus, the latent vectors z ~ e \\tilde{z}_{e} and z ~ p \\tilde{z}_{p} can be used as input for the IFED module. Then, the latent vectors z ^ e \\hat{z}_{e} and z ^ p \\hat{z}_{p} are concatenated together, and along with the text vector x t x^{t} , they are passed through the IFED module to obtain the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} as x ^ e â€‹ m â€‹ b e , x ^ e â€‹ m â€‹ b p = I â€‹ F â€‹ E â€‹ D â€‹ ( z ^ p âŠ— z ^ e , x t ) \\displaystyle\\hat{x}_{emb}^{e},\\hat{x}_{emb}^{p}=IFED(\\hat{z}_{p}\\otimes\\hat{z}_{e},x^{t}) (8) This approach enhances the semantic representation of latent vectors by refining the conditional feature vectors for decoders based on the learned latent vectors throughout the training process. The decoders are defined as follows ğ’Ÿ p t â€‹ ( z ~ p âŠ— x ^ e â€‹ m â€‹ b p ) âŸ¶ ( Î¸ ^ 0 , Î¸ ^ 1 ) \\mathcal{D}^{t}_{p}(\\tilde{z}_{p}\\otimes\\hat{x}^{p}_{emb})\\longrightarrow(\\hat{\\theta}_{0},\\hat{\\theta}_{1}) (9) and ğ’Ÿ e t â€‹ ( z ~ e âŠ— x ^ e â€‹ m â€‹ b e ) âŸ¶ ( e ^ 0 , e ^ 1 ) \\mathcal{D}^{t}_{e}(\\tilde{z}_{e}\\otimes\\hat{x}^{e}_{emb})\\longrightarrow(\\hat{e}_{0},\\hat{e}_{1}) (10) The parameters of the I2FET model are optimized by minimizing the reconstruction loss function â„’ M â€‹ S â€‹ E ( . , . ) \\mathcal{L}_{MSE}(.,.) between the ground truth coefficients e e , Î¸ \\theta , and the predicted coefficients e ^ , Î¸ ^ \\hat{e},\\hat{\\theta} . In addition, the Kullback-Leibler divergence loss function is utilized to minimize the distance between the probability distribution of the ground truth and that of the predicted values. â„’ e = â„’ M â€‹ S â€‹ E â€‹ ( e , e ^ ) \\displaystyle\\mathcal{L}_{e}=\\mathcal{L}_{MSE}(e,\\hat{e}) (11) + 0.5 â€‹ [ âˆ’ âˆ‘ i ( l â€‹ o â€‹ g â€‹ Ïƒ i 2 + 1 ) + âˆ‘ i Ïƒ i 2 + âˆ‘ i Î¼ i 2 ] \\displaystyle+5\\left[-\\sum_{i}(log\\>\\sigma_{i}^{2}+1)+\\sum_{i}\\sigma^{2}_{i}+\\sum_{i}\\mu_{i}^{2}\\right] â„’ p = â„’ M â€‹ S â€‹ E â€‹ ( Î¸ , Î¸ ^ ) \\displaystyle\\mathcal{L}_{p}=\\mathcal{L}_{MSE}(\\theta,\\hat{\\theta}) (12) + 0.5 â€‹ [ âˆ’ âˆ‘ i ( l â€‹ o â€‹ g â€‹ Ïƒ i 2 + 1 ) + âˆ‘ i Ïƒ i 2 + âˆ‘ i Î¼ i 2 ] \\displaystyle+5\\left[-\\sum_{i}(log\\>\\sigma_{i}^{2}+1)+\\sum_{i}\\sigma^{2}_{i}+\\sum_{i}\\mu_{i}^{2}\\right] where â„’ e \\mathcal{L}_{e} and â„’ p \\mathcal{L}_{p} denote the expression and pose loss function, respectively. Furthermore, we utilize the pretrained FLAME [ 22 ] , a parametric model of a 3D head that is integrated into DECA [ 9 ] , which has N = 5023 N=5023 base vertices V b âˆˆ â„› N Ã— 3 V_{b}\\in\\mathcal{R}^{N\\times 3} and two sets of M M and K K basis vectors that encode shape blendshapes ğ’® âˆˆ R N Ã— 3 Ã— M \\mathcal{S}\\in R^{N\\times 3\\times M} , and expression blendshapes B âˆˆ R N Ã— 3 Ã— K B\\in R^{N\\times 3\\times K} . The basic vectors are first blended and then Linear Blend Skinning (LBS) ğ’² ( . ) \\mathcal{W}(.) is applied to rotate the vertices following the pose Î¸ \\theta . The final reconstruction in world coordinates can be computed as follows ğ’± â€‹ ( Ï• , e , Î¸ ) = ğ’² â€‹ ( V b + ğ’® â€‹ Ï• + â„¬ â€‹ e , Î¸ ) \\mathcal{V}(\\phi,e,\\theta)=\\mathcal{W}(V_{b}+\\mathcal{S}\\phi+\\mathcal{B}e,\\theta) (13) where Ï• \\phi , e e , and Î¸ \\theta denote the identity shape, facial expression, and head pose parameters, respectively. In this case, FLAME is used to reconstruct vertices to improve the performance of I2FET. â„’ v = â„’ M â€‹ S â€‹ E â€‹ ( v , v ^ ) \\mathcal{L}_{v}=\\mathcal{L}_{MSE}(v,\\hat{v}) (14) where the vertex coordinates v v and v ^ \\hat{v} are computed by feeding the ground truth { Î¸ , e , Ï• } \\{\\theta,e,\\phi\\} and the reconstructed { Î¸ ^ , e ^ , Ï• } \\{\\hat{\\theta},\\hat{e},\\phi\\} through FLAME. Finally, the total loss function is given as â„’ t â€‹ o â€‹ t â€‹ a â€‹ l = â„’ e + â„’ p + â„’ v \\mathcal{L}_{total}=\\mathcal{L}_{e}+\\mathcal{L}_{p}+\\mathcal{L}_{v} (15) For the second branch, the pre-trained DECA [ 9 ] is used to encode the facial coefficients of the source face I s I_{s} including shape Ï• s âˆˆ R 1 Ã— 100 \\phi_{s}\\in R^{1\\times 100} , expression e s âˆˆ R 1 Ã— 50 e_{s}\\in R^{1\\times 50} , pose Î¸ s âˆˆ R 1 Ã— 6 \\theta_{s}\\in R^{1\\times 6} and camera c s âˆˆ R 1 Ã— 3 c_{s}\\in R^{1\\times 3} . Following this step, { Ï• s , e s , Î¸ s , c s } \\{\\phi_{s},e_{s},\\theta_{s},c_{s}\\} is integrated with the sequence of facial expression coefficients { ( e 0 , Î¸ 0 ) , ( e 1 , Î¸ 1 ) } \\{(e_{0},\\theta_{0}),(e_{1},\\theta_{1})\\} , generated by the first branch to generate the anchor facial expressions { s s ( 1 ) , s e 0 ( k ) , s e 1 ( T ) } \\{s^{(1)}_{s},s^{(k)}_{e_{0}},s^{(T)}_{e_{1}}\\} . Then, a linear interpolation function Î¨ ( . ) \\Psi(.) is applied to smoothen the transition between the two anchored facial expressions. The Î¨ â€‹ ( s ( l ) , s ( n ) ) \\Psi(s^{(l)},s^{(n)}) function for interpolation between facial expression sequences s ( l ) s^{(l)} and s ( n ) s^{(n)} can be defined as e ( k ) = Î´ âˆ— e ( l ) + ( 1 âˆ’ Î´ ) âˆ— e ( n ) \\displaystyle e^{(k)}=\\delta*e^{(l)}+(1-\\delta)*e^{(n)} (16) Î¸ ( k ) = Î´ âˆ— Î¸ ( l ) + ( 1 âˆ’ Î´ ) âˆ— Î¸ ( n ) \\displaystyle\\theta^{(k)}=\\delta*\\theta^{(l)}+(1-\\delta)*\\theta^{(n)} where 0 â‰¤ Î´ â‰¤ 1 0\\leq\\delta\\leq 1 is a linear coefficient and { e ( l ) , Î¸ ( l ) } âˆˆ s ( l ) \\{e^{(l)},\\theta^{(l)}\\}\\in s^{(l)} . This allows our framework to ensure temporal consistency when the motion changes are sampled uniformly. III-B 2 Face Rendering This module renders the facial appearance for the facial expression trajectories, generated by FET. In this work, the face rendering approach is based on the facial parameters of FLAME for integration with FET. Head Mesh Reconstruction: It generates the vertex coordinates corresponding to the facial expressions using Eq. 13 by using the sequence of facial expression transitions as the input. Then, the target vertex coordinates are obtained by combining these expressions with the deformation of hair and shoulder regions f H ( . ) f_{H}(.) from I s I_{s} as follows â„³ t = ğ’± â€‹ ( Ï• , e , Î¸ ) + f H â€‹ ( I s ) \\mathcal{M}^{t}=\\mathcal{V}(\\phi,e,\\theta)+f_{H}(I_{s}) (17) where f H ( . ) f_{H}(.) is the pre-trained linear deformation model [ 18 ] that is used to refine the vertices locations. Rendering : Following the head mesh reconstruction step, facial appearances can be rendered from the reconstructed head vertices â„³ = { m s ( 1 ) , m ( i ) , m e 0 ( k ) , m ( j ) , m e 1 ( T ) } \\mathcal{M}=\\{m_{s}^{(1)},m^{(i)},m_{e_{0}}^{(k)},m^{(j)},m_{e_{1}}^{(T)}\\} and the extracted neutral texture of the source image I s I_{s} by the texture decoder â„° t â€‹ e â€‹ x â€‹ ( I s ) \\mathcal{E}_{tex}(I_{s}) . The present work explores a face-rendering model suitable for facial expression transition. Therefore, we utilize pre-trained facial rendering models, known as state-of-the-art models [ 18 , 26 ] , capable of rendering facial appearances based on the facial parameters calculated with FLAME. Subsequently, the sequence of facial appearances ğ’´ = { y s ( 1 ) , y ( i ) , y e 0 ( k ) , y ( j ) , y e 1 ( T ) } \\mathcal{Y}=\\{y_{s}^{(1)},y^{(i)},y_{e_{0}}^{(k)},y^{(j)},y_{e_{1}}^{(T)}\\} is generated as the output of the proposed framework. (a) (b) (c) Figure 5 : Statistics of the facial expressions utilized in this study for the CK+ (a) and CelebV-HQ (b) datasets, respectively, and a collection of sampled words extracted from the text instructions (c).\n\nIII-A Problem Statement\nWe aim to develop a model to generate 3D facial expression with two inputs: a face image I s I_{s} and a text instruction t t . The facial expression transition model ğ’¯ w â€‹ ( t , I s ) \\mathcal{T}_{w}(t,I_{s}) , having weights w w , is trained with a training set ğ’³ = { ( t , ( e 0 , Î¸ 0 ) , ( e 1 , Î¸ 1 ) ) ( j ) } j = 1 n \\mathcal{X}=\\{(t,(e_{0},\\theta_{0}),(e_{1},\\theta_{1}))^{(j)}\\}_{j=1}^{n} , where e 0 , e 1 âˆˆ R 1 Ã— 50 e_{0},e_{1}\\in R^{1\\times 50} denote expression vectors with 50 dimensions and Î¸ 0 , Î¸ 1 âˆˆ R 1 Ã— 6 \\theta_{0},\\theta_{1}\\in R^{1\\times 6} represent pose vectors with 6 dimensions corresponding to the first and next expressions in the facial expression sequence specified in instruction t t , respectively. n n is the number of samples in the training dataset. Our network learns ğ’¯ w ( . ) \\mathcal{T}_{w}(.) that approximates the data distribution p â€‹ ( ğ’³ ) p(\\mathcal{X}) and then generates a facial expression coefficient sequence { ( e 0 , Î¸ 0 ) , ( e 1 , Î¸ 1 ) } \\{(e_{0},\\theta_{0}),(e_{1},\\theta_{1})\\} with a given instruction t t . Next, a face rendering module ğ’¢ â€‹ ( I s , ğ’® ) \\mathcal{G}(I_{s},\\mathcal{S}) is used to render a sequence of facial appearances ğ’´ = { y s ( 1 ) , y ( i ) , y e 0 ( k ) , y ( j ) , y e 1 ( T ) } \\mathcal{Y}=\\{y_{s}^{(1)},y^{(i)},y_{e_{0}}^{(k)},y^{(j)},y_{e_{1}}^{(T)}\\} , with inputs, consisting of the texture of source image I s I_{s} and facial expression trajectories S = { s s ( 1 ) , s ( i ) , s e 0 ( k ) , s ( j ) , s e 1 ( T ) } S=\\{s_{s}^{(1)},s^{(i)},s_{e_{0}}^{(k)},s^{(j)},s_{e_{1}}^{(T)}\\} , where { s s ( 1 ) , s e 0 ( k ) , s e 1 ( T ) } \\{s_{s}^{(1)},s_{e_{0}}^{(k)},s_{e_{1}}^{(T)}\\} are anchor facial expression sequences corresponding to the source image, and specific facial expressions, generated by ğ’¯ w ( . ) \\mathcal{T}_{w}(.) . Then, facial expression sequences s ( i ) s^{(i)} and s ( j ) s^{(j)} transitioning between these anchor facial expressions can be inferred by a linear interpolation function to ensure the smoothness of the sequence of facial expressions.\n\nIII-B The Proposed Method\nFigure 2 provides an overview of the proposed framework whereby one can control 3D facial expressions of the monocular image I s I_{s} following an input instruction t t . The framework consists of two major modules: (1) Facial Expression Transition (FET), ğ’¯ w â€‹ ( t , I s ) \\mathcal{T}_{w}(t,I_{s}) , for generating diverse expression trajectories; (2) Face Rendering (FR), ğ’¢ â€‹ ( I s , S ) \\mathcal{G}(I_{s},S) , for rendering facial appearances based on the texture of the source image I s I_{s} and the expression trajectories S S generated by the FET module. Figure 2 : Overview of the proposed framework consisting of two major modules: Facial Expression Transition (FET) and Face Rendering (FR). First, textual vector x t x_{t} is obtained from the pre-trained CLIP encoder for the FET module. Simultaneously, latent representations z p z_{p} and z e z_{e} are drawn from ğ’© â€‹ ( 0 , I ) \\mathcal{N}(0,I) . Then, the latent vectors are processed and concatenated to obtain x ^ f \\hat{x}^{f} . Afterward, the IFED module utilizes x t x^{t} and x ^ f \\hat{x}^{f} as inputs to create conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} , and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} for the pose and expression decoders. Subsequently, for smoothness of the facial expression sequence, the facial expression in the source image ( e s , Î¸ s ) (e_{s},\\theta_{s}) is subjected to linear interpolation and the specific facial expressions ( e 0 , Î¸ 0 ) , ( e 1 , Î¸ 1 ) (e_{0},\\theta_{0}),(e_{1},\\theta_{1}) , generated by the I2FET decoders. These sequences are then combined with the shape Ï• s \\phi_{s} and camera c s c_{s} parameters of the source image, obtained from DECA, to form facial expression trajectories { s s ( 1 ) , s ( i ) , s e 0 ( k ) , s ( j ) , s e 1 ( T ) } \\{s_{s}^{(1)},s^{(i)},s^{(k)}_{e_{0}},s^{(j)},s_{e_{1}}^{(T)}\\} . For FR, head mesh reconstruction produces a flame mesh sequence { m s ( 1 ) , m ( i ) , m e 0 ( k ) , m ( j ) , m e 1 ( T ) } \\{m_{s}^{(1)},m^{(i)},m^{(k)}_{e_{0}},m^{(j)},m_{e_{1}}^{(T)}\\} aligned with the facial expression trajectories and the texture encoder extracts the facial appearance from the source image. Finally, the rendering module generates the facial appearances { y s ( 1 ) , y ( i ) , y e 0 ( k ) , y ( j ) , y e 1 ( T ) } \\{y_{s}^{(1)},y^{(i)},y^{(k)}_{e_{0}},y^{(j)},y_{e_{1}}^{(T)}\\} within the facial expression trajectories using the flame mesh sequence and the extracted facial appearance. III-B 1 Facial Expression Transition This module is designed to generate diverse expression trajectories that correspond to the facial parameters of the source face image I s I_{s} and the parameters of specific facial expressions, described in an instruction t t . The module has two primary branches: the first receives an instruction to generate the corresponding sequence of facial expression based on instruction t t , whereas the second takes a 2D face image to encode its facial coefficients and create the facial expression trajectories. Figure 3 : Flow diagram of IFED (Instruction-Driven Facial Expression Decomposer). It consists of CAFT and the decomposition of parameters. The major function of CAFT is to introduce cross-attention between the facial parameters and text instruction. The two outputs from CAFT pass through layer normalization and are combined to create the fused feature vector x f â€‹ u â€‹ s â€‹ e â€‹ d x^{fused} . Then, pose projection function ğ’« p \\mathcal{P}_{p} and expression projection function ğ’« e \\mathcal{P}_{e} are used to decompose x f â€‹ u â€‹ s â€‹ e â€‹ d x^{fused} into two conditional vectors for facial expression ( x e â€‹ m â€‹ b e x_{emb}^{e} ) and pose ( x e â€‹ m â€‹ b p x_{emb}^{p} ). Figure 4 : Detailed architecture of the I2FET (Instruction to Facial Expression Transition) method. Given an instruction and a sequence of facial parameters, including pose and expression, the IFED module utilizes the facial parameters and textual description extracted by CLIP to produce the corresponding conditional feature vectors for pose x e â€‹ m â€‹ b p x_{emb}^{p} and expression x e â€‹ m â€‹ b e x_{emb}^{e} , respectively. The pose and expression encoders then map this sequence of facial parameters and conditional feature vectors to latent representations corresponding to pose and expression. These latent vectors, along with the textual description, are passed through the IFED module to generate the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} . Following this, the expression and pose decoders reconstruct expression e ^ \\hat{e} and pose Î¸ ^ \\hat{\\theta} parameters using these latent representations and the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} , respectively. Finally, the pre-trained FLAME decoder reconstructs the vertex coordinates v ^ \\hat{v} based on e ^ \\hat{e} and Î¸ ^ \\hat{\\theta} . In the former, the pre-trained CLIP encoder [ 33 ] extracts text description x t âˆˆ R 77 Ã— 768 x^{t}\\in R^{77\\times 768} according to the given instruction t t . Then, the I2FET model generates a sequence of expression and pose coefficients for the facial expression trajectories. Instruction-driven Facial Expression Decomposer : A dual-branch vision transformer [ 3 ] is adopted for extracting multi-scale features and fusing cross-attention information at different scales because it is efficient for the image classification task. Here, the Cross-Attention for Facial Expression Parameter and Text (CAFT) is used to learn the relationship between instruction and facial expression parameters by enhancing the understanding of multimodal data and refining the latent representation for the Instruction to Facial Expression Transition model. The proposed IFED module is illustrated in Fig. 3 . In particular, we directly incorporate facial expression parameters and textual descriptions into CAFT instead of adding class tokens as in [ 3 ] . With respect to the transformer encoders, the first branch learns the correlation between the facial expression parameters, while the second branch focuses on learning the textual descriptions. To begin with, each vector representing facial expression parameters comprises two elements: expression and pose parameters, denoted as x f = { e , Î¸ } âˆˆ R m Ã— 56 x^{f}=\\{e,\\theta\\}\\in R^{m\\times 56} , where m m denotes the number of expressions in a given instruction. As [ 24 , 6 ] , we only utilize jaw pose Î¸ j â€‹ a â€‹ w \\theta^{jaw} and expression e e to convey facial expression information. Therefore, we rewrite x f = { e , Î¸ j â€‹ a â€‹ w } âˆˆ R m Ã— 53 x^{f}=\\{e,\\theta^{jaw}\\}\\in R^{m\\times 53} as the input of the first branch. Then, the transformer encoder is utilized as y f = x f + M â€‹ S â€‹ A â€‹ ( L â€‹ N â€‹ ( x f ) ) \\displaystyle y^{f}=x^{f}+MSA(LN(x^{f})) (1) x ^ f = y f + F â€‹ F â€‹ N â€‹ ( L â€‹ N â€‹ ( y f ) ) \\displaystyle\\hat{x}^{f}=y^{f}+FFN(LN(y^{f})) where x ^ f \\hat{x}^{f} denotes the output of the transformer encoder in the facial expression parameters branch â„° P \\mathcal{E}_{P} . L N ( . ) LN(.) , M S A ( . ) MSA(.) , and F F N ( . ) FFN(.) are the layer norm, multi-head self-attention, and feed-forward network, respectively. Likewise, in the text branch, we replicate x t x^{t} to create a vector { x } i = { 0 , 1 } t \\{x\\}^{t}_{i=\\{0,1\\}} for computational purposes. Subsequently, the input for the transformer encoder is generated using a text projection function P t ( . ) P_{t}(.) on { x } i = { 0 , 1 } t \\{x\\}^{t}_{i=\\{0,1\\}} . As a result, the textual description x ^ t \\hat{x}^{t} is obtained through the transformer encoder â„° t \\mathcal{E}_{t} . Both branches are then fused by cross-attention to capture the relationship between the facial expression parameters and textual descriptions. The fusion process can be expressed as y c f = h f â€‹ 2 â€‹ t â€‹ ( x ^ 0 f ) , y c t = h t â€‹ 2 â€‹ f â€‹ ( x ^ 0 t ) , \\displaystyle y_{c}^{f}=h^{f2t}(\\hat{x}_{0}^{f}),\\quad y_{c}^{t}=h^{t2f}(\\hat{x}_{0}^{t}), (2) x o f = g t â€‹ 2 â€‹ f â€‹ ( [ C â€‹ A â€‹ ( y c f âŠ— x 1 t ) + y c f ] ) âŠ— x ^ 1 f \\displaystyle x_{o}^{f}=g^{t2f}([CA(y^{f}_{c}\\otimes x_{1}^{t})+y_{c}^{f}])\\otimes\\hat{x}_{1}^{f} x o t = g f â€‹ 2 â€‹ t â€‹ ( [ C â€‹ A â€‹ ( y c t âŠ— x 1 f ) + y c t ] ) âŠ— x ^ 1 t \\displaystyle x_{o}^{t}=g^{f2t}([CA(y_{c}^{t}\\otimes x_{1}^{f})+y_{c}^{t}])\\otimes\\hat{x}_{1}^{t} where h t â€‹ 2 â€‹ f , h f â€‹ 2 â€‹ t , g t â€‹ 2 â€‹ f h^{t2f},h^{f2t},g^{t2f} , and g f â€‹ 2 â€‹ t g^{f2t} are the projection function and back-projection function between the vectors that represent the text and facial expression parameters, x o f âˆˆ R m Ã— 53 x_{o}^{f}\\in R^{m\\times 53} and x o t âˆˆ R m Ã— 768 x_{o}^{t}\\in R^{m\\times 768} are the outputs of CAFT, C â€‹ A CA denotes cross-attention and âŠ— \\otimes denotes the concatenation operator. Next, the fused feature vector is obtained by concatenating x o f x_{o}^{f} and x o t x_{o}^{t} , after passing through L N ( . ) LN(.) , as follows: x f â€‹ u â€‹ s â€‹ e â€‹ d = L â€‹ N â€‹ ( x o t ) âŠ— L â€‹ N â€‹ ( x o f ) x^{fused}=LN(x_{o}^{t})\\otimes LN(x_{o}^{f}) (3) Finally, the projection functions for pose ğ’« p ( . ) \\mathcal{P}_{p}(.) and expression ğ’« e ( . ) \\mathcal{P}_{e}(.) are applied to the fused feature vector as follows: x e â€‹ m â€‹ b e = ğ’« e â€‹ ( x f â€‹ u â€‹ s â€‹ e â€‹ d ) \\displaystyle x^{e}_{emb}=\\mathcal{P}_{e}(x^{fused}) (4) x e â€‹ m â€‹ b p = ğ’« p â€‹ ( x f â€‹ u â€‹ s â€‹ e â€‹ d ) \\displaystyle x^{p}_{emb}=\\mathcal{P}_{p}(x^{fused}) where x e â€‹ m â€‹ b e âˆˆ R m Ã— 50 x^{e}_{emb}\\in R^{m\\times 50} and x e â€‹ m â€‹ b p âˆˆ R m Ã— 6 x^{p}_{emb}\\in R^{m\\times 6} are the conditional feature vectors for pose and expression, respectively. In this work, ğ’« p ( . ) \\mathcal{P}_{p}(.) and ğ’« e ( . ) \\mathcal{P}_{e}(.) are designed as linear transformation functions to map the feature x f â€‹ u â€‹ s â€‹ e â€‹ d x^{fused} to the corresponding pose and expression feature vectors. Instruction to Facial Expression Transition : The proposed I2FET method is based on a conditional variational autoencoder architecture. The method includes encoders â„° e t ( . ) \\mathcal{E}_{e}^{t}(.) and â„° p t ( . ) \\mathcal{E}_{p}^{t}(.) , as well as decoders ğ’Ÿ e t ( . ) \\mathcal{D}^{t}_{e}(.) and ğ’Ÿ p t ( . ) \\mathcal{D}^{t}_{p}(.) , which are defined using three Multi-Layer Perceptrons (MLPs). These components are conditioned on feature vectors that represent expression and pose information, respectively. An overview of the proposed I2FET architecture is presented in Fig. 4 . Here, the encoders are defined as follows: â„° e t â€‹ ( e âŠ— x e â€‹ m â€‹ b e ) âŸ¶ ( Î¼ e , Ïƒ e ) \\mathcal{E}_{e}^{t}(e\\otimes x^{e}_{emb})\\longrightarrow(\\mu_{e},\\sigma_{e}) (5) and â„° p t â€‹ ( Î¸ âŠ— x e â€‹ m â€‹ b p ) âŸ¶ ( Î¼ p , Ïƒ p ) \\mathcal{E}_{p}^{t}(\\theta\\otimes x^{p}_{emb})\\longrightarrow(\\mu_{p},\\sigma_{p}) (6) where âŠ— \\otimes denotes the concatenation operator and x e â€‹ m â€‹ b e x^{e}_{emb} and x e â€‹ m â€‹ b p x^{p}_{emb} are conditional feature vectors for expression and pose, respectively. For the encoder stage, x e â€‹ m â€‹ b e x^{e}_{emb} and x e â€‹ m â€‹ b p x^{p}_{emb} are created by textual description x t x_{t} and the facial expression parameters ( e , Î¸ ) (e,\\theta) . Î¼ e \\mu_{e} and Î¼ p \\mu_{p} denote the mean and standard deviation of the expression distribution, whereas Ïƒ e \\sigma_{e} , and Ïƒ p \\sigma_{p} are the mean and standard deviation of the pose distribution. In this study, the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}^{e}_{emb} and x ^ e â€‹ m â€‹ b p \\hat{x}^{p}_{emb} in the decoder are computed by textual description x t x_{t} and the latent vectors z ^ p \\hat{z}_{p} and z ^ e \\hat{z}_{e} representing pose and expression, respectively. These latent vectors are calculated as follows: z ~ p = Ïƒ p âˆ— z p + Î¼ p \\displaystyle\\tilde{z}_{p}=\\sigma_{p}*z_{p}+\\mu_{p} (7) z ^ p = ğ’¯ p â€‹ ( z ~ p ) \\displaystyle\\hat{z}_{p}=\\mathcal{T}_{p}(\\tilde{z}_{p}) z ~ e = Ïƒ e âˆ— z e + Î¼ e \\displaystyle\\tilde{z}_{e}=\\sigma_{e}*z_{e}+\\mu_{e} z ^ e = ğ’¯ e â€‹ ( z ~ e ) \\displaystyle\\hat{z}_{e}=\\mathcal{T}_{e}(\\tilde{z}_{e}) where z p z_{p} and z e z_{e} are the latent vectors sampled from ğ’© â€‹ ( 0 , I ) \\mathcal{N}(0,I) . In this case, we utilize linear transformation functions ğ’¯ p ( . ) \\mathcal{T}_{p}(.) and ğ’¯ e ( . ) \\mathcal{T}_{e}(.) to transform z ~ p \\tilde{z}_{p} and z ~ e âˆˆ R m Ã— 16 \\tilde{z}_{e}\\in R^{m\\times 16} into z ^ p âˆˆ R m Ã— 6 \\hat{z}_{p}\\in R^{m\\times 6} and z ^ e âˆˆ R m Ã— 50 \\hat{z}_{e}\\in R^{m\\times 50} , respectively, similar to the expression and pose dimensions. Thus, the latent vectors z ~ e \\tilde{z}_{e} and z ~ p \\tilde{z}_{p} can be used as input for the IFED module. Then, the latent vectors z ^ e \\hat{z}_{e} and z ^ p \\hat{z}_{p} are concatenated together, and along with the text vector x t x^{t} , they are passed through the IFED module to obtain the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} as x ^ e â€‹ m â€‹ b e , x ^ e â€‹ m â€‹ b p = I â€‹ F â€‹ E â€‹ D â€‹ ( z ^ p âŠ— z ^ e , x t ) \\displaystyle\\hat{x}_{emb}^{e},\\hat{x}_{emb}^{p}=IFED(\\hat{z}_{p}\\otimes\\hat{z}_{e},x^{t}) (8) This approach enhances the semantic representation of latent vectors by refining the conditional feature vectors for decoders based on the learned latent vectors throughout the training process. The decoders are defined as follows ğ’Ÿ p t â€‹ ( z ~ p âŠ— x ^ e â€‹ m â€‹ b p ) âŸ¶ ( Î¸ ^ 0 , Î¸ ^ 1 ) \\mathcal{D}^{t}_{p}(\\tilde{z}_{p}\\otimes\\hat{x}^{p}_{emb})\\longrightarrow(\\hat{\\theta}_{0},\\hat{\\theta}_{1}) (9) and ğ’Ÿ e t â€‹ ( z ~ e âŠ— x ^ e â€‹ m â€‹ b e ) âŸ¶ ( e ^ 0 , e ^ 1 ) \\mathcal{D}^{t}_{e}(\\tilde{z}_{e}\\otimes\\hat{x}^{e}_{emb})\\longrightarrow(\\hat{e}_{0},\\hat{e}_{1}) (10) The parameters of the I2FET model are optimized by minimizing the reconstruction loss function â„’ M â€‹ S â€‹ E ( . , . ) \\mathcal{L}_{MSE}(.,.) between the ground truth coefficients e e , Î¸ \\theta , and the predicted coefficients e ^ , Î¸ ^ \\hat{e},\\hat{\\theta} . In addition, the Kullback-Leibler divergence loss function is utilized to minimize the distance between the probability distribution of the ground truth and that of the predicted values. â„’ e = â„’ M â€‹ S â€‹ E â€‹ ( e , e ^ ) \\displaystyle\\mathcal{L}_{e}=\\mathcal{L}_{MSE}(e,\\hat{e}) (11) + 0.5 â€‹ [ âˆ’ âˆ‘ i ( l â€‹ o â€‹ g â€‹ Ïƒ i 2 + 1 ) + âˆ‘ i Ïƒ i 2 + âˆ‘ i Î¼ i 2 ] \\displaystyle+5\\left[-\\sum_{i}(log\\>\\sigma_{i}^{2}+1)+\\sum_{i}\\sigma^{2}_{i}+\\sum_{i}\\mu_{i}^{2}\\right] â„’ p = â„’ M â€‹ S â€‹ E â€‹ ( Î¸ , Î¸ ^ ) \\displaystyle\\mathcal{L}_{p}=\\mathcal{L}_{MSE}(\\theta,\\hat{\\theta}) (12) + 0.5 â€‹ [ âˆ’ âˆ‘ i ( l â€‹ o â€‹ g â€‹ Ïƒ i 2 + 1 ) + âˆ‘ i Ïƒ i 2 + âˆ‘ i Î¼ i 2 ] \\displaystyle+5\\left[-\\sum_{i}(log\\>\\sigma_{i}^{2}+1)+\\sum_{i}\\sigma^{2}_{i}+\\sum_{i}\\mu_{i}^{2}\\right] where â„’ e \\mathcal{L}_{e} and â„’ p \\mathcal{L}_{p} denote the expression and pose loss function, respectively. Furthermore, we utilize the pretrained FLAME [ 22 ] , a parametric model of a 3D head that is integrated into DECA [ 9 ] , which has N = 5023 N=5023 base vertices V b âˆˆ â„› N Ã— 3 V_{b}\\in\\mathcal{R}^{N\\times 3} and two sets of M M and K K basis vectors that encode shape blendshapes ğ’® âˆˆ R N Ã— 3 Ã— M \\mathcal{S}\\in R^{N\\times 3\\times M} , and expression blendshapes B âˆˆ R N Ã— 3 Ã— K B\\in R^{N\\times 3\\times K} . The basic vectors are first blended and then Linear Blend Skinning (LBS) ğ’² ( . ) \\mathcal{W}(.) is applied to rotate the vertices following the pose Î¸ \\theta . The final reconstruction in world coordinates can be computed as follows ğ’± â€‹ ( Ï• , e , Î¸ ) = ğ’² â€‹ ( V b + ğ’® â€‹ Ï• + â„¬ â€‹ e , Î¸ ) \\mathcal{V}(\\phi,e,\\theta)=\\mathcal{W}(V_{b}+\\mathcal{S}\\phi+\\mathcal{B}e,\\theta) (13) where Ï• \\phi , e e , and Î¸ \\theta denote the identity shape, facial expression, and head pose parameters, respectively. In this case, FLAME is used to reconstruct vertices to improve the performance of I2FET. â„’ v = â„’ M â€‹ S â€‹ E â€‹ ( v , v ^ ) \\mathcal{L}_{v}=\\mathcal{L}_{MSE}(v,\\hat{v}) (14) where the vertex coordinates v v and v ^ \\hat{v} are computed by feeding the ground truth { Î¸ , e , Ï• } \\{\\theta,e,\\phi\\} and the reconstructed { Î¸ ^ , e ^ , Ï• } \\{\\hat{\\theta},\\hat{e},\\phi\\} through FLAME. Finally, the total loss function is given as â„’ t â€‹ o â€‹ t â€‹ a â€‹ l = â„’ e + â„’ p + â„’ v \\mathcal{L}_{total}=\\mathcal{L}_{e}+\\mathcal{L}_{p}+\\mathcal{L}_{v} (15) For the second branch, the pre-trained DECA [ 9 ] is used to encode the facial coefficients of the source face I s I_{s} including shape Ï• s âˆˆ R 1 Ã— 100 \\phi_{s}\\in R^{1\\times 100} , expression e s âˆˆ R 1 Ã— 50 e_{s}\\in R^{1\\times 50} , pose Î¸ s âˆˆ R 1 Ã— 6 \\theta_{s}\\in R^{1\\times 6} and camera c s âˆˆ R 1 Ã— 3 c_{s}\\in R^{1\\times 3} . Following this step, { Ï• s , e s , Î¸ s , c s } \\{\\phi_{s},e_{s},\\theta_{s},c_{s}\\} is integrated with the sequence of facial expression coefficients { ( e 0 , Î¸ 0 ) , ( e 1 , Î¸ 1 ) } \\{(e_{0},\\theta_{0}),(e_{1},\\theta_{1})\\} , generated by the first branch to generate the anchor facial expressions { s s ( 1 ) , s e 0 ( k ) , s e 1 ( T ) } \\{s^{(1)}_{s},s^{(k)}_{e_{0}},s^{(T)}_{e_{1}}\\} . Then, a linear interpolation function Î¨ ( . ) \\Psi(.) is applied to smoothen the transition between the two anchored facial expressions. The Î¨ â€‹ ( s ( l ) , s ( n ) ) \\Psi(s^{(l)},s^{(n)}) function for interpolation between facial expression sequences s ( l ) s^{(l)} and s ( n ) s^{(n)} can be defined as e ( k ) = Î´ âˆ— e ( l ) + ( 1 âˆ’ Î´ ) âˆ— e ( n ) \\displaystyle e^{(k)}=\\delta*e^{(l)}+(1-\\delta)*e^{(n)} (16) Î¸ ( k ) = Î´ âˆ— Î¸ ( l ) + ( 1 âˆ’ Î´ ) âˆ— Î¸ ( n ) \\displaystyle\\theta^{(k)}=\\delta*\\theta^{(l)}+(1-\\delta)*\\theta^{(n)} where 0 â‰¤ Î´ â‰¤ 1 0\\leq\\delta\\leq 1 is a linear coefficient and { e ( l ) , Î¸ ( l ) } âˆˆ s ( l ) \\{e^{(l)},\\theta^{(l)}\\}\\in s^{(l)} . This allows our framework to ensure temporal consistency when the motion changes are sampled uniformly.\n\nIII-B 1 Facial Expression Transition\nThis module is designed to generate diverse expression trajectories that correspond to the facial parameters of the source face image I s I_{s} and the parameters of specific facial expressions, described in an instruction t t . The module has two primary branches: the first receives an instruction to generate the corresponding sequence of facial expression based on instruction t t , whereas the second takes a 2D face image to encode its facial coefficients and create the facial expression trajectories. Figure 3 : Flow diagram of IFED (Instruction-Driven Facial Expression Decomposer). It consists of CAFT and the decomposition of parameters. The major function of CAFT is to introduce cross-attention between the facial parameters and text instruction. The two outputs from CAFT pass through layer normalization and are combined to create the fused feature vector x f â€‹ u â€‹ s â€‹ e â€‹ d x^{fused} . Then, pose projection function ğ’« p \\mathcal{P}_{p} and expression projection function ğ’« e \\mathcal{P}_{e} are used to decompose x f â€‹ u â€‹ s â€‹ e â€‹ d x^{fused} into two conditional vectors for facial expression ( x e â€‹ m â€‹ b e x_{emb}^{e} ) and pose ( x e â€‹ m â€‹ b p x_{emb}^{p} ). Figure 4 : Detailed architecture of the I2FET (Instruction to Facial Expression Transition) method. Given an instruction and a sequence of facial parameters, including pose and expression, the IFED module utilizes the facial parameters and textual description extracted by CLIP to produce the corresponding conditional feature vectors for pose x e â€‹ m â€‹ b p x_{emb}^{p} and expression x e â€‹ m â€‹ b e x_{emb}^{e} , respectively. The pose and expression encoders then map this sequence of facial parameters and conditional feature vectors to latent representations corresponding to pose and expression. These latent vectors, along with the textual description, are passed through the IFED module to generate the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} . Following this, the expression and pose decoders reconstruct expression e ^ \\hat{e} and pose Î¸ ^ \\hat{\\theta} parameters using these latent representations and the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} , respectively. Finally, the pre-trained FLAME decoder reconstructs the vertex coordinates v ^ \\hat{v} based on e ^ \\hat{e} and Î¸ ^ \\hat{\\theta} . In the former, the pre-trained CLIP encoder [ 33 ] extracts text description x t âˆˆ R 77 Ã— 768 x^{t}\\in R^{77\\times 768} according to the given instruction t t . Then, the I2FET model generates a sequence of expression and pose coefficients for the facial expression trajectories. Instruction-driven Facial Expression Decomposer : A dual-branch vision transformer [ 3 ] is adopted for extracting multi-scale features and fusing cross-attention information at different scales because it is efficient for the image classification task. Here, the Cross-Attention for Facial Expression Parameter and Text (CAFT) is used to learn the relationship between instruction and facial expression parameters by enhancing the understanding of multimodal data and refining the latent representation for the Instruction to Facial Expression Transition model. The proposed IFED module is illustrated in Fig. 3 . In particular, we directly incorporate facial expression parameters and textual descriptions into CAFT instead of adding class tokens as in [ 3 ] . With respect to the transformer encoders, the first branch learns the correlation between the facial expression parameters, while the second branch focuses on learning the textual descriptions. To begin with, each vector representing facial expression parameters comprises two elements: expression and pose parameters, denoted as x f = { e , Î¸ } âˆˆ R m Ã— 56 x^{f}=\\{e,\\theta\\}\\in R^{m\\times 56} , where m m denotes the number of expressions in a given instruction. As [ 24 , 6 ] , we only utilize jaw pose Î¸ j â€‹ a â€‹ w \\theta^{jaw} and expression e e to convey facial expression information. Therefore, we rewrite x f = { e , Î¸ j â€‹ a â€‹ w } âˆˆ R m Ã— 53 x^{f}=\\{e,\\theta^{jaw}\\}\\in R^{m\\times 53} as the input of the first branch. Then, the transformer encoder is utilized as y f = x f + M â€‹ S â€‹ A â€‹ ( L â€‹ N â€‹ ( x f ) ) \\displaystyle y^{f}=x^{f}+MSA(LN(x^{f})) (1) x ^ f = y f + F â€‹ F â€‹ N â€‹ ( L â€‹ N â€‹ ( y f ) ) \\displaystyle\\hat{x}^{f}=y^{f}+FFN(LN(y^{f})) where x ^ f \\hat{x}^{f} denotes the output of the transformer encoder in the facial expression parameters branch â„° P \\mathcal{E}_{P} . L N ( . ) LN(.) , M S A ( . ) MSA(.) , and F F N ( . ) FFN(.) are the layer norm, multi-head self-attention, and feed-forward network, respectively. Likewise, in the text branch, we replicate x t x^{t} to create a vector { x } i = { 0 , 1 } t \\{x\\}^{t}_{i=\\{0,1\\}} for computational purposes. Subsequently, the input for the transformer encoder is generated using a text projection function P t ( . ) P_{t}(.) on { x } i = { 0 , 1 } t \\{x\\}^{t}_{i=\\{0,1\\}} . As a result, the textual description x ^ t \\hat{x}^{t} is obtained through the transformer encoder â„° t \\mathcal{E}_{t} . Both branches are then fused by cross-attention to capture the relationship between the facial expression parameters and textual descriptions. The fusion process can be expressed as y c f = h f â€‹ 2 â€‹ t â€‹ ( x ^ 0 f ) , y c t = h t â€‹ 2 â€‹ f â€‹ ( x ^ 0 t ) , \\displaystyle y_{c}^{f}=h^{f2t}(\\hat{x}_{0}^{f}),\\quad y_{c}^{t}=h^{t2f}(\\hat{x}_{0}^{t}), (2) x o f = g t â€‹ 2 â€‹ f â€‹ ( [ C â€‹ A â€‹ ( y c f âŠ— x 1 t ) + y c f ] ) âŠ— x ^ 1 f \\displaystyle x_{o}^{f}=g^{t2f}([CA(y^{f}_{c}\\otimes x_{1}^{t})+y_{c}^{f}])\\otimes\\hat{x}_{1}^{f} x o t = g f â€‹ 2 â€‹ t â€‹ ( [ C â€‹ A â€‹ ( y c t âŠ— x 1 f ) + y c t ] ) âŠ— x ^ 1 t \\displaystyle x_{o}^{t}=g^{f2t}([CA(y_{c}^{t}\\otimes x_{1}^{f})+y_{c}^{t}])\\otimes\\hat{x}_{1}^{t} where h t â€‹ 2 â€‹ f , h f â€‹ 2 â€‹ t , g t â€‹ 2 â€‹ f h^{t2f},h^{f2t},g^{t2f} , and g f â€‹ 2 â€‹ t g^{f2t} are the projection function and back-projection function between the vectors that represent the text and facial expression parameters, x o f âˆˆ R m Ã— 53 x_{o}^{f}\\in R^{m\\times 53} and x o t âˆˆ R m Ã— 768 x_{o}^{t}\\in R^{m\\times 768} are the outputs of CAFT, C â€‹ A CA denotes cross-attention and âŠ— \\otimes denotes the concatenation operator. Next, the fused feature vector is obtained by concatenating x o f x_{o}^{f} and x o t x_{o}^{t} , after passing through L N ( . ) LN(.) , as follows: x f â€‹ u â€‹ s â€‹ e â€‹ d = L â€‹ N â€‹ ( x o t ) âŠ— L â€‹ N â€‹ ( x o f ) x^{fused}=LN(x_{o}^{t})\\otimes LN(x_{o}^{f}) (3) Finally, the projection functions for pose ğ’« p ( . ) \\mathcal{P}_{p}(.) and expression ğ’« e ( . ) \\mathcal{P}_{e}(.) are applied to the fused feature vector as follows: x e â€‹ m â€‹ b e = ğ’« e â€‹ ( x f â€‹ u â€‹ s â€‹ e â€‹ d ) \\displaystyle x^{e}_{emb}=\\mathcal{P}_{e}(x^{fused}) (4) x e â€‹ m â€‹ b p = ğ’« p â€‹ ( x f â€‹ u â€‹ s â€‹ e â€‹ d ) \\displaystyle x^{p}_{emb}=\\mathcal{P}_{p}(x^{fused}) where x e â€‹ m â€‹ b e âˆˆ R m Ã— 50 x^{e}_{emb}\\in R^{m\\times 50} and x e â€‹ m â€‹ b p âˆˆ R m Ã— 6 x^{p}_{emb}\\in R^{m\\times 6} are the conditional feature vectors for pose and expression, respectively. In this work, ğ’« p ( . ) \\mathcal{P}_{p}(.) and ğ’« e ( . ) \\mathcal{P}_{e}(.) are designed as linear transformation functions to map the feature x f â€‹ u â€‹ s â€‹ e â€‹ d x^{fused} to the corresponding pose and expression feature vectors. Instruction to Facial Expression Transition : The proposed I2FET method is based on a conditional variational autoencoder architecture. The method includes encoders â„° e t ( . ) \\mathcal{E}_{e}^{t}(.) and â„° p t ( . ) \\mathcal{E}_{p}^{t}(.) , as well as decoders ğ’Ÿ e t ( . ) \\mathcal{D}^{t}_{e}(.) and ğ’Ÿ p t ( . ) \\mathcal{D}^{t}_{p}(.) , which are defined using three Multi-Layer Perceptrons (MLPs). These components are conditioned on feature vectors that represent expression and pose information, respectively. An overview of the proposed I2FET architecture is presented in Fig. 4 . Here, the encoders are defined as follows: â„° e t â€‹ ( e âŠ— x e â€‹ m â€‹ b e ) âŸ¶ ( Î¼ e , Ïƒ e ) \\mathcal{E}_{e}^{t}(e\\otimes x^{e}_{emb})\\longrightarrow(\\mu_{e},\\sigma_{e}) (5) and â„° p t â€‹ ( Î¸ âŠ— x e â€‹ m â€‹ b p ) âŸ¶ ( Î¼ p , Ïƒ p ) \\mathcal{E}_{p}^{t}(\\theta\\otimes x^{p}_{emb})\\longrightarrow(\\mu_{p},\\sigma_{p}) (6) where âŠ— \\otimes denotes the concatenation operator and x e â€‹ m â€‹ b e x^{e}_{emb} and x e â€‹ m â€‹ b p x^{p}_{emb} are conditional feature vectors for expression and pose, respectively. For the encoder stage, x e â€‹ m â€‹ b e x^{e}_{emb} and x e â€‹ m â€‹ b p x^{p}_{emb} are created by textual description x t x_{t} and the facial expression parameters ( e , Î¸ ) (e,\\theta) . Î¼ e \\mu_{e} and Î¼ p \\mu_{p} denote the mean and standard deviation of the expression distribution, whereas Ïƒ e \\sigma_{e} , and Ïƒ p \\sigma_{p} are the mean and standard deviation of the pose distribution. In this study, the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}^{e}_{emb} and x ^ e â€‹ m â€‹ b p \\hat{x}^{p}_{emb} in the decoder are computed by textual description x t x_{t} and the latent vectors z ^ p \\hat{z}_{p} and z ^ e \\hat{z}_{e} representing pose and expression, respectively. These latent vectors are calculated as follows: z ~ p = Ïƒ p âˆ— z p + Î¼ p \\displaystyle\\tilde{z}_{p}=\\sigma_{p}*z_{p}+\\mu_{p} (7) z ^ p = ğ’¯ p â€‹ ( z ~ p ) \\displaystyle\\hat{z}_{p}=\\mathcal{T}_{p}(\\tilde{z}_{p}) z ~ e = Ïƒ e âˆ— z e + Î¼ e \\displaystyle\\tilde{z}_{e}=\\sigma_{e}*z_{e}+\\mu_{e} z ^ e = ğ’¯ e â€‹ ( z ~ e ) \\displaystyle\\hat{z}_{e}=\\mathcal{T}_{e}(\\tilde{z}_{e}) where z p z_{p} and z e z_{e} are the latent vectors sampled from ğ’© â€‹ ( 0 , I ) \\mathcal{N}(0,I) . In this case, we utilize linear transformation functions ğ’¯ p ( . ) \\mathcal{T}_{p}(.) and ğ’¯ e ( . ) \\mathcal{T}_{e}(.) to transform z ~ p \\tilde{z}_{p} and z ~ e âˆˆ R m Ã— 16 \\tilde{z}_{e}\\in R^{m\\times 16} into z ^ p âˆˆ R m Ã— 6 \\hat{z}_{p}\\in R^{m\\times 6} and z ^ e âˆˆ R m Ã— 50 \\hat{z}_{e}\\in R^{m\\times 50} , respectively, similar to the expression and pose dimensions. Thus, the latent vectors z ~ e \\tilde{z}_{e} and z ~ p \\tilde{z}_{p} can be used as input for the IFED module. Then, the latent vectors z ^ e \\hat{z}_{e} and z ^ p \\hat{z}_{p} are concatenated together, and along with the text vector x t x^{t} , they are passed through the IFED module to obtain the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} as x ^ e â€‹ m â€‹ b e , x ^ e â€‹ m â€‹ b p = I â€‹ F â€‹ E â€‹ D â€‹ ( z ^ p âŠ— z ^ e , x t ) \\displaystyle\\hat{x}_{emb}^{e},\\hat{x}_{emb}^{p}=IFED(\\hat{z}_{p}\\otimes\\hat{z}_{e},x^{t}) (8)\n\nIII-B 2 Face Rendering\nThis module renders the facial appearance for the facial expression trajectories, generated by FET. In this work, the face rendering approach is based on the facial parameters of FLAME for integration with FET. Head Mesh Reconstruction: It generates the vertex coordinates corresponding to the facial expressions using Eq. 13 by using the sequence of facial expression transitions as the input. Then, the target vertex coordinates are obtained by combining these expressions with the deformation of hair and shoulder regions f H ( . ) f_{H}(.) from I s I_{s} as follows â„³ t = ğ’± â€‹ ( Ï• , e , Î¸ ) + f H â€‹ ( I s ) \\mathcal{M}^{t}=\\mathcal{V}(\\phi,e,\\theta)+f_{H}(I_{s}) (17) where f H ( . ) f_{H}(.) is the pre-trained linear deformation model [ 18 ] that is used to refine the vertices locations. Rendering : Following the head mesh reconstruction step, facial appearances can be rendered from the reconstructed head vertices â„³ = { m s ( 1 ) , m ( i ) , m e 0 ( k ) , m ( j ) , m e 1 ( T ) } \\mathcal{M}=\\{m_{s}^{(1)},m^{(i)},m_{e_{0}}^{(k)},m^{(j)},m_{e_{1}}^{(T)}\\} and the extracted neutral texture of the source image I s I_{s} by the texture decoder â„° t â€‹ e â€‹ x â€‹ ( I s ) \\mathcal{E}_{tex}(I_{s}) . The present work explores a face-rendering model suitable for facial expression transition. Therefore, we utilize pre-trained facial rendering models, known as state-of-the-art models [ 18 , 26 ] , capable of rendering facial appearances based on the facial parameters calculated with FLAME. Subsequently, the sequence of facial appearances ğ’´ = { y s ( 1 ) , y ( i ) , y e 0 ( k ) , y ( j ) , y e 1 ( T ) } \\mathcal{Y}=\\{y_{s}^{(1)},y^{(i)},y_{e_{0}}^{(k)},y^{(j)},y_{e_{1}}^{(T)}\\} is generated as the output of the proposed framework. (a) (b) (c) Figure 5 : Statistics of the facial expressions utilized in this study for the CK+ (a) and CelebV-HQ (b) datasets, respectively, and a collection of sampled words extracted from the text instructions (c).\n\nIV Experiments\nIV-A Dataset TABLE I : N 1 , N 2 âˆˆ { N_{1},N_{2}\\in\\{ HAPPINESS, DISGUST, ANGER, FEAR, SURPRISE, CONTEMPT, SADNESS } \\} IN THE CK+ DATASET AND N 1 , N 2 âˆˆ { N_{1},N_{2}\\in\\{ HAPPINESS, DISGUST, ANGER, FEAR, SURPRISE, CONTEMPT, SADNESS, NEUTRAL } \\} IN THE CELEBV-HQ DATASET. No Instruction/ Prompts 1 Turn this face from [ N 1 N_{1} ] to [ N 2 N_{2} ]. 2 Change this face from [ N 1 N_{1} ] to [ N 2 N_{2} ]. 3 Transform this face from [ N 1 N_{1} ] to [ N 2 N_{2} ]. 4 Modify this face, changing it from [ N 1 N_{1} ] to [ N 2 N_{2} ]. 5 Replace this face from [ N 1 N_{1} to [ N 2 N_{2} ]. IV-A 1 CK+ dataset The CK+ dataset was supplemented with text instructions for facial expression generation and transition. The original CK+ dataset [ 25 ] includes 117 subjects and 7 emotions, such as happiness, disgust, anger, fear, surprise, contempt, and sadness . The instructions are created to describe the transitions between these facial expressions. To extract the facial parameters and the shape image corresponding to the facial expression in the image, we utilize the pre-trained DECA model. For each sample, a tuple is created with an instruction, the target shape image, and facial parameters that correspond to the same subject. The CK+ dataset, in this study, contains a total of 26,352 samples, and the composition of this dataset in terms of the seven facial expressions is shown in Fig. 5(a) . IV-A 2 CelebV-HQ dataset Similar to the CK+ case, text instructions were added to the CelebV-HQ dataset for our experiments [ 44 ] . The original CelebV-HQ dataset included 15,653 subjects and 8 expressions of neutral, happiness, disgust, anger, fear, surprise, contempt, and sadness . The images in the CelebV-HQ dataset were spontaneously collected and presented many challenges such as highly non-frontal head poses, blur, and brightness. This problem was overcome by using a head pose algorithm [ 23 ] to estimate parameters such as the yaw, roll, and pitch , of the head pose ranging from -15 to 15 degrees. Given that the subjects were chosen indiscriminately, we select those with more than two facial expressions. Then, the pre-trained DECA was applied to extract the corresponding shape images and facial parameters. The CelebV-HQ dataset adopted in this study includes 614 subjects and 28,335 samples, and Fig. 5(b) shows a statistical breakdown of the eight facial expressions. Table I lists the template sentences that are used to generate guided instructions for facial expression transitions. We generated five instructions for each sample and depicted the transition from expression N 1 N_{1} to expression N 2 N_{2} . Subsequently, one presentation sentence is randomly chosen from these instructions. Fig. 5(c) illustrates some sampled words used in the text instructions. Figure 6 : The shapes of facial expression are generated using images from the CK+ dataset. The top and bottom rows illustrate surprised and angry expressions with various head poses. IV-B Implementation The I2FET model was trained for 200 epochs with a batch size of 128 and a learning rate of 8 â€‹ e âˆ’ 4 8e^{-4} . The Adam optimizer [ 20 ] was adopted to optimize the model. For the experiments, 10% of the dataset was allocated to the testing set, whereas the remaining portion was used for training. Then, a validation set was drawn from 10% of the training set. In this work, we repeated each experiment ten times and used the average results to evaluate the overall performance of the proposed I2FET. Because our task is new and a specific method that would allow a fair comparison is not available, we chose MotionClip [ 39 ] as a baseline. For the face rendering experiment, the configuration by [ 18 , 26 ] was used. IV-C Classification of Facial Expressions In this study, it was important to employ an appropriate classifier to evaluate the method that was used to generate facial expressions as prior studies [ 27 , 46 ] . Given that the CK+ and CelebV-HQ datasets had skewed distributions, the performance for minor classes could be affected by the facial expression classifiers trained on these imbalanced datasets, in which the results would be biased towards the major class. Moreover, the effectiveness of the classification model could also be affected by head movements, which were commonly encountered as shown in Fig. 6 . These problems were addressed by conducting an experiment to identify an appropriate classifier that can handle not only imbalanced datasets but also invariant rotation transformations. ResNet-101 [ 14 ] , MEK [ 43 ] , and ResNet-101 integrated with Reweighted Focal Loss (RFL) [ 5 ] were utilized to evaluate the performance of facial expression classification. For this purpose, augmented datasets were created by rotating the original CK+ and CelebV-HQ datasets at angles of Â± 10 o , Â± 15 o \\pm 10^{o},\\pm 15^{o} , and Â± 30 o \\pm 30^{o} . Then, the three networks are trained using the original datasets and evaluated on all datasets. The Accuracy was employed to assess whether samples had been correctly classified across all classes, while G-mean was used to evaluate the ability of the model to effectively recognize minor classes that are predicted by the facial expression classification model. The supplementary data in the ablation study in Section IV-E suggests that MEK can address imbalanced datasets but is sensitive to rotational transformations. ResNet101 demonstrates positive results with rotation transformations but performs worse on datasets that are highly imbalanced. ResNet101 integrated with RFL outperforms ResNet101 and MEK in both scenarios. As a result, ResNet101 with RFL was selected as our classifier for evaluating the ability of our models to generate facial expressions. IV-D Results (a) (b) (c) Figure 7 : Visualization of the learned latent space for seven facial expressions by (a) MotionClip, (b) I2FET without IFED, and (c) I2FET with IFED, respectively, using t-SNE. (a) (b) Figure 8 : Confusion matrices for seven facial expressions by (a) MotionClip and (b) our method on CK+. (a) (b) Figure 9 : Confusion matrices for eight facial expressions by (a) MotionClip and (b) our method on CelebV-HQ. TABLE II : COMPARISON OF ACCURACY BETWEEN OUR METHOD AND MOTIONCLIP ON THE CK+ DATASET Method Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow Ground-truth 99.69 Â± \\pm 0.0001 99.39 Â± \\pm 0.0002 99.69 Â± \\pm 9.3e -5 MotionClip 52.00 Â± \\pm 0.0058 20.00 Â± \\pm 0.0082 40.48 Â± \\pm 0.0060 Ours 91.44 Â± \\pm 0.0024 84.03 Â± \\pm 0.0034 80.30 Â± \\pm 0.0064 TABLE III : COMPARISON OF ACCURACY BETWEEN OUR METHOD AND MOTIONCLIP ON THE CELEBV-HQ DATASET Method Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow Ground-truth 93.67 Â± \\pm 0.0000 87.89 Â± \\pm 0.0000 95.04 Â± \\pm 1.1e -16 MotionClip 40.00 Â± \\pm 0.0039 13.73 Â± \\pm 0.0056 34.42 Â± \\pm 0.0069 Ours 58.24 Â± \\pm 0.0028 33.45 Â± \\pm 0.0052 46.47 Â± \\pm 0.0065 TABLE IV : RESULT OBTAINED BY COMBINING OUR METHOD WITH ROME OR CVTHEAD FOR TEXTURE GENERATION ON THE CK+ AND CELEBV-HQ DATASETS, RESPECTIVELY. NOTE THAT â†“ \\downarrow INDICATES THAT LOWER IS BETTER, WHEREAS â†‘ \\uparrow DENOTES THAT THE HIGHER IS BETTER. Dataset Method L â†“ 1 {}_{1}\\downarrow PSNR â†‘ {\\uparrow} LPIPS â†“ {\\downarrow} MS_SSIM â†‘ {\\uparrow} CK+ Ours+ROME 0.199 9.939 0.490 0.347 Ours+CVTHead 0.005 33.74 0.021 0.978 CelebV-HQ Ours+ROME 0.168 11.316 0.505 0.418 Ours+CVTHead 0.003 37.08 0.010 0.990 Figure 10 : Qualitative comparison between 3D expressive heads that were generated by MotionClip and our method, respectively. The input, consisting of a shape image and instruction, was used for generating expressions 1 and 2 using either MotionClip (b) or Ours (c) on CK+ and CelebV-HQ (d, e), respectively. Figure 11 : Qualitative comparison between generated 3D expressive faces when our method is combined with texture generation [ 18 , 26 ] to generate the desired expressions. The input (a), consisting of the source image and instruction, was used for generating expressions 1 and 2 using either Ours+ROME (b) or Ours+CVTHead (c) on CK+ and CelebV-HQ (d, e), respectively. Figure 12 : Successive snapshots of the generated 3D expressive faces that include a neutral expression between two facial expressions instructed by text prompts. As the present study aims to generate an avatar with a 3D facial expression from a 2D image of a face and transform the facial emotion according to the given instructions, two major challenges arise (1) facial expression transition based on expression information given by the instruction and (2) rendering the facial appearance such that it corresponds with the desired facial expression. Two experiments were prepared to evaluate the results obtained with the proposed framework both quantitatively and qualitatively. IV-D 1 Quantitative Comparison Two tasks are prepared for quantitative comparison. Facial Expression Transition The DECA encoder extracts the shape, facial expression, and pose from the source image. At the same time, the CLIP encoder combined with IFED and I2FET was used to interpret the text instruction: the initial facial expression, the final facial expression, and pose variation. To compare our approach with MotionClip, the first accuracy metric (Acc 1 ) was used to count the correctly classified samples for all expression labels in the instructions. The other one (Acc 2 ) was used to measure the correctly classified samples corresponding to both expression labels presented in each input instruction. The result suggests that the accuracy of our model is higher than that of MotionClip on both datasets as summarized in Table II . For example, our model reaches 91.44 % accuracy for generating an independent expression and 84.03 % for generating expression sequences on the CK+ dataset. The Gmean of our model is 80.30%. For the CelebV-HQ dataset, which is the real-world dataset, our model scores 58.24 %, 33.45 %, and 46.47 % for Acc 1 , Acc 2 , and Gmean, respectively, as indicated in Table III . This result suggests that our model outperforms MotionClip for learning the facial expression parameters. Visualization with t-SNE : Fig. 7 shows three learned latent representations, i.e. with MotionClip, I2FET, and I2FET with IFED, for seven facial expressions. These visual representations indicate that segregation between facial expressions is highly distinctive when IFED is combined with I2FET, compared with either MotionClip or I2FET alone. Confusion Matrices Analysis : Fig. 8 and Fig. 9 show that IFED enhances the accuracy across most classes compared to MotionClip on both datasets. Yet, a few case of misclassification occurred between certain classes of expressions, such as anger and sadness, or contempt and anger with sadness on CK+. In real-world datasets such as CelebV-HQ, some misclassification occurs between classes, particularly those with challenging expressions such as fear and surprise, or contempt and neutral. Face Rendering : For the face-rendering task, FET was combined with state-of-the-art models [ 18 , 26 ] for rendering facial appearances that correspond to specific expressions requested by instruction. This was accomplished by integrating whether ROME [ 18 ] or CVTHead [ 26 ] with facial expression transition. In this case, DECA extracts facial parameters to render geometrical information for a specific face. Instead of shapes, the RGB images and instructions are used to evaluate the performance. The L 1 loss, Peak Signal to Noise Ratio (PSNR), Learned Perceptual Image Patch Similarity (LPIPS) [ 42 ] , and Multi-Scale Structured Similarity (MS-SSIM) were used to measure the quality of reconstruction between the target image and synthesized image in the generated expression. As indicated by Table IV , our model with CVTHead produces the best result. IV-D 2 Qualitative Comparison In a similar vein, two tasks were prepared for qualitative comparisons. Facial Expression Transition : Fig. 10 compares sequences of facial expressions, generated by Motionclip and our method on CK+ and CelebV-HQ, respectively. These results clearly show that our method can generate facial expressions that convey more accurate expressions than MotionClip. Face Rendering : The resulting expressions can be qualitatively compared by viewing and deciding which expression looks better and more natural than the other. Fig. 11 shows the generated facial expressions based on learning the latent space of facial parameters on both datasets. The viewer may notice the difference between (b) and (c) for the CK+ dataset, and between (d) and (e) for the CelebV-HQ dataset, respectively, in terms of facial expressions and facial texture. For instance, the facial transition from surprise to happiness seen in the 2nd row is clearer in (c) than in (b), and the facial texture seen in the 4th row is much smoother in (c) than in (b). A similar observation can be made between (d) and (e), suggesting that Ours+CVTHead is superior to than Ours+ROME. For instance, in some cases, those enclosed within dashed boxes in Fig. 11 , a subtle distortion arose either when the facial region of the source image was occluded or when the instructions describe expressions with significant pose changes, such as transitioning from surprise to happiness or from fear to surprise. This led to inconsistencies, and missing details in the generated facial regions compared to Ours+CVTHead. A comparison between the two datasets, such as between (b) and (d) or between (c) and (e), reveals that the visual quality of CK+ is better than that of CelebV-HQ. These results confirm that our model has outstanding ability to transform facial expressions, learned on CK+ or CelebV-HQ, to a specific facial image. In addition, a user study was carried out by requesting 24 subjects to view 60 randomly paired images of various identities selected from either CVTHead + MotionClip or CVTHead + Ours. Subjects were asked to choose the expression closest to the expressions specified in the instructions or indicate whether none of them matched the provided expressions. Naturally, they were unaware as to which algorithm was used to the presented images. Four subjects viewed each video and voted for the one that more closely resembled the expression in the instructions. In cases in which the votes were tied, an additional subject was asked to make the final decision. Based on these outcomes, CVTHead + Ours achieved 73% accuracy, whereas CVTHead + MotionClip yielded 58% accuracy for CK+. For CelebV-HQ, CVTHead + Ours and CVTHead + MotionClip achieved 77% and 70.5% accuracy, respectively. IV-D 3 Inference Time During inference, CVTHead + MotionClip takes 3.97 seconds to generate a video, whereas CVTHead + Ours only takes 3.92 seconds on a single NVIDIA RTX A6000, suggesting that our model is a slightly faster than MotionClip.\n\nIV-A Dataset\nTABLE I : N 1 , N 2 âˆˆ { N_{1},N_{2}\\in\\{ HAPPINESS, DISGUST, ANGER, FEAR, SURPRISE, CONTEMPT, SADNESS } \\} IN THE CK+ DATASET AND N 1 , N 2 âˆˆ { N_{1},N_{2}\\in\\{ HAPPINESS, DISGUST, ANGER, FEAR, SURPRISE, CONTEMPT, SADNESS, NEUTRAL } \\} IN THE CELEBV-HQ DATASET. No Instruction/ Prompts 1 Turn this face from [ N 1 N_{1} ] to [ N 2 N_{2} ]. 2 Change this face from [ N 1 N_{1} ] to [ N 2 N_{2} ]. 3 Transform this face from [ N 1 N_{1} ] to [ N 2 N_{2} ]. 4 Modify this face, changing it from [ N 1 N_{1} ] to [ N 2 N_{2} ]. 5 Replace this face from [ N 1 N_{1} to [ N 2 N_{2} ]. IV-A 1 CK+ dataset The CK+ dataset was supplemented with text instructions for facial expression generation and transition. The original CK+ dataset [ 25 ] includes 117 subjects and 7 emotions, such as happiness, disgust, anger, fear, surprise, contempt, and sadness . The instructions are created to describe the transitions between these facial expressions. To extract the facial parameters and the shape image corresponding to the facial expression in the image, we utilize the pre-trained DECA model. For each sample, a tuple is created with an instruction, the target shape image, and facial parameters that correspond to the same subject. The CK+ dataset, in this study, contains a total of 26,352 samples, and the composition of this dataset in terms of the seven facial expressions is shown in Fig. 5(a) . IV-A 2 CelebV-HQ dataset Similar to the CK+ case, text instructions were added to the CelebV-HQ dataset for our experiments [ 44 ] . The original CelebV-HQ dataset included 15,653 subjects and 8 expressions of neutral, happiness, disgust, anger, fear, surprise, contempt, and sadness . The images in the CelebV-HQ dataset were spontaneously collected and presented many challenges such as highly non-frontal head poses, blur, and brightness. This problem was overcome by using a head pose algorithm [ 23 ] to estimate parameters such as the yaw, roll, and pitch , of the head pose ranging from -15 to 15 degrees. Given that the subjects were chosen indiscriminately, we select those with more than two facial expressions. Then, the pre-trained DECA was applied to extract the corresponding shape images and facial parameters. The CelebV-HQ dataset adopted in this study includes 614 subjects and 28,335 samples, and Fig. 5(b) shows a statistical breakdown of the eight facial expressions. Table I lists the template sentences that are used to generate guided instructions for facial expression transitions. We generated five instructions for each sample and depicted the transition from expression N 1 N_{1} to expression N 2 N_{2} . Subsequently, one presentation sentence is randomly chosen from these instructions. Fig. 5(c) illustrates some sampled words used in the text instructions. Figure 6 : The shapes of facial expression are generated using images from the CK+ dataset. The top and bottom rows illustrate surprised and angry expressions with various head poses.\n\nIV-A 1 CK+ dataset\nThe CK+ dataset was supplemented with text instructions for facial expression generation and transition. The original CK+ dataset [ 25 ] includes 117 subjects and 7 emotions, such as happiness, disgust, anger, fear, surprise, contempt, and sadness . The instructions are created to describe the transitions between these facial expressions. To extract the facial parameters and the shape image corresponding to the facial expression in the image, we utilize the pre-trained DECA model. For each sample, a tuple is created with an instruction, the target shape image, and facial parameters that correspond to the same subject. The CK+ dataset, in this study, contains a total of 26,352 samples, and the composition of this dataset in terms of the seven facial expressions is shown in Fig. 5(a) .\n\nIV-A 2 CelebV-HQ dataset\nSimilar to the CK+ case, text instructions were added to the CelebV-HQ dataset for our experiments [ 44 ] . The original CelebV-HQ dataset included 15,653 subjects and 8 expressions of neutral, happiness, disgust, anger, fear, surprise, contempt, and sadness . The images in the CelebV-HQ dataset were spontaneously collected and presented many challenges such as highly non-frontal head poses, blur, and brightness. This problem was overcome by using a head pose algorithm [ 23 ] to estimate parameters such as the yaw, roll, and pitch , of the head pose ranging from -15 to 15 degrees. Given that the subjects were chosen indiscriminately, we select those with more than two facial expressions. Then, the pre-trained DECA was applied to extract the corresponding shape images and facial parameters. The CelebV-HQ dataset adopted in this study includes 614 subjects and 28,335 samples, and Fig. 5(b) shows a statistical breakdown of the eight facial expressions. Table I lists the template sentences that are used to generate guided instructions for facial expression transitions. We generated five instructions for each sample and depicted the transition from expression N 1 N_{1} to expression N 2 N_{2} . Subsequently, one presentation sentence is randomly chosen from these instructions. Fig. 5(c) illustrates some sampled words used in the text instructions. Figure 6 : The shapes of facial expression are generated using images from the CK+ dataset. The top and bottom rows illustrate surprised and angry expressions with various head poses.\n\nIV-B Implementation\nThe I2FET model was trained for 200 epochs with a batch size of 128 and a learning rate of 8 â€‹ e âˆ’ 4 8e^{-4} . The Adam optimizer [ 20 ] was adopted to optimize the model. For the experiments, 10% of the dataset was allocated to the testing set, whereas the remaining portion was used for training. Then, a validation set was drawn from 10% of the training set. In this work, we repeated each experiment ten times and used the average results to evaluate the overall performance of the proposed I2FET. Because our task is new and a specific method that would allow a fair comparison is not available, we chose MotionClip [ 39 ] as a baseline. For the face rendering experiment, the configuration by [ 18 , 26 ] was used.\n\nIV-C Classification of Facial Expressions\nIn this study, it was important to employ an appropriate classifier to evaluate the method that was used to generate facial expressions as prior studies [ 27 , 46 ] . Given that the CK+ and CelebV-HQ datasets had skewed distributions, the performance for minor classes could be affected by the facial expression classifiers trained on these imbalanced datasets, in which the results would be biased towards the major class. Moreover, the effectiveness of the classification model could also be affected by head movements, which were commonly encountered as shown in Fig. 6 . These problems were addressed by conducting an experiment to identify an appropriate classifier that can handle not only imbalanced datasets but also invariant rotation transformations. ResNet-101 [ 14 ] , MEK [ 43 ] , and ResNet-101 integrated with Reweighted Focal Loss (RFL) [ 5 ] were utilized to evaluate the performance of facial expression classification. For this purpose, augmented datasets were created by rotating the original CK+ and CelebV-HQ datasets at angles of Â± 10 o , Â± 15 o \\pm 10^{o},\\pm 15^{o} , and Â± 30 o \\pm 30^{o} . Then, the three networks are trained using the original datasets and evaluated on all datasets. The Accuracy was employed to assess whether samples had been correctly classified across all classes, while G-mean was used to evaluate the ability of the model to effectively recognize minor classes that are predicted by the facial expression classification model. The supplementary data in the ablation study in Section IV-E suggests that MEK can address imbalanced datasets but is sensitive to rotational transformations. ResNet101 demonstrates positive results with rotation transformations but performs worse on datasets that are highly imbalanced. ResNet101 integrated with RFL outperforms ResNet101 and MEK in both scenarios. As a result, ResNet101 with RFL was selected as our classifier for evaluating the ability of our models to generate facial expressions.\n\nIV-D Results\n(a) (b) (c) Figure 7 : Visualization of the learned latent space for seven facial expressions by (a) MotionClip, (b) I2FET without IFED, and (c) I2FET with IFED, respectively, using t-SNE. (a) (b) Figure 8 : Confusion matrices for seven facial expressions by (a) MotionClip and (b) our method on CK+. (a) (b) Figure 9 : Confusion matrices for eight facial expressions by (a) MotionClip and (b) our method on CelebV-HQ. TABLE II : COMPARISON OF ACCURACY BETWEEN OUR METHOD AND MOTIONCLIP ON THE CK+ DATASET Method Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow Ground-truth 99.69 Â± \\pm 0.0001 99.39 Â± \\pm 0.0002 99.69 Â± \\pm 9.3e -5 MotionClip 52.00 Â± \\pm 0.0058 20.00 Â± \\pm 0.0082 40.48 Â± \\pm 0.0060 Ours 91.44 Â± \\pm 0.0024 84.03 Â± \\pm 0.0034 80.30 Â± \\pm 0.0064 TABLE III : COMPARISON OF ACCURACY BETWEEN OUR METHOD AND MOTIONCLIP ON THE CELEBV-HQ DATASET Method Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow Ground-truth 93.67 Â± \\pm 0.0000 87.89 Â± \\pm 0.0000 95.04 Â± \\pm 1.1e -16 MotionClip 40.00 Â± \\pm 0.0039 13.73 Â± \\pm 0.0056 34.42 Â± \\pm 0.0069 Ours 58.24 Â± \\pm 0.0028 33.45 Â± \\pm 0.0052 46.47 Â± \\pm 0.0065 TABLE IV : RESULT OBTAINED BY COMBINING OUR METHOD WITH ROME OR CVTHEAD FOR TEXTURE GENERATION ON THE CK+ AND CELEBV-HQ DATASETS, RESPECTIVELY. NOTE THAT â†“ \\downarrow INDICATES THAT LOWER IS BETTER, WHEREAS â†‘ \\uparrow DENOTES THAT THE HIGHER IS BETTER. Dataset Method L â†“ 1 {}_{1}\\downarrow PSNR â†‘ {\\uparrow} LPIPS â†“ {\\downarrow} MS_SSIM â†‘ {\\uparrow} CK+ Ours+ROME 0.199 9.939 0.490 0.347 Ours+CVTHead 0.005 33.74 0.021 0.978 CelebV-HQ Ours+ROME 0.168 11.316 0.505 0.418 Ours+CVTHead 0.003 37.08 0.010 0.990 Figure 10 : Qualitative comparison between 3D expressive heads that were generated by MotionClip and our method, respectively. The input, consisting of a shape image and instruction, was used for generating expressions 1 and 2 using either MotionClip (b) or Ours (c) on CK+ and CelebV-HQ (d, e), respectively. Figure 11 : Qualitative comparison between generated 3D expressive faces when our method is combined with texture generation [ 18 , 26 ] to generate the desired expressions. The input (a), consisting of the source image and instruction, was used for generating expressions 1 and 2 using either Ours+ROME (b) or Ours+CVTHead (c) on CK+ and CelebV-HQ (d, e), respectively. Figure 12 : Successive snapshots of the generated 3D expressive faces that include a neutral expression between two facial expressions instructed by text prompts. As the present study aims to generate an avatar with a 3D facial expression from a 2D image of a face and transform the facial emotion according to the given instructions, two major challenges arise (1) facial expression transition based on expression information given by the instruction and (2) rendering the facial appearance such that it corresponds with the desired facial expression. Two experiments were prepared to evaluate the results obtained with the proposed framework both quantitatively and qualitatively. IV-D 1 Quantitative Comparison Two tasks are prepared for quantitative comparison. Facial Expression Transition The DECA encoder extracts the shape, facial expression, and pose from the source image. At the same time, the CLIP encoder combined with IFED and I2FET was used to interpret the text instruction: the initial facial expression, the final facial expression, and pose variation. To compare our approach with MotionClip, the first accuracy metric (Acc 1 ) was used to count the correctly classified samples for all expression labels in the instructions. The other one (Acc 2 ) was used to measure the correctly classified samples corresponding to both expression labels presented in each input instruction. The result suggests that the accuracy of our model is higher than that of MotionClip on both datasets as summarized in Table II . For example, our model reaches 91.44 % accuracy for generating an independent expression and 84.03 % for generating expression sequences on the CK+ dataset. The Gmean of our model is 80.30%. For the CelebV-HQ dataset, which is the real-world dataset, our model scores 58.24 %, 33.45 %, and 46.47 % for Acc 1 , Acc 2 , and Gmean, respectively, as indicated in Table III . This result suggests that our model outperforms MotionClip for learning the facial expression parameters. Visualization with t-SNE : Fig. 7 shows three learned latent representations, i.e. with MotionClip, I2FET, and I2FET with IFED, for seven facial expressions. These visual representations indicate that segregation between facial expressions is highly distinctive when IFED is combined with I2FET, compared with either MotionClip or I2FET alone. Confusion Matrices Analysis : Fig. 8 and Fig. 9 show that IFED enhances the accuracy across most classes compared to MotionClip on both datasets. Yet, a few case of misclassification occurred between certain classes of expressions, such as anger and sadness, or contempt and anger with sadness on CK+. In real-world datasets such as CelebV-HQ, some misclassification occurs between classes, particularly those with challenging expressions such as fear and surprise, or contempt and neutral. Face Rendering : For the face-rendering task, FET was combined with state-of-the-art models [ 18 , 26 ] for rendering facial appearances that correspond to specific expressions requested by instruction. This was accomplished by integrating whether ROME [ 18 ] or CVTHead [ 26 ] with facial expression transition. In this case, DECA extracts facial parameters to render geometrical information for a specific face. Instead of shapes, the RGB images and instructions are used to evaluate the performance. The L 1 loss, Peak Signal to Noise Ratio (PSNR), Learned Perceptual Image Patch Similarity (LPIPS) [ 42 ] , and Multi-Scale Structured Similarity (MS-SSIM) were used to measure the quality of reconstruction between the target image and synthesized image in the generated expression. As indicated by Table IV , our model with CVTHead produces the best result. IV-D 2 Qualitative Comparison In a similar vein, two tasks were prepared for qualitative comparisons. Facial Expression Transition : Fig. 10 compares sequences of facial expressions, generated by Motionclip and our method on CK+ and CelebV-HQ, respectively. These results clearly show that our method can generate facial expressions that convey more accurate expressions than MotionClip. Face Rendering : The resulting expressions can be qualitatively compared by viewing and deciding which expression looks better and more natural than the other. Fig. 11 shows the generated facial expressions based on learning the latent space of facial parameters on both datasets. The viewer may notice the difference between (b) and (c) for the CK+ dataset, and between (d) and (e) for the CelebV-HQ dataset, respectively, in terms of facial expressions and facial texture. For instance, the facial transition from surprise to happiness seen in the 2nd row is clearer in (c) than in (b), and the facial texture seen in the 4th row is much smoother in (c) than in (b). A similar observation can be made between (d) and (e), suggesting that Ours+CVTHead is superior to than Ours+ROME. For instance, in some cases, those enclosed within dashed boxes in Fig. 11 , a subtle distortion arose either when the facial region of the source image was occluded or when the instructions describe expressions with significant pose changes, such as transitioning from surprise to happiness or from fear to surprise. This led to inconsistencies, and missing details in the generated facial regions compared to Ours+CVTHead. A comparison between the two datasets, such as between (b) and (d) or between (c) and (e), reveals that the visual quality of CK+ is better than that of CelebV-HQ. These results confirm that our model has outstanding ability to transform facial expressions, learned on CK+ or CelebV-HQ, to a specific facial image. In addition, a user study was carried out by requesting 24 subjects to view 60 randomly paired images of various identities selected from either CVTHead + MotionClip or CVTHead + Ours. Subjects were asked to choose the expression closest to the expressions specified in the instructions or indicate whether none of them matched the provided expressions. Naturally, they were unaware as to which algorithm was used to the presented images. Four subjects viewed each video and voted for the one that more closely resembled the expression in the instructions. In cases in which the votes were tied, an additional subject was asked to make the final decision. Based on these outcomes, CVTHead + Ours achieved 73% accuracy, whereas CVTHead + MotionClip yielded 58% accuracy for CK+. For CelebV-HQ, CVTHead + Ours and CVTHead + MotionClip achieved 77% and 70.5% accuracy, respectively.\n\nIV-D 1 Quantitative Comparison\nTwo tasks are prepared for quantitative comparison. Facial Expression Transition The DECA encoder extracts the shape, facial expression, and pose from the source image. At the same time, the CLIP encoder combined with IFED and I2FET was used to interpret the text instruction: the initial facial expression, the final facial expression, and pose variation. To compare our approach with MotionClip, the first accuracy metric (Acc 1 ) was used to count the correctly classified samples for all expression labels in the instructions. The other one (Acc 2 ) was used to measure the correctly classified samples corresponding to both expression labels presented in each input instruction. The result suggests that the accuracy of our model is higher than that of MotionClip on both datasets as summarized in Table II . For example, our model reaches 91.44 % accuracy for generating an independent expression and 84.03 % for generating expression sequences on the CK+ dataset. The Gmean of our model is 80.30%. For the CelebV-HQ dataset, which is the real-world dataset, our model scores 58.24 %, 33.45 %, and 46.47 % for Acc 1 , Acc 2 , and Gmean, respectively, as indicated in Table III . This result suggests that our model outperforms MotionClip for learning the facial expression parameters. Visualization with t-SNE : Fig. 7 shows three learned latent representations, i.e. with MotionClip, I2FET, and I2FET with IFED, for seven facial expressions. These visual representations indicate that segregation between facial expressions is highly distinctive when IFED is combined with I2FET, compared with either MotionClip or I2FET alone. Confusion Matrices Analysis : Fig. 8 and Fig. 9 show that IFED enhances the accuracy across most classes compared to MotionClip on both datasets. Yet, a few case of misclassification occurred between certain classes of expressions, such as anger and sadness, or contempt and anger with sadness on CK+. In real-world datasets such as CelebV-HQ, some misclassification occurs between classes, particularly those with challenging expressions such as fear and surprise, or contempt and neutral. Face Rendering : For the face-rendering task, FET was combined with state-of-the-art models [ 18 , 26 ] for rendering facial appearances that correspond to specific expressions requested by instruction. This was accomplished by integrating whether ROME [ 18 ] or CVTHead [ 26 ] with facial expression transition. In this case, DECA extracts facial parameters to render geometrical information for a specific face. Instead of shapes, the RGB images and instructions are used to evaluate the performance. The L 1 loss, Peak Signal to Noise Ratio (PSNR), Learned Perceptual Image Patch Similarity (LPIPS) [ 42 ] , and Multi-Scale Structured Similarity (MS-SSIM) were used to measure the quality of reconstruction between the target image and synthesized image in the generated expression. As indicated by Table IV , our model with CVTHead produces the best result.\n\nIV-D 2 Qualitative Comparison\nIn a similar vein, two tasks were prepared for qualitative comparisons. Facial Expression Transition : Fig. 10 compares sequences of facial expressions, generated by Motionclip and our method on CK+ and CelebV-HQ, respectively. These results clearly show that our method can generate facial expressions that convey more accurate expressions than MotionClip. Face Rendering : The resulting expressions can be qualitatively compared by viewing and deciding which expression looks better and more natural than the other. Fig. 11 shows the generated facial expressions based on learning the latent space of facial parameters on both datasets. The viewer may notice the difference between (b) and (c) for the CK+ dataset, and between (d) and (e) for the CelebV-HQ dataset, respectively, in terms of facial expressions and facial texture. For instance, the facial transition from surprise to happiness seen in the 2nd row is clearer in (c) than in (b), and the facial texture seen in the 4th row is much smoother in (c) than in (b). A similar observation can be made between (d) and (e), suggesting that Ours+CVTHead is superior to than Ours+ROME. For instance, in some cases, those enclosed within dashed boxes in Fig. 11 , a subtle distortion arose either when the facial region of the source image was occluded or when the instructions describe expressions with significant pose changes, such as transitioning from surprise to happiness or from fear to surprise. This led to inconsistencies, and missing details in the generated facial regions compared to Ours+CVTHead. A comparison between the two datasets, such as between (b) and (d) or between (c) and (e), reveals that the visual quality of CK+ is better than that of CelebV-HQ. These results confirm that our model has outstanding ability to transform facial expressions, learned on CK+ or CelebV-HQ, to a specific facial image. In addition, a user study was carried out by requesting 24 subjects to view 60 randomly paired images of various identities selected from either CVTHead + MotionClip or CVTHead + Ours. Subjects were asked to choose the expression closest to the expressions specified in the instructions or indicate whether none of them matched the provided expressions. Naturally, they were unaware as to which algorithm was used to the presented images. Four subjects viewed each video and voted for the one that more closely resembled the expression in the instructions. In cases in which the votes were tied, an additional subject was asked to make the final decision. Based on these outcomes, CVTHead + Ours achieved 73% accuracy, whereas CVTHead + MotionClip yielded 58% accuracy for CK+. For CelebV-HQ, CVTHead + Ours and CVTHead + MotionClip achieved 77% and 70.5% accuracy, respectively.\n\nIV-D 3 Inference Time\nDuring inference, CVTHead + MotionClip takes 3.97 seconds to generate a video, whereas CVTHead + Ours only takes 3.92 seconds on a single NVIDIA RTX A6000, suggesting that our model is a slightly faster than MotionClip.\n\nIV-E Ablation Study\nThis section presents a series of ablation studies that were carried out to evaluate the performance of our model. IV-E 1 Effectiveness of the Proposed Components To demonstrate the effectiveness of each proposed component, we conducted a comparative experiment, as shown in Table V . It can be observed that the IFED module significantly improves the performance of I2FET compared to I2FET without IFED on both CK+ and CelebV-HQ datasets. Additionally, integration of the vertex loss function with IFED increases Acc 1 and Gmean on both datasets. Despite a slight decrease in Acc 2 on CK+, it still performs well on CelebV-HQ. TABLE V : COMPARISON OF THE ACCURACY BETWEEN THE PROPOSED DIFFERENT COMPONENTS FOR OUR METHOD (I2FET) ON THE TWO DATASETS Dataset Method Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow w/o IFED 76.89 Â± \\pm 0.0029 63.23 Â± \\pm 0.0047 63.59 Â± \\pm 0.0059 w/ IFED 91.30 Â± \\pm 0.0017 84.06 Â± \\pm 0.0026 79.41 Â± \\pm 0.0061 CK+ w/ IFED 91.44 Â± \\pm 0.0024 84.03 Â± \\pm 0.0034 80.30 Â± \\pm 0.0064 + w/ â„’ v \\mathcal{L}_{v} w/o IFED 48.35 Â± \\pm 0.0037 22.1 Â± \\pm 0.0040 32.88 Â± \\pm 0.0058 w/ IFED 57.80 Â± \\pm 0.0020 32.60 Â± \\pm 0.0037 46.04 Â± \\pm 0.0062 CelebV-HQ w/ IFED 58.24 Â± \\pm 0.0028 33.45 Â± \\pm 0.0052 46.47 Â± \\pm 0.0065 + w/ â„’ v \\mathcal{L}_{v} IV-E 2 Impact of Three Loss Functions To analyze the impact of each loss function in I2FET, an experiment was carried out to compare the performance of the model in various scenarios. Specifically, IFED was integrated with the I2FET model and each loss function was incrementally added to determine its impact on the model performance of the model. The results in Table VI indicate that the performance improved when the pose loss function integrated with the expression loss function, whereas the vertex loss function yielded a slight improvement when used alongside both the pose and expression loss functions. In this study, the vertex loss was integrated with other loss functions to capture the geometry of the face, to enhance the accuracy of facial expression recognition. Fig 13 and 14 are the confusion matrix of our model when the total loss function integrated with â„’ v \\mathcal{L}_{v} or without â„’ v \\mathcal{L}_{v} on the CK+ and CelebV-HQ datasets. Note that the fear accuracies for both datasets rather decrease slightly when the vertex loss is applied. It is known that the fear expression often overlaps with other expressions (e.g., surprise and happiness), thereby making it challenging to distinguish. Moreover, since the fear portion of CK+ is much larger than that of CelebV-HQ in Fig. 5, it appears that confusion during the facial expression transition has a reverse impact on Acc 2 as shown in Table VI (a) (b) Figure 13 : Confusion matrices for seven facial expressions by I2FET (a) without â„’ v \\mathcal{L}_{v} and (b) with â„’ v \\mathcal{L}_{v} on CK+. (a) (b) Figure 14 : Confusion matrices for eight facial expressions by I2FET (a) without â„’ v \\mathcal{L}_{v} and (b) with â„’ v \\mathcal{L}_{v} on CelebV-HQ. TABLE VI : COMPARISON OF THE IMPACT OF LOSS FUNCTIONS ON THE PERFORMANCE OF OUR METHOD (I2FET) ACROSS THE TWO DATASETS Dataset Method Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow â„’ e \\mathcal{L}_{e} 66.80 Â± \\pm 0.0056 45.96 Â± \\pm 0.0112 52.42 Â± \\pm 0.0730 â„’ e \\mathcal{L}_{e} + â„’ p \\mathcal{L}_{p} 91.30 Â± \\pm 0.0017 84.06 Â± \\pm 0.0026 79.41 Â± \\pm 0.0061 CK+ â„’ e \\mathcal{L}_{e} + â„’ p \\mathcal{L}_{p} + â„’ v \\mathcal{L}_{v} 91.44 Â± \\pm 0.0024 84.03 Â± \\pm 0.0034 80.30 Â± \\pm 0.0064 â„’ e \\mathcal{L}_{e} 37.57 Â± \\pm 0.0052 11.99 Â± \\pm 0.0326 25.82 Â± \\pm 0.0114 â„’ e \\mathcal{L}_{e} + â„’ p \\mathcal{L}_{p} 57.80 Â± \\pm 0.0020 32.60 Â± \\pm 0.0037 46.04 Â± \\pm 0.0062 CelebV-HQ â„’ e \\mathcal{L}_{e} + â„’ p \\mathcal{L}_{p} + â„’ v \\mathcal{L}_{v} 58.24 Â± \\pm 0.0028 33.45 Â± \\pm 0.0052 46.47 Â± \\pm 0.0065 IV-E 3 Impact of IFED on the Training Process This experiment was conducted to compare the two loss functions with and without incorporating the IFED module in I2FET. Fig. 15(a) and Fig. 15(b) show the results for CK+ and CelebV-HQ, respectively, confirming that I2FED plays an important role. IV-E 4 Effect of IFED on Intensity of Facial Expression Fig. 16 demonstrates that IFED helps I2FED to generate high-intensity facial expressions. (a) (b) Figure 15 : Comparison of the validation loss during the training process of I2FET w/ and w/o IFED, respectively, on (a) CK+ and (b) CelebV-HQ. The green curves represent I2FET without IFED, whereas the blue curves are the results for I2FET with IFED. Figure 16 : Qualitative comparison between 3D expressive faces generated (a) w/o and (b) w/ IFED. IV-E 5 Impact of Quantity of CAFT Layer The experiments were conducted to evaluate the results of the proposed I2FET model by designing the IFED module with varying numbers of CAFT layers. We fixed the number of transformer layers to 1 in both branches and adjusted the number of CAFT layers. The results in Table VII indicate that increasing the quantity of CAFT layers does not improve the performance of the proposed I2FET. IV-E 6 Impact of Quantity of Transformer Encoder in Facial Parameter Branch and Text Branch In this experiment, we utilized different configurations by adjusting the number of transformer encoder layers of IFED. The findings suggest that the I2FET model performs better when N = 2 and M = 1 compared to other configurations, particularly in terms of Acc 1 and G-mean, as shown in Table VIII . TABLE VII : COMPARISON OF THE ACCURACY OF THE I2FET METHOD ON BOTH DATASETS BY VARYING THE NUMBER OF CAFT LAYERS IN I2FED Dataset #Layers Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow 1 91.36 Â± \\pm 0.0023 84.13 Â± \\pm 0.0046 79.65 Â± \\pm 0.0080 CK+ 2 88.95 Â± \\pm 0.0019 80.95 Â± \\pm 0.0038 78.72 Â± \\pm 0.0071 1 58.25 Â± \\pm 0.0034 33.03 Â± \\pm 0.0056 46.46 Â± \\pm 0.0066 CelebV-HQ 2 55.68 Â± \\pm 0.0034 29.82 Â± \\pm 0.0066 42.59 Â± \\pm 0.0085 TABLE VIII : COMPARISON OF THE ACCURACY OF THE PROPOSED METHOD WITH THE NUMBER OF TRANSFORMER LAYERS FOR FACIAL PARAMETERS BRANCH ( N ) AND TEXT BRANCH ( M ) IN TFED ON CK+ Mode N M Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow A 1 1 91.36 Â± \\pm 0.0023 84.13 Â± \\pm 0.0046 79.67 Â± \\pm 0.008 B 2 1 91.44 Â± \\pm 0.0024 84.03 Â± \\pm 0.0034 80.30 Â± \\pm 0.0064 C 1 2 91.27 Â± \\pm 0.0019 83.84 Â± \\pm 0.0037 79.69 Â± \\pm 0.0052 D 2 2 89.07 Â± \\pm 0.0021 81.13 Â± \\pm 0.0034 78.71 Â± \\pm 0.0070\n\nIV-E 1 Effectiveness of the Proposed Components\nTo demonstrate the effectiveness of each proposed component, we conducted a comparative experiment, as shown in Table V . It can be observed that the IFED module significantly improves the performance of I2FET compared to I2FET without IFED on both CK+ and CelebV-HQ datasets. Additionally, integration of the vertex loss function with IFED increases Acc 1 and Gmean on both datasets. Despite a slight decrease in Acc 2 on CK+, it still performs well on CelebV-HQ. TABLE V : COMPARISON OF THE ACCURACY BETWEEN THE PROPOSED DIFFERENT COMPONENTS FOR OUR METHOD (I2FET) ON THE TWO DATASETS Dataset Method Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow w/o IFED 76.89 Â± \\pm 0.0029 63.23 Â± \\pm 0.0047 63.59 Â± \\pm 0.0059 w/ IFED 91.30 Â± \\pm 0.0017 84.06 Â± \\pm 0.0026 79.41 Â± \\pm 0.0061 CK+ w/ IFED 91.44 Â± \\pm 0.0024 84.03 Â± \\pm 0.0034 80.30 Â± \\pm 0.0064 + w/ â„’ v \\mathcal{L}_{v} w/o IFED 48.35 Â± \\pm 0.0037 22.1 Â± \\pm 0.0040 32.88 Â± \\pm 0.0058 w/ IFED 57.80 Â± \\pm 0.0020 32.60 Â± \\pm 0.0037 46.04 Â± \\pm 0.0062 CelebV-HQ w/ IFED 58.24 Â± \\pm 0.0028 33.45 Â± \\pm 0.0052 46.47 Â± \\pm 0.0065 + w/ â„’ v \\mathcal{L}_{v}\n\nIV-E 2 Impact of Three Loss Functions\nTo analyze the impact of each loss function in I2FET, an experiment was carried out to compare the performance of the model in various scenarios. Specifically, IFED was integrated with the I2FET model and each loss function was incrementally added to determine its impact on the model performance of the model. The results in Table VI indicate that the performance improved when the pose loss function integrated with the expression loss function, whereas the vertex loss function yielded a slight improvement when used alongside both the pose and expression loss functions. In this study, the vertex loss was integrated with other loss functions to capture the geometry of the face, to enhance the accuracy of facial expression recognition. Fig 13 and 14 are the confusion matrix of our model when the total loss function integrated with â„’ v \\mathcal{L}_{v} or without â„’ v \\mathcal{L}_{v} on the CK+ and CelebV-HQ datasets. Note that the fear accuracies for both datasets rather decrease slightly when the vertex loss is applied. It is known that the fear expression often overlaps with other expressions (e.g., surprise and happiness), thereby making it challenging to distinguish. Moreover, since the fear portion of CK+ is much larger than that of CelebV-HQ in Fig. 5, it appears that confusion during the facial expression transition has a reverse impact on Acc 2 as shown in Table VI (a) (b) Figure 13 : Confusion matrices for seven facial expressions by I2FET (a) without â„’ v \\mathcal{L}_{v} and (b) with â„’ v \\mathcal{L}_{v} on CK+. (a) (b) Figure 14 : Confusion matrices for eight facial expressions by I2FET (a) without â„’ v \\mathcal{L}_{v} and (b) with â„’ v \\mathcal{L}_{v} on CelebV-HQ. TABLE VI : COMPARISON OF THE IMPACT OF LOSS FUNCTIONS ON THE PERFORMANCE OF OUR METHOD (I2FET) ACROSS THE TWO DATASETS Dataset Method Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow â„’ e \\mathcal{L}_{e} 66.80 Â± \\pm 0.0056 45.96 Â± \\pm 0.0112 52.42 Â± \\pm 0.0730 â„’ e \\mathcal{L}_{e} + â„’ p \\mathcal{L}_{p} 91.30 Â± \\pm 0.0017 84.06 Â± \\pm 0.0026 79.41 Â± \\pm 0.0061 CK+ â„’ e \\mathcal{L}_{e} + â„’ p \\mathcal{L}_{p} + â„’ v \\mathcal{L}_{v} 91.44 Â± \\pm 0.0024 84.03 Â± \\pm 0.0034 80.30 Â± \\pm 0.0064 â„’ e \\mathcal{L}_{e} 37.57 Â± \\pm 0.0052 11.99 Â± \\pm 0.0326 25.82 Â± \\pm 0.0114 â„’ e \\mathcal{L}_{e} + â„’ p \\mathcal{L}_{p} 57.80 Â± \\pm 0.0020 32.60 Â± \\pm 0.0037 46.04 Â± \\pm 0.0062 CelebV-HQ â„’ e \\mathcal{L}_{e} + â„’ p \\mathcal{L}_{p} + â„’ v \\mathcal{L}_{v} 58.24 Â± \\pm 0.0028 33.45 Â± \\pm 0.0052 46.47 Â± \\pm 0.0065\n\nIV-E 3 Impact of IFED on the Training Process\nThis experiment was conducted to compare the two loss functions with and without incorporating the IFED module in I2FET. Fig. 15(a) and Fig. 15(b) show the results for CK+ and CelebV-HQ, respectively, confirming that I2FED plays an important role.\n\nIV-E 4 Effect of IFED on Intensity of Facial Expression\nFig. 16 demonstrates that IFED helps I2FED to generate high-intensity facial expressions. (a) (b) Figure 15 : Comparison of the validation loss during the training process of I2FET w/ and w/o IFED, respectively, on (a) CK+ and (b) CelebV-HQ. The green curves represent I2FET without IFED, whereas the blue curves are the results for I2FET with IFED. Figure 16 : Qualitative comparison between 3D expressive faces generated (a) w/o and (b) w/ IFED.\n\nIV-E 5 Impact of Quantity of CAFT Layer\nThe experiments were conducted to evaluate the results of the proposed I2FET model by designing the IFED module with varying numbers of CAFT layers. We fixed the number of transformer layers to 1 in both branches and adjusted the number of CAFT layers. The results in Table VII indicate that increasing the quantity of CAFT layers does not improve the performance of the proposed I2FET.\n\nIV-E 6 Impact of Quantity of Transformer Encoder in Facial Parameter Branch and Text Branch\nIn this experiment, we utilized different configurations by adjusting the number of transformer encoder layers of IFED. The findings suggest that the I2FET model performs better when N = 2 and M = 1 compared to other configurations, particularly in terms of Acc 1 and G-mean, as shown in Table VIII . TABLE VII : COMPARISON OF THE ACCURACY OF THE I2FET METHOD ON BOTH DATASETS BY VARYING THE NUMBER OF CAFT LAYERS IN I2FED Dataset #Layers Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow 1 91.36 Â± \\pm 0.0023 84.13 Â± \\pm 0.0046 79.65 Â± \\pm 0.0080 CK+ 2 88.95 Â± \\pm 0.0019 80.95 Â± \\pm 0.0038 78.72 Â± \\pm 0.0071 1 58.25 Â± \\pm 0.0034 33.03 Â± \\pm 0.0056 46.46 Â± \\pm 0.0066 CelebV-HQ 2 55.68 Â± \\pm 0.0034 29.82 Â± \\pm 0.0066 42.59 Â± \\pm 0.0085 TABLE VIII : COMPARISON OF THE ACCURACY OF THE PROPOSED METHOD WITH THE NUMBER OF TRANSFORMER LAYERS FOR FACIAL PARAMETERS BRANCH ( N ) AND TEXT BRANCH ( M ) IN TFED ON CK+ Mode N M Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow A 1 1 91.36 Â± \\pm 0.0023 84.13 Â± \\pm 0.0046 79.67 Â± \\pm 0.008 B 2 1 91.44 Â± \\pm 0.0024 84.03 Â± \\pm 0.0034 80.30 Â± \\pm 0.0064 C 1 2 91.27 Â± \\pm 0.0019 83.84 Â± \\pm 0.0037 79.69 Â± \\pm 0.0052 D 2 2 89.07 Â± \\pm 0.0021 81.13 Â± \\pm 0.0034 78.71 Â± \\pm 0.0070\n\nIV-F Longer Facial Expression Generation with Neutral Expressions\nAccording to appraisal theory [ 35 ] , which is one of the major emotion theories, the dynamic of emotion is described as the sequential check theory of emotion differentiation. In other words, the differentiation of emotional states of humans is the result of a sequence of specific stimulus evaluation appraisal checks, rather than an action of hopping from one emotional state to another in a discrete manner. In our framework, the transition of the facial expression from A to B is similar to the differentiation of emotional states in a sequence: starting from A facial expression with strong emotion a neutral expression and then moving to strong facial expression B. In addition to appraisal theory, the present approach is suitable for investigating facial behaviors within the pleasure-arousal (P-A) space [ 34 ] , where the neutral expression serves as a transitional expression between two specific expressions. For this purpose, a neutral encoder-decoder (NED) was adopted to generate neutral faces. The aim thereof was to improve the performance of our framework when a neutral expression is not available in the instruction to increase its flexibility for extending facial expression sequences. IV-F 1 Implementation Details In this scenario, two datasets containing the parameters of neutral expressions and poses were constructed the pretrained DECA to extract the data from the CelebV-HQ and CK+ datasets. As the CK+ dataset does not contain neutral expressions, the first frame of each video was extracted and considered as the representative neutral case. Afterward, NED was trained on these two neutral datasets. IV-F 2 Network Architecture The neutral encoder-decoder model was constructed with fully connected layers. The encoder consisted of four fully connected layers, whereas the decoder was structured with four fully connected layers dedicated to pose and expression, respectively. IV-F 3 Analysis Similarly, we also conducted experiments to evaluate neutral expression generation for both quantitative and qualitative comparisons. Quantitative comparison : Specifically, the accuracy of NED on the CK+ and CelebV-HQ datasets was 49.5% and 51.33% for generating the neutral expression, respectively. These results are presented in Table IX , which confirms that NED + CVTHead outperforms NED + ROME. Qualitative comparison : Fig. 17 visualizes neutral expressions generated by NED + CVTHead on CK+ and CelebV-HQ. Moreover, we also illustrate neutral expressions generated by FLAME with the expression parameter set to zero, as shown in Fig. 17 c. The results show that the facial units differ, depending on whether the environment is controllable or non-controllable, or whether FLAME was used. This indicates that neutral expressions are more complex in real-world scenarios, leading us to prefer using a neural encoder-decoder instead of the expression parameter settings provided by FLAME Figure 17 : Samples of facial images with a neutral expression generated by NED+CVTHead. The input images are shown in the leftmost column (Source). The images generated using CK+ dataset (controllable environment) are shown in (a), whereas those using CelebV-HQ dataset (non-controllable environment) are shown in (b). On the other hand, the generated 3D face with FLAME is depicted by setting the expression parameter to zero in (c). Note that subtle differences exist in the image quality between the three generated cases. TABLE IX : RESULT OBTAINED WHEN NED IS COMBINED WITH ROME OR CVTHEAD FOR TEXTURE GENERATION ON CK+ AND CELEBV-HQ, RESPECTIVELY. Dataset Method L â†“ 1 {}_{1}\\downarrow PSNR â†‘ {\\uparrow} LPIPS â†“ {\\downarrow} MS_SSIM â†‘ {\\uparrow} CK+ NED + ROME 0.193 10.207 0.516 0.348 NED + CVTHead 0.004 35.158 0.014 0.987 CelebV-HQ NED + ROME 0.183 10.818 0.498 0.407 NED + CVTHead 0.004 36.347 0.0117 0.989 In a user study similar to that described in Section IV-D 2 , we asked subjects to determine whether the generated images display neutral expressions. Most subjects succeeded in identifying the neutral expressions produced by our model on CK+, and they had difficulties with only 4% of the samples generated by the learned model on CelebV-HQ. TABLE X : COMPARISON OF FACIAL EXPRESSION CLASSIFIERS ON CK+ AND CELEBV-HQ WITH SAMPLES ROTATED AT VARIOUS ANGLES { 0 o , Â± 10 o , Â± 15 o , Â± 30 o } \\{0^{o},\\pm 10^{o},\\pm 15^{o},\\pm 30^{o}\\} 0 o Â± \\pm 10 o Â± \\pm 15 o Â± \\pm 30 o Dataset Method Acc 1 â†‘ \\uparrow G-mean â†‘ \\uparrow Acc 1 â†‘ \\uparrow G-mean â†‘ \\uparrow Acc 1 â†‘ \\uparrow G-mean â†‘ \\uparrow Acc 1 â†‘ \\uparrow G-mean â†‘ \\uparrow MEK [ 43 ] 100 100 86.41 76.20 76.01 52.16 32.56 0.0 CK+ ResNet-101 [ 14 ] 99.56 99.4 84.40 73.28 79.50 67.6 75.29 33.39 ResNet-101 with RFL 99.69 99.69 87.18 75.85 83.66 66.13 77.48 31.5 MEK [ 43 ] 99.75 99.83 60.49 55.07 49.08 33.84 32.05 9.64 CelebV-HQ ResNet-101 [ 14 ] 37 12.77 35.77 13.39 34.97 0.0 28.92 0.0 ResNet-101 with RFL 93.67 95.04 58.54 49.92 51.53 41.22 40.71 26.80\n\nIV-F 1 Implementation Details\nIn this scenario, two datasets containing the parameters of neutral expressions and poses were constructed the pretrained DECA to extract the data from the CelebV-HQ and CK+ datasets. As the CK+ dataset does not contain neutral expressions, the first frame of each video was extracted and considered as the representative neutral case. Afterward, NED was trained on these two neutral datasets.\n\nIV-F 2 Network Architecture\nThe neutral encoder-decoder model was constructed with fully connected layers. The encoder consisted of four fully connected layers, whereas the decoder was structured with four fully connected layers dedicated to pose and expression, respectively.\n\nIV-F 3 Analysis\nSimilarly, we also conducted experiments to evaluate neutral expression generation for both quantitative and qualitative comparisons. Quantitative comparison : Specifically, the accuracy of NED on the CK+ and CelebV-HQ datasets was 49.5% and 51.33% for generating the neutral expression, respectively. These results are presented in Table IX , which confirms that NED + CVTHead outperforms NED + ROME. Qualitative comparison : Fig. 17 visualizes neutral expressions generated by NED + CVTHead on CK+ and CelebV-HQ. Moreover, we also illustrate neutral expressions generated by FLAME with the expression parameter set to zero, as shown in Fig. 17 c. The results show that the facial units differ, depending on whether the environment is controllable or non-controllable, or whether FLAME was used. This indicates that neutral expressions are more complex in real-world scenarios, leading us to prefer using a neural encoder-decoder instead of the expression parameter settings provided by FLAME Figure 17 : Samples of facial images with a neutral expression generated by NED+CVTHead. The input images are shown in the leftmost column (Source). The images generated using CK+ dataset (controllable environment) are shown in (a), whereas those using CelebV-HQ dataset (non-controllable environment) are shown in (b). On the other hand, the generated 3D face with FLAME is depicted by setting the expression parameter to zero in (c). Note that subtle differences exist in the image quality between the three generated cases. TABLE IX : RESULT OBTAINED WHEN NED IS COMBINED WITH ROME OR CVTHEAD FOR TEXTURE GENERATION ON CK+ AND CELEBV-HQ, RESPECTIVELY. Dataset Method L â†“ 1 {}_{1}\\downarrow PSNR â†‘ {\\uparrow} LPIPS â†“ {\\downarrow} MS_SSIM â†‘ {\\uparrow} CK+ NED + ROME 0.193 10.207 0.516 0.348 NED + CVTHead 0.004 35.158 0.014 0.987 CelebV-HQ NED + ROME 0.183 10.818 0.498 0.407 NED + CVTHead 0.004 36.347 0.0117 0.989 In a user study similar to that described in Section IV-D 2 , we asked subjects to determine whether the generated images display neutral expressions. Most subjects succeeded in identifying the neutral expressions produced by our model on CK+, and they had difficulties with only 4% of the samples generated by the learned model on CelebV-HQ. TABLE X : COMPARISON OF FACIAL EXPRESSION CLASSIFIERS ON CK+ AND CELEBV-HQ WITH SAMPLES ROTATED AT VARIOUS ANGLES { 0 o , Â± 10 o , Â± 15 o , Â± 30 o } \\{0^{o},\\pm 10^{o},\\pm 15^{o},\\pm 30^{o}\\} 0 o Â± \\pm 10 o Â± \\pm 15 o Â± \\pm 30 o Dataset Method Acc 1 â†‘ \\uparrow G-mean â†‘ \\uparrow Acc 1 â†‘ \\uparrow G-mean â†‘ \\uparrow Acc 1 â†‘ \\uparrow G-mean â†‘ \\uparrow Acc 1 â†‘ \\uparrow G-mean â†‘ \\uparrow MEK [ 43 ] 100 100 86.41 76.20 76.01 52.16 32.56 0.0 CK+ ResNet-101 [ 14 ] 99.56 99.4 84.40 73.28 79.50 67.6 75.29 33.39 ResNet-101 with RFL 99.69 99.69 87.18 75.85 83.66 66.13 77.48 31.5 MEK [ 43 ] 99.75 99.83 60.49 55.07 49.08 33.84 32.05 9.64 CelebV-HQ ResNet-101 [ 14 ] 37 12.77 35.77 13.39 34.97 0.0 28.92 0.0 ResNet-101 with RFL 93.67 95.04 58.54 49.92 51.53 41.22 40.71 26.80\n\nIV-G Facial Expression Classification\nIV-G 1 Implementation For ResNet-101 [ 14 ] and ResNet-101 with RFL [ 5 ] , the learning rate was 1 â€‹ e âˆ’ 4 1e^{-4} and the batch size was 64. Adam was used to optimize the models with 50 epochs per cycle. For the MEK model, the configuration in [ 43 ] was used to optimize the training process for both the CK+ and CelebV-HQ datasets. IV-G 2 Analysis Based on the results in Table X , ResNet-101 with RFL was more stable compared to ResNet-101 or MEK in terms of their classification performance, suggesting that ResNet-101 with RFL can handle both imbalanced datasets and rotational transformations fairly well. In particular, MEK achieved the best result on the original images although its performance was affected by rotated images. ResNet-101 with RFL demonstrated more stable performance than ResNet-101 and MEK on CK+. For real-world scenarios such as those in the CelebV-HQ dataset, we obtained similar results. These findings indicate that the performance of facial expression classifiers deteriorates for faces that are rotated by Â± 15 \\pm 15 degrees or more. In addition, MEK performs better for the original datasets, but worse for the augmented datasets. The performance of ResNet-101 is high on datasets with low imbalance rates, whereas it is low on datasets with high imbalance rates. Meanwhile, ResNet-101 with RFL delivers stable performance on imbalanced and rotated datasets. Based on this result, we adopted ResNet-101 by default during evaluation. (a) (b) Figure 18 : Generated 3D faces (a) by changing the template sentences and (b) a few unsuccessful cases.\n\nIV-G 1 Implementation\nFor ResNet-101 [ 14 ] and ResNet-101 with RFL [ 5 ] , the learning rate was 1 â€‹ e âˆ’ 4 1e^{-4} and the batch size was 64. Adam was used to optimize the models with 50 epochs per cycle. For the MEK model, the configuration in [ 43 ] was used to optimize the training process for both the CK+ and CelebV-HQ datasets.\n\nIV-G 2 Analysis\nBased on the results in Table X , ResNet-101 with RFL was more stable compared to ResNet-101 or MEK in terms of their classification performance, suggesting that ResNet-101 with RFL can handle both imbalanced datasets and rotational transformations fairly well. In particular, MEK achieved the best result on the original images although its performance was affected by rotated images. ResNet-101 with RFL demonstrated more stable performance than ResNet-101 and MEK on CK+. For real-world scenarios such as those in the CelebV-HQ dataset, we obtained similar results. These findings indicate that the performance of facial expression classifiers deteriorates for faces that are rotated by Â± 15 \\pm 15 degrees or more. In addition, MEK performs better for the original datasets, but worse for the augmented datasets. The performance of ResNet-101 is high on datasets with low imbalance rates, whereas it is low on datasets with high imbalance rates. Meanwhile, ResNet-101 with RFL delivers stable performance on imbalanced and rotated datasets. Based on this result, we adopted ResNet-101 by default during evaluation. (a) (b) Figure 18 : Generated 3D faces (a) by changing the template sentences and (b) a few unsuccessful cases.\n\nIV-H Discussion\nIV-H 1 Diversity of Facial Expression Sequences Our framework is capable of animating various facial expressions guided by instruction and facial images with pose variation, as illustrated in Fig 12 . Moreover, it can generate facial expressions corresponding to both controlled and real-world environments, enabling the generation of a diverse range of expressions for creating a long video. IV-H 2 Results on Instructions with Changes in Template Sentences Fig. 18(a) shows the outcomes obtained by changing the template sentences. The result suggests that our model can successfully generate facial expressions that closely resemble specific emotions described in the instruction. IV-H 3 Failure Cases Although our model performs well in many cases, the model failed in certain instances as exemplified in Fig 18(b) . In the top row, significant pose variation in the input image leads to regions of the shoulders being missing from the resulting image due to constraints within the pre-trained face-rendering model. Likewise, images in which regions of the head and those containing hair were missing from the source image also led to the lack of information in the face rendering process because of this constraint, as shown in the bottom row. In the second row, the generated result is restricted by the size of the expression vocabulary, causing a scared face to be transformed into an angry face.\n\nIV-H 1 Diversity of Facial Expression Sequences\nOur framework is capable of animating various facial expressions guided by instruction and facial images with pose variation, as illustrated in Fig 12 . Moreover, it can generate facial expressions corresponding to both controlled and real-world environments, enabling the generation of a diverse range of expressions for creating a long video.\n\nIV-H 2 Results on Instructions with Changes in Template Sentences\nFig. 18(a) shows the outcomes obtained by changing the template sentences. The result suggests that our model can successfully generate facial expressions that closely resemble specific emotions described in the instruction.\n\nIV-H 3 Failure Cases\nAlthough our model performs well in many cases, the model failed in certain instances as exemplified in Fig 18(b) . In the top row, significant pose variation in the input image leads to regions of the shoulders being missing from the resulting image due to constraints within the pre-trained face-rendering model. Likewise, images in which regions of the head and those containing hair were missing from the source image also led to the lack of information in the face rendering process because of this constraint, as shown in the bottom row. In the second row, the generated result is restricted by the size of the expression vocabulary, causing a scared face to be transformed into an angry face.\n\nV Conclusion\nIn this study, we present a novel framework for generating 3D facial expressions from an RGB image and animating facial transitions between two expressions as specified by entering text instructions. First, FET was introduced to produce a sequence of facial expressions. The Instruction-Driven Facial Expression Decomposer was designed to learn from multimodal data and capture correlations between textual descriptions and facial expression features. We proposed an Instruction to Facial Expression Transition model to generate target facial expressions guided by a single instruction. Our framework integrates FET and a pre-trained face rendering model to generate facial appearances aligned with the expected expression sequence. Furthermore, our framework can be expanded to include the generation of a neutral expression, thereby enabling the creation of diverse expressions in a video, thus closely simulating facial expression behaviors in real-world scenarios. Extensive experiments were conducted on the CK+ and CelebV-HQ datasets to demonstrate the effectiveness of our framework. Although our proposed approach demonstrates positive outcomes in terms of controlling the facial expression of an RGB image according to the provided instructions, it does have certain limitations. Similar to previous studies [ 18 , 26 ] in which facial parameters were used, the performance of our model relies on the precision of 3D face coefficients, especially using DECA [ 9 ] within our configurations since it may encounter difficulty, often seen in challenging scenarios, in disentangling facial factors. Also, even though linear interpolation can maintain temporal consistency, it often compromises the realism of the synthesized videos. Although the present work uses either basic facial expressions or two designated expressions, the vocabulary number can be easily expanded by combining a Large Language Model (LLM) with our model. Given that text prompting is a powerful tool by which various emotional states can be expressed on a 3D avatar via our framework, we expect it to find various applications in the future.\n\nAcknowledgment\nThis work was supported by the Information Technology Research Center (ITRC) support program (IITP-2022-RS-2022-00156354) and a Korean government grant (MSIT) (No.RS-2019-II190231) from the Institute of Information & Communications Technology Planning & Evaluation (IITP) as well as by the Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (2020R1A6A1A03038540).\n\nReferences\n[1] S. Aneja, J. Thies, A. Dai, and M. NieÃŸner (2023) ClipFace: text-guided editing of textured 3d morphable models . In SIGGRAPH â€™23 Conference Proceedings , Cited by: Â§I , Â§I . [2] M. C. Buehler, A. Meka, G. Li, T. Beeler, and O. Hilliges (2021) VariTex: variational neural face textures . In Proceedings of the IEEE/CVF International Conference on Computer Vision , Cited by: Â§I , Â§ II-B . [3] C. Chen, Q. Fan, and R. Panda (2021) CrossViT: cross-attention multi-scale vision transformer for image classification . 2021 IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 347â€“356 . Cited by: Â§ III-B 1 . [4] Y. Chen and S. Xiong (2023) Rethinking one-shot face reenactment: a spatialâ€“temporal reconstruction view . Knowledge Based Systems 277 ( C ). Cited by: Â§I . [5] Y. Cui, M. Jia, T. Lin, Y. Song, and S. J. Belongie (2019) Class-balanced loss based on effective number of samples . 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 9260â€“9269 . Cited by: Â§ IV-C , Â§ IV-G 1 . [6] R. DanÄ›Äek, K. Chhatre, S. Tripathi, Y. Wen, M. Black, and T. Bolkart (2023) Emotional speech-driven animation with content-emotion disentanglement . In SIGGRAPH Asia 2023 Conference Papers , Cited by: Â§ II-A , Â§ II-A , Â§ III-B 1 . [7] H. Ding, K. Sricharan, and R. Chellappa (2018) Exprgan: facial expression editing with controllable expression intensity . AAAI . Cited by: Â§I . [8] M. C. Doukas, S. Zafeiriou, and V. Sharmanska (2021-10) HeadGAN: one-shot neural head synthesis and editing . In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 14398â€“14407 . Cited by: Â§I . [9] Y. Feng, H. Feng, M. J. Black, and T. Bolkart (2021) Learning an animatable detailed 3D face model from in-the-wild images . In ACM Transactions on Graphics, (Proc. SIGGRAPH) , Vol. 40 . Cited by: Â§ III-B 1 , Â§ III-B 1 , Â§V . [10] P. Grassal, M. Prinzler, T. Leistner, C. Rother, M. NieÃŸner, and J. Thies (2022) Neural head avatars from monocular rgb videos . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 18653â€“18664 . Cited by: Â§I . [11] X. Gu, C. Wen, W. Ye, J. Song, and Y. Gao (2023) Seer: language instructed video prediction with latent diffusion models . ICLR 2024 . Cited by: Â§I . [12] J. Guo, H. Manukyan, C. Yang, C. Wang, L. Khachatryan, S. Navasardyan, S. Song, H. Shi, and G. Huang (2023) FaceCLIP: facial image-to-video translation via a brief text description . IEEE Transactions on Circuits and Systems for Video Technology , pp. 1â€“1 . Cited by: Â§I . [13] T. Hang, H. Yang, B. Liu, J. Fu, X. Geng, and B. Guo (2023) Language-guided face animation by recurrent stylegan-based generator . IEEE Transactions on Multimedia 25 ( ), pp. 1â€“12 . Cited by: Â§I , Â§I . [14] K. He, X. Zhang, S. Ren, and J. Sun (2016) Deep Residual Learning for Image Recognition . In Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition , CVPR â€™16 , pp. 770â€“778 . External Links: Document , ISSN 1063-6919 Cited by: Â§ IV-C , Â§ IV-G 1 , TABLE X , TABLE X . [15] Z. Huang, K. C.K. Chan, Y. Jiang, and Z. Liu (2023) Collaborative diffusion for multi-modal face generation and editing . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , Cited by: Â§I . [16] S. Hwang, J. Hyung, D. Kim, M. Kim, and J. Choo (2023-10) FaceCLIPNeRF: text-driven 3d face manipulation using deformable neural radiance fields . In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 3469â€“3479 . Cited by: Â§I , Â§I . [17] Y. Jiang, Z. Huang, X. Pan, C. C. Loy, and Z. Liu (2021) Talk-to-edit: fine-grained facial editing via dialog . In Proceedings of the IEEE/CVF International Conference on Computer Vision , Cited by: Â§I . [18] T. Khakhulin, V. Sklyarova, V. Lempitsky, and E. Zakharov (2022) Realistic one-shot mesh-based head avatars . In European Conference of Computer vision (ECCV) , Cited by: Â§I , Â§ II-B , Â§ III-B 2 , Â§ III-B 2 , Figure 11 , Figure 11 , Â§ IV-B , Â§ IV-D 1 , Â§V . [19] M. Kim, F. Liu, A. Jain, and X. Liu (2023-06) DCFace: synthetic face generation with dual condition diffusion model . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 12715â€“12725 . Cited by: Â§I . [20] D. P. Kingma and J. Ba (2014) Adam: a method for stochastic optimization . ICLR abs/1412.6980 . Cited by: Â§ IV-B . [21] Y. Lee, T. Choi, H. Go, H. Lee, S. Cho, and J. Kim (2023) Exp-gan: 3d-aware facial image generation with expression control . In Computer Vision â€“ ACCV 2022: 16th Asian Conference on Computer Vision , pp. 151â€“167 . External Links: ISBN 978-3-031-26292-0 Cited by: Â§ II-B . [22] T. Li, T. Bolkart, M. J. Black, H. Li, and J. Romero (2017) Learning a model of facial shape and expression from 4d scans . ACM Transactions on Graphics (TOG) 36 , pp. 1 â€“ 17 . Cited by: Â§I , Â§ II-A , Â§ III-B 1 . [23] X. Li, D. Zhang, M. Li, and D. Lee (2023) Accurate head pose estimation using image rectification and a lightweight convolutional neural network . IEEE Transactions on Multimedia 25 ( ), pp. 2239â€“2251 . Cited by: Â§ IV-A 2 . [24] K. Lin, A. Trevithick, K. Cheng, M. Sarkis, M. Ghafoorian, N. Bi, G. Reitmayr, and R. Ramamoorthi (2023) PVP: Personalized Video Prior for Editable Dynamic Portraits using StyleGAN . Computer Graphics Forum 42 ( 4 ), pp. 13 pages . Cited by: Â§ III-B 1 . [25] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and I. Matthews (2010) The extended cohn-kanade dataset (ck+): a complete dataset for action unit and emotion-specified expression . In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops , Vol. , pp. 94â€“101 . Cited by: Â§ IV-A 1 . [26] H. Ma, T. Zhang, S. Sun, X. Yan, K. Han, and X. Xie (2024) CVTHead: one-shot controllable head avatar with vertex-feature transformer . IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) . Cited by: Â§I , Â§ II-B , Â§ III-B 2 , Figure 11 , Figure 11 , Â§ IV-B , Â§ IV-D 1 , Â§V . [27] N. Otberdout, C. Ferrari, M. Daoudi, S. Berretti, and A. Bimbo (2023) Generating complex 4d expression transitions by learning face landmark trajectories . IEEE Transactions on Affective Computing . Cited by: Â§I , Â§ II-A , Â§ II-A , Â§ IV-C . [28] N. Otberdout, C. Ferrari, M. Daoudi, S. Berritti, and A. Del Bimbo (2022-06) Sparse to dense dynamic 3d facial expression generation . In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , Cited by: Â§ II-A . [29] F. Paraperas Papantoniou, P. P. Filntisis, P. Maragos, and A. Roussos (2022) Neural emotion director: speech-preserving semantic control of facial expressions in â€in-the-wildâ€ videos . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , Cited by: Â§I . [30] O. Patashnik, Z. Wu, E. Shechtman, D. Cohen-Or, and D. Lischinski (2021-10) StyleCLIP: text-driven manipulation of stylegan imagery . In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 2085â€“2094 . Cited by: Â§I . [31] B. Peng, H. Fan, W. Wang, J. Dong, and S. Lyu (2022) A unified framework for high fidelity face swap and expression reenactment . IEEE Transactions on Circuits and Systems for Video Technology 32 ( 6 ), pp. 3673â€“3684 . Cited by: Â§I . [32] S. Qian, T. Kirschstein, L. Schoneveld, D. Davoli, S. Giebenhain, and M. NieÃŸner (2024) GaussianAvatars: photorealistic head avatars with rigged 3d gaussians . In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , Cited by: Â§I . [33] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. (2021) Learning transferable visual models from natural language supervision . In International conference on machine learning , pp. 8748â€“8763 . Cited by: Â§ III-B 1 . [34] J. Russell and J. M. F. dez Dols (1997) Reading emotions from and into faces: resurrecting dimensional-contextual perspective . The psychology of facial expression , pp. 295â€“320 . Note: Cambridge University Press Cited by: Â§ IV-F . [35] K. R. Scherer (2001) Appraisal considered as a process of multilevel sequential checking. . Oxford University Press , pp. 92 â€“ 120 . Cited by: Â§ IV-F . [36] S. Su, J. Zhu, L. Gao, and J. Song (2024) Utilizing greedy nature for multimodal conditional image synthesis in transformers . IEEE Transactions on Multimedia 26 ( ), pp. 2354â€“2366 . Cited by: Â§I . [37] T. Sutter, I. Daunhawer, and J. Vogt (2020) Multimodal generative learning utilizing jensen-shannon-divergence . Vol. 33 , pp. 6100â€“6110 . Cited by: Â§I . [38] M. Suzuki and Y. Matsuo (2022) A survey of multimodal deep generative models . Advanced Robotics 36 , pp. 261 â€“ 278 . Cited by: Â§I . [39] G. Tevet, B. Gordon, A. Hertz, A. H. Bermano, and D. Cohen-Or (2022) Motionclip: exposing human motion generation to clip space . In Computer Visionâ€“ECCV 2022: 17th European Conference , pp. 358â€“374 . Cited by: Â§ II-A , Â§ IV-B . [40] Y. Wu, S. Xu, J. Xiang, F. Wei, Q. Chen, J. Yang, and X. Tong (2023) AniPortraitGAN: animatable 3d portrait generation from 2d image collections . In SIGGRAPH Asia 2023 Conference Proceedings , Cited by: Â§I . [41] W. Xia, Y. Yang, J. Xue, and B. Wu (2021) TediGAN: text-guided diverse face image generation and manipulation . In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , Cited by: Â§I . [42] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang (2018) The unreasonable effectiveness of deep features as a perceptual metric . In CVPR , Cited by: Â§ IV-D 1 . [43] Y. Zhang, Y. Li, lixiong Qin, X. Liu, and W. Deng (2023) Leave no stone unturned: mine extra knowledge for imbalanced facial expression recognition . In Thirty-seventh Conference on Neural Information Processing Systems , Cited by: Â§ IV-C , Â§ IV-G 1 , TABLE X , TABLE X . [44] H. Zhu, W. Wu, W. Zhu, L. Jiang, S. Tang, L. Zhang, Z. Liu, and C. C. Loy (2022) CelebV-HQ: a large-scale video facial attributes dataset . In ECCV , Cited by: Â§ IV-A 2 . [45] W. Zielonka, T. Bolkart, and J. Thies (2023-06) Instant volumetric head avatars . In IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) , External Links: Document Cited by: Â§I . [46] K. Zou, S. Faisan, B. Yu, S. Valette, and H. Seo (2023) 4D facial expression diffusion model . External Links: 2303.16611 Cited by: Â§I , Â§ II-A , Â§ II-A , Â§ IV-C .",
      "references": [
        {
          "raw_text": "[1] S. Aneja, J. Thies, A. Dai, and M. NieÃŸner (2023) ClipFace: text-guided editing of textured 3d morphable models . In SIGGRAPH â€™23 Conference Proceedings , Cited by: Â§I , Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p2.1",
            "https://arxiv.org/html/2601.08179v1#S1.p3.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[2] M. C. Buehler, A. Meka, G. Li, T. Beeler, and O. Hilliges (2021) VariTex: variational neural face textures . In Proceedings of the IEEE/CVF International Conference on Computer Vision , Cited by: Â§I , Â§ II-B .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p2.1",
            "https://arxiv.org/html/2601.08179v1#S2.SS2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[3] C. Chen, Q. Fan, and R. Panda (2021) CrossViT: cross-attention multi-scale vision transformer for image classification . 2021 IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 347â€“356 . Cited by: Â§ III-B 1 .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S3.SS2.SSS1.p2.7"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[4] Y. Chen and S. Xiong (2023) Rethinking one-shot face reenactment: a spatialâ€“temporal reconstruction view . Knowledge Based Systems 277 ( C ). Cited by: Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[5] Y. Cui, M. Jia, T. Lin, Y. Song, and S. J. Belongie (2019) Class-balanced loss based on effective number of samples . 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 9260â€“9269 . Cited by: Â§ IV-C , Â§ IV-G 1 .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S4.SS3.p2.2",
            "https://arxiv.org/html/2601.08179v1#S4.SS7.SSS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[6] R. DanÄ›Äek, K. Chhatre, S. Tripathi, Y. Wen, M. Black, and T. Bolkart (2023) Emotional speech-driven animation with content-emotion disentanglement . In SIGGRAPH Asia 2023 Conference Papers , Cited by: Â§ II-A , Â§ II-A , Â§ III-B 1 .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S2.SS1.p1.1",
            "https://arxiv.org/html/2601.08179v1#S2.SS1.p2.1",
            "https://arxiv.org/html/2601.08179v1#S3.SS2.SSS1.p2.7"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[7] H. Ding, K. Sricharan, and R. Chellappa (2018) Exprgan: facial expression editing with controllable expression intensity . AAAI . Cited by: Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[8] M. C. Doukas, S. Zafeiriou, and V. Sharmanska (2021-10) HeadGAN: one-shot neural head synthesis and editing . In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 14398â€“14407 . Cited by: Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p2.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[9] Y. Feng, H. Feng, M. J. Black, and T. Bolkart (2021) Learning an animatable detailed 3D face model from in-the-wild images . In ACM Transactions on Graphics, (Proc. SIGGRAPH) , Vol. 40 . Cited by: Â§ III-B 1 , Â§ III-B 1 , Â§V .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S3.SS2.SSS1.p7.8",
            "https://arxiv.org/html/2601.08179v1#S3.SS2.SSS1.p8.12",
            "https://arxiv.org/html/2601.08179v1#S5.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[10] P. Grassal, M. Prinzler, T. Leistner, C. Rother, M. NieÃŸner, and J. Thies (2022) Neural head avatars from monocular rgb videos . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 18653â€“18664 . Cited by: Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p2.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[11] X. Gu, C. Wen, W. Ye, J. Song, and Y. Gao (2023) Seer: language instructed video prediction with latent diffusion models . ICLR 2024 . Cited by: Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p3.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[12] J. Guo, H. Manukyan, C. Yang, C. Wang, L. Khachatryan, S. Navasardyan, S. Song, H. Shi, and G. Huang (2023) FaceCLIP: facial image-to-video translation via a brief text description . IEEE Transactions on Circuits and Systems for Video Technology , pp. 1â€“1 . Cited by: Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[13] T. Hang, H. Yang, B. Liu, J. Fu, X. Geng, and B. Guo (2023) Language-guided face animation by recurrent stylegan-based generator . IEEE Transactions on Multimedia 25 ( ), pp. 1â€“12 . Cited by: Â§I , Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p1.1",
            "https://arxiv.org/html/2601.08179v1#S1.p3.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[14] K. He, X. Zhang, S. Ren, and J. Sun (2016) Deep Residual Learning for Image Recognition . In Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition , CVPR â€™16 , pp. 770â€“778 . External Links: Document , ISSN 1063-6919 Cited by: Â§ IV-C , Â§ IV-G 1 , TABLE X , TABLE X .",
          "urls": [
            "https://dx.doi.org/10.1109/CVPR.2016.90",
            "https://arxiv.org/html/2601.08179v1#S4.SS3.p2.2",
            "https://arxiv.org/html/2601.08179v1#S4.SS7.SSS1.p1.1",
            "https://arxiv.org/html/2601.08179v1#S4.T10.21.21.2.2",
            "https://arxiv.org/html/2601.08179v1#S4.T10.21.24.5.2"
          ],
          "dois": [
            "https://dx.doi.org/10.1109/CVPR.2016.90"
          ],
          "pdf_links": []
        },
        {
          "raw_text": "[15] Z. Huang, K. C.K. Chan, Y. Jiang, and Z. Liu (2023) Collaborative diffusion for multi-modal face generation and editing . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , Cited by: Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[16] S. Hwang, J. Hyung, D. Kim, M. Kim, and J. Choo (2023-10) FaceCLIPNeRF: text-driven 3d face manipulation using deformable neural radiance fields . In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 3469â€“3479 . Cited by: Â§I , Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p2.1",
            "https://arxiv.org/html/2601.08179v1#S1.p3.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[17] Y. Jiang, Z. Huang, X. Pan, C. C. Loy, and Z. Liu (2021) Talk-to-edit: fine-grained facial editing via dialog . In Proceedings of the IEEE/CVF International Conference on Computer Vision , Cited by: Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[18] T. Khakhulin, V. Sklyarova, V. Lempitsky, and E. Zakharov (2022) Realistic one-shot mesh-based head avatars . In European Conference of Computer vision (ECCV) , Cited by: Â§I , Â§ II-B , Â§ III-B 2 , Â§ III-B 2 , Figure 11 , Figure 11 , Â§ IV-B , Â§ IV-D 1 , Â§V .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p1.1",
            "https://arxiv.org/html/2601.08179v1#S2.SS2.p1.1",
            "https://arxiv.org/html/2601.08179v1#S3.SS2.SSS2.p2.3",
            "https://arxiv.org/html/2601.08179v1#S3.SS2.SSS2.p3.4",
            "https://arxiv.org/html/2601.08179v1#S4.F11",
            "https://arxiv.org/html/2601.08179v1#S4.F11.3.2",
            "https://arxiv.org/html/2601.08179v1#S4.SS2.p1.1",
            "https://arxiv.org/html/2601.08179v1#S4.SS4.SSS1.p4.1",
            "https://arxiv.org/html/2601.08179v1#S5.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[19] M. Kim, F. Liu, A. Jain, and X. Liu (2023-06) DCFace: synthetic face generation with dual condition diffusion model . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 12715â€“12725 . Cited by: Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[20] D. P. Kingma and J. Ba (2014) Adam: a method for stochastic optimization . ICLR abs/1412.6980 . Cited by: Â§ IV-B .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S4.SS2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[21] Y. Lee, T. Choi, H. Go, H. Lee, S. Cho, and J. Kim (2023) Exp-gan: 3d-aware facial image generation with expression control . In Computer Vision â€“ ACCV 2022: 16th Asian Conference on Computer Vision , pp. 151â€“167 . External Links: ISBN 978-3-031-26292-0 Cited by: Â§ II-B .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S2.SS2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[22] T. Li, T. Bolkart, M. J. Black, H. Li, and J. Romero (2017) Learning a model of facial shape and expression from 4d scans . ACM Transactions on Graphics (TOG) 36 , pp. 1 â€“ 17 . Cited by: Â§I , Â§ II-A , Â§ III-B 1 .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p3.1",
            "https://arxiv.org/html/2601.08179v1#S2.SS1.p1.1",
            "https://arxiv.org/html/2601.08179v1#S3.SS2.SSS1.p7.8"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[23] X. Li, D. Zhang, M. Li, and D. Lee (2023) Accurate head pose estimation using image rectification and a lightweight convolutional neural network . IEEE Transactions on Multimedia 25 ( ), pp. 2239â€“2251 . Cited by: Â§ IV-A 2 .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S4.SS1.SSS2.p1.2"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[24] K. Lin, A. Trevithick, K. Cheng, M. Sarkis, M. Ghafoorian, N. Bi, G. Reitmayr, and R. Ramamoorthi (2023) PVP: Personalized Video Prior for Editable Dynamic Portraits using StyleGAN . Computer Graphics Forum 42 ( 4 ), pp. 13 pages . Cited by: Â§ III-B 1 .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S3.SS2.SSS1.p2.7"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[25] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and I. Matthews (2010) The extended cohn-kanade dataset (ck+): a complete dataset for action unit and emotion-specified expression . In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops , Vol. , pp. 94â€“101 . Cited by: Â§ IV-A 1 .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S4.SS1.SSS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[26] H. Ma, T. Zhang, S. Sun, X. Yan, K. Han, and X. Xie (2024) CVTHead: one-shot controllable head avatar with vertex-feature transformer . IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) . Cited by: Â§I , Â§ II-B , Â§ III-B 2 , Figure 11 , Figure 11 , Â§ IV-B , Â§ IV-D 1 , Â§V .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p1.1",
            "https://arxiv.org/html/2601.08179v1#S2.SS2.p1.1",
            "https://arxiv.org/html/2601.08179v1#S3.SS2.SSS2.p3.4",
            "https://arxiv.org/html/2601.08179v1#S4.F11",
            "https://arxiv.org/html/2601.08179v1#S4.F11.3.2",
            "https://arxiv.org/html/2601.08179v1#S4.SS2.p1.1",
            "https://arxiv.org/html/2601.08179v1#S4.SS4.SSS1.p4.1",
            "https://arxiv.org/html/2601.08179v1#S5.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[27] N. Otberdout, C. Ferrari, M. Daoudi, S. Berretti, and A. Bimbo (2023) Generating complex 4d expression transitions by learning face landmark trajectories . IEEE Transactions on Affective Computing . Cited by: Â§I , Â§ II-A , Â§ II-A , Â§ IV-C .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p2.1",
            "https://arxiv.org/html/2601.08179v1#S2.SS1.p1.1",
            "https://arxiv.org/html/2601.08179v1#S2.SS1.p2.1",
            "https://arxiv.org/html/2601.08179v1#S4.SS3.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[28] N. Otberdout, C. Ferrari, M. Daoudi, S. Berritti, and A. Del Bimbo (2022-06) Sparse to dense dynamic 3d facial expression generation . In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , Cited by: Â§ II-A .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S2.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[29] F. Paraperas Papantoniou, P. P. Filntisis, P. Maragos, and A. Roussos (2022) Neural emotion director: speech-preserving semantic control of facial expressions in â€in-the-wildâ€ videos . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , Cited by: Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p2.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[30] O. Patashnik, Z. Wu, E. Shechtman, D. Cohen-Or, and D. Lischinski (2021-10) StyleCLIP: text-driven manipulation of stylegan imagery . In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 2085â€“2094 . Cited by: Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p3.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[31] B. Peng, H. Fan, W. Wang, J. Dong, and S. Lyu (2022) A unified framework for high fidelity face swap and expression reenactment . IEEE Transactions on Circuits and Systems for Video Technology 32 ( 6 ), pp. 3673â€“3684 . Cited by: Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[32] S. Qian, T. Kirschstein, L. Schoneveld, D. Davoli, S. Giebenhain, and M. NieÃŸner (2024) GaussianAvatars: photorealistic head avatars with rigged 3d gaussians . In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , Cited by: Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p2.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[33] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. (2021) Learning transferable visual models from natural language supervision . In International conference on machine learning , pp. 8748â€“8763 . Cited by: Â§ III-B 1 .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S3.SS2.SSS1.p2.7"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[34] J. Russell and J. M. F. dez Dols (1997) Reading emotions from and into faces: resurrecting dimensional-contextual perspective . The psychology of facial expression , pp. 295â€“320 . Note: Cambridge University Press Cited by: Â§ IV-F .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S4.SS6.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[35] K. R. Scherer (2001) Appraisal considered as a process of multilevel sequential checking. . Oxford University Press , pp. 92 â€“ 120 . Cited by: Â§ IV-F .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S4.SS6.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[36] S. Su, J. Zhu, L. Gao, and J. Song (2024) Utilizing greedy nature for multimodal conditional image synthesis in transformers . IEEE Transactions on Multimedia 26 ( ), pp. 2354â€“2366 . Cited by: Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[37] T. Sutter, I. Daunhawer, and J. Vogt (2020) Multimodal generative learning utilizing jensen-shannon-divergence . Vol. 33 , pp. 6100â€“6110 . Cited by: Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p3.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[38] M. Suzuki and Y. Matsuo (2022) A survey of multimodal deep generative models . Advanced Robotics 36 , pp. 261 â€“ 278 . Cited by: Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p3.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[39] G. Tevet, B. Gordon, A. Hertz, A. H. Bermano, and D. Cohen-Or (2022) Motionclip: exposing human motion generation to clip space . In Computer Visionâ€“ECCV 2022: 17th European Conference , pp. 358â€“374 . Cited by: Â§ II-A , Â§ IV-B .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S2.SS1.p1.1",
            "https://arxiv.org/html/2601.08179v1#S4.SS2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[40] Y. Wu, S. Xu, J. Xiang, F. Wei, Q. Chen, J. Yang, and X. Tong (2023) AniPortraitGAN: animatable 3d portrait generation from 2d image collections . In SIGGRAPH Asia 2023 Conference Proceedings , Cited by: Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p2.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[41] W. Xia, Y. Yang, J. Xue, and B. Wu (2021) TediGAN: text-guided diverse face image generation and manipulation . In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , Cited by: Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[42] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang (2018) The unreasonable effectiveness of deep features as a perceptual metric . In CVPR , Cited by: Â§ IV-D 1 .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S4.SS4.SSS1.p4.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[43] Y. Zhang, Y. Li, lixiong Qin, X. Liu, and W. Deng (2023) Leave no stone unturned: mine extra knowledge for imbalanced facial expression recognition . In Thirty-seventh Conference on Neural Information Processing Systems , Cited by: Â§ IV-C , Â§ IV-G 1 , TABLE X , TABLE X .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S4.SS3.p2.2",
            "https://arxiv.org/html/2601.08179v1#S4.SS7.SSS1.p1.1",
            "https://arxiv.org/html/2601.08179v1#S4.T10.21.20.1.2",
            "https://arxiv.org/html/2601.08179v1#S4.T10.21.23.4.2"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[44] H. Zhu, W. Wu, W. Zhu, L. Jiang, S. Tang, L. Zhang, Z. Liu, and C. C. Loy (2022) CelebV-HQ: a large-scale video facial attributes dataset . In ECCV , Cited by: Â§ IV-A 2 .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S4.SS1.SSS2.p1.2"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[45] W. Zielonka, T. Bolkart, and J. Thies (2023-06) Instant volumetric head avatars . In IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) , External Links: Document Cited by: Â§I .",
          "urls": [
            "https://dx.doi.org/",
            "https://arxiv.org/html/2601.08179v1#S1.p1.1"
          ],
          "dois": [
            "https://dx.doi.org/"
          ],
          "pdf_links": []
        },
        {
          "raw_text": "[46] K. Zou, S. Faisan, B. Yu, S. Valette, and H. Seo (2023) 4D facial expression diffusion model . External Links: 2303.16611 Cited by: Â§I , Â§ II-A , Â§ II-A , Â§ IV-C .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p2.1",
            "https://arxiv.org/html/2601.08179v1#S2.SS1.p1.1",
            "https://arxiv.org/html/2601.08179v1#S2.SS1.p2.1",
            "https://arxiv.org/html/2601.08179v1#S4.SS3.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        }
      ],
      "references_dois": [
        "https://dx.doi.org/10.1109/CVPR.2016.90",
        "https://dx.doi.org/"
      ],
      "fallback_urls": [],
      "errors": [],
      "missing_fields": [
        "versions",
        "last_updated_raw",
        "published_date",
        "license"
      ],
      "url_hint_if_missing": "Champs manquants: versions, last_updated_raw, published_date, license. Tu peux vÃ©rifier ici: abs=https://arxiv.org/abs/2601.08179 | html=https://arxiv.org/html/2601.08179v1 | pdf=https://arxiv.org/pdf/2601.08179"
    },
    {
      "arxiv_id": "2601.08151",
      "title": "Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention",
      "authors": [
        "Shezheng Song",
        "Shasha Li",
        "Jie Yu"
      ],
      "abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable progress in vision-language understanding, yet how they internally integrate visual and textual information remains poorly understood. To bridge this gap, we perform a systematic layer-wise masking analysis across multiple architectures, revealing how visual-text fusion evolves within MLLMs. The results show that fusion emerges at several specific layers rather than being uniformly distributed across the network, and certain models exhibit a late-stage \"review\" phenomenon where visual signals are reactivated before output generation. Besides, we further analyze layer-wise attention evolution and observe persistent high-attention noise on irrelevant regions, along with gradually increasing attention on text-aligned areas. Guided by these insights, we introduce a training-free contrastive attention framework that models the transformation between early fusion and final layers to highlight meaningful attention shifts. Extensive experiments across various MLLMs and benchmarks validate our analysis and demonstrate that the proposed approach improves multimodal reasoning performance. Code will be released.",
      "submitted_date": "12 January, 2026",
      "abs_url": "https://arxiv.org/abs/2601.08151",
      "pdf_url": "https://arxiv.org/pdf/2601.08151",
      "doi": "",
      "versions": [],
      "last_updated_raw": "",
      "html_url": "https://arxiv.org/html/2601.08151v1",
      "published_date": "",
      "license": "",
      "sections": [
        {
          "section_index": 1,
          "heading_level": "h1",
          "heading": "Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention",
          "text": "Shezheng Song, Shasha Li, Jie Yu Abstract Recent advances in Multimodal Large Language Models (MLLMs) have achieved impressive results in vision-language tasks. However, the internal mechanism of how visual information is integrated across layers remains poorly understood. In this work, we investigate the hierarchical process of visual integration in MLLMs through a series of layer-wise masking experiments. Our findings reveal that vision-language fusion primarily occurs at several shallow layers. We also discover a review-like behavior at a deep layer, where the model re-attends to the image before producing the final output. We further analyze attention distributions and uncover a systematic bias: irrelevant image regions often receive consistently high attention across layers, resulting in noisy and suboptimal final predictions. To address this issue, we propose a training-free method that leverages contrastive attention, defined as the difference in attention maps between a pre-integrated layer and a post-integrated layer. This captures the evolving focus of the model as it integrates visual information. We apply the contrastive attention at the review layer to selectively mask irrelevant image regions, guiding the model to attend more effectively to task-relevant content without requiring additional training. Extensive experiments on multiple multimodal benchmarks demonstrate that our method significantly boosts performance, achieving new state-of-the-art results on the LLaVA series. The code will be released. 1 Introduction In recent years, Multimodal Large Language Models (MLLMs) (Liu et al. 2023 ; Bai et al. 2023 ) have shown strong performance on tasks such as visual question answering (Goyal et al. 2017 ; Marino et al. 2019 ) . However, how information flows between layers in MLLMs, especially how visual signals are gradually integrated and utilized across layers, has not been fully explored. This gap in understanding limits the development of effective training strategies and model architectures. To address this gap, we focus on the internal process of visual-textual information fusion: At which layers is visual information integrated, and how does integration in each layer affect task performance? To explore this, we design a series of layer-wise masking experiments to systematically evaluate the role of each layer in cross-modal fusion. Specifically, we apply masking to visual feature at different layers and observe the resulting changes in model performance and inference speed. A significant performance drop after masking indicates that the corresponding layer plays a critical role in visual integration, whereas a minimal impact suggests a weaker dependence on visual information. This approach provides a functional perspective on the hierarchical dynamics of vision-language fusion in MLLMs. Our experiments show that visual-text fusion mainly happens at several shallow layers. Masking visual inputs at these layers causes performance to drop nearly to zero, highlighting their role as fusion layers . This effect is consistent across different datasets and MLLMs, showing stable layer-wise patterns. As depth increases, masking has less impact, suggesting that visual information has already been integrated. Notably, a sharp drop reappears at a late layer (e.g., layer 29), indicating the model briefly revisits the image before producing the final output. We call this a review mechanism , and the corresponding layer the review layer. In addition to analyzing the functional roles of each layer through masking, we further investigate how visual attention is distributed across layers. This analysis reveals a fundamental issue in current MLLMs: Systematic bias in visual attention allocation . As illustrated in Figure 1 , the final layer often fails to accurately attend to regions in the image that are truly relevant to the question. Prior work such as Zhang et al. ( 2025 ) has noted this limitation and attempted to address it by directly using attention maps from a fixed intermediate layer (e.g., layer 14 in LLaVA (Liu et al. 2023 ) , layer 15 in InstructBLIP (Dai et al. 2023 ) ). However, relying on a single fixed layer is both rigid and suboptimal, as it overlooks the dynamic nature of visual reasoning across tasks and model architectures. Furthermore, we observe that some irrelevant image regions consistently receive high attention across layers, a phenomenon we refer to as high-attention noise . These regions tend to be activated in early layers and remain prominent in later ones. For example, as shown in Figure 1 , the yellow dashed region maintains high attention values from shallow to deep layers, despite its irrelevance to the question. This layer-to-layer consistency suggests that early attention biases may propagate through the network and contribute to the suboptimal final attention patterns. Figure 1: Contrastive attention (pre-integrated â†’ \\rightarrow last layer) showing focus shift and persistent high-attention noise. Recent work such as DoLa (Chuang et al. 2024 ) has demonstrated that contrasting internal representations across layers can better reveal what the model truly learns. Specifically, DoLa computes the difference between the logits of early and later layers, producing contrastive logits that better reflect the modelâ€™s factual knowledge compared to relying solely on the final layer output. Inspired by this idea, we extend the notion of inter-layer contrast from output space to the attention space. In particular, we observe that as MLLMs process inputs layer by layer, their attention gradually shifts from task-irrelevant regions to more semantically meaningful areas. This shift reflects how the model integrates visual cues in a progressive manner. Shallow layers tend to contain noisy or indiscriminate attention, while deeper layers refine this focus toward task-relevant content. We refer to this evolving focus as contrastive attention , which captures not the static attention at any single layer, but the transformation in attention patterns across layers that reveals what the model incrementally learns to attend to . To quantify the contrastive attention, we introduce two layer concepts: (a) the post-integrated layer , where the model has already combined visual and linguistic information and formed a representation capable of solving the task. We identify the post-integrated layer as the one immediately preceding the review layer (see section 5.4 for detailed exploration). (b) the pre-integrated layer , which represents the initial perception of the image before substantial semantic fusion occurs. The pre-integrated layer is selected from the identified fusion layers as the one whose attention map exhibits the largest Hellinger distance from the post-integrated layer, indicating the point before major vision-language integration takes place (see section 5.5 ). Based on the above analysis, we introduce a training-free approach to enhance vision-language understanding of MLLMs. Specifically, we compute the contrastive attention by comparing attention maps between the pre-integrated layer and the post-integrated layer. It captures the actual contribution of visual information as it flows through the model, revealing how visual semantics are progressively integrated. Moreover, it effectively suppresses spurious high-attention noise in later layers. Then we apply contrastive attention at review layer to mask visual regions. This selective masking reduces the influence of irrelevant content and enables the model to concentrate more effectively on task-relevant areas without additional training. Experiments show that our method significantly improves the performance of the widely used LLaVA series across six visual question answering benchmarks, achieving state-of-the-art results. Our contributions are as follows: â€¢ We conduct a systematic investigation into how visual information is integrated across layers in MLLMs. By applying layer-wise visual masking, we identify key visual-textual fusion layers and a review-like mechanism. â€¢ We propose a training-free method that exploits contrastive attention, the divergence between pre-integrated and post-integrated attention maps, to refine visual focus. â€¢ Our method consistently outperforms existing techniques and achieves state-of-the-art results across multiple multimodal benchmarks, validating its generality and effectiveness."
        },
        {
          "section_index": 2,
          "heading_level": "h6",
          "heading": "Abstract",
          "text": "Recent advances in Multimodal Large Language Models (MLLMs) have achieved impressive results in vision-language tasks. However, the internal mechanism of how visual information is integrated across layers remains poorly understood. In this work, we investigate the hierarchical process of visual integration in MLLMs through a series of layer-wise masking experiments. Our findings reveal that vision-language fusion primarily occurs at several shallow layers. We also discover a review-like behavior at a deep layer, where the model re-attends to the image before producing the final output. We further analyze attention distributions and uncover a systematic bias: irrelevant image regions often receive consistently high attention across layers, resulting in noisy and suboptimal final predictions. To address this issue, we propose a training-free method that leverages contrastive attention, defined as the difference in attention maps between a pre-integrated layer and a post-integrated layer. This captures the evolving focus of the model as it integrates visual information. We apply the contrastive attention at the review layer to selectively mask irrelevant image regions, guiding the model to attend more effectively to task-relevant content without requiring additional training. Extensive experiments on multiple multimodal benchmarks demonstrate that our method significantly boosts performance, achieving new state-of-the-art results on the LLaVA series. The code will be released."
        },
        {
          "section_index": 3,
          "heading_level": "h2",
          "heading": "1 Introduction",
          "text": "In recent years, Multimodal Large Language Models (MLLMs) (Liu et al. 2023 ; Bai et al. 2023 ) have shown strong performance on tasks such as visual question answering (Goyal et al. 2017 ; Marino et al. 2019 ) . However, how information flows between layers in MLLMs, especially how visual signals are gradually integrated and utilized across layers, has not been fully explored. This gap in understanding limits the development of effective training strategies and model architectures. To address this gap, we focus on the internal process of visual-textual information fusion: At which layers is visual information integrated, and how does integration in each layer affect task performance? To explore this, we design a series of layer-wise masking experiments to systematically evaluate the role of each layer in cross-modal fusion. Specifically, we apply masking to visual feature at different layers and observe the resulting changes in model performance and inference speed. A significant performance drop after masking indicates that the corresponding layer plays a critical role in visual integration, whereas a minimal impact suggests a weaker dependence on visual information. This approach provides a functional perspective on the hierarchical dynamics of vision-language fusion in MLLMs. Our experiments show that visual-text fusion mainly happens at several shallow layers. Masking visual inputs at these layers causes performance to drop nearly to zero, highlighting their role as fusion layers . This effect is consistent across different datasets and MLLMs, showing stable layer-wise patterns. As depth increases, masking has less impact, suggesting that visual information has already been integrated. Notably, a sharp drop reappears at a late layer (e.g., layer 29), indicating the model briefly revisits the image before producing the final output. We call this a review mechanism , and the corresponding layer the review layer. In addition to analyzing the functional roles of each layer through masking, we further investigate how visual attention is distributed across layers. This analysis reveals a fundamental issue in current MLLMs: Systematic bias in visual attention allocation . As illustrated in Figure 1 , the final layer often fails to accurately attend to regions in the image that are truly relevant to the question. Prior work such as Zhang et al. ( 2025 ) has noted this limitation and attempted to address it by directly using attention maps from a fixed intermediate layer (e.g., layer 14 in LLaVA (Liu et al. 2023 ) , layer 15 in InstructBLIP (Dai et al. 2023 ) ). However, relying on a single fixed layer is both rigid and suboptimal, as it overlooks the dynamic nature of visual reasoning across tasks and model architectures. Furthermore, we observe that some irrelevant image regions consistently receive high attention across layers, a phenomenon we refer to as high-attention noise . These regions tend to be activated in early layers and remain prominent in later ones. For example, as shown in Figure 1 , the yellow dashed region maintains high attention values from shallow to deep layers, despite its irrelevance to the question. This layer-to-layer consistency suggests that early attention biases may propagate through the network and contribute to the suboptimal final attention patterns. Figure 1: Contrastive attention (pre-integrated â†’ \\rightarrow last layer) showing focus shift and persistent high-attention noise. Recent work such as DoLa (Chuang et al. 2024 ) has demonstrated that contrasting internal representations across layers can better reveal what the model truly learns. Specifically, DoLa computes the difference between the logits of early and later layers, producing contrastive logits that better reflect the modelâ€™s factual knowledge compared to relying solely on the final layer output. Inspired by this idea, we extend the notion of inter-layer contrast from output space to the attention space. In particular, we observe that as MLLMs process inputs layer by layer, their attention gradually shifts from task-irrelevant regions to more semantically meaningful areas. This shift reflects how the model integrates visual cues in a progressive manner. Shallow layers tend to contain noisy or indiscriminate attention, while deeper layers refine this focus toward task-relevant content. We refer to this evolving focus as contrastive attention , which captures not the static attention at any single layer, but the transformation in attention patterns across layers that reveals what the model incrementally learns to attend to . To quantify the contrastive attention, we introduce two layer concepts: (a) the post-integrated layer , where the model has already combined visual and linguistic information and formed a representation capable of solving the task. We identify the post-integrated layer as the one immediately preceding the review layer (see section 5.4 for detailed exploration). (b) the pre-integrated layer , which represents the initial perception of the image before substantial semantic fusion occurs. The pre-integrated layer is selected from the identified fusion layers as the one whose attention map exhibits the largest Hellinger distance from the post-integrated layer, indicating the point before major vision-language integration takes place (see section 5.5 ). Based on the above analysis, we introduce a training-free approach to enhance vision-language understanding of MLLMs. Specifically, we compute the contrastive attention by comparing attention maps between the pre-integrated layer and the post-integrated layer. It captures the actual contribution of visual information as it flows through the model, revealing how visual semantics are progressively integrated. Moreover, it effectively suppresses spurious high-attention noise in later layers. Then we apply contrastive attention at review layer to mask visual regions. This selective masking reduces the influence of irrelevant content and enables the model to concentrate more effectively on task-relevant areas without additional training. Experiments show that our method significantly improves the performance of the widely used LLaVA series across six visual question answering benchmarks, achieving state-of-the-art results. Our contributions are as follows: â€¢ We conduct a systematic investigation into how visual information is integrated across layers in MLLMs. By applying layer-wise visual masking, we identify key visual-textual fusion layers and a review-like mechanism. â€¢ We propose a training-free method that exploits contrastive attention, the divergence between pre-integrated and post-integrated attention maps, to refine visual focus. â€¢ Our method consistently outperforms existing techniques and achieves state-of-the-art results across multiple multimodal benchmarks, validating its generality and effectiveness."
        },
        {
          "section_index": 4,
          "heading_level": "h2",
          "heading": "2 Related Work",
          "text": "Early efforts to understand how information flows within language models primarily focused on transformers in unimodal settings. For example, studies like (Cao et al. 2020 ; Frank et al. 2021 ) analyzed how attention mechanisms propagate syntactic and semantic information in transformers, while others (Aflalo et al. 2022 ; Chefer et al. 2021 ; Lyu et al. 2022 ; Stan et al. 2024 ) explored the roles of feed-forward networks and memory representation in information transformation. These works provided foundational insights into token-level dependency and hierarchical abstraction but largely focused on language-only settings. Building on this, recent studies examined how LLMs acquire knowledge across layers. Lin et al. ( 2025 ) investigates the impact of visual fusion positions on MLLM performance, comparing external and internal integration strategies for incorporating visual information. Jin et al. ( 2024 ) introduced the concept of â€œdepthâ€ in reasoning, and Ju et al. ( 2024 ) showed context is unevenly distributed across layers. Figure 2: Layer-wise masking effects on accuracy and inference time across multimodal datasets using LLaVA-1.5. Acc is the performance after completely masking the visual information at the corresponding layer. Figure 3: Layer-wise masking effects on accuracy and inference time across multimodal datasets using LLaVA-1.6. A growing body of work has recently turned to the interpretability of MLLMs. For example, Palit et al. ( 2023 ) extended causal tracing methods to MLLMs and showed that visual information progressively influences generation in later layers. Zhao et al. ( 2024 ) surveyed various interpretability techniques for MLLMs, categorizing insights at input, token, and attention levels. However, most of these efforts focus on saliency or input-output attribution and rarely dissect the internal fusion mechanism of visual features across layers. Despite these efforts, what remains largely missing is a comprehensive understanding of how visual information is gradually injected and transformed across different layers within MLLMs. Existing works often rely on single-layer attention analysis or fixed fusion strategies, overlooking the progressive, multi-stage nature of visual integration. A comprehensive understanding of how visual information is progressively injected across layers in MLLMs is still lacking. This gap limits both interpretability and usage of MLLMs in complex vision-language tasks."
        },
        {
          "section_index": 5,
          "heading_level": "h2",
          "heading": "3 Exploration of Visual Information Fusion",
          "text": "3.1 Layer-wise Visual Information Masking To better understand how multimodal information flows and is integrated across layers in MLLMs, we conduct a series of layer-wise image masking experiments on the LLaVA series. Specifically, we follow the input format used in LLaVA and identify the position of image tokens based on the special marker <image> . In LLaVA, which uses a ViT-based image encoder, the number of image tokens is 576, allowing us to locate the image segment within the input sequence. Thus, we apply a masking operation to the image tokens at each layer by directly setting their embeddings to zero, effectively removing visual information while preserving the input shape. We then assess how each layer depends on image features by measuring performance changes across datasets. A significant drop in performance after masking indicates that the layer relies heavily on image for multimodal integration. The results are shown in Figure 2 and 3 . 3.2 Analysis and Observations Layer-wise Differences in Visual-Language Fusion. Our results show that masking visual information at shallow layers (layers 0â€“4) leads to a sharp performance drop across all datasets. This suggests that image features are not yet integrated into the multimodal representations at early layers. Removing image at these layers deprives the model of essential visual cues, severely affecting its output. As the layer depth increases, the model begins to incorporate visual information gradually. The layer at which this integration completes varies by dataset: (1) For simple tasks like VQAv2, image-text fusion appears to complete earlier, as performance stabilizes after around layer 16. (2) For complex tasks such as OKVQA, the integration occurs at deeper layers, suggesting that more abstract reasoning requires longer propagation of visual context. In summary, masking the image after layer 19 results in no substantial performance degradation, indicating that the visual semantics have already been sufficiently integrated. Identifying Critical Fusion Layers. In the layer-wise masking experiments, we find that masking visual information at certain specific layers leads to a dramatic drop in model performance, sometimes approaching zero. Notably, these critical layers show strong consistency across different datasets and across various MLLMs, indicating that the same set of layers consistently exhibits such behavior under different settings. Furthermore, we observe that masking at these layers not only degrades performance but also significantly increases inference time (measured as 1 / t 1/t ). We hypothesize that this results from the disruption of efficient reasoning pathways within the model. These critical layers could be categorized into two groups: â€¢ Early fusion layers ( ğ’® \\mathcal{S} = 2, 4, 8, 11, 12, 13): These critical layers are mostly located in the shallow stages of the model, where visual information has not yet been fully fused with textual input. Masking at these layers removes essential visual cues, leading to a sharp performance drop, indicating active fusion is still in progress. â€¢ Review layer (29): After the early fusion layers, model performance stabilizes and becomes largely insensitive to visual masking, suggesting that fusion is mostly complete. However, at layer 29, masking again causes a significant drop, resembling the behavior of early fusion layers. We hypothesize that this layer performs a â€œ final visual check â€ before the model generates its output. This mirrors human behavior, where one might glance back at the image one last time before making a decision. The experiment reveals key phenomena: hierarchical differences in visual information fusion and the presence of critical fusion layer. Specifically, we observe: (1) A set of shallow layers serve as essential early fusion layers for integrating multimodal information. (2) At a deep layer (review layer), the LLM exhibits a review-like behavior, where visual information is revisited even after the integration process has been completed. This suggests that the model still relies on visual cues for final decision verification."
        },
        {
          "section_index": 6,
          "heading_level": "h3",
          "heading": "3.1 Layer-wise Visual Information Masking",
          "text": "To better understand how multimodal information flows and is integrated across layers in MLLMs, we conduct a series of layer-wise image masking experiments on the LLaVA series. Specifically, we follow the input format used in LLaVA and identify the position of image tokens based on the special marker <image> . In LLaVA, which uses a ViT-based image encoder, the number of image tokens is 576, allowing us to locate the image segment within the input sequence. Thus, we apply a masking operation to the image tokens at each layer by directly setting their embeddings to zero, effectively removing visual information while preserving the input shape. We then assess how each layer depends on image features by measuring performance changes across datasets. A significant drop in performance after masking indicates that the layer relies heavily on image for multimodal integration. The results are shown in Figure 2 and 3 ."
        },
        {
          "section_index": 7,
          "heading_level": "h3",
          "heading": "3.2 Analysis and Observations",
          "text": "Layer-wise Differences in Visual-Language Fusion. Our results show that masking visual information at shallow layers (layers 0â€“4) leads to a sharp performance drop across all datasets. This suggests that image features are not yet integrated into the multimodal representations at early layers. Removing image at these layers deprives the model of essential visual cues, severely affecting its output. As the layer depth increases, the model begins to incorporate visual information gradually. The layer at which this integration completes varies by dataset: (1) For simple tasks like VQAv2, image-text fusion appears to complete earlier, as performance stabilizes after around layer 16. (2) For complex tasks such as OKVQA, the integration occurs at deeper layers, suggesting that more abstract reasoning requires longer propagation of visual context. In summary, masking the image after layer 19 results in no substantial performance degradation, indicating that the visual semantics have already been sufficiently integrated. Identifying Critical Fusion Layers. In the layer-wise masking experiments, we find that masking visual information at certain specific layers leads to a dramatic drop in model performance, sometimes approaching zero. Notably, these critical layers show strong consistency across different datasets and across various MLLMs, indicating that the same set of layers consistently exhibits such behavior under different settings. Furthermore, we observe that masking at these layers not only degrades performance but also significantly increases inference time (measured as 1 / t 1/t ). We hypothesize that this results from the disruption of efficient reasoning pathways within the model. These critical layers could be categorized into two groups: â€¢ Early fusion layers ( ğ’® \\mathcal{S} = 2, 4, 8, 11, 12, 13): These critical layers are mostly located in the shallow stages of the model, where visual information has not yet been fully fused with textual input. Masking at these layers removes essential visual cues, leading to a sharp performance drop, indicating active fusion is still in progress. â€¢ Review layer (29): After the early fusion layers, model performance stabilizes and becomes largely insensitive to visual masking, suggesting that fusion is mostly complete. However, at layer 29, masking again causes a significant drop, resembling the behavior of early fusion layers. We hypothesize that this layer performs a â€œ final visual check â€ before the model generates its output. This mirrors human behavior, where one might glance back at the image one last time before making a decision. The experiment reveals key phenomena: hierarchical differences in visual information fusion and the presence of critical fusion layer. Specifically, we observe: (1) A set of shallow layers serve as essential early fusion layers for integrating multimodal information. (2) At a deep layer (review layer), the LLM exhibits a review-like behavior, where visual information is revisited even after the integration process has been completed. This suggests that the model still relies on visual cues for final decision verification."
        },
        {
          "section_index": 8,
          "heading_level": "h4",
          "heading": "Layer-wise Differences in Visual-Language Fusion.",
          "text": "Our results show that masking visual information at shallow layers (layers 0â€“4) leads to a sharp performance drop across all datasets. This suggests that image features are not yet integrated into the multimodal representations at early layers. Removing image at these layers deprives the model of essential visual cues, severely affecting its output. As the layer depth increases, the model begins to incorporate visual information gradually. The layer at which this integration completes varies by dataset: (1) For simple tasks like VQAv2, image-text fusion appears to complete earlier, as performance stabilizes after around layer 16. (2) For complex tasks such as OKVQA, the integration occurs at deeper layers, suggesting that more abstract reasoning requires longer propagation of visual context. In summary, masking the image after layer 19 results in no substantial performance degradation, indicating that the visual semantics have already been sufficiently integrated."
        },
        {
          "section_index": 9,
          "heading_level": "h4",
          "heading": "Identifying Critical Fusion Layers.",
          "text": "In the layer-wise masking experiments, we find that masking visual information at certain specific layers leads to a dramatic drop in model performance, sometimes approaching zero. Notably, these critical layers show strong consistency across different datasets and across various MLLMs, indicating that the same set of layers consistently exhibits such behavior under different settings. Furthermore, we observe that masking at these layers not only degrades performance but also significantly increases inference time (measured as 1 / t 1/t ). We hypothesize that this results from the disruption of efficient reasoning pathways within the model. These critical layers could be categorized into two groups: â€¢ Early fusion layers ( ğ’® \\mathcal{S} = 2, 4, 8, 11, 12, 13): These critical layers are mostly located in the shallow stages of the model, where visual information has not yet been fully fused with textual input. Masking at these layers removes essential visual cues, leading to a sharp performance drop, indicating active fusion is still in progress. â€¢ Review layer (29): After the early fusion layers, model performance stabilizes and becomes largely insensitive to visual masking, suggesting that fusion is mostly complete. However, at layer 29, masking again causes a significant drop, resembling the behavior of early fusion layers. We hypothesize that this layer performs a â€œ final visual check â€ before the model generates its output. This mirrors human behavior, where one might glance back at the image one last time before making a decision. The experiment reveals key phenomena: hierarchical differences in visual information fusion and the presence of critical fusion layer. Specifically, we observe: (1) A set of shallow layers serve as essential early fusion layers for integrating multimodal information. (2) At a deep layer (review layer), the LLM exhibits a review-like behavior, where visual information is revisited even after the integration process has been completed. This suggests that the model still relies on visual cues for final decision verification."
        },
        {
          "section_index": 10,
          "heading_level": "h2",
          "heading": "4 Method",
          "text": "As shown in Figure 4 , we compute the contrastive attention by selecting appropriate pre-integrated and post-integrated layers, in section 4.1 . The contrastive attention is defined as the difference in attention distribution, capturing how attention shifts as visual information is fused. Then, in section 4.2 , we leverage the contrastive attention to guide the review process, helping suppress irrelevant information and focus on content that is more relevant to the task. Figure 4: overview of contrastive attention and review-stage masking. 4.1 Selecting Contrastive Layers To investigate how the attention shifts during the process of task understanding, we explore two contrastive layers: the post-integrated layer and the pre-integrated layer. The post-integrated layer represents the stage at which task semantics and visual information have been fully fused, while the pre-integrated layer captures the initial perception of the image. By comparing attention maps at these two layers, we aim to reveal which image regions become increasingly aligned with task objectives. We begin by locating the post-integrated layer. Experiment shows that review behavior occurs at layer 29. Layer 28 masking has minimal impact on performance, indicating that the fusion of visual features is largely complete. Thus, we designate layer 28 as the post-integrated layer. MLLMs form an initial, task-agnostic perception of the image at shallow layers, which may lead to high-attention noise unrelated to task. To mitigate this, we propose contrastive attention, which captures the difference between the attention maps of the post-integrated layer and a pre-integrated layer. This difference is designed to suppress irrelevant high-attention regions and enhance task-relevant visual focus. We compute the Hellinger distance between each layerâ€™s attention map and that of the post-integrated layer, and select the one with the highest distance as the pre-integrated layer. We explore multiple strategies to define the candidate set ğ’ \\mathcal{C} for selecting the pre-integrated layer: (a) without any constraint, (b) constrained to shallow layers (layers 1â€“16), (c) constrained to deep layers (layers 17â€“28), and (d) constrained to a predefined early fusion set ğ’® \\mathcal{S} identified through our analysis. Details are provided in Table 3 and section 5.5 . Table 1: Effectiveness of our method across multiple MLLM baselines. MLLMs GQA VQAv2 OKVQA VizWiz TextVQA DocVQA Average BLIP-2 (Li et al. 2023 ) 41.0 41.0 45.9 19.6 42.5 - 38.00 Flamingo-80B (Alayrac et al. 2022 ) 43.3 56.3 50.6 31.6 - - 45.45 InstructBILP-8B (Dai et al. 2023 ) 49.2 51.9 49.9 34.5 50.1 9.2 40.80 IDEFICS (LaurenÃ§on et al. 2023 ) 38.4 50.9 38.4 35.5 25.9 - 37.82 LLaVA (Liu et al. 2023 ) 49.5 63.6 45.2 30.7 38.9 10.5 39.72 InternVL-MLP (Chen et al. 2024 ) 62.9 79.3 42.9 52.5 57.0 23.2 52.97 InternVL-QLLaMA (Chen et al. 2024 ) 57.7 72.3 51.0 44.5 42.1 25.0 48.77 Qwen-VL (Bai et al. 2023 ) 59.3 78.8 46.9 35.2 57.2 25.5 50.49 Qwen2-VL (Wang et al. 2024 ) 63.7 79.8 48.5 60.4 71.9 59.8 64.02 LLaVA-v1.5 (Liu et al. 2023 ) 66.0 74.0 51.6 57.8 57.2 24.6 55.19 + Ours 69.4 77.6 55.4 60.4 59.8 26.9 58.25 LLaVA-v1.6 (Liu et al. 2024 ) 69.3 79.9 52.5 59.6 72.0 65.8 66.52 + Ours 71.6 80.7 56.9 62.0 75.8 68.1 69.18 We formalize the selection process as follows: given a set of candidate attention matrices A ( i ) {A^{(i)}} , where each A ( i ) âˆˆ â„ d Ã— d A^{(i)}\\in\\mathbb{R}^{d\\times d} represents the attention weights at layer i i , we define the attention at the post-integrated layer (layer k = 28 k=28 ) as A ( k ) A^{(k)} . To identify the attention matrix that deviates most significantly from A ( k ) A^{(k)} in terms of distributional difference, we compute the Hellinger distance between each A ( i ) A^{(i)} and the reference A ( k ) A^{(k)} , and select the one with the maximal distance from candidate layers ğ’ \\mathcal{C} : i âˆ— = arg â¡ max i âˆˆ ğ’ â¡ H â€‹ ( A ( i ) , A ( k ) ) i^{*}=\\arg\\max_{i\\in\\mathcal{C}}H\\left(A^{(i)},A^{(k)}\\right) (1) where H â€‹ ( P , Q ) H(P,Q) denotes the Hellinger distance between two probability distributions P P and Q Q , defined as: H â€‹ ( P , Q ) = 1 2 â€‹ âˆ‘ j = 1 d ( p j âˆ’ q j ) 2 H(P,Q)=\\frac{1}{\\sqrt{2}}\\sqrt{\\sum_{j=1}^{d}\\left(\\sqrt{p_{j}}-\\sqrt{q_{j}}\\right)^{2}} (2) where d d denotes the number of image tokens involved in the attention distribution. 4.2 Contrastive Attention for Review As shown in Figure 4 , we leverage the observed layer-wise fusion phenomenon in MLLMs, particularly the review-like behavior, where the model reattends to the image even after the primary fusion appears to be completed. Based on this, we apply the contrastive attention to guide the masking of visual information during this review stage. Specifically, the contrastive attention is computed as the difference between the attention at pre-integrated layer ğ€ ( i âˆ— ) \\mathbf{A}^{(i^{*})} and that at post-integrated layer ğ€ ( k ) \\mathbf{A}^{(k)} . IA = | ğ€ ( k ) âˆ’ ğ€ ( i âˆ— ) | \\text{IA}=\\left|\\mathbf{A}^{(k)}-\\mathbf{A}^{(i^{*})}\\right| (3) The difference between the attention from the pre-integrated layer and the post-integrated layer captures how visual information shifts the attention during the visual integration and understanding. We leverage this signal to refine the modelâ€™s attention at review layer through a soft masking strategy. Specifically, we identify visual tokens whose contrastive attention scores fall below the Ï \\rho -th percentile and softly suppress their influence by scaling down features. Formally, let Q Ï â€‹ ( IA ) Q_{\\rho}(\\text{IA}) denote the Ï \\rho -th quantile of the contrastive attention scores IA . The masked visual features are computed as: ğ„ j masked = Î» â‹… ğ„ j , if IA â€‹ j < Q Ï â€‹ ( IA ) \\mathbf{E}_{j}^{\\text{masked}}=\\lambda\\cdot\\mathbf{E}_{j},\\quad\\text{if }\\text{IA}j<Q_{\\rho}(\\text{IA}) (4) where ğ„ j \\mathbf{E}_{j} is the embedding of the j j -th visual token, and Î» â‰ª 1 \\lambda\\ll 1 is a constant that softly downweights low-relevance regions without fully discarding them. We analyze the effect of varying the masking ratio Ï \\rho in section 5.6 ."
        },
        {
          "section_index": 11,
          "heading_level": "h3",
          "heading": "4.1 Selecting Contrastive Layers",
          "text": "To investigate how the attention shifts during the process of task understanding, we explore two contrastive layers: the post-integrated layer and the pre-integrated layer. The post-integrated layer represents the stage at which task semantics and visual information have been fully fused, while the pre-integrated layer captures the initial perception of the image. By comparing attention maps at these two layers, we aim to reveal which image regions become increasingly aligned with task objectives. We begin by locating the post-integrated layer. Experiment shows that review behavior occurs at layer 29. Layer 28 masking has minimal impact on performance, indicating that the fusion of visual features is largely complete. Thus, we designate layer 28 as the post-integrated layer. MLLMs form an initial, task-agnostic perception of the image at shallow layers, which may lead to high-attention noise unrelated to task. To mitigate this, we propose contrastive attention, which captures the difference between the attention maps of the post-integrated layer and a pre-integrated layer. This difference is designed to suppress irrelevant high-attention regions and enhance task-relevant visual focus. We compute the Hellinger distance between each layerâ€™s attention map and that of the post-integrated layer, and select the one with the highest distance as the pre-integrated layer. We explore multiple strategies to define the candidate set ğ’ \\mathcal{C} for selecting the pre-integrated layer: (a) without any constraint, (b) constrained to shallow layers (layers 1â€“16), (c) constrained to deep layers (layers 17â€“28), and (d) constrained to a predefined early fusion set ğ’® \\mathcal{S} identified through our analysis. Details are provided in Table 3 and section 5.5 . Table 1: Effectiveness of our method across multiple MLLM baselines. MLLMs GQA VQAv2 OKVQA VizWiz TextVQA DocVQA Average BLIP-2 (Li et al. 2023 ) 41.0 41.0 45.9 19.6 42.5 - 38.00 Flamingo-80B (Alayrac et al. 2022 ) 43.3 56.3 50.6 31.6 - - 45.45 InstructBILP-8B (Dai et al. 2023 ) 49.2 51.9 49.9 34.5 50.1 9.2 40.80 IDEFICS (LaurenÃ§on et al. 2023 ) 38.4 50.9 38.4 35.5 25.9 - 37.82 LLaVA (Liu et al. 2023 ) 49.5 63.6 45.2 30.7 38.9 10.5 39.72 InternVL-MLP (Chen et al. 2024 ) 62.9 79.3 42.9 52.5 57.0 23.2 52.97 InternVL-QLLaMA (Chen et al. 2024 ) 57.7 72.3 51.0 44.5 42.1 25.0 48.77 Qwen-VL (Bai et al. 2023 ) 59.3 78.8 46.9 35.2 57.2 25.5 50.49 Qwen2-VL (Wang et al. 2024 ) 63.7 79.8 48.5 60.4 71.9 59.8 64.02 LLaVA-v1.5 (Liu et al. 2023 ) 66.0 74.0 51.6 57.8 57.2 24.6 55.19 + Ours 69.4 77.6 55.4 60.4 59.8 26.9 58.25 LLaVA-v1.6 (Liu et al. 2024 ) 69.3 79.9 52.5 59.6 72.0 65.8 66.52 + Ours 71.6 80.7 56.9 62.0 75.8 68.1 69.18 We formalize the selection process as follows: given a set of candidate attention matrices A ( i ) {A^{(i)}} , where each A ( i ) âˆˆ â„ d Ã— d A^{(i)}\\in\\mathbb{R}^{d\\times d} represents the attention weights at layer i i , we define the attention at the post-integrated layer (layer k = 28 k=28 ) as A ( k ) A^{(k)} . To identify the attention matrix that deviates most significantly from A ( k ) A^{(k)} in terms of distributional difference, we compute the Hellinger distance between each A ( i ) A^{(i)} and the reference A ( k ) A^{(k)} , and select the one with the maximal distance from candidate layers ğ’ \\mathcal{C} : i âˆ— = arg â¡ max i âˆˆ ğ’ â¡ H â€‹ ( A ( i ) , A ( k ) ) i^{*}=\\arg\\max_{i\\in\\mathcal{C}}H\\left(A^{(i)},A^{(k)}\\right) (1) where H â€‹ ( P , Q ) H(P,Q) denotes the Hellinger distance between two probability distributions P P and Q Q , defined as: H â€‹ ( P , Q ) = 1 2 â€‹ âˆ‘ j = 1 d ( p j âˆ’ q j ) 2 H(P,Q)=\\frac{1}{\\sqrt{2}}\\sqrt{\\sum_{j=1}^{d}\\left(\\sqrt{p_{j}}-\\sqrt{q_{j}}\\right)^{2}} (2) where d d denotes the number of image tokens involved in the attention distribution."
        },
        {
          "section_index": 12,
          "heading_level": "h3",
          "heading": "4.2 Contrastive Attention for Review",
          "text": "As shown in Figure 4 , we leverage the observed layer-wise fusion phenomenon in MLLMs, particularly the review-like behavior, where the model reattends to the image even after the primary fusion appears to be completed. Based on this, we apply the contrastive attention to guide the masking of visual information during this review stage. Specifically, the contrastive attention is computed as the difference between the attention at pre-integrated layer ğ€ ( i âˆ— ) \\mathbf{A}^{(i^{*})} and that at post-integrated layer ğ€ ( k ) \\mathbf{A}^{(k)} . IA = | ğ€ ( k ) âˆ’ ğ€ ( i âˆ— ) | \\text{IA}=\\left|\\mathbf{A}^{(k)}-\\mathbf{A}^{(i^{*})}\\right| (3) The difference between the attention from the pre-integrated layer and the post-integrated layer captures how visual information shifts the attention during the visual integration and understanding. We leverage this signal to refine the modelâ€™s attention at review layer through a soft masking strategy. Specifically, we identify visual tokens whose contrastive attention scores fall below the Ï \\rho -th percentile and softly suppress their influence by scaling down features. Formally, let Q Ï â€‹ ( IA ) Q_{\\rho}(\\text{IA}) denote the Ï \\rho -th quantile of the contrastive attention scores IA . The masked visual features are computed as: ğ„ j masked = Î» â‹… ğ„ j , if IA â€‹ j < Q Ï â€‹ ( IA ) \\mathbf{E}_{j}^{\\text{masked}}=\\lambda\\cdot\\mathbf{E}_{j},\\quad\\text{if }\\text{IA}j<Q_{\\rho}(\\text{IA}) (4) where ğ„ j \\mathbf{E}_{j} is the embedding of the j j -th visual token, and Î» â‰ª 1 \\lambda\\ll 1 is a constant that softly downweights low-relevance regions without fully discarding them. We analyze the effect of varying the masking ratio Ï \\rho in section 5.6 ."
        },
        {
          "section_index": 13,
          "heading_level": "h2",
          "heading": "5 Experiments",
          "text": "Table 2: Effectiveness of contrastive attention compared to existing enhanced methods. MLLMs Methods GQA VQAv2 OKVQA VizWiz TextVQA DocVQA Average LLaVA-v1.5 + DoLA (Chuang et al. 2024 ) 66.8 70.1 52.8 58.4 56.1 26.0 55.04 + ViCrop (Zhang et al. 2025 ) 67.0 76.0 54.8 59.6 56.6 25.1 56.52 + Ours 69.4 77.6 55.4 60.4 59.8 26.9 58.25 LLaVA-v1.6 + DoLA (Chuang et al. 2024 ) 71.0 75.4 53.6 60.9 72.1 67.2 66.69 + ViCrop (Zhang et al. 2025 ) 70.2 81.4 54.1 61.3 73.4 65.6 67.00 + Ours 71.6 80.7 56.9 62.0 75.8 68.1 69.18 5.1 Datasets and Setting The proposed method is evaluated on six widely-used multimodal datasets: VQAv2 (Goyal et al. 2017 ) , GQA (Hudson and Manning 2019 ) , TextVQA (Singh et al. 2019 ) , OKVQA (Marino et al. 2019 ) , VizWiz (Gurari et al. 2018 ) , and DocVQA (Mathew et al. 2021 ) . We report accuracy on each dataset as the evaluation metric. All experiments are conducted based on the open-source and SOTA LLaVA series (Liu et al. 2023 , 2024 ) : LLaVA-1.5-7B, LLaVA-1.6-Vicuna-7B. The experiments were conducted on an RTX A800 GPU using PyTorch 2.0, and the system was based on Ubuntu 20.04. 5.2 Comparison with MLLMs The baseline MLLMs include BLIP-2 (Li et al. 2023 ) , Flamingo (Alayrac et al. 2022 ) , InstructBLIP (Dai et al. 2023 ) , IDEFICS (LaurenÃ§on et al. 2023 ) , LLaVA (Liu et al. 2023 ) , LLaVA-1.5 (Liu et al. 2024 ) , InternVL-MLP (Chen et al. 2024 ) , InternVL-QLLaMA (Chen et al. 2024 ) , and Qwen-VL (Bai et al. 2023 ) . As shown in Table 1 , integrating our method into LLaVA-1.5 yields state-of-the-art performance across six VQA benchmarks, achieving an average score of 58.17. This surpasses strong recent baselines such as InternVL-MLP (52.97) and Qwen-VL (50.49). 5.3 Comparison with Training-free Methods Table 3: Pre-integrated layer selection on LLaVA-v1.5-7B. GQA VQAv2 OKVQA VizWiz TextVQA DocVQA Average all (0-28) 65.40 72.97 51.34 57.77 52.50 26.10 54.35 deep (16-28) 60.30 70.11 52.80 57.40 56.10 26.00 53.79 shallow (0-15) 68.30 76.49 53.92 59.30 57.00 26.40 56.90 fusion ( ğ’® \\mathcal{S} ) 69.40 77.59 55.40 60.40 59.80 26.90 58.25 To evaluate the effectiveness of our proposed training-free method, as shown in Table 2 , we also compare it against two representative methods: (1) DoLA (Chuang et al. 2024 ) proposes a training-free decoding strategy that contrasts the output logits between earlier and later transformer layers, amplifying deep-layer factual knowledge and reducing hallucination without requiring additional supervision. (2) ViCrop (Zhang et al. 2025 ) introduces a training-free adversarial framework that manipulates image inputs by strategically masking and cropping regions to challenge the visual reasoning of MLLMs, revealing their sensitivity to localized visual changes. As illustrated in Table 2 , our approach consistently outperforms existing representative methods, demonstrating improved stability and transferability during inference. These results indicate that contrastive attention effectively identifies and enhances critical visual regions, providing a lightweight yet effective enhancement to the inference process without requiring any additional training. 5.4 Selection of post-integrated layer Figure 5: Layer-wise Hellinger distance to the final layer. Before identifying the post-integrated layer for applying contrastive attention, we first clarify two key questions: 1. Where and how does contrastive attention function? Contrastive attention functions by suppressing image regions that are irrelevant to the task. Our exploratory experiments show that image has little effect on deeper layers (e.g., beyond the 20th layer), where visual information has already been fully integrated. However, we observe that a secondary integration of visual and textual information occurs at layer 29, referred to as the â€œreviewâ€ layer. Therefore, to influence the final reasoning, contrastive attention should take effect at layer 29 . 2. Why is layer 28 selected as the post-integrated layer? While the final layer (layer 31) represents the modelâ€™s fully aggregated understanding, it cannot be directly used for contrastive attention. This is because contrastive attention must be computed before layer 29 in order to take effect at the point where visual-textual fusion reoccurs. Therefore, the post-integrated layer needs to be selected before layer 29 . To approximate the role of the final layer, we calculate the Hellinger Distance between the attention distributions of each layer and that of layer 31. The results show that both layer 28 and layer 30 exhibit minimal distance from the final layer, indicating that their attention patterns closely resemble the final attention. Thus, we select layer 28 as the post-integrated layer for computing contrastive attention. 5.5 Selection of Pre-Integrated Layers We systematically explored different strategies for selecting the pre-integrated layer, considering four selection scope from: (a) all layers (unconstrained setting, 0â€“28), (b) deep layers (16â€“28), (c) shallow layers (0â€“16), and (d) the set of fusion layers (2, 4, 8, 11, 12, 13) identified in section 3.2 . The pre-integrated layer refers to the stage where visual information is initially processed, before being influenced by textual context. Therefore, layers with already fused representations should be excluded. As shown in Table 3 , the deep strategy performs poorly because these layers contain cross-modal information, which degrades the quality of contrastive attention. Similarly, the all strategy includes deep layers, introducing noisy and biased attention. In comparison, the shallow strategy achieves better results. The best performance comes from the fusion strategy, which limits selection to empirically identified fusion layers ğ’® \\mathcal{S} . As shown in Figure 6 , we further investigate the distribution of automatically selected pre-integrated layers when no constraints are imposed on the selection range. Specifically, we apply Equation ( 1 ) to identify the layer that exhibits the largest attention divergence from the post-integrated layer. As illustrated, layer 2 is selected with overwhelmingly high frequency across all datasets, indicating a strong concentration around this early layer. For instance, in DocVQA, over 86% of samples select layer 2 as the pre-integrated layer. Figure 6 shows that fusion layers ğ’® \\mathcal{S} are favored as pre-integrated layers, reinforcing our claim that they are well-suited for contrastive attention. Figure 6: Distribution of selected pre-integrated layers across tasks under unconstrained candidate setting. 5.6 Masking Ratio Based on Contrastive Attention Figure 7: Effect of visual token masking ratio on accuracy and inference time at the review layer. Based on the observed review-like behavior, we applied contrastive attention at review layer of the MLLM and used a soft mask strategy to selectively suppress irrelevant visual information. In this section, we investigate how different masking ratios affect model performance and efficiency. As shown in Figure 7 , we evaluated the accuracy and inference time across six benchmarks under varying mask ratios on LLaVA-1.5. We observe a consistent trend: accuracy peaks when 20% of the tokens are masked, but degrades as the masking ratio increases further. Meanwhile, inference time decreases steadily as the masking ratio increases. When the masking ratio reaches 1.0â€”i.e., the model receives no visual inputâ€”the accuracy drops to zero on all datasets. This clearly confirms the critical role of visual signals at the review layer during final-stage reasoning."
        },
        {
          "section_index": 14,
          "heading_level": "h3",
          "heading": "5.1 Datasets and Setting",
          "text": "The proposed method is evaluated on six widely-used multimodal datasets: VQAv2 (Goyal et al. 2017 ) , GQA (Hudson and Manning 2019 ) , TextVQA (Singh et al. 2019 ) , OKVQA (Marino et al. 2019 ) , VizWiz (Gurari et al. 2018 ) , and DocVQA (Mathew et al. 2021 ) . We report accuracy on each dataset as the evaluation metric. All experiments are conducted based on the open-source and SOTA LLaVA series (Liu et al. 2023 , 2024 ) : LLaVA-1.5-7B, LLaVA-1.6-Vicuna-7B. The experiments were conducted on an RTX A800 GPU using PyTorch 2.0, and the system was based on Ubuntu 20.04."
        },
        {
          "section_index": 15,
          "heading_level": "h3",
          "heading": "5.2 Comparison with MLLMs",
          "text": "The baseline MLLMs include BLIP-2 (Li et al. 2023 ) , Flamingo (Alayrac et al. 2022 ) , InstructBLIP (Dai et al. 2023 ) , IDEFICS (LaurenÃ§on et al. 2023 ) , LLaVA (Liu et al. 2023 ) , LLaVA-1.5 (Liu et al. 2024 ) , InternVL-MLP (Chen et al. 2024 ) , InternVL-QLLaMA (Chen et al. 2024 ) , and Qwen-VL (Bai et al. 2023 ) . As shown in Table 1 , integrating our method into LLaVA-1.5 yields state-of-the-art performance across six VQA benchmarks, achieving an average score of 58.17. This surpasses strong recent baselines such as InternVL-MLP (52.97) and Qwen-VL (50.49)."
        },
        {
          "section_index": 16,
          "heading_level": "h3",
          "heading": "5.3 Comparison with Training-free Methods",
          "text": "Table 3: Pre-integrated layer selection on LLaVA-v1.5-7B. GQA VQAv2 OKVQA VizWiz TextVQA DocVQA Average all (0-28) 65.40 72.97 51.34 57.77 52.50 26.10 54.35 deep (16-28) 60.30 70.11 52.80 57.40 56.10 26.00 53.79 shallow (0-15) 68.30 76.49 53.92 59.30 57.00 26.40 56.90 fusion ( ğ’® \\mathcal{S} ) 69.40 77.59 55.40 60.40 59.80 26.90 58.25 To evaluate the effectiveness of our proposed training-free method, as shown in Table 2 , we also compare it against two representative methods: (1) DoLA (Chuang et al. 2024 ) proposes a training-free decoding strategy that contrasts the output logits between earlier and later transformer layers, amplifying deep-layer factual knowledge and reducing hallucination without requiring additional supervision. (2) ViCrop (Zhang et al. 2025 ) introduces a training-free adversarial framework that manipulates image inputs by strategically masking and cropping regions to challenge the visual reasoning of MLLMs, revealing their sensitivity to localized visual changes. As illustrated in Table 2 , our approach consistently outperforms existing representative methods, demonstrating improved stability and transferability during inference. These results indicate that contrastive attention effectively identifies and enhances critical visual regions, providing a lightweight yet effective enhancement to the inference process without requiring any additional training."
        },
        {
          "section_index": 17,
          "heading_level": "h3",
          "heading": "5.4 Selection of post-integrated layer",
          "text": "Figure 5: Layer-wise Hellinger distance to the final layer. Before identifying the post-integrated layer for applying contrastive attention, we first clarify two key questions: 1. Where and how does contrastive attention function? Contrastive attention functions by suppressing image regions that are irrelevant to the task. Our exploratory experiments show that image has little effect on deeper layers (e.g., beyond the 20th layer), where visual information has already been fully integrated. However, we observe that a secondary integration of visual and textual information occurs at layer 29, referred to as the â€œreviewâ€ layer. Therefore, to influence the final reasoning, contrastive attention should take effect at layer 29 . 2. Why is layer 28 selected as the post-integrated layer? While the final layer (layer 31) represents the modelâ€™s fully aggregated understanding, it cannot be directly used for contrastive attention. This is because contrastive attention must be computed before layer 29 in order to take effect at the point where visual-textual fusion reoccurs. Therefore, the post-integrated layer needs to be selected before layer 29 . To approximate the role of the final layer, we calculate the Hellinger Distance between the attention distributions of each layer and that of layer 31. The results show that both layer 28 and layer 30 exhibit minimal distance from the final layer, indicating that their attention patterns closely resemble the final attention. Thus, we select layer 28 as the post-integrated layer for computing contrastive attention."
        },
        {
          "section_index": 18,
          "heading_level": "h4",
          "heading": "1. Where and how does contrastive attention function?",
          "text": "Contrastive attention functions by suppressing image regions that are irrelevant to the task. Our exploratory experiments show that image has little effect on deeper layers (e.g., beyond the 20th layer), where visual information has already been fully integrated. However, we observe that a secondary integration of visual and textual information occurs at layer 29, referred to as the â€œreviewâ€ layer. Therefore, to influence the final reasoning, contrastive attention should take effect at layer 29 ."
        },
        {
          "section_index": 19,
          "heading_level": "h4",
          "heading": "2. Why is layer 28 selected as the post-integrated layer?",
          "text": "While the final layer (layer 31) represents the modelâ€™s fully aggregated understanding, it cannot be directly used for contrastive attention. This is because contrastive attention must be computed before layer 29 in order to take effect at the point where visual-textual fusion reoccurs. Therefore, the post-integrated layer needs to be selected before layer 29 . To approximate the role of the final layer, we calculate the Hellinger Distance between the attention distributions of each layer and that of layer 31. The results show that both layer 28 and layer 30 exhibit minimal distance from the final layer, indicating that their attention patterns closely resemble the final attention. Thus, we select layer 28 as the post-integrated layer for computing contrastive attention."
        },
        {
          "section_index": 20,
          "heading_level": "h3",
          "heading": "5.5 Selection of Pre-Integrated Layers",
          "text": "We systematically explored different strategies for selecting the pre-integrated layer, considering four selection scope from: (a) all layers (unconstrained setting, 0â€“28), (b) deep layers (16â€“28), (c) shallow layers (0â€“16), and (d) the set of fusion layers (2, 4, 8, 11, 12, 13) identified in section 3.2 . The pre-integrated layer refers to the stage where visual information is initially processed, before being influenced by textual context. Therefore, layers with already fused representations should be excluded. As shown in Table 3 , the deep strategy performs poorly because these layers contain cross-modal information, which degrades the quality of contrastive attention. Similarly, the all strategy includes deep layers, introducing noisy and biased attention. In comparison, the shallow strategy achieves better results. The best performance comes from the fusion strategy, which limits selection to empirically identified fusion layers ğ’® \\mathcal{S} . As shown in Figure 6 , we further investigate the distribution of automatically selected pre-integrated layers when no constraints are imposed on the selection range. Specifically, we apply Equation ( 1 ) to identify the layer that exhibits the largest attention divergence from the post-integrated layer. As illustrated, layer 2 is selected with overwhelmingly high frequency across all datasets, indicating a strong concentration around this early layer. For instance, in DocVQA, over 86% of samples select layer 2 as the pre-integrated layer. Figure 6 shows that fusion layers ğ’® \\mathcal{S} are favored as pre-integrated layers, reinforcing our claim that they are well-suited for contrastive attention. Figure 6: Distribution of selected pre-integrated layers across tasks under unconstrained candidate setting."
        },
        {
          "section_index": 21,
          "heading_level": "h3",
          "heading": "5.6 Masking Ratio Based on Contrastive Attention",
          "text": "Figure 7: Effect of visual token masking ratio on accuracy and inference time at the review layer. Based on the observed review-like behavior, we applied contrastive attention at review layer of the MLLM and used a soft mask strategy to selectively suppress irrelevant visual information. In this section, we investigate how different masking ratios affect model performance and efficiency. As shown in Figure 7 , we evaluated the accuracy and inference time across six benchmarks under varying mask ratios on LLaVA-1.5. We observe a consistent trend: accuracy peaks when 20% of the tokens are masked, but degrades as the masking ratio increases further. Meanwhile, inference time decreases steadily as the masking ratio increases. When the masking ratio reaches 1.0â€”i.e., the model receives no visual inputâ€”the accuracy drops to zero on all datasets. This clearly confirms the critical role of visual signals at the review layer during final-stage reasoning."
        },
        {
          "section_index": 22,
          "heading_level": "h2",
          "heading": "6 Conclusion",
          "text": "In this paper, we present a systematic analysis of visual information integration in MLLMs by conducting layer-wise masking experiments. Our findings reveal that visual-text fusion primarily occurs in a certain number of shallow layers, while a review-like re-attention behavior emerges just before the final output. Building on these findings, we propose a training-free method that enhances vision-language reasoning using contrastive attention. This attention is computed by comparing the attention maps between a pre-integrated layer, which represents early visual perception, and an post-integrated layer, where vision and language are fully fused. The resulting contrastive signal reveals the modelâ€™s evolving visual focus and helps suppress irrelevant high-attention noise in later layers. When applied at the review layer, this approach guides the model to concentrate on task-relevant visual regions more effectively, without any additional training. Extensive experiments across six VQA benchmarks demonstrate that our method consistently improves the performance of LLaVA series models, achieving new state-of-the-art results. Our work provides deeper insight into the internal mechanisms of MLLMs and offers a lightweight, effective strategy for enhancing multimodal understanding."
        },
        {
          "section_index": 23,
          "heading_level": "h2",
          "heading": "References",
          "text": "E. Aflalo, M. Du, S. Tseng, Y. Liu, C. Wu, N. Duan, and V. Lal (2022) Vl-interpret: an interactive visualization tool for interpreting vision-language transformers . In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition , pp. 21406â€“21415 . Cited by: Â§2 . J. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. (2022) Flamingo: a visual language model for few-shot learning . Advances in neural information processing systems 35 , pp. 23716â€“23736 . Cited by: Table 1 , Â§5.2 . J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou (2023) Qwen-vl: a versatile vision-language model for understanding, localization, text reading, and beyond . arXiv preprint arXiv:2308.12966 . Cited by: Â§1 , Table 1 , Â§5.2 . J. Cao, Z. Gan, Y. Cheng, L. Yu, Y. Chen, and J. Liu (2020) Behind the scene: revealing the secrets of pre-trained vision-and-language models . In Computer Visionâ€“ECCV 2020: 16th European Conference, Glasgow, UK, August 23â€“28, 2020, Proceedings, Part VI 16 , pp. 565â€“580 . Cited by: Â§2 . H. Chefer, S. Gur, and L. Wolf (2021) Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers . In Proceedings of the IEEE/CVF international conference on computer vision , pp. 397â€“406 . Cited by: Â§2 . Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, M. Zhong, Q. Zhang, X. Zhu, L. Lu, et al. (2024) Internvl: scaling up vision foundation models and aligning for generic visual-linguistic tasks . In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 24185â€“24198 . Cited by: Table 1 , Table 1 , Â§5.2 . Y. Chuang, Y. Xie, H. Luo, Y. Kim, J. Glass, and P. He (2024) Dola: decoding by contrasting layers improves factuality in large language models . arXiv preprint arXiv:2309.03883 . Cited by: Â§1 , Â§5.3 , Table 2 , Table 2 . W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi (2023) InstructBLIP: towards general-purpose vision-language models with instruction tuning . External Links: 2305.06500 Cited by: Â§1 , Table 1 , Â§5.2 . S. Frank, E. Bugliarello, and D. Elliott (2021) Vision-and-language or vision-for-language? on cross-modal influence in multimodal transformers . arXiv preprint arXiv:2109.04448 . Cited by: Â§2 . Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh (2017) Making the v in vqa matter: elevating the role of image understanding in visual question answering . In Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 6904â€“6913 . Cited by: Â§1 , Â§5.1 . D. Gurari, Q. Li, A. J. Stangl, A. Guo, C. Lin, K. Grauman, J. Luo, and J. P. Bigham (2018) Vizwiz grand challenge: answering visual questions from blind people . In Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 3608â€“3617 . Cited by: Â§5.1 . D. A. Hudson and C. D. Manning (2019) Gqa: a new dataset for real-world visual reasoning and compositional question answering . In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 6700â€“6709 . Cited by: Â§5.1 . M. Jin, Q. Yu, J. Huang, Q. Zeng, Z. Wang, W. Hua, H. Zhao, K. Mei, Y. Meng, K. Ding, et al. (2024) Exploring concept depth: how large language models acquire knowledge and concept at different layers? . arXiv preprint arXiv:2404.07066 . Cited by: Â§2 . T. Ju, W. Sun, W. Du, X. Yuan, Z. Ren, and G. Liu (2024) How large language models encode context knowledge? a layer-wise probing study . arXiv preprint arXiv:2402.16061 . Cited by: Â§2 . H. LaurenÃ§on, L. Saulnier, L. Tronchon, S. Bekman, A. Singh, A. Lozhkov, T. Wang, S. Karamcheti, A. Rush, D. Kiela, et al. (2023) Obelics: an open web-scale filtered dataset of interleaved image-text documents . Advances in Neural Information Processing Systems 36 , pp. 71683â€“71702 . Cited by: Table 1 , Â§5.2 . J. Li, D. Li, S. Savarese, and S. Hoi (2023) Blip-2: bootstrapping language-image pre-training with frozen image encoders and large language models . In International conference on machine learning , pp. 19730â€“19742 . Cited by: Table 1 , Â§5.2 . J. Lin, H. Chen, Y. Fan, Y. Fan, X. Jin, H. Su, J. Fu, and X. Shen (2025) Multi-layer visual feature fusion in multimodal llms: methods, analysis, and best practices . arXiv preprint arXiv:2503.06063 . Cited by: Â§2 . H. Liu, C. Li, Y. Li, and Y. J. Lee (2024) Improved baselines with visual instruction tuning . arXiv:2310.03744 . Cited by: Table 1 , Â§5.1 , Â§5.2 . H. Liu, C. Li, Q. Wu, and Y. J. Lee (2023) Visual instruction tuning . NeurIPS . Cited by: Â§1 , Â§1 , Table 1 , Table 1 , Â§5.1 , Â§5.2 . Y. Lyu, P. P. Liang, Z. Deng, R. Salakhutdinov, and L. Morency (2022) Dime: fine-grained interpretations of multimodal models via disentangled local explanations . In Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society , pp. 455â€“467 . Cited by: Â§2 . K. Marino, M. Rastegari, A. Farhadi, and R. Mottaghi (2019) Ok-vqa: a visual question answering benchmark requiring external knowledge . In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition , pp. 3195â€“3204 . Cited by: Â§1 , Â§5.1 . M. Mathew, D. Karatzas, and C. Jawahar (2021) Docvqa: a dataset for vqa on document images . In Proceedings of the IEEE/CVF winter conference on applications of computer vision , pp. 2200â€“2209 . Cited by: Â§5.1 . V. Palit, R. Pandey, A. Arora, and P. P. Liang (2023) Towards vision-language mechanistic interpretability: a causal tracing tool for blip . In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 2856â€“2861 . Cited by: Â§2 . A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach (2019) Towards vqa models that can read . In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 8317â€“8326 . Cited by: Â§5.1 . G. B. M. Stan, E. Aflalo, R. Y. Rohekar, A. Bhiwandiwalla, S. Tseng, M. L. Olson, Y. Gurwicz, C. Wu, N. Duan, and V. Lal (2024) LVLM-interpret: an interpretability tool for large vision-language models . arXiv preprint arXiv:2404.03118 . Cited by: Â§2 . P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K. Chen, X. Liu, J. Wang, W. Ge, et al. (2024) Qwen2-vl: enhancing vision-language modelâ€™s perception of the world at any resolution . arXiv preprint arXiv:2409.12191 . Cited by: Table 1 . J. Zhang, M. Khayatkhoei, P. Chhikara, and F. Ilievski (2025) MLLMs know where to look: training-free perception of small visual details with multimodal llms . arXiv preprint arXiv:2502.17422 . Cited by: Â§1 , Â§5.3 , Table 2 , Table 2 . Q. Zhao, M. Xu, K. Gupta, A. Asthana, L. Zheng, and S. Gould (2024) The first to know: how token distributions reveal hidden knowledge in large vision-language models? . In European Conference on Computer Vision , pp. 127â€“142 . Cited by: Â§2 ."
        },
        {
          "section_index": 52,
          "heading_level": "h2",
          "heading": "Reproducibility Checklist",
          "text": "Instructions for Authors: This document outlines key aspects for assessing reproducibility. Please provide your input by editing this .tex file directly. For each question (that applies), replace the â€œType your response hereâ€ text with your answer. Example: If a question appears as \\question{Proofs of all novel claims are included} {(yes/partial/no)} Type your response here you would change it to: \\question{Proofs of all novel claims are included} {(yes/partial/no)} yes Please make sure to: â€¢ Replace ONLY the â€œType your response hereâ€ text and nothing else. â€¢ Use one of the options listed for that question (e.g., yes , no , partial , or NA ). â€¢ Not modify any other part of the \\question command or any other lines in this document. You can \\input this .tex file right before \\end{document} of your main file or compile it as a stand-alone document. Check the instructions on your conferenceâ€™s website to see if you will be asked to provide this checklist with your paper or separately. 1. General Paper Structure 1.1. Includes a conceptual outline and/or pseudocode description of AI methods introduced (yes/partial/no/NA) yes 1.2. Clearly delineates statements that are opinions, hypothesis, and speculation from objective facts and results (yes/no) yes 1.3. Provides well-marked pedagogical references for less-familiar readers to gain background necessary to replicate the paper (yes/no) yes 2. Theoretical Contributions 2.1. Does this paper make theoretical contributions? (yes/no) yes If yes, please address the following points: 2.2. All assumptions and restrictions are stated clearly and formally (yes/partial/no) yes 2.3. All novel claims are stated formally (e.g., in theorem statements) (yes/partial/no) yes 2.4. Proofs of all novel claims are included (yes/partial/no) yes 2.5. Proof sketches or intuitions are given for complex and/or novel results (yes/partial/no) yes 2.6. Appropriate citations to theoretical tools used are given (yes/partial/no) yes 2.7. All theoretical claims are demonstrated empirically to hold (yes/partial/no/NA) yes 2.8. All experimental code used to eliminate or disprove claims is included (yes/no/NA) yes 3. Dataset Usage 3.1. Does this paper rely on one or more datasets? (yes/no) yes If yes, please address the following points: 3.2. A motivation is given for why the experiments are conducted on the selected datasets (yes/partial/no/NA) yes 3.3. All novel datasets introduced in this paper are included in a data appendix (yes/partial/no/NA) NA 3.4. All novel datasets introduced in this paper will be made publicly available upon publication of the paper with a license that allows free usage for research purposes (yes/partial/no/NA) NA 3.5. All datasets drawn from the existing literature (potentially including authorsâ€™ own previously published work) are accompanied by appropriate citations (yes/no/NA) yes 3.6. All datasets drawn from the existing literature (potentially including authorsâ€™ own previously published work) are publicly available (yes/partial/no/NA) yes 3.7. All datasets that are not publicly available are described in detail, with explanation why publicly available alternatives are not scientifically satisficing (yes/partial/no/NA) NA 4. Computational Experiments 4.1. Does this paper include computational experiments? (yes/no) yes If yes, please address the following points: 4.2. This paper states the number and range of values tried per (hyper-) parameter during development of the paper, along with the criterion used for selecting the final parameter setting (yes/partial/no/NA) yes 4.3. Any code required for pre-processing data is included in the appendix (yes/partial/no) yes 4.4. All source code required for conducting and analyzing the experiments is included in a code appendix (yes/partial/no) yes 4.5. All source code required for conducting and analyzing the experiments will be made publicly available upon publication of the paper with a license that allows free usage for research purposes (yes/partial/no) yes 4.6. All source code implementing new methods have comments detailing the implementation, with references to the paper where each step comes from (yes/partial/no) yes 4.7. If an algorithm depends on randomness, then the method used for setting seeds is described in a way sufficient to allow replication of results (yes/partial/no/NA) yes 4.8. This paper specifies the computing infrastructure used for running experiments (hardware and software), including GPU/CPU models; amount of memory; operating system; names and versions of relevant software libraries and frameworks (yes/partial/no) yes 4.9. This paper formally describes evaluation metrics used and explains the motivation for choosing these metrics (yes/partial/no) yes 4.10. This paper states the number of algorithm runs used to compute each reported result (yes/no) yes 4.11. Analysis of experiments goes beyond single-dimensional summaries of performance (e.g., average; median) to include measures of variation, confidence, or other distributional information (yes/no) no 4.12. The significance of any improvement or decrease in performance is judged using appropriate statistical tests (e.g., Wilcoxon signed-rank) (yes/partial/no) no 4.13. This paper lists all final (hyper-)parameters used for each model/algorithm in the paperâ€™s experiments (yes/partial/no/NA) yes"
        },
        {
          "section_index": 53,
          "heading_level": "h4",
          "heading": "1. General Paper Structure",
          "text": "1.1. Includes a conceptual outline and/or pseudocode description of AI methods introduced (yes/partial/no/NA) yes 1.2. Clearly delineates statements that are opinions, hypothesis, and speculation from objective facts and results (yes/no) yes 1.3. Provides well-marked pedagogical references for less-familiar readers to gain background necessary to replicate the paper (yes/no) yes"
        },
        {
          "section_index": 54,
          "heading_level": "h4",
          "heading": "2. Theoretical Contributions",
          "text": "2.1. Does this paper make theoretical contributions? (yes/no) yes If yes, please address the following points: 2.2. All assumptions and restrictions are stated clearly and formally (yes/partial/no) yes 2.3. All novel claims are stated formally (e.g., in theorem statements) (yes/partial/no) yes 2.4. Proofs of all novel claims are included (yes/partial/no) yes 2.5. Proof sketches or intuitions are given for complex and/or novel results (yes/partial/no) yes 2.6. Appropriate citations to theoretical tools used are given (yes/partial/no) yes 2.7. All theoretical claims are demonstrated empirically to hold (yes/partial/no/NA) yes 2.8. All experimental code used to eliminate or disprove claims is included (yes/no/NA) yes"
        },
        {
          "section_index": 55,
          "heading_level": "h4",
          "heading": "3. Dataset Usage",
          "text": "3.1. Does this paper rely on one or more datasets? (yes/no) yes If yes, please address the following points: 3.2. A motivation is given for why the experiments are conducted on the selected datasets (yes/partial/no/NA) yes 3.3. All novel datasets introduced in this paper are included in a data appendix (yes/partial/no/NA) NA 3.4. All novel datasets introduced in this paper will be made publicly available upon publication of the paper with a license that allows free usage for research purposes (yes/partial/no/NA) NA 3.5. All datasets drawn from the existing literature (potentially including authorsâ€™ own previously published work) are accompanied by appropriate citations (yes/no/NA) yes 3.6. All datasets drawn from the existing literature (potentially including authorsâ€™ own previously published work) are publicly available (yes/partial/no/NA) yes 3.7. All datasets that are not publicly available are described in detail, with explanation why publicly available alternatives are not scientifically satisficing (yes/partial/no/NA) NA"
        },
        {
          "section_index": 56,
          "heading_level": "h4",
          "heading": "4. Computational Experiments",
          "text": "4.1. Does this paper include computational experiments? (yes/no) yes If yes, please address the following points: 4.2. This paper states the number and range of values tried per (hyper-) parameter during development of the paper, along with the criterion used for selecting the final parameter setting (yes/partial/no/NA) yes 4.3. Any code required for pre-processing data is included in the appendix (yes/partial/no) yes 4.4. All source code required for conducting and analyzing the experiments is included in a code appendix (yes/partial/no) yes 4.5. All source code required for conducting and analyzing the experiments will be made publicly available upon publication of the paper with a license that allows free usage for research purposes (yes/partial/no) yes 4.6. All source code implementing new methods have comments detailing the implementation, with references to the paper where each step comes from (yes/partial/no) yes 4.7. If an algorithm depends on randomness, then the method used for setting seeds is described in a way sufficient to allow replication of results (yes/partial/no/NA) yes 4.8. This paper specifies the computing infrastructure used for running experiments (hardware and software), including GPU/CPU models; amount of memory; operating system; names and versions of relevant software libraries and frameworks (yes/partial/no) yes 4.9. This paper formally describes evaluation metrics used and explains the motivation for choosing these metrics (yes/partial/no) yes 4.10. This paper states the number of algorithm runs used to compute each reported result (yes/no) yes 4.11. Analysis of experiments goes beyond single-dimensional summaries of performance (e.g., average; median) to include measures of variation, confidence, or other distributional information (yes/no) no 4.12. The significance of any improvement or decrease in performance is judged using appropriate statistical tests (e.g., Wilcoxon signed-rank) (yes/partial/no) no 4.13. This paper lists all final (hyper-)parameters used for each model/algorithm in the paperâ€™s experiments (yes/partial/no/NA) yes"
        }
      ],
      "content_text": "Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention\nShezheng Song, Shasha Li, Jie Yu Abstract Recent advances in Multimodal Large Language Models (MLLMs) have achieved impressive results in vision-language tasks. However, the internal mechanism of how visual information is integrated across layers remains poorly understood. In this work, we investigate the hierarchical process of visual integration in MLLMs through a series of layer-wise masking experiments. Our findings reveal that vision-language fusion primarily occurs at several shallow layers. We also discover a review-like behavior at a deep layer, where the model re-attends to the image before producing the final output. We further analyze attention distributions and uncover a systematic bias: irrelevant image regions often receive consistently high attention across layers, resulting in noisy and suboptimal final predictions. To address this issue, we propose a training-free method that leverages contrastive attention, defined as the difference in attention maps between a pre-integrated layer and a post-integrated layer. This captures the evolving focus of the model as it integrates visual information. We apply the contrastive attention at the review layer to selectively mask irrelevant image regions, guiding the model to attend more effectively to task-relevant content without requiring additional training. Extensive experiments on multiple multimodal benchmarks demonstrate that our method significantly boosts performance, achieving new state-of-the-art results on the LLaVA series. The code will be released. 1 Introduction In recent years, Multimodal Large Language Models (MLLMs) (Liu et al. 2023 ; Bai et al. 2023 ) have shown strong performance on tasks such as visual question answering (Goyal et al. 2017 ; Marino et al. 2019 ) . However, how information flows between layers in MLLMs, especially how visual signals are gradually integrated and utilized across layers, has not been fully explored. This gap in understanding limits the development of effective training strategies and model architectures. To address this gap, we focus on the internal process of visual-textual information fusion: At which layers is visual information integrated, and how does integration in each layer affect task performance? To explore this, we design a series of layer-wise masking experiments to systematically evaluate the role of each layer in cross-modal fusion. Specifically, we apply masking to visual feature at different layers and observe the resulting changes in model performance and inference speed. A significant performance drop after masking indicates that the corresponding layer plays a critical role in visual integration, whereas a minimal impact suggests a weaker dependence on visual information. This approach provides a functional perspective on the hierarchical dynamics of vision-language fusion in MLLMs. Our experiments show that visual-text fusion mainly happens at several shallow layers. Masking visual inputs at these layers causes performance to drop nearly to zero, highlighting their role as fusion layers . This effect is consistent across different datasets and MLLMs, showing stable layer-wise patterns. As depth increases, masking has less impact, suggesting that visual information has already been integrated. Notably, a sharp drop reappears at a late layer (e.g., layer 29), indicating the model briefly revisits the image before producing the final output. We call this a review mechanism , and the corresponding layer the review layer. In addition to analyzing the functional roles of each layer through masking, we further investigate how visual attention is distributed across layers. This analysis reveals a fundamental issue in current MLLMs: Systematic bias in visual attention allocation . As illustrated in Figure 1 , the final layer often fails to accurately attend to regions in the image that are truly relevant to the question. Prior work such as Zhang et al. ( 2025 ) has noted this limitation and attempted to address it by directly using attention maps from a fixed intermediate layer (e.g., layer 14 in LLaVA (Liu et al. 2023 ) , layer 15 in InstructBLIP (Dai et al. 2023 ) ). However, relying on a single fixed layer is both rigid and suboptimal, as it overlooks the dynamic nature of visual reasoning across tasks and model architectures. Furthermore, we observe that some irrelevant image regions consistently receive high attention across layers, a phenomenon we refer to as high-attention noise . These regions tend to be activated in early layers and remain prominent in later ones. For example, as shown in Figure 1 , the yellow dashed region maintains high attention values from shallow to deep layers, despite its irrelevance to the question. This layer-to-layer consistency suggests that early attention biases may propagate through the network and contribute to the suboptimal final attention patterns. Figure 1: Contrastive attention (pre-integrated â†’ \\rightarrow last layer) showing focus shift and persistent high-attention noise. Recent work such as DoLa (Chuang et al. 2024 ) has demonstrated that contrasting internal representations across layers can better reveal what the model truly learns. Specifically, DoLa computes the difference between the logits of early and later layers, producing contrastive logits that better reflect the modelâ€™s factual knowledge compared to relying solely on the final layer output. Inspired by this idea, we extend the notion of inter-layer contrast from output space to the attention space. In particular, we observe that as MLLMs process inputs layer by layer, their attention gradually shifts from task-irrelevant regions to more semantically meaningful areas. This shift reflects how the model integrates visual cues in a progressive manner. Shallow layers tend to contain noisy or indiscriminate attention, while deeper layers refine this focus toward task-relevant content. We refer to this evolving focus as contrastive attention , which captures not the static attention at any single layer, but the transformation in attention patterns across layers that reveals what the model incrementally learns to attend to . To quantify the contrastive attention, we introduce two layer concepts: (a) the post-integrated layer , where the model has already combined visual and linguistic information and formed a representation capable of solving the task. We identify the post-integrated layer as the one immediately preceding the review layer (see section 5.4 for detailed exploration). (b) the pre-integrated layer , which represents the initial perception of the image before substantial semantic fusion occurs. The pre-integrated layer is selected from the identified fusion layers as the one whose attention map exhibits the largest Hellinger distance from the post-integrated layer, indicating the point before major vision-language integration takes place (see section 5.5 ). Based on the above analysis, we introduce a training-free approach to enhance vision-language understanding of MLLMs. Specifically, we compute the contrastive attention by comparing attention maps between the pre-integrated layer and the post-integrated layer. It captures the actual contribution of visual information as it flows through the model, revealing how visual semantics are progressively integrated. Moreover, it effectively suppresses spurious high-attention noise in later layers. Then we apply contrastive attention at review layer to mask visual regions. This selective masking reduces the influence of irrelevant content and enables the model to concentrate more effectively on task-relevant areas without additional training. Experiments show that our method significantly improves the performance of the widely used LLaVA series across six visual question answering benchmarks, achieving state-of-the-art results. Our contributions are as follows: â€¢ We conduct a systematic investigation into how visual information is integrated across layers in MLLMs. By applying layer-wise visual masking, we identify key visual-textual fusion layers and a review-like mechanism. â€¢ We propose a training-free method that exploits contrastive attention, the divergence between pre-integrated and post-integrated attention maps, to refine visual focus. â€¢ Our method consistently outperforms existing techniques and achieves state-of-the-art results across multiple multimodal benchmarks, validating its generality and effectiveness.\n\nAbstract\nRecent advances in Multimodal Large Language Models (MLLMs) have achieved impressive results in vision-language tasks. However, the internal mechanism of how visual information is integrated across layers remains poorly understood. In this work, we investigate the hierarchical process of visual integration in MLLMs through a series of layer-wise masking experiments. Our findings reveal that vision-language fusion primarily occurs at several shallow layers. We also discover a review-like behavior at a deep layer, where the model re-attends to the image before producing the final output. We further analyze attention distributions and uncover a systematic bias: irrelevant image regions often receive consistently high attention across layers, resulting in noisy and suboptimal final predictions. To address this issue, we propose a training-free method that leverages contrastive attention, defined as the difference in attention maps between a pre-integrated layer and a post-integrated layer. This captures the evolving focus of the model as it integrates visual information. We apply the contrastive attention at the review layer to selectively mask irrelevant image regions, guiding the model to attend more effectively to task-relevant content without requiring additional training. Extensive experiments on multiple multimodal benchmarks demonstrate that our method significantly boosts performance, achieving new state-of-the-art results on the LLaVA series. The code will be released.\n\n1 Introduction\nIn recent years, Multimodal Large Language Models (MLLMs) (Liu et al. 2023 ; Bai et al. 2023 ) have shown strong performance on tasks such as visual question answering (Goyal et al. 2017 ; Marino et al. 2019 ) . However, how information flows between layers in MLLMs, especially how visual signals are gradually integrated and utilized across layers, has not been fully explored. This gap in understanding limits the development of effective training strategies and model architectures. To address this gap, we focus on the internal process of visual-textual information fusion: At which layers is visual information integrated, and how does integration in each layer affect task performance? To explore this, we design a series of layer-wise masking experiments to systematically evaluate the role of each layer in cross-modal fusion. Specifically, we apply masking to visual feature at different layers and observe the resulting changes in model performance and inference speed. A significant performance drop after masking indicates that the corresponding layer plays a critical role in visual integration, whereas a minimal impact suggests a weaker dependence on visual information. This approach provides a functional perspective on the hierarchical dynamics of vision-language fusion in MLLMs. Our experiments show that visual-text fusion mainly happens at several shallow layers. Masking visual inputs at these layers causes performance to drop nearly to zero, highlighting their role as fusion layers . This effect is consistent across different datasets and MLLMs, showing stable layer-wise patterns. As depth increases, masking has less impact, suggesting that visual information has already been integrated. Notably, a sharp drop reappears at a late layer (e.g., layer 29), indicating the model briefly revisits the image before producing the final output. We call this a review mechanism , and the corresponding layer the review layer. In addition to analyzing the functional roles of each layer through masking, we further investigate how visual attention is distributed across layers. This analysis reveals a fundamental issue in current MLLMs: Systematic bias in visual attention allocation . As illustrated in Figure 1 , the final layer often fails to accurately attend to regions in the image that are truly relevant to the question. Prior work such as Zhang et al. ( 2025 ) has noted this limitation and attempted to address it by directly using attention maps from a fixed intermediate layer (e.g., layer 14 in LLaVA (Liu et al. 2023 ) , layer 15 in InstructBLIP (Dai et al. 2023 ) ). However, relying on a single fixed layer is both rigid and suboptimal, as it overlooks the dynamic nature of visual reasoning across tasks and model architectures. Furthermore, we observe that some irrelevant image regions consistently receive high attention across layers, a phenomenon we refer to as high-attention noise . These regions tend to be activated in early layers and remain prominent in later ones. For example, as shown in Figure 1 , the yellow dashed region maintains high attention values from shallow to deep layers, despite its irrelevance to the question. This layer-to-layer consistency suggests that early attention biases may propagate through the network and contribute to the suboptimal final attention patterns. Figure 1: Contrastive attention (pre-integrated â†’ \\rightarrow last layer) showing focus shift and persistent high-attention noise. Recent work such as DoLa (Chuang et al. 2024 ) has demonstrated that contrasting internal representations across layers can better reveal what the model truly learns. Specifically, DoLa computes the difference between the logits of early and later layers, producing contrastive logits that better reflect the modelâ€™s factual knowledge compared to relying solely on the final layer output. Inspired by this idea, we extend the notion of inter-layer contrast from output space to the attention space. In particular, we observe that as MLLMs process inputs layer by layer, their attention gradually shifts from task-irrelevant regions to more semantically meaningful areas. This shift reflects how the model integrates visual cues in a progressive manner. Shallow layers tend to contain noisy or indiscriminate attention, while deeper layers refine this focus toward task-relevant content. We refer to this evolving focus as contrastive attention , which captures not the static attention at any single layer, but the transformation in attention patterns across layers that reveals what the model incrementally learns to attend to . To quantify the contrastive attention, we introduce two layer concepts: (a) the post-integrated layer , where the model has already combined visual and linguistic information and formed a representation capable of solving the task. We identify the post-integrated layer as the one immediately preceding the review layer (see section 5.4 for detailed exploration). (b) the pre-integrated layer , which represents the initial perception of the image before substantial semantic fusion occurs. The pre-integrated layer is selected from the identified fusion layers as the one whose attention map exhibits the largest Hellinger distance from the post-integrated layer, indicating the point before major vision-language integration takes place (see section 5.5 ). Based on the above analysis, we introduce a training-free approach to enhance vision-language understanding of MLLMs. Specifically, we compute the contrastive attention by comparing attention maps between the pre-integrated layer and the post-integrated layer. It captures the actual contribution of visual information as it flows through the model, revealing how visual semantics are progressively integrated. Moreover, it effectively suppresses spurious high-attention noise in later layers. Then we apply contrastive attention at review layer to mask visual regions. This selective masking reduces the influence of irrelevant content and enables the model to concentrate more effectively on task-relevant areas without additional training. Experiments show that our method significantly improves the performance of the widely used LLaVA series across six visual question answering benchmarks, achieving state-of-the-art results. Our contributions are as follows: â€¢ We conduct a systematic investigation into how visual information is integrated across layers in MLLMs. By applying layer-wise visual masking, we identify key visual-textual fusion layers and a review-like mechanism. â€¢ We propose a training-free method that exploits contrastive attention, the divergence between pre-integrated and post-integrated attention maps, to refine visual focus. â€¢ Our method consistently outperforms existing techniques and achieves state-of-the-art results across multiple multimodal benchmarks, validating its generality and effectiveness.\n\n2 Related Work\nEarly efforts to understand how information flows within language models primarily focused on transformers in unimodal settings. For example, studies like (Cao et al. 2020 ; Frank et al. 2021 ) analyzed how attention mechanisms propagate syntactic and semantic information in transformers, while others (Aflalo et al. 2022 ; Chefer et al. 2021 ; Lyu et al. 2022 ; Stan et al. 2024 ) explored the roles of feed-forward networks and memory representation in information transformation. These works provided foundational insights into token-level dependency and hierarchical abstraction but largely focused on language-only settings. Building on this, recent studies examined how LLMs acquire knowledge across layers. Lin et al. ( 2025 ) investigates the impact of visual fusion positions on MLLM performance, comparing external and internal integration strategies for incorporating visual information. Jin et al. ( 2024 ) introduced the concept of â€œdepthâ€ in reasoning, and Ju et al. ( 2024 ) showed context is unevenly distributed across layers. Figure 2: Layer-wise masking effects on accuracy and inference time across multimodal datasets using LLaVA-1.5. Acc is the performance after completely masking the visual information at the corresponding layer. Figure 3: Layer-wise masking effects on accuracy and inference time across multimodal datasets using LLaVA-1.6. A growing body of work has recently turned to the interpretability of MLLMs. For example, Palit et al. ( 2023 ) extended causal tracing methods to MLLMs and showed that visual information progressively influences generation in later layers. Zhao et al. ( 2024 ) surveyed various interpretability techniques for MLLMs, categorizing insights at input, token, and attention levels. However, most of these efforts focus on saliency or input-output attribution and rarely dissect the internal fusion mechanism of visual features across layers. Despite these efforts, what remains largely missing is a comprehensive understanding of how visual information is gradually injected and transformed across different layers within MLLMs. Existing works often rely on single-layer attention analysis or fixed fusion strategies, overlooking the progressive, multi-stage nature of visual integration. A comprehensive understanding of how visual information is progressively injected across layers in MLLMs is still lacking. This gap limits both interpretability and usage of MLLMs in complex vision-language tasks.\n\n3 Exploration of Visual Information Fusion\n3.1 Layer-wise Visual Information Masking To better understand how multimodal information flows and is integrated across layers in MLLMs, we conduct a series of layer-wise image masking experiments on the LLaVA series. Specifically, we follow the input format used in LLaVA and identify the position of image tokens based on the special marker <image> . In LLaVA, which uses a ViT-based image encoder, the number of image tokens is 576, allowing us to locate the image segment within the input sequence. Thus, we apply a masking operation to the image tokens at each layer by directly setting their embeddings to zero, effectively removing visual information while preserving the input shape. We then assess how each layer depends on image features by measuring performance changes across datasets. A significant drop in performance after masking indicates that the layer relies heavily on image for multimodal integration. The results are shown in Figure 2 and 3 . 3.2 Analysis and Observations Layer-wise Differences in Visual-Language Fusion. Our results show that masking visual information at shallow layers (layers 0â€“4) leads to a sharp performance drop across all datasets. This suggests that image features are not yet integrated into the multimodal representations at early layers. Removing image at these layers deprives the model of essential visual cues, severely affecting its output. As the layer depth increases, the model begins to incorporate visual information gradually. The layer at which this integration completes varies by dataset: (1) For simple tasks like VQAv2, image-text fusion appears to complete earlier, as performance stabilizes after around layer 16. (2) For complex tasks such as OKVQA, the integration occurs at deeper layers, suggesting that more abstract reasoning requires longer propagation of visual context. In summary, masking the image after layer 19 results in no substantial performance degradation, indicating that the visual semantics have already been sufficiently integrated. Identifying Critical Fusion Layers. In the layer-wise masking experiments, we find that masking visual information at certain specific layers leads to a dramatic drop in model performance, sometimes approaching zero. Notably, these critical layers show strong consistency across different datasets and across various MLLMs, indicating that the same set of layers consistently exhibits such behavior under different settings. Furthermore, we observe that masking at these layers not only degrades performance but also significantly increases inference time (measured as 1 / t 1/t ). We hypothesize that this results from the disruption of efficient reasoning pathways within the model. These critical layers could be categorized into two groups: â€¢ Early fusion layers ( ğ’® \\mathcal{S} = 2, 4, 8, 11, 12, 13): These critical layers are mostly located in the shallow stages of the model, where visual information has not yet been fully fused with textual input. Masking at these layers removes essential visual cues, leading to a sharp performance drop, indicating active fusion is still in progress. â€¢ Review layer (29): After the early fusion layers, model performance stabilizes and becomes largely insensitive to visual masking, suggesting that fusion is mostly complete. However, at layer 29, masking again causes a significant drop, resembling the behavior of early fusion layers. We hypothesize that this layer performs a â€œ final visual check â€ before the model generates its output. This mirrors human behavior, where one might glance back at the image one last time before making a decision. The experiment reveals key phenomena: hierarchical differences in visual information fusion and the presence of critical fusion layer. Specifically, we observe: (1) A set of shallow layers serve as essential early fusion layers for integrating multimodal information. (2) At a deep layer (review layer), the LLM exhibits a review-like behavior, where visual information is revisited even after the integration process has been completed. This suggests that the model still relies on visual cues for final decision verification.\n\n3.1 Layer-wise Visual Information Masking\nTo better understand how multimodal information flows and is integrated across layers in MLLMs, we conduct a series of layer-wise image masking experiments on the LLaVA series. Specifically, we follow the input format used in LLaVA and identify the position of image tokens based on the special marker <image> . In LLaVA, which uses a ViT-based image encoder, the number of image tokens is 576, allowing us to locate the image segment within the input sequence. Thus, we apply a masking operation to the image tokens at each layer by directly setting their embeddings to zero, effectively removing visual information while preserving the input shape. We then assess how each layer depends on image features by measuring performance changes across datasets. A significant drop in performance after masking indicates that the layer relies heavily on image for multimodal integration. The results are shown in Figure 2 and 3 .\n\n3.2 Analysis and Observations\nLayer-wise Differences in Visual-Language Fusion. Our results show that masking visual information at shallow layers (layers 0â€“4) leads to a sharp performance drop across all datasets. This suggests that image features are not yet integrated into the multimodal representations at early layers. Removing image at these layers deprives the model of essential visual cues, severely affecting its output. As the layer depth increases, the model begins to incorporate visual information gradually. The layer at which this integration completes varies by dataset: (1) For simple tasks like VQAv2, image-text fusion appears to complete earlier, as performance stabilizes after around layer 16. (2) For complex tasks such as OKVQA, the integration occurs at deeper layers, suggesting that more abstract reasoning requires longer propagation of visual context. In summary, masking the image after layer 19 results in no substantial performance degradation, indicating that the visual semantics have already been sufficiently integrated. Identifying Critical Fusion Layers. In the layer-wise masking experiments, we find that masking visual information at certain specific layers leads to a dramatic drop in model performance, sometimes approaching zero. Notably, these critical layers show strong consistency across different datasets and across various MLLMs, indicating that the same set of layers consistently exhibits such behavior under different settings. Furthermore, we observe that masking at these layers not only degrades performance but also significantly increases inference time (measured as 1 / t 1/t ). We hypothesize that this results from the disruption of efficient reasoning pathways within the model. These critical layers could be categorized into two groups: â€¢ Early fusion layers ( ğ’® \\mathcal{S} = 2, 4, 8, 11, 12, 13): These critical layers are mostly located in the shallow stages of the model, where visual information has not yet been fully fused with textual input. Masking at these layers removes essential visual cues, leading to a sharp performance drop, indicating active fusion is still in progress. â€¢ Review layer (29): After the early fusion layers, model performance stabilizes and becomes largely insensitive to visual masking, suggesting that fusion is mostly complete. However, at layer 29, masking again causes a significant drop, resembling the behavior of early fusion layers. We hypothesize that this layer performs a â€œ final visual check â€ before the model generates its output. This mirrors human behavior, where one might glance back at the image one last time before making a decision. The experiment reveals key phenomena: hierarchical differences in visual information fusion and the presence of critical fusion layer. Specifically, we observe: (1) A set of shallow layers serve as essential early fusion layers for integrating multimodal information. (2) At a deep layer (review layer), the LLM exhibits a review-like behavior, where visual information is revisited even after the integration process has been completed. This suggests that the model still relies on visual cues for final decision verification.\n\nLayer-wise Differences in Visual-Language Fusion.\nOur results show that masking visual information at shallow layers (layers 0â€“4) leads to a sharp performance drop across all datasets. This suggests that image features are not yet integrated into the multimodal representations at early layers. Removing image at these layers deprives the model of essential visual cues, severely affecting its output. As the layer depth increases, the model begins to incorporate visual information gradually. The layer at which this integration completes varies by dataset: (1) For simple tasks like VQAv2, image-text fusion appears to complete earlier, as performance stabilizes after around layer 16. (2) For complex tasks such as OKVQA, the integration occurs at deeper layers, suggesting that more abstract reasoning requires longer propagation of visual context. In summary, masking the image after layer 19 results in no substantial performance degradation, indicating that the visual semantics have already been sufficiently integrated.\n\nIdentifying Critical Fusion Layers.\nIn the layer-wise masking experiments, we find that masking visual information at certain specific layers leads to a dramatic drop in model performance, sometimes approaching zero. Notably, these critical layers show strong consistency across different datasets and across various MLLMs, indicating that the same set of layers consistently exhibits such behavior under different settings. Furthermore, we observe that masking at these layers not only degrades performance but also significantly increases inference time (measured as 1 / t 1/t ). We hypothesize that this results from the disruption of efficient reasoning pathways within the model. These critical layers could be categorized into two groups: â€¢ Early fusion layers ( ğ’® \\mathcal{S} = 2, 4, 8, 11, 12, 13): These critical layers are mostly located in the shallow stages of the model, where visual information has not yet been fully fused with textual input. Masking at these layers removes essential visual cues, leading to a sharp performance drop, indicating active fusion is still in progress. â€¢ Review layer (29): After the early fusion layers, model performance stabilizes and becomes largely insensitive to visual masking, suggesting that fusion is mostly complete. However, at layer 29, masking again causes a significant drop, resembling the behavior of early fusion layers. We hypothesize that this layer performs a â€œ final visual check â€ before the model generates its output. This mirrors human behavior, where one might glance back at the image one last time before making a decision. The experiment reveals key phenomena: hierarchical differences in visual information fusion and the presence of critical fusion layer. Specifically, we observe: (1) A set of shallow layers serve as essential early fusion layers for integrating multimodal information. (2) At a deep layer (review layer), the LLM exhibits a review-like behavior, where visual information is revisited even after the integration process has been completed. This suggests that the model still relies on visual cues for final decision verification.\n\n4 Method\nAs shown in Figure 4 , we compute the contrastive attention by selecting appropriate pre-integrated and post-integrated layers, in section 4.1 . The contrastive attention is defined as the difference in attention distribution, capturing how attention shifts as visual information is fused. Then, in section 4.2 , we leverage the contrastive attention to guide the review process, helping suppress irrelevant information and focus on content that is more relevant to the task. Figure 4: overview of contrastive attention and review-stage masking. 4.1 Selecting Contrastive Layers To investigate how the attention shifts during the process of task understanding, we explore two contrastive layers: the post-integrated layer and the pre-integrated layer. The post-integrated layer represents the stage at which task semantics and visual information have been fully fused, while the pre-integrated layer captures the initial perception of the image. By comparing attention maps at these two layers, we aim to reveal which image regions become increasingly aligned with task objectives. We begin by locating the post-integrated layer. Experiment shows that review behavior occurs at layer 29. Layer 28 masking has minimal impact on performance, indicating that the fusion of visual features is largely complete. Thus, we designate layer 28 as the post-integrated layer. MLLMs form an initial, task-agnostic perception of the image at shallow layers, which may lead to high-attention noise unrelated to task. To mitigate this, we propose contrastive attention, which captures the difference between the attention maps of the post-integrated layer and a pre-integrated layer. This difference is designed to suppress irrelevant high-attention regions and enhance task-relevant visual focus. We compute the Hellinger distance between each layerâ€™s attention map and that of the post-integrated layer, and select the one with the highest distance as the pre-integrated layer. We explore multiple strategies to define the candidate set ğ’ \\mathcal{C} for selecting the pre-integrated layer: (a) without any constraint, (b) constrained to shallow layers (layers 1â€“16), (c) constrained to deep layers (layers 17â€“28), and (d) constrained to a predefined early fusion set ğ’® \\mathcal{S} identified through our analysis. Details are provided in Table 3 and section 5.5 . Table 1: Effectiveness of our method across multiple MLLM baselines. MLLMs GQA VQAv2 OKVQA VizWiz TextVQA DocVQA Average BLIP-2 (Li et al. 2023 ) 41.0 41.0 45.9 19.6 42.5 - 38.00 Flamingo-80B (Alayrac et al. 2022 ) 43.3 56.3 50.6 31.6 - - 45.45 InstructBILP-8B (Dai et al. 2023 ) 49.2 51.9 49.9 34.5 50.1 9.2 40.80 IDEFICS (LaurenÃ§on et al. 2023 ) 38.4 50.9 38.4 35.5 25.9 - 37.82 LLaVA (Liu et al. 2023 ) 49.5 63.6 45.2 30.7 38.9 10.5 39.72 InternVL-MLP (Chen et al. 2024 ) 62.9 79.3 42.9 52.5 57.0 23.2 52.97 InternVL-QLLaMA (Chen et al. 2024 ) 57.7 72.3 51.0 44.5 42.1 25.0 48.77 Qwen-VL (Bai et al. 2023 ) 59.3 78.8 46.9 35.2 57.2 25.5 50.49 Qwen2-VL (Wang et al. 2024 ) 63.7 79.8 48.5 60.4 71.9 59.8 64.02 LLaVA-v1.5 (Liu et al. 2023 ) 66.0 74.0 51.6 57.8 57.2 24.6 55.19 + Ours 69.4 77.6 55.4 60.4 59.8 26.9 58.25 LLaVA-v1.6 (Liu et al. 2024 ) 69.3 79.9 52.5 59.6 72.0 65.8 66.52 + Ours 71.6 80.7 56.9 62.0 75.8 68.1 69.18 We formalize the selection process as follows: given a set of candidate attention matrices A ( i ) {A^{(i)}} , where each A ( i ) âˆˆ â„ d Ã— d A^{(i)}\\in\\mathbb{R}^{d\\times d} represents the attention weights at layer i i , we define the attention at the post-integrated layer (layer k = 28 k=28 ) as A ( k ) A^{(k)} . To identify the attention matrix that deviates most significantly from A ( k ) A^{(k)} in terms of distributional difference, we compute the Hellinger distance between each A ( i ) A^{(i)} and the reference A ( k ) A^{(k)} , and select the one with the maximal distance from candidate layers ğ’ \\mathcal{C} : i âˆ— = arg â¡ max i âˆˆ ğ’ â¡ H â€‹ ( A ( i ) , A ( k ) ) i^{*}=\\arg\\max_{i\\in\\mathcal{C}}H\\left(A^{(i)},A^{(k)}\\right) (1) where H â€‹ ( P , Q ) H(P,Q) denotes the Hellinger distance between two probability distributions P P and Q Q , defined as: H â€‹ ( P , Q ) = 1 2 â€‹ âˆ‘ j = 1 d ( p j âˆ’ q j ) 2 H(P,Q)=\\frac{1}{\\sqrt{2}}\\sqrt{\\sum_{j=1}^{d}\\left(\\sqrt{p_{j}}-\\sqrt{q_{j}}\\right)^{2}} (2) where d d denotes the number of image tokens involved in the attention distribution. 4.2 Contrastive Attention for Review As shown in Figure 4 , we leverage the observed layer-wise fusion phenomenon in MLLMs, particularly the review-like behavior, where the model reattends to the image even after the primary fusion appears to be completed. Based on this, we apply the contrastive attention to guide the masking of visual information during this review stage. Specifically, the contrastive attention is computed as the difference between the attention at pre-integrated layer ğ€ ( i âˆ— ) \\mathbf{A}^{(i^{*})} and that at post-integrated layer ğ€ ( k ) \\mathbf{A}^{(k)} . IA = | ğ€ ( k ) âˆ’ ğ€ ( i âˆ— ) | \\text{IA}=\\left|\\mathbf{A}^{(k)}-\\mathbf{A}^{(i^{*})}\\right| (3) The difference between the attention from the pre-integrated layer and the post-integrated layer captures how visual information shifts the attention during the visual integration and understanding. We leverage this signal to refine the modelâ€™s attention at review layer through a soft masking strategy. Specifically, we identify visual tokens whose contrastive attention scores fall below the Ï \\rho -th percentile and softly suppress their influence by scaling down features. Formally, let Q Ï â€‹ ( IA ) Q_{\\rho}(\\text{IA}) denote the Ï \\rho -th quantile of the contrastive attention scores IA . The masked visual features are computed as: ğ„ j masked = Î» â‹… ğ„ j , if IA â€‹ j < Q Ï â€‹ ( IA ) \\mathbf{E}_{j}^{\\text{masked}}=\\lambda\\cdot\\mathbf{E}_{j},\\quad\\text{if }\\text{IA}j<Q_{\\rho}(\\text{IA}) (4) where ğ„ j \\mathbf{E}_{j} is the embedding of the j j -th visual token, and Î» â‰ª 1 \\lambda\\ll 1 is a constant that softly downweights low-relevance regions without fully discarding them. We analyze the effect of varying the masking ratio Ï \\rho in section 5.6 .\n\n4.1 Selecting Contrastive Layers\nTo investigate how the attention shifts during the process of task understanding, we explore two contrastive layers: the post-integrated layer and the pre-integrated layer. The post-integrated layer represents the stage at which task semantics and visual information have been fully fused, while the pre-integrated layer captures the initial perception of the image. By comparing attention maps at these two layers, we aim to reveal which image regions become increasingly aligned with task objectives. We begin by locating the post-integrated layer. Experiment shows that review behavior occurs at layer 29. Layer 28 masking has minimal impact on performance, indicating that the fusion of visual features is largely complete. Thus, we designate layer 28 as the post-integrated layer. MLLMs form an initial, task-agnostic perception of the image at shallow layers, which may lead to high-attention noise unrelated to task. To mitigate this, we propose contrastive attention, which captures the difference between the attention maps of the post-integrated layer and a pre-integrated layer. This difference is designed to suppress irrelevant high-attention regions and enhance task-relevant visual focus. We compute the Hellinger distance between each layerâ€™s attention map and that of the post-integrated layer, and select the one with the highest distance as the pre-integrated layer. We explore multiple strategies to define the candidate set ğ’ \\mathcal{C} for selecting the pre-integrated layer: (a) without any constraint, (b) constrained to shallow layers (layers 1â€“16), (c) constrained to deep layers (layers 17â€“28), and (d) constrained to a predefined early fusion set ğ’® \\mathcal{S} identified through our analysis. Details are provided in Table 3 and section 5.5 . Table 1: Effectiveness of our method across multiple MLLM baselines. MLLMs GQA VQAv2 OKVQA VizWiz TextVQA DocVQA Average BLIP-2 (Li et al. 2023 ) 41.0 41.0 45.9 19.6 42.5 - 38.00 Flamingo-80B (Alayrac et al. 2022 ) 43.3 56.3 50.6 31.6 - - 45.45 InstructBILP-8B (Dai et al. 2023 ) 49.2 51.9 49.9 34.5 50.1 9.2 40.80 IDEFICS (LaurenÃ§on et al. 2023 ) 38.4 50.9 38.4 35.5 25.9 - 37.82 LLaVA (Liu et al. 2023 ) 49.5 63.6 45.2 30.7 38.9 10.5 39.72 InternVL-MLP (Chen et al. 2024 ) 62.9 79.3 42.9 52.5 57.0 23.2 52.97 InternVL-QLLaMA (Chen et al. 2024 ) 57.7 72.3 51.0 44.5 42.1 25.0 48.77 Qwen-VL (Bai et al. 2023 ) 59.3 78.8 46.9 35.2 57.2 25.5 50.49 Qwen2-VL (Wang et al. 2024 ) 63.7 79.8 48.5 60.4 71.9 59.8 64.02 LLaVA-v1.5 (Liu et al. 2023 ) 66.0 74.0 51.6 57.8 57.2 24.6 55.19 + Ours 69.4 77.6 55.4 60.4 59.8 26.9 58.25 LLaVA-v1.6 (Liu et al. 2024 ) 69.3 79.9 52.5 59.6 72.0 65.8 66.52 + Ours 71.6 80.7 56.9 62.0 75.8 68.1 69.18 We formalize the selection process as follows: given a set of candidate attention matrices A ( i ) {A^{(i)}} , where each A ( i ) âˆˆ â„ d Ã— d A^{(i)}\\in\\mathbb{R}^{d\\times d} represents the attention weights at layer i i , we define the attention at the post-integrated layer (layer k = 28 k=28 ) as A ( k ) A^{(k)} . To identify the attention matrix that deviates most significantly from A ( k ) A^{(k)} in terms of distributional difference, we compute the Hellinger distance between each A ( i ) A^{(i)} and the reference A ( k ) A^{(k)} , and select the one with the maximal distance from candidate layers ğ’ \\mathcal{C} : i âˆ— = arg â¡ max i âˆˆ ğ’ â¡ H â€‹ ( A ( i ) , A ( k ) ) i^{*}=\\arg\\max_{i\\in\\mathcal{C}}H\\left(A^{(i)},A^{(k)}\\right) (1) where H â€‹ ( P , Q ) H(P,Q) denotes the Hellinger distance between two probability distributions P P and Q Q , defined as: H â€‹ ( P , Q ) = 1 2 â€‹ âˆ‘ j = 1 d ( p j âˆ’ q j ) 2 H(P,Q)=\\frac{1}{\\sqrt{2}}\\sqrt{\\sum_{j=1}^{d}\\left(\\sqrt{p_{j}}-\\sqrt{q_{j}}\\right)^{2}} (2) where d d denotes the number of image tokens involved in the attention distribution.\n\n4.2 Contrastive Attention for Review\nAs shown in Figure 4 , we leverage the observed layer-wise fusion phenomenon in MLLMs, particularly the review-like behavior, where the model reattends to the image even after the primary fusion appears to be completed. Based on this, we apply the contrastive attention to guide the masking of visual information during this review stage. Specifically, the contrastive attention is computed as the difference between the attention at pre-integrated layer ğ€ ( i âˆ— ) \\mathbf{A}^{(i^{*})} and that at post-integrated layer ğ€ ( k ) \\mathbf{A}^{(k)} . IA = | ğ€ ( k ) âˆ’ ğ€ ( i âˆ— ) | \\text{IA}=\\left|\\mathbf{A}^{(k)}-\\mathbf{A}^{(i^{*})}\\right| (3) The difference between the attention from the pre-integrated layer and the post-integrated layer captures how visual information shifts the attention during the visual integration and understanding. We leverage this signal to refine the modelâ€™s attention at review layer through a soft masking strategy. Specifically, we identify visual tokens whose contrastive attention scores fall below the Ï \\rho -th percentile and softly suppress their influence by scaling down features. Formally, let Q Ï â€‹ ( IA ) Q_{\\rho}(\\text{IA}) denote the Ï \\rho -th quantile of the contrastive attention scores IA . The masked visual features are computed as: ğ„ j masked = Î» â‹… ğ„ j , if IA â€‹ j < Q Ï â€‹ ( IA ) \\mathbf{E}_{j}^{\\text{masked}}=\\lambda\\cdot\\mathbf{E}_{j},\\quad\\text{if }\\text{IA}j<Q_{\\rho}(\\text{IA}) (4) where ğ„ j \\mathbf{E}_{j} is the embedding of the j j -th visual token, and Î» â‰ª 1 \\lambda\\ll 1 is a constant that softly downweights low-relevance regions without fully discarding them. We analyze the effect of varying the masking ratio Ï \\rho in section 5.6 .\n\n5 Experiments\nTable 2: Effectiveness of contrastive attention compared to existing enhanced methods. MLLMs Methods GQA VQAv2 OKVQA VizWiz TextVQA DocVQA Average LLaVA-v1.5 + DoLA (Chuang et al. 2024 ) 66.8 70.1 52.8 58.4 56.1 26.0 55.04 + ViCrop (Zhang et al. 2025 ) 67.0 76.0 54.8 59.6 56.6 25.1 56.52 + Ours 69.4 77.6 55.4 60.4 59.8 26.9 58.25 LLaVA-v1.6 + DoLA (Chuang et al. 2024 ) 71.0 75.4 53.6 60.9 72.1 67.2 66.69 + ViCrop (Zhang et al. 2025 ) 70.2 81.4 54.1 61.3 73.4 65.6 67.00 + Ours 71.6 80.7 56.9 62.0 75.8 68.1 69.18 5.1 Datasets and Setting The proposed method is evaluated on six widely-used multimodal datasets: VQAv2 (Goyal et al. 2017 ) , GQA (Hudson and Manning 2019 ) , TextVQA (Singh et al. 2019 ) , OKVQA (Marino et al. 2019 ) , VizWiz (Gurari et al. 2018 ) , and DocVQA (Mathew et al. 2021 ) . We report accuracy on each dataset as the evaluation metric. All experiments are conducted based on the open-source and SOTA LLaVA series (Liu et al. 2023 , 2024 ) : LLaVA-1.5-7B, LLaVA-1.6-Vicuna-7B. The experiments were conducted on an RTX A800 GPU using PyTorch 2.0, and the system was based on Ubuntu 20.04. 5.2 Comparison with MLLMs The baseline MLLMs include BLIP-2 (Li et al. 2023 ) , Flamingo (Alayrac et al. 2022 ) , InstructBLIP (Dai et al. 2023 ) , IDEFICS (LaurenÃ§on et al. 2023 ) , LLaVA (Liu et al. 2023 ) , LLaVA-1.5 (Liu et al. 2024 ) , InternVL-MLP (Chen et al. 2024 ) , InternVL-QLLaMA (Chen et al. 2024 ) , and Qwen-VL (Bai et al. 2023 ) . As shown in Table 1 , integrating our method into LLaVA-1.5 yields state-of-the-art performance across six VQA benchmarks, achieving an average score of 58.17. This surpasses strong recent baselines such as InternVL-MLP (52.97) and Qwen-VL (50.49). 5.3 Comparison with Training-free Methods Table 3: Pre-integrated layer selection on LLaVA-v1.5-7B. GQA VQAv2 OKVQA VizWiz TextVQA DocVQA Average all (0-28) 65.40 72.97 51.34 57.77 52.50 26.10 54.35 deep (16-28) 60.30 70.11 52.80 57.40 56.10 26.00 53.79 shallow (0-15) 68.30 76.49 53.92 59.30 57.00 26.40 56.90 fusion ( ğ’® \\mathcal{S} ) 69.40 77.59 55.40 60.40 59.80 26.90 58.25 To evaluate the effectiveness of our proposed training-free method, as shown in Table 2 , we also compare it against two representative methods: (1) DoLA (Chuang et al. 2024 ) proposes a training-free decoding strategy that contrasts the output logits between earlier and later transformer layers, amplifying deep-layer factual knowledge and reducing hallucination without requiring additional supervision. (2) ViCrop (Zhang et al. 2025 ) introduces a training-free adversarial framework that manipulates image inputs by strategically masking and cropping regions to challenge the visual reasoning of MLLMs, revealing their sensitivity to localized visual changes. As illustrated in Table 2 , our approach consistently outperforms existing representative methods, demonstrating improved stability and transferability during inference. These results indicate that contrastive attention effectively identifies and enhances critical visual regions, providing a lightweight yet effective enhancement to the inference process without requiring any additional training. 5.4 Selection of post-integrated layer Figure 5: Layer-wise Hellinger distance to the final layer. Before identifying the post-integrated layer for applying contrastive attention, we first clarify two key questions: 1. Where and how does contrastive attention function? Contrastive attention functions by suppressing image regions that are irrelevant to the task. Our exploratory experiments show that image has little effect on deeper layers (e.g., beyond the 20th layer), where visual information has already been fully integrated. However, we observe that a secondary integration of visual and textual information occurs at layer 29, referred to as the â€œreviewâ€ layer. Therefore, to influence the final reasoning, contrastive attention should take effect at layer 29 . 2. Why is layer 28 selected as the post-integrated layer? While the final layer (layer 31) represents the modelâ€™s fully aggregated understanding, it cannot be directly used for contrastive attention. This is because contrastive attention must be computed before layer 29 in order to take effect at the point where visual-textual fusion reoccurs. Therefore, the post-integrated layer needs to be selected before layer 29 . To approximate the role of the final layer, we calculate the Hellinger Distance between the attention distributions of each layer and that of layer 31. The results show that both layer 28 and layer 30 exhibit minimal distance from the final layer, indicating that their attention patterns closely resemble the final attention. Thus, we select layer 28 as the post-integrated layer for computing contrastive attention. 5.5 Selection of Pre-Integrated Layers We systematically explored different strategies for selecting the pre-integrated layer, considering four selection scope from: (a) all layers (unconstrained setting, 0â€“28), (b) deep layers (16â€“28), (c) shallow layers (0â€“16), and (d) the set of fusion layers (2, 4, 8, 11, 12, 13) identified in section 3.2 . The pre-integrated layer refers to the stage where visual information is initially processed, before being influenced by textual context. Therefore, layers with already fused representations should be excluded. As shown in Table 3 , the deep strategy performs poorly because these layers contain cross-modal information, which degrades the quality of contrastive attention. Similarly, the all strategy includes deep layers, introducing noisy and biased attention. In comparison, the shallow strategy achieves better results. The best performance comes from the fusion strategy, which limits selection to empirically identified fusion layers ğ’® \\mathcal{S} . As shown in Figure 6 , we further investigate the distribution of automatically selected pre-integrated layers when no constraints are imposed on the selection range. Specifically, we apply Equation ( 1 ) to identify the layer that exhibits the largest attention divergence from the post-integrated layer. As illustrated, layer 2 is selected with overwhelmingly high frequency across all datasets, indicating a strong concentration around this early layer. For instance, in DocVQA, over 86% of samples select layer 2 as the pre-integrated layer. Figure 6 shows that fusion layers ğ’® \\mathcal{S} are favored as pre-integrated layers, reinforcing our claim that they are well-suited for contrastive attention. Figure 6: Distribution of selected pre-integrated layers across tasks under unconstrained candidate setting. 5.6 Masking Ratio Based on Contrastive Attention Figure 7: Effect of visual token masking ratio on accuracy and inference time at the review layer. Based on the observed review-like behavior, we applied contrastive attention at review layer of the MLLM and used a soft mask strategy to selectively suppress irrelevant visual information. In this section, we investigate how different masking ratios affect model performance and efficiency. As shown in Figure 7 , we evaluated the accuracy and inference time across six benchmarks under varying mask ratios on LLaVA-1.5. We observe a consistent trend: accuracy peaks when 20% of the tokens are masked, but degrades as the masking ratio increases further. Meanwhile, inference time decreases steadily as the masking ratio increases. When the masking ratio reaches 1.0â€”i.e., the model receives no visual inputâ€”the accuracy drops to zero on all datasets. This clearly confirms the critical role of visual signals at the review layer during final-stage reasoning.\n\n5.1 Datasets and Setting\nThe proposed method is evaluated on six widely-used multimodal datasets: VQAv2 (Goyal et al. 2017 ) , GQA (Hudson and Manning 2019 ) , TextVQA (Singh et al. 2019 ) , OKVQA (Marino et al. 2019 ) , VizWiz (Gurari et al. 2018 ) , and DocVQA (Mathew et al. 2021 ) . We report accuracy on each dataset as the evaluation metric. All experiments are conducted based on the open-source and SOTA LLaVA series (Liu et al. 2023 , 2024 ) : LLaVA-1.5-7B, LLaVA-1.6-Vicuna-7B. The experiments were conducted on an RTX A800 GPU using PyTorch 2.0, and the system was based on Ubuntu 20.04.\n\n5.2 Comparison with MLLMs\nThe baseline MLLMs include BLIP-2 (Li et al. 2023 ) , Flamingo (Alayrac et al. 2022 ) , InstructBLIP (Dai et al. 2023 ) , IDEFICS (LaurenÃ§on et al. 2023 ) , LLaVA (Liu et al. 2023 ) , LLaVA-1.5 (Liu et al. 2024 ) , InternVL-MLP (Chen et al. 2024 ) , InternVL-QLLaMA (Chen et al. 2024 ) , and Qwen-VL (Bai et al. 2023 ) . As shown in Table 1 , integrating our method into LLaVA-1.5 yields state-of-the-art performance across six VQA benchmarks, achieving an average score of 58.17. This surpasses strong recent baselines such as InternVL-MLP (52.97) and Qwen-VL (50.49).\n\n5.3 Comparison with Training-free Methods\nTable 3: Pre-integrated layer selection on LLaVA-v1.5-7B. GQA VQAv2 OKVQA VizWiz TextVQA DocVQA Average all (0-28) 65.40 72.97 51.34 57.77 52.50 26.10 54.35 deep (16-28) 60.30 70.11 52.80 57.40 56.10 26.00 53.79 shallow (0-15) 68.30 76.49 53.92 59.30 57.00 26.40 56.90 fusion ( ğ’® \\mathcal{S} ) 69.40 77.59 55.40 60.40 59.80 26.90 58.25 To evaluate the effectiveness of our proposed training-free method, as shown in Table 2 , we also compare it against two representative methods: (1) DoLA (Chuang et al. 2024 ) proposes a training-free decoding strategy that contrasts the output logits between earlier and later transformer layers, amplifying deep-layer factual knowledge and reducing hallucination without requiring additional supervision. (2) ViCrop (Zhang et al. 2025 ) introduces a training-free adversarial framework that manipulates image inputs by strategically masking and cropping regions to challenge the visual reasoning of MLLMs, revealing their sensitivity to localized visual changes. As illustrated in Table 2 , our approach consistently outperforms existing representative methods, demonstrating improved stability and transferability during inference. These results indicate that contrastive attention effectively identifies and enhances critical visual regions, providing a lightweight yet effective enhancement to the inference process without requiring any additional training.\n\n5.4 Selection of post-integrated layer\nFigure 5: Layer-wise Hellinger distance to the final layer. Before identifying the post-integrated layer for applying contrastive attention, we first clarify two key questions: 1. Where and how does contrastive attention function? Contrastive attention functions by suppressing image regions that are irrelevant to the task. Our exploratory experiments show that image has little effect on deeper layers (e.g., beyond the 20th layer), where visual information has already been fully integrated. However, we observe that a secondary integration of visual and textual information occurs at layer 29, referred to as the â€œreviewâ€ layer. Therefore, to influence the final reasoning, contrastive attention should take effect at layer 29 . 2. Why is layer 28 selected as the post-integrated layer? While the final layer (layer 31) represents the modelâ€™s fully aggregated understanding, it cannot be directly used for contrastive attention. This is because contrastive attention must be computed before layer 29 in order to take effect at the point where visual-textual fusion reoccurs. Therefore, the post-integrated layer needs to be selected before layer 29 . To approximate the role of the final layer, we calculate the Hellinger Distance between the attention distributions of each layer and that of layer 31. The results show that both layer 28 and layer 30 exhibit minimal distance from the final layer, indicating that their attention patterns closely resemble the final attention. Thus, we select layer 28 as the post-integrated layer for computing contrastive attention.\n\n1. Where and how does contrastive attention function?\nContrastive attention functions by suppressing image regions that are irrelevant to the task. Our exploratory experiments show that image has little effect on deeper layers (e.g., beyond the 20th layer), where visual information has already been fully integrated. However, we observe that a secondary integration of visual and textual information occurs at layer 29, referred to as the â€œreviewâ€ layer. Therefore, to influence the final reasoning, contrastive attention should take effect at layer 29 .\n\n2. Why is layer 28 selected as the post-integrated layer?\nWhile the final layer (layer 31) represents the modelâ€™s fully aggregated understanding, it cannot be directly used for contrastive attention. This is because contrastive attention must be computed before layer 29 in order to take effect at the point where visual-textual fusion reoccurs. Therefore, the post-integrated layer needs to be selected before layer 29 . To approximate the role of the final layer, we calculate the Hellinger Distance between the attention distributions of each layer and that of layer 31. The results show that both layer 28 and layer 30 exhibit minimal distance from the final layer, indicating that their attention patterns closely resemble the final attention. Thus, we select layer 28 as the post-integrated layer for computing contrastive attention.\n\n5.5 Selection of Pre-Integrated Layers\nWe systematically explored different strategies for selecting the pre-integrated layer, considering four selection scope from: (a) all layers (unconstrained setting, 0â€“28), (b) deep layers (16â€“28), (c) shallow layers (0â€“16), and (d) the set of fusion layers (2, 4, 8, 11, 12, 13) identified in section 3.2 . The pre-integrated layer refers to the stage where visual information is initially processed, before being influenced by textual context. Therefore, layers with already fused representations should be excluded. As shown in Table 3 , the deep strategy performs poorly because these layers contain cross-modal information, which degrades the quality of contrastive attention. Similarly, the all strategy includes deep layers, introducing noisy and biased attention. In comparison, the shallow strategy achieves better results. The best performance comes from the fusion strategy, which limits selection to empirically identified fusion layers ğ’® \\mathcal{S} . As shown in Figure 6 , we further investigate the distribution of automatically selected pre-integrated layers when no constraints are imposed on the selection range. Specifically, we apply Equation ( 1 ) to identify the layer that exhibits the largest attention divergence from the post-integrated layer. As illustrated, layer 2 is selected with overwhelmingly high frequency across all datasets, indicating a strong concentration around this early layer. For instance, in DocVQA, over 86% of samples select layer 2 as the pre-integrated layer. Figure 6 shows that fusion layers ğ’® \\mathcal{S} are favored as pre-integrated layers, reinforcing our claim that they are well-suited for contrastive attention. Figure 6: Distribution of selected pre-integrated layers across tasks under unconstrained candidate setting.\n\n5.6 Masking Ratio Based on Contrastive Attention\nFigure 7: Effect of visual token masking ratio on accuracy and inference time at the review layer. Based on the observed review-like behavior, we applied contrastive attention at review layer of the MLLM and used a soft mask strategy to selectively suppress irrelevant visual information. In this section, we investigate how different masking ratios affect model performance and efficiency. As shown in Figure 7 , we evaluated the accuracy and inference time across six benchmarks under varying mask ratios on LLaVA-1.5. We observe a consistent trend: accuracy peaks when 20% of the tokens are masked, but degrades as the masking ratio increases further. Meanwhile, inference time decreases steadily as the masking ratio increases. When the masking ratio reaches 1.0â€”i.e., the model receives no visual inputâ€”the accuracy drops to zero on all datasets. This clearly confirms the critical role of visual signals at the review layer during final-stage reasoning.\n\n6 Conclusion\nIn this paper, we present a systematic analysis of visual information integration in MLLMs by conducting layer-wise masking experiments. Our findings reveal that visual-text fusion primarily occurs in a certain number of shallow layers, while a review-like re-attention behavior emerges just before the final output. Building on these findings, we propose a training-free method that enhances vision-language reasoning using contrastive attention. This attention is computed by comparing the attention maps between a pre-integrated layer, which represents early visual perception, and an post-integrated layer, where vision and language are fully fused. The resulting contrastive signal reveals the modelâ€™s evolving visual focus and helps suppress irrelevant high-attention noise in later layers. When applied at the review layer, this approach guides the model to concentrate on task-relevant visual regions more effectively, without any additional training. Extensive experiments across six VQA benchmarks demonstrate that our method consistently improves the performance of LLaVA series models, achieving new state-of-the-art results. Our work provides deeper insight into the internal mechanisms of MLLMs and offers a lightweight, effective strategy for enhancing multimodal understanding.\n\nReferences\nE. Aflalo, M. Du, S. Tseng, Y. Liu, C. Wu, N. Duan, and V. Lal (2022) Vl-interpret: an interactive visualization tool for interpreting vision-language transformers . In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition , pp. 21406â€“21415 . Cited by: Â§2 . J. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. (2022) Flamingo: a visual language model for few-shot learning . Advances in neural information processing systems 35 , pp. 23716â€“23736 . Cited by: Table 1 , Â§5.2 . J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou (2023) Qwen-vl: a versatile vision-language model for understanding, localization, text reading, and beyond . arXiv preprint arXiv:2308.12966 . Cited by: Â§1 , Table 1 , Â§5.2 . J. Cao, Z. Gan, Y. Cheng, L. Yu, Y. Chen, and J. Liu (2020) Behind the scene: revealing the secrets of pre-trained vision-and-language models . In Computer Visionâ€“ECCV 2020: 16th European Conference, Glasgow, UK, August 23â€“28, 2020, Proceedings, Part VI 16 , pp. 565â€“580 . Cited by: Â§2 . H. Chefer, S. Gur, and L. Wolf (2021) Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers . In Proceedings of the IEEE/CVF international conference on computer vision , pp. 397â€“406 . Cited by: Â§2 . Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, M. Zhong, Q. Zhang, X. Zhu, L. Lu, et al. (2024) Internvl: scaling up vision foundation models and aligning for generic visual-linguistic tasks . In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 24185â€“24198 . Cited by: Table 1 , Table 1 , Â§5.2 . Y. Chuang, Y. Xie, H. Luo, Y. Kim, J. Glass, and P. He (2024) Dola: decoding by contrasting layers improves factuality in large language models . arXiv preprint arXiv:2309.03883 . Cited by: Â§1 , Â§5.3 , Table 2 , Table 2 . W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi (2023) InstructBLIP: towards general-purpose vision-language models with instruction tuning . External Links: 2305.06500 Cited by: Â§1 , Table 1 , Â§5.2 . S. Frank, E. Bugliarello, and D. Elliott (2021) Vision-and-language or vision-for-language? on cross-modal influence in multimodal transformers . arXiv preprint arXiv:2109.04448 . Cited by: Â§2 . Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh (2017) Making the v in vqa matter: elevating the role of image understanding in visual question answering . In Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 6904â€“6913 . Cited by: Â§1 , Â§5.1 . D. Gurari, Q. Li, A. J. Stangl, A. Guo, C. Lin, K. Grauman, J. Luo, and J. P. Bigham (2018) Vizwiz grand challenge: answering visual questions from blind people . In Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 3608â€“3617 . Cited by: Â§5.1 . D. A. Hudson and C. D. Manning (2019) Gqa: a new dataset for real-world visual reasoning and compositional question answering . In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 6700â€“6709 . Cited by: Â§5.1 . M. Jin, Q. Yu, J. Huang, Q. Zeng, Z. Wang, W. Hua, H. Zhao, K. Mei, Y. Meng, K. Ding, et al. (2024) Exploring concept depth: how large language models acquire knowledge and concept at different layers? . arXiv preprint arXiv:2404.07066 . Cited by: Â§2 . T. Ju, W. Sun, W. Du, X. Yuan, Z. Ren, and G. Liu (2024) How large language models encode context knowledge? a layer-wise probing study . arXiv preprint arXiv:2402.16061 . Cited by: Â§2 . H. LaurenÃ§on, L. Saulnier, L. Tronchon, S. Bekman, A. Singh, A. Lozhkov, T. Wang, S. Karamcheti, A. Rush, D. Kiela, et al. (2023) Obelics: an open web-scale filtered dataset of interleaved image-text documents . Advances in Neural Information Processing Systems 36 , pp. 71683â€“71702 . Cited by: Table 1 , Â§5.2 . J. Li, D. Li, S. Savarese, and S. Hoi (2023) Blip-2: bootstrapping language-image pre-training with frozen image encoders and large language models . In International conference on machine learning , pp. 19730â€“19742 . Cited by: Table 1 , Â§5.2 . J. Lin, H. Chen, Y. Fan, Y. Fan, X. Jin, H. Su, J. Fu, and X. Shen (2025) Multi-layer visual feature fusion in multimodal llms: methods, analysis, and best practices . arXiv preprint arXiv:2503.06063 . Cited by: Â§2 . H. Liu, C. Li, Y. Li, and Y. J. Lee (2024) Improved baselines with visual instruction tuning . arXiv:2310.03744 . Cited by: Table 1 , Â§5.1 , Â§5.2 . H. Liu, C. Li, Q. Wu, and Y. J. Lee (2023) Visual instruction tuning . NeurIPS . Cited by: Â§1 , Â§1 , Table 1 , Table 1 , Â§5.1 , Â§5.2 . Y. Lyu, P. P. Liang, Z. Deng, R. Salakhutdinov, and L. Morency (2022) Dime: fine-grained interpretations of multimodal models via disentangled local explanations . In Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society , pp. 455â€“467 . Cited by: Â§2 . K. Marino, M. Rastegari, A. Farhadi, and R. Mottaghi (2019) Ok-vqa: a visual question answering benchmark requiring external knowledge . In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition , pp. 3195â€“3204 . Cited by: Â§1 , Â§5.1 . M. Mathew, D. Karatzas, and C. Jawahar (2021) Docvqa: a dataset for vqa on document images . In Proceedings of the IEEE/CVF winter conference on applications of computer vision , pp. 2200â€“2209 . Cited by: Â§5.1 . V. Palit, R. Pandey, A. Arora, and P. P. Liang (2023) Towards vision-language mechanistic interpretability: a causal tracing tool for blip . In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 2856â€“2861 . Cited by: Â§2 . A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach (2019) Towards vqa models that can read . In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 8317â€“8326 . Cited by: Â§5.1 . G. B. M. Stan, E. Aflalo, R. Y. Rohekar, A. Bhiwandiwalla, S. Tseng, M. L. Olson, Y. Gurwicz, C. Wu, N. Duan, and V. Lal (2024) LVLM-interpret: an interpretability tool for large vision-language models . arXiv preprint arXiv:2404.03118 . Cited by: Â§2 . P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K. Chen, X. Liu, J. Wang, W. Ge, et al. (2024) Qwen2-vl: enhancing vision-language modelâ€™s perception of the world at any resolution . arXiv preprint arXiv:2409.12191 . Cited by: Table 1 . J. Zhang, M. Khayatkhoei, P. Chhikara, and F. Ilievski (2025) MLLMs know where to look: training-free perception of small visual details with multimodal llms . arXiv preprint arXiv:2502.17422 . Cited by: Â§1 , Â§5.3 , Table 2 , Table 2 . Q. Zhao, M. Xu, K. Gupta, A. Asthana, L. Zheng, and S. Gould (2024) The first to know: how token distributions reveal hidden knowledge in large vision-language models? . In European Conference on Computer Vision , pp. 127â€“142 . Cited by: Â§2 .\n\nReproducibility Checklist\nInstructions for Authors: This document outlines key aspects for assessing reproducibility. Please provide your input by editing this .tex file directly. For each question (that applies), replace the â€œType your response hereâ€ text with your answer. Example: If a question appears as \\question{Proofs of all novel claims are included} {(yes/partial/no)} Type your response here you would change it to: \\question{Proofs of all novel claims are included} {(yes/partial/no)} yes Please make sure to: â€¢ Replace ONLY the â€œType your response hereâ€ text and nothing else. â€¢ Use one of the options listed for that question (e.g., yes , no , partial , or NA ). â€¢ Not modify any other part of the \\question command or any other lines in this document. You can \\input this .tex file right before \\end{document} of your main file or compile it as a stand-alone document. Check the instructions on your conferenceâ€™s website to see if you will be asked to provide this checklist with your paper or separately. 1. General Paper Structure 1.1. Includes a conceptual outline and/or pseudocode description of AI methods introduced (yes/partial/no/NA) yes 1.2. Clearly delineates statements that are opinions, hypothesis, and speculation from objective facts and results (yes/no) yes 1.3. Provides well-marked pedagogical references for less-familiar readers to gain background necessary to replicate the paper (yes/no) yes 2. Theoretical Contributions 2.1. Does this paper make theoretical contributions? (yes/no) yes If yes, please address the following points: 2.2. All assumptions and restrictions are stated clearly and formally (yes/partial/no) yes 2.3. All novel claims are stated formally (e.g., in theorem statements) (yes/partial/no) yes 2.4. Proofs of all novel claims are included (yes/partial/no) yes 2.5. Proof sketches or intuitions are given for complex and/or novel results (yes/partial/no) yes 2.6. Appropriate citations to theoretical tools used are given (yes/partial/no) yes 2.7. All theoretical claims are demonstrated empirically to hold (yes/partial/no/NA) yes 2.8. All experimental code used to eliminate or disprove claims is included (yes/no/NA) yes 3. Dataset Usage 3.1. Does this paper rely on one or more datasets? (yes/no) yes If yes, please address the following points: 3.2. A motivation is given for why the experiments are conducted on the selected datasets (yes/partial/no/NA) yes 3.3. All novel datasets introduced in this paper are included in a data appendix (yes/partial/no/NA) NA 3.4. All novel datasets introduced in this paper will be made publicly available upon publication of the paper with a license that allows free usage for research purposes (yes/partial/no/NA) NA 3.5. All datasets drawn from the existing literature (potentially including authorsâ€™ own previously published work) are accompanied by appropriate citations (yes/no/NA) yes 3.6. All datasets drawn from the existing literature (potentially including authorsâ€™ own previously published work) are publicly available (yes/partial/no/NA) yes 3.7. All datasets that are not publicly available are described in detail, with explanation why publicly available alternatives are not scientifically satisficing (yes/partial/no/NA) NA 4. Computational Experiments 4.1. Does this paper include computational experiments? (yes/no) yes If yes, please address the following points: 4.2. This paper states the number and range of values tried per (hyper-) parameter during development of the paper, along with the criterion used for selecting the final parameter setting (yes/partial/no/NA) yes 4.3. Any code required for pre-processing data is included in the appendix (yes/partial/no) yes 4.4. All source code required for conducting and analyzing the experiments is included in a code appendix (yes/partial/no) yes 4.5. All source code required for conducting and analyzing the experiments will be made publicly available upon publication of the paper with a license that allows free usage for research purposes (yes/partial/no) yes 4.6. All source code implementing new methods have comments detailing the implementation, with references to the paper where each step comes from (yes/partial/no) yes 4.7. If an algorithm depends on randomness, then the method used for setting seeds is described in a way sufficient to allow replication of results (yes/partial/no/NA) yes 4.8. This paper specifies the computing infrastructure used for running experiments (hardware and software), including GPU/CPU models; amount of memory; operating system; names and versions of relevant software libraries and frameworks (yes/partial/no) yes 4.9. This paper formally describes evaluation metrics used and explains the motivation for choosing these metrics (yes/partial/no) yes 4.10. This paper states the number of algorithm runs used to compute each reported result (yes/no) yes 4.11. Analysis of experiments goes beyond single-dimensional summaries of performance (e.g., average; median) to include measures of variation, confidence, or other distributional information (yes/no) no 4.12. The significance of any improvement or decrease in performance is judged using appropriate statistical tests (e.g., Wilcoxon signed-rank) (yes/partial/no) no 4.13. This paper lists all final (hyper-)parameters used for each model/algorithm in the paperâ€™s experiments (yes/partial/no/NA) yes\n\n1. General Paper Structure\n1.1. Includes a conceptual outline and/or pseudocode description of AI methods introduced (yes/partial/no/NA) yes 1.2. Clearly delineates statements that are opinions, hypothesis, and speculation from objective facts and results (yes/no) yes 1.3. Provides well-marked pedagogical references for less-familiar readers to gain background necessary to replicate the paper (yes/no) yes\n\n2. Theoretical Contributions\n2.1. Does this paper make theoretical contributions? (yes/no) yes If yes, please address the following points: 2.2. All assumptions and restrictions are stated clearly and formally (yes/partial/no) yes 2.3. All novel claims are stated formally (e.g., in theorem statements) (yes/partial/no) yes 2.4. Proofs of all novel claims are included (yes/partial/no) yes 2.5. Proof sketches or intuitions are given for complex and/or novel results (yes/partial/no) yes 2.6. Appropriate citations to theoretical tools used are given (yes/partial/no) yes 2.7. All theoretical claims are demonstrated empirically to hold (yes/partial/no/NA) yes 2.8. All experimental code used to eliminate or disprove claims is included (yes/no/NA) yes\n\n3. Dataset Usage\n3.1. Does this paper rely on one or more datasets? (yes/no) yes If yes, please address the following points: 3.2. A motivation is given for why the experiments are conducted on the selected datasets (yes/partial/no/NA) yes 3.3. All novel datasets introduced in this paper are included in a data appendix (yes/partial/no/NA) NA 3.4. All novel datasets introduced in this paper will be made publicly available upon publication of the paper with a license that allows free usage for research purposes (yes/partial/no/NA) NA 3.5. All datasets drawn from the existing literature (potentially including authorsâ€™ own previously published work) are accompanied by appropriate citations (yes/no/NA) yes 3.6. All datasets drawn from the existing literature (potentially including authorsâ€™ own previously published work) are publicly available (yes/partial/no/NA) yes 3.7. All datasets that are not publicly available are described in detail, with explanation why publicly available alternatives are not scientifically satisficing (yes/partial/no/NA) NA\n\n4. Computational Experiments\n4.1. Does this paper include computational experiments? (yes/no) yes If yes, please address the following points: 4.2. This paper states the number and range of values tried per (hyper-) parameter during development of the paper, along with the criterion used for selecting the final parameter setting (yes/partial/no/NA) yes 4.3. Any code required for pre-processing data is included in the appendix (yes/partial/no) yes 4.4. All source code required for conducting and analyzing the experiments is included in a code appendix (yes/partial/no) yes 4.5. All source code required for conducting and analyzing the experiments will be made publicly available upon publication of the paper with a license that allows free usage for research purposes (yes/partial/no) yes 4.6. All source code implementing new methods have comments detailing the implementation, with references to the paper where each step comes from (yes/partial/no) yes 4.7. If an algorithm depends on randomness, then the method used for setting seeds is described in a way sufficient to allow replication of results (yes/partial/no/NA) yes 4.8. This paper specifies the computing infrastructure used for running experiments (hardware and software), including GPU/CPU models; amount of memory; operating system; names and versions of relevant software libraries and frameworks (yes/partial/no) yes 4.9. This paper formally describes evaluation metrics used and explains the motivation for choosing these metrics (yes/partial/no) yes 4.10. This paper states the number of algorithm runs used to compute each reported result (yes/no) yes 4.11. Analysis of experiments goes beyond single-dimensional summaries of performance (e.g., average; median) to include measures of variation, confidence, or other distributional information (yes/no) no 4.12. The significance of any improvement or decrease in performance is judged using appropriate statistical tests (e.g., Wilcoxon signed-rank) (yes/partial/no) no 4.13. This paper lists all final (hyper-)parameters used for each model/algorithm in the paperâ€™s experiments (yes/partial/no/NA) yes",
      "references": [
        {
          "raw_text": "E. Aflalo, M. Du, S. Tseng, Y. Liu, C. Wu, N. Duan, and V. Lal (2022) Vl-interpret: an interactive visualization tool for interpreting vision-language transformers . In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition , pp. 21406â€“21415 . Cited by: Â§2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "J. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al. (2022) Flamingo: a visual language model for few-shot learning . Advances in neural information processing systems 35 , pp. 23716â€“23736 . Cited by: Table 1 , Â§5.2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S4.T1.1.3.3.1",
            "https://arxiv.org/html/2601.08151v1#S5.SS2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou (2023) Qwen-vl: a versatile vision-language model for understanding, localization, text reading, and beyond . arXiv preprint arXiv:2308.12966 . Cited by: Â§1 , Table 1 , Â§5.2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S1.p1.1",
            "https://arxiv.org/html/2601.08151v1#S4.T1.1.9.9.1",
            "https://arxiv.org/html/2601.08151v1#S5.SS2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "J. Cao, Z. Gan, Y. Cheng, L. Yu, Y. Chen, and J. Liu (2020) Behind the scene: revealing the secrets of pre-trained vision-and-language models . In Computer Visionâ€“ECCV 2020: 16th European Conference, Glasgow, UK, August 23â€“28, 2020, Proceedings, Part VI 16 , pp. 565â€“580 . Cited by: Â§2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "H. Chefer, S. Gur, and L. Wolf (2021) Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers . In Proceedings of the IEEE/CVF international conference on computer vision , pp. 397â€“406 . Cited by: Â§2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, M. Zhong, Q. Zhang, X. Zhu, L. Lu, et al. (2024) Internvl: scaling up vision foundation models and aligning for generic visual-linguistic tasks . In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 24185â€“24198 . Cited by: Table 1 , Table 1 , Â§5.2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S4.T1.1.7.7.1",
            "https://arxiv.org/html/2601.08151v1#S4.T1.1.8.8.1",
            "https://arxiv.org/html/2601.08151v1#S5.SS2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "Y. Chuang, Y. Xie, H. Luo, Y. Kim, J. Glass, and P. He (2024) Dola: decoding by contrasting layers improves factuality in large language models . arXiv preprint arXiv:2309.03883 . Cited by: Â§1 , Â§5.3 , Table 2 , Table 2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S1.p4.1",
            "https://arxiv.org/html/2601.08151v1#S5.SS3.p1.1",
            "https://arxiv.org/html/2601.08151v1#S5.T2.1.1.2.1.2",
            "https://arxiv.org/html/2601.08151v1#S5.T2.1.1.5.4.2"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi (2023) InstructBLIP: towards general-purpose vision-language models with instruction tuning . External Links: 2305.06500 Cited by: Â§1 , Table 1 , Â§5.2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S1.p3.1",
            "https://arxiv.org/html/2601.08151v1#S4.T1.1.4.4.1",
            "https://arxiv.org/html/2601.08151v1#S5.SS2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "S. Frank, E. Bugliarello, and D. Elliott (2021) Vision-and-language or vision-for-language? on cross-modal influence in multimodal transformers . arXiv preprint arXiv:2109.04448 . Cited by: Â§2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh (2017) Making the v in vqa matter: elevating the role of image understanding in visual question answering . In Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 6904â€“6913 . Cited by: Â§1 , Â§5.1 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S1.p1.1",
            "https://arxiv.org/html/2601.08151v1#S5.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "D. Gurari, Q. Li, A. J. Stangl, A. Guo, C. Lin, K. Grauman, J. Luo, and J. P. Bigham (2018) Vizwiz grand challenge: answering visual questions from blind people . In Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 3608â€“3617 . Cited by: Â§5.1 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S5.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "D. A. Hudson and C. D. Manning (2019) Gqa: a new dataset for real-world visual reasoning and compositional question answering . In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 6700â€“6709 . Cited by: Â§5.1 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S5.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "M. Jin, Q. Yu, J. Huang, Q. Zeng, Z. Wang, W. Hua, H. Zhao, K. Mei, Y. Meng, K. Ding, et al. (2024) Exploring concept depth: how large language models acquire knowledge and concept at different layers? . arXiv preprint arXiv:2404.07066 . Cited by: Â§2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "T. Ju, W. Sun, W. Du, X. Yuan, Z. Ren, and G. Liu (2024) How large language models encode context knowledge? a layer-wise probing study . arXiv preprint arXiv:2402.16061 . Cited by: Â§2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "H. LaurenÃ§on, L. Saulnier, L. Tronchon, S. Bekman, A. Singh, A. Lozhkov, T. Wang, S. Karamcheti, A. Rush, D. Kiela, et al. (2023) Obelics: an open web-scale filtered dataset of interleaved image-text documents . Advances in Neural Information Processing Systems 36 , pp. 71683â€“71702 . Cited by: Table 1 , Â§5.2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S4.T1.1.5.5.1",
            "https://arxiv.org/html/2601.08151v1#S5.SS2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "J. Li, D. Li, S. Savarese, and S. Hoi (2023) Blip-2: bootstrapping language-image pre-training with frozen image encoders and large language models . In International conference on machine learning , pp. 19730â€“19742 . Cited by: Table 1 , Â§5.2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S4.T1.1.2.2.1",
            "https://arxiv.org/html/2601.08151v1#S5.SS2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "J. Lin, H. Chen, Y. Fan, Y. Fan, X. Jin, H. Su, J. Fu, and X. Shen (2025) Multi-layer visual feature fusion in multimodal llms: methods, analysis, and best practices . arXiv preprint arXiv:2503.06063 . Cited by: Â§2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "H. Liu, C. Li, Y. Li, and Y. J. Lee (2024) Improved baselines with visual instruction tuning . arXiv:2310.03744 . Cited by: Table 1 , Â§5.1 , Â§5.2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S4.T1.1.13.13.1",
            "https://arxiv.org/html/2601.08151v1#S5.SS1.p1.1",
            "https://arxiv.org/html/2601.08151v1#S5.SS2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "H. Liu, C. Li, Q. Wu, and Y. J. Lee (2023) Visual instruction tuning . NeurIPS . Cited by: Â§1 , Â§1 , Table 1 , Table 1 , Â§5.1 , Â§5.2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S1.p1.1",
            "https://arxiv.org/html/2601.08151v1#S1.p3.1",
            "https://arxiv.org/html/2601.08151v1#S4.T1.1.11.11.1",
            "https://arxiv.org/html/2601.08151v1#S4.T1.1.6.6.1",
            "https://arxiv.org/html/2601.08151v1#S5.SS1.p1.1",
            "https://arxiv.org/html/2601.08151v1#S5.SS2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "Y. Lyu, P. P. Liang, Z. Deng, R. Salakhutdinov, and L. Morency (2022) Dime: fine-grained interpretations of multimodal models via disentangled local explanations . In Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society , pp. 455â€“467 . Cited by: Â§2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "K. Marino, M. Rastegari, A. Farhadi, and R. Mottaghi (2019) Ok-vqa: a visual question answering benchmark requiring external knowledge . In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition , pp. 3195â€“3204 . Cited by: Â§1 , Â§5.1 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S1.p1.1",
            "https://arxiv.org/html/2601.08151v1#S5.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "M. Mathew, D. Karatzas, and C. Jawahar (2021) Docvqa: a dataset for vqa on document images . In Proceedings of the IEEE/CVF winter conference on applications of computer vision , pp. 2200â€“2209 . Cited by: Â§5.1 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S5.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "V. Palit, R. Pandey, A. Arora, and P. P. Liang (2023) Towards vision-language mechanistic interpretability: a causal tracing tool for blip . In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp. 2856â€“2861 . Cited by: Â§2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S2.p2.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach (2019) Towards vqa models that can read . In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp. 8317â€“8326 . Cited by: Â§5.1 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S5.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "G. B. M. Stan, E. Aflalo, R. Y. Rohekar, A. Bhiwandiwalla, S. Tseng, M. L. Olson, Y. Gurwicz, C. Wu, N. Duan, and V. Lal (2024) LVLM-interpret: an interpretability tool for large vision-language models . arXiv preprint arXiv:2404.03118 . Cited by: Â§2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K. Chen, X. Liu, J. Wang, W. Ge, et al. (2024) Qwen2-vl: enhancing vision-language modelâ€™s perception of the world at any resolution . arXiv preprint arXiv:2409.12191 . Cited by: Table 1 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S4.T1.1.10.10.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "J. Zhang, M. Khayatkhoei, P. Chhikara, and F. Ilievski (2025) MLLMs know where to look: training-free perception of small visual details with multimodal llms . arXiv preprint arXiv:2502.17422 . Cited by: Â§1 , Â§5.3 , Table 2 , Table 2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S1.p3.1",
            "https://arxiv.org/html/2601.08151v1#S5.SS3.p1.1",
            "https://arxiv.org/html/2601.08151v1#S5.T2.1.1.3.2.1",
            "https://arxiv.org/html/2601.08151v1#S5.T2.1.1.6.5.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "Q. Zhao, M. Xu, K. Gupta, A. Asthana, L. Zheng, and S. Gould (2024) The first to know: how token distributions reveal hidden knowledge in large vision-language models? . In European Conference on Computer Vision , pp. 127â€“142 . Cited by: Â§2 .",
          "urls": [
            "https://arxiv.org/html/2601.08151v1#S2.p2.1"
          ],
          "dois": [],
          "pdf_links": []
        }
      ],
      "references_dois": [],
      "fallback_urls": [],
      "errors": [],
      "missing_fields": [
        "doi",
        "versions",
        "last_updated_raw",
        "published_date",
        "license",
        "references_dois"
      ],
      "url_hint_if_missing": "Champs manquants: doi, versions, last_updated_raw, published_date, license, references_dois. Tu peux vÃ©rifier ici: abs=https://arxiv.org/abs/2601.08151 | html=https://arxiv.org/html/2601.08151v1 | pdf=https://arxiv.org/pdf/2601.08151"
    }
  ],
  "bundle_html_file": "data_lake\\raw\\arxiv_bundle_20260114_095654.html",
  "supported_fields": [
    "arxiv_id",
    "title",
    "authors",
    "abstract",
    "submitted_date",
    "abs_url",
    "pdf_url",
    "doi",
    "versions",
    "last_updated_raw",
    "html_url",
    "published_date",
    "license",
    "sections",
    "content_text",
    "references",
    "references_dois"
  ]
}