<!-- ===== SEARCH URL: https://arxiv.org/search/cs?query=multimodal%20transformer&searchtype=all&abstracts=show&size=50&start=0 | HTTP 200 ===== -->

<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/1.0.0a5/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;50 of 2,808 results for all: <span class="mathjax">multimodal transformer</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/cs"  aria-role="search">
    
      Searching in archive <strong>cs</strong>. <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=0">Search in all archives.</a>
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="multimodal transformer">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option selected value="all">All fields</option><option value="title">Title</option><option value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=multimodal+transformer&amp;terms-0-field=all&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option selected value="all">All fields</option><option value="title">Title</option><option value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="multimodal transformer">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                     
          
          <li>
            <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=50"
              class="pagination-link "
              aria-label="Page 2"
              aria-current="page">2
            </a>
          </li>
          
          <li>
            <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=100"
              class="pagination-link "
              aria-label="Page 3"
              aria-current="page">3
            </a>
          </li>
          
          <li>
            <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=150"
              class="pagination-link "
              aria-label="Page 4"
              aria-current="page">4
            </a>
          </li>
          
          <li>
            <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=200"
              class="pagination-link "
              aria-label="Page 5"
              aria-current="page">5
            </a>
          </li>
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.07581">arXiv:2601.07581</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.07581">pdf</a>, <a href="https://arxiv.org/ps/2601.07581">ps</a>, <a href="https://arxiv.org/format/2601.07581">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        BenchSeg: A Large-Scale Dataset and Benchmark for Multi-View Food Video Segmentation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=AlMughrabi%2C+A">Ahmad AlMughrabi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rivo%2C+G">Guillermo Rivo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jim%C3%A9nez-Farf%C3%A1n%2C+C">Carlos Jiménez-Farfán</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Haroon%2C+U">Umair Haroon</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Al-Areqi%2C+F">Farid Al-Areqi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jung%2C+H">Hyunjun Jung</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Busam%2C+B">Benjamin Busam</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Marques%2C+R">Ricardo Marques</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Radeva%2C+P">Petia Radeva</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.07581v1-abstract-short" style="display: inline;">
        &hellip;284 meticulously annotated frames, capturing each dish under free 360° camera motion. We evaluate a diverse set of 20 state-of-the-art segmentation models (e.g., SAM-based, <span class="search-hit mathjax">transformer</span>, CNN, and large <span class="search-hit mathjax">multimodal</span>) on the existing FoodSeg103 dataset and evaluate them (alone and combined with video-memory modules) on Benc&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.07581v1-abstract-full').style.display = 'inline'; document.getElementById('2601.07581v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.07581v1-abstract-full" style="display: none;">
        Food image segmentation is a critical task for dietary analysis, enabling accurate estimation of food volume and nutrients. However, current methods suffer from limited multi-view data and poor generalization to new viewpoints. We introduce BenchSeg, a novel multi-view food video segmentation dataset and benchmark. BenchSeg aggregates 55 dish scenes (from Nutrition5k, Vegetables &amp; Fruits, MetaFood3D, and FoodKit) with 25,284 meticulously annotated frames, capturing each dish under free 360° camera motion. We evaluate a diverse set of 20 state-of-the-art segmentation models (e.g., SAM-based, <span class="search-hit mathjax">transformer</span>, CNN, and large <span class="search-hit mathjax">multimodal</span>) on the existing FoodSeg103 dataset and evaluate them (alone and combined with video-memory modules) on BenchSeg. Quantitative and qualitative results demonstrate that while standard image segmenters degrade sharply under novel viewpoints, memory-augmented methods maintain temporal consistency across frames. Our best model based on a combination of SeTR-MLA+XMem2 outperforms prior work (e.g., improving over FoodMem by ~2.63% mAP), offering new insights into food segmentation and tracking for dietary analysis. We release BenchSeg to foster future research. The project page including the dataset annotations and the food segmentation models can be found at https://amughrabi.github.io/benchseg.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.07581v1-abstract-full').style.display = 'none'; document.getElementById('2601.07581v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.07516">arXiv:2601.07516</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.07516">pdf</a>, <a href="https://arxiv.org/ps/2601.07516">ps</a>, <a href="https://arxiv.org/format/2601.07516">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Controlling <span class="search-hit mathjax">Multimodal</span> Conversational Agents with Coverage-Enhanced Latent Actions
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Y">Yongqi Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lang%2C+H">Hao Lang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qian%2C+T">Tieyun Qian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Y">Yongbin Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.07516v1-abstract-short" style="display: inline;">
        Vision-language models are increasingly employed as <span class="search-hit mathjax">multimodal</span> conversational agents (MCAs) for diverse conversational tasks. Recently, reinforcement learning (RL) has been widely explored for adapting MCAs to various human-AI interaction scenarios. Despite showing great enhancement in generalization performance, fine-tuning MCAs via RL still faces challenge&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.07516v1-abstract-full').style.display = 'inline'; document.getElementById('2601.07516v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.07516v1-abstract-full" style="display: none;">
        Vision-language models are increasingly employed as <span class="search-hit mathjax">multimodal</span> conversational agents (MCAs) for diverse conversational tasks. Recently, reinforcement learning (RL) has been widely explored for adapting MCAs to various human-AI interaction scenarios. Despite showing great enhancement in generalization performance, fine-tuning MCAs via RL still faces challenges in handling the extremely large text token space. To address this, we learn a compact latent action space for RL fine-tuning instead. Specifically, we adopt the learning from observation mechanism to construct the codebook for the latent action space, where future observations are leveraged to estimate current latent actions that could further be used to reconstruct future observations. However, the scarcity of paired image-text data hinders learning a codebook with sufficient coverage. Thus, we leverage both paired image-text data and text-only data to construct the latent action space, using a cross-modal projector for <span class="search-hit mathjax">transforming</span> text embeddings into image-text embeddings. We initialize the cross-modal projector on paired image-text data, and further train it on massive text-only data with a novel cycle consistency loss to enhance its robustness. We show that our latent action based method outperforms competitive baselines on two conversation tasks across various RL algorithms.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.07516v1-abstract-full').style.display = 'none'; document.getElementById('2601.07516v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.07235">arXiv:2601.07235</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.07235">pdf</a>, <a href="https://arxiv.org/ps/2601.07235">ps</a>, <a href="https://arxiv.org/format/2601.07235">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Gosai%2C+A">Agnivo Gosai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=De%2C+S">Shuvodeep De</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Thankachan%2C+K">Karun Thankachan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.07235v1-abstract-short" style="display: inline;">
        &hellip;widely used datasets such as IMDb, Rotten Tomatoes, and SST-2, and models ranging from Naive Bayes and support vector machines to LSTM networks, BERT, and attention-based <span class="search-hit mathjax">transformers</span>. Beyond summarizing prior work, this survey differentiates itself by offering a comparative, challenge-driven analysis of how these modeling paradigms address domain-specific i&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.07235v1-abstract-full').style.display = 'inline'; document.getElementById('2601.07235v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.07235v1-abstract-full" style="display: none;">
        This paper presents a comprehensive survey of sentiment analysis methods for movie reviews, a benchmark task that has played a central role in advancing natural language processing. We review the evolution of techniques from early lexicon-based and classical machine learning approaches to modern deep learning architectures and large language models, covering widely used datasets such as IMDb, Rotten Tomatoes, and SST-2, and models ranging from Naive Bayes and support vector machines to LSTM networks, BERT, and attention-based <span class="search-hit mathjax">transformers</span>. Beyond summarizing prior work, this survey differentiates itself by offering a comparative, challenge-driven analysis of how these modeling paradigms address domain-specific issues such as sarcasm, negation, contextual ambiguity, and domain shift, which remain open problems in existing literature. Unlike earlier reviews that focus primarily on text-only pipelines, we also synthesize recent advances in <span class="search-hit mathjax">multimodal</span> sentiment analysis that integrate textual, audio, and visual cues from movie trailers and clips. In addition, we examine emerging concerns related to interpretability, fairness, and robustness that are often underexplored in prior surveys, and we outline future research directions including zero-shot and few-shot learning, hybrid symbolic--neural models, and real-time deployment considerations. Overall, this abstract provides a domain-focused roadmap that highlights both established solutions and unresolved challenges toward building more accurate, generalizable, and explainable sentiment analysis systems for movie review data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.07235v1-abstract-full').style.display = 'none'; document.getElementById('2601.07235v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">31 Pages; 1 figure; 108 references; ongoing paper that would be submitted to suitable Wiley journal</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.06874">arXiv:2601.06874</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.06874">pdf</a>, <a href="https://arxiv.org/ps/2601.06874">ps</a>, <a href="https://arxiv.org/format/2601.06874">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MVGGT: <span class="search-hit mathjax">Multimodal</span> Visual Geometry Grounded <span class="search-hit mathjax">Transformer</span> for Multiview 3D Referring Expression Segmentation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+C">Changli Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+H">Haodong Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ji%2C+J">Jiayi Ji</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yao%2C+Y">Yutian Yao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Du%2C+C">Chunsai Du</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kang%2C+J">Jihua Kang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fu%2C+Y">Yanwei Fu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cao%2C+L">Liujuan Cao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.06874v1-abstract-short" style="display: inline;">
        &hellip;first reconstruct a point cloud and then perform segmentation, often yield low-quality geometry, produce coarse or degraded target regions, and run slowly. We propose the <span class="search-hit mathjax">Multimodal</span> Visual Geometry Grounded <span class="search-hit mathjax">Transformer</span> (MVGGT), an efficient end-to-end framework that integrates language information into sparse-view geom&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06874v1-abstract-full').style.display = 'inline'; document.getElementById('2601.06874v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.06874v1-abstract-full" style="display: none;">
        Most existing 3D referring expression segmentation (3DRES) methods rely on dense, high-quality point clouds, while real-world agents such as robots and mobile phones operate with only a few sparse RGB views and strict latency constraints. We introduce Multi-view 3D Referring Expression Segmentation (MV-3DRES), where the model must recover scene structure and segment the referred object directly from sparse multi-view images. Traditional two-stage pipelines, which first reconstruct a point cloud and then perform segmentation, often yield low-quality geometry, produce coarse or degraded target regions, and run slowly. We propose the <span class="search-hit mathjax">Multimodal</span> Visual Geometry Grounded <span class="search-hit mathjax">Transformer</span> (MVGGT), an efficient end-to-end framework that integrates language information into sparse-view geometric reasoning through a dual-branch design. Training in this setting exposes a critical optimization barrier, termed Foreground Gradient Dilution (FGD), where sparse 3D signals lead to weak supervision. To resolve this, we introduce Per-view No-target Suppression Optimization (PVSO), which provides stronger and more balanced gradients across views, enabling stable and efficient learning. To support consistent evaluation, we build MVRefer, a benchmark that defines standardized settings and metrics for MV-3DRES. Experiments show that MVGGT establishes the first strong baseline and achieves both high accuracy and fast inference, outperforming existing alternatives. Code and models are publicly available at https://mvggt.github.io.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06874v1-abstract-full').style.display = 'none'; document.getElementById('2601.06874v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Project Website: https://mvggt.github.io</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.06847">arXiv:2601.06847</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.06847">pdf</a>, <a href="https://arxiv.org/ps/2601.06847">ps</a>, <a href="https://arxiv.org/format/2601.06847">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+M">Mengmeng Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+X">Xiaoping Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Luo%2C+H">Hao Luo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+F">Fan Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lv%2C+Y">Yisheng Lv</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.06847v1-abstract-short" style="display: inline;">
        &hellip;limitation arises from the scarcity of high-quality, large-scale clinical referring-localization pairs. To address this, we introduce MedGround, an automated pipeline that <span class="search-hit mathjax">transforms</span> segmentation resources into high-quality medical referring grounding data. Leveraging expert masks as spatial anchors, MedGround precisely derives localization targets, extracts&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06847v1-abstract-full').style.display = 'inline'; document.getElementById('2601.06847v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.06847v1-abstract-full" style="display: none;">
        Vision-Language Models (VLMs) can generate convincing clinical narratives, yet frequently struggle to visually ground their statements. We posit this limitation arises from the scarcity of high-quality, large-scale clinical referring-localization pairs. To address this, we introduce MedGround, an automated pipeline that <span class="search-hit mathjax">transforms</span> segmentation resources into high-quality medical referring grounding data. Leveraging expert masks as spatial anchors, MedGround precisely derives localization targets, extracts shape and spatial cues, and guides VLMs to synthesize natural, clinically grounded queries that reflect morphology and location. To ensure data rigor, a multi-stage verification system integrates strict formatting checks, geometry- and medical-prior rules, and image-based visual judging to filter out ambiguous or visually unsupported samples. Finally, we present MedGround-35K, a novel <span class="search-hit mathjax">multimodal</span> medical dataset. Extensive experiments demonstrate that VLMs trained with MedGround-35K consistently achieve improved referring grounding performance, enhance multi-object semantic disambiguation, and exhibit strong generalization to unseen grounding settings. This work highlights MedGround as a scalable, data-driven approach to anchor medical language to verifiable visual evidence. Dataset and code will be released publicly upon acceptance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06847v1-abstract-full').style.display = 'none'; document.getElementById('2601.06847v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">18 pages, 10 figures</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          J.1
        

        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.06753">arXiv:2601.06753</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.06753">pdf</a>, <a href="https://arxiv.org/ps/2601.06753">ps</a>, <a href="https://arxiv.org/format/2601.06753">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards Computational Chinese Paleography
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ma%2C+Y+R">Yiran Rex Ma</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.06753v1-abstract-short" style="display: inline;">
        &hellip;for automated decipherment and human-AI collaboration. We examine the technological shift from classical computer vision to modern deep learning paradigms, including <span class="search-hit mathjax">transformers</span> and large&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06753v1-abstract-full').style.display = 'inline'; document.getElementById('2601.06753v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.06753v1-abstract-full" style="display: none;">
        Chinese paleography, the study of ancient Chinese writing, is undergoing a computational turn powered by artificial intelligence. This position paper charts the trajectory of this emerging field, arguing that it is evolving from automating isolated visual tasks to creating integrated digital ecosystems for scholarly research. We first map the landscape of digital resources, analyzing critical datasets for oracle bone, bronze, and bamboo slip scripts. The core of our analysis follows the field&#39;s methodological pipeline: from foundational visual processing (image restoration, character recognition), through contextual analysis (artifact rejoining, dating), to the advanced reasoning required for automated decipherment and human-AI collaboration. We examine the technological shift from classical computer vision to modern deep learning paradigms, including <span class="search-hit mathjax">transformers</span> and large <span class="search-hit mathjax">multimodal</span> models. Finally, we synthesize the field&#39;s core challenges -- notably data scarcity and a disconnect between current AI capabilities and the holistic nature of humanistic inquiry -- and advocate for a future research agenda focused on creating <span class="search-hit mathjax">multimodal</span>, few-shot, and human-centric systems to augment scholarly expertise.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06753v1-abstract-full').style.display = 'none'; document.getElementById('2601.06753v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">A position paper in progress with Peking University &amp; ByteDance Digital Humanities Open Lab</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.06616">arXiv:2601.06616</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.06616">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        LLM-Driven Accessible Interface: A Model-Based Approach
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Jerry%2C+B">Blessing Jerry</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Moreno%2C+L">Lourdes Moreno</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Francisco%2C+V">Virginia Francisco</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hervas%2C+R">Raquel Hervas</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.06616v1-abstract-short" style="display: inline;">
        &hellip;raises challenges regarding accessibility, explainability, and normative compliance. This paper presents an implemented model-driven architecture for generating personalised, <span class="search-hit mathjax">multimodal</span>, and accessibility-aligned user interfaces. The approach combines structured user profiles, declarative adaptation rules, and validated prompt templates to refine baseline ac&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06616v1-abstract-full').style.display = 'inline'; document.getElementById('2601.06616v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.06616v1-abstract-full" style="display: none;">
        The integration of Large Language Models (LLMs) into interactive systems opens new opportunities for adaptive user experiences, yet it also raises challenges regarding accessibility, explainability, and normative compliance. This paper presents an implemented model-driven architecture for generating personalised, <span class="search-hit mathjax">multimodal</span>, and accessibility-aligned user interfaces. The approach combines structured user profiles, declarative adaptation rules, and validated prompt templates to refine baseline accessible UI templates that conform to WCAG 2.2 and EN 301 549, tailored to cognitive and sensory support needs. LLMs dynamically <span class="search-hit mathjax">transform</span> language complexity, modality, and visual structure, producing outputs such as Plain-Language text, pictograms, and high-contrast layouts aligned with ISO 24495-1 and W3C COGA guidance. A healthcare use case demonstrates how the system generates accessible post-consultation medication instructions tailored to a user profile comprising cognitive disability and hearing impairment. SysML v2 models provide explicit traceability between user needs, adaptation rules, and normative requirements, ensuring explainable and auditable <span class="search-hit mathjax">transformations</span>. Grounded in Human-Centered AI (HCAI), the framework incorporates co-design processes and structured feedback mechanisms to guide iterative refinement and support trustworthy generative behaviour.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06616v1-abstract-full').style.display = 'none'; document.getElementById('2601.06616v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          H.5.2; J.3; K.4.2; H.1.2; I.2.7
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.06475">arXiv:2601.06475</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.06475">pdf</a>, <a href="https://arxiv.org/ps/2601.06475">ps</a>, <a href="https://arxiv.org/format/2601.06475">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        VVTRec: Radio Interferometric Reconstruction through Visual and Textual Modality Enrichment
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Cheng%2C+K">Kai Cheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+R">Ruoqi Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Luo%2C+Q">Qiong Luo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.06475v1-abstract-short" style="display: inline;">
        Radio astronomy is an indispensable discipline for observing distant celestial objects. Measurements of wave signals from radio telescopes, called visibility, need to be <span class="search-hit mathjax">transformed</span> into images for astronomical observations. These dirty images blend information from real sources and artifacts. Therefore, astronomers usually perform reconstruction before imag&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06475v1-abstract-full').style.display = 'inline'; document.getElementById('2601.06475v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.06475v1-abstract-full" style="display: none;">
        Radio astronomy is an indispensable discipline for observing distant celestial objects. Measurements of wave signals from radio telescopes, called visibility, need to be <span class="search-hit mathjax">transformed</span> into images for astronomical observations. These dirty images blend information from real sources and artifacts. Therefore, astronomers usually perform reconstruction before imaging to obtain cleaner images. Existing methods consider only a single modality of sparse visibility data, resulting in images with remaining artifacts and insufficient modeling of correlation. To enhance the extraction of visibility information and emphasize output quality in the image domain, we propose VVTRec, a <span class="search-hit mathjax">multimodal</span> radio interferometric data reconstruction method with visibility-guided visual and textual modality enrichment. In our VVTRec, sparse visibility is <span class="search-hit mathjax">transformed</span> into image-form and text-form features to obtain enhancements in terms of spatial and semantic information, improving the structural integrity and accuracy of images. Also, we leverage Vision-Language Models (VLMs) to achieve additional training-free performance improvements. VVTRec enables sparse visibility, as a foreign modality unseen by VLMs, to accurately extract pre-trained knowledge as a supplement. Our experiments demonstrate that VVTRec effectively enhances imaging results by exploiting <span class="search-hit mathjax">multimodal</span> information without introducing excessive computational overhead.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06475v1-abstract-full').style.display = 'none'; document.getElementById('2601.06475v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.06212">arXiv:2601.06212</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.06212">pdf</a>, <a href="https://arxiv.org/ps/2601.06212">ps</a>, <a href="https://arxiv.org/format/2601.06212">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Akasha 2: Hamiltonian State Space Duality and Visual-Language Joint Embedding Predictive Architectur
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Meziani%2C+Y">Yani Meziani</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.06212v1-abstract-short" style="display: inline;">
        We present Akasha 2, a state-of-the-art <span class="search-hit mathjax">multimodal</span> architecture that integrates Hamiltonian State Space Duality (H-SSD) with Visual-Language Joint Embedding Predictive Architecture (VL-JEPA). The system leverages the Mamba-3 Selective State Space Model (SSM) augmented by a Sparse Mixture of Hamiltonian Experts (SMoE-HE) that enforces latent physical conserva&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06212v1-abstract-full').style.display = 'inline'; document.getElementById('2601.06212v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.06212v1-abstract-full" style="display: none;">
        We present Akasha 2, a state-of-the-art <span class="search-hit mathjax">multimodal</span> architecture that integrates Hamiltonian State Space Duality (H-SSD) with Visual-Language Joint Embedding Predictive Architecture (VL-JEPA). The system leverages the Mamba-3 Selective State Space Model (SSM) augmented by a Sparse Mixture of Hamiltonian Experts (SMoE-HE) that enforces latent physical conservation laws through symplectic integration. For visual synthesis, we introduce Hamiltonian Flow Matching (HFM) and persistent 3D Gaussian Splatting (3DGS), enabling ultra-low latency (&lt;50ms) on mobile hardware. This work establishes a new paradigm in latent world models, achieving unprecedented spatiotemporal coherence through a holographic memory architecture. Our approach demonstrates that incorporating physics-inspired inductive biases into neural architectures yields significant improvements: state-of-the-art video prediction (FVD: 287), 4x faster visual synthesis than diffusion models, and 3-18x inference speedup over <span class="search-hit mathjax">transformer</span> baselines while maintaining energy conservation over extended horizons.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06212v1-abstract-full').style.display = 'none'; document.getElementById('2601.06212v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">12 pages, 6 figures, 3 tables. Includes appendices with pseudocode and implementation details. Supplementary materials eventually at github.com/yanimeziani/akasha</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68T07; 68T45; 70H05
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.6; I.2.10; I.4.8
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.06199">arXiv:2601.06199</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.06199">pdf</a>, <a href="https://arxiv.org/ps/2601.06199">ps</a>, <a href="https://arxiv.org/format/2601.06199">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        FastSLM: Hierarchical Frame Q-Former for Effective Speech Modality Adaptation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+J">Junseok Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+S">Sangyong Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chun%2C+C">Chang-Jae Chun</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.06199v1-abstract-short" style="display: inline;">
        &hellip;general intelligence (AGI). In particular, there is growing momentum in adapting LLMs to various modalities, including vision, video, and speech, through the development of <span class="search-hit mathjax">multimodal</span> LLMs (MLLMs). However, existing speech-language model (SLM) research has largely overlooked cost-effective adaptation strategies for leveraging LLMs in the speech domain. In th&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06199v1-abstract-full').style.display = 'inline'; document.getElementById('2601.06199v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.06199v1-abstract-full" style="display: none;">
        Recent advances in large language models (LLMs) have demonstrated human-expert-level capabilities, driving significant interest in their potential for achieving artificial general intelligence (AGI). In particular, there is growing momentum in adapting LLMs to various modalities, including vision, video, and speech, through the development of <span class="search-hit mathjax">multimodal</span> LLMs (MLLMs). However, existing speech-language model (SLM) research has largely overlooked cost-effective adaptation strategies for leveraging LLMs in the speech domain. In this paper, we propose FastSLM, a lightweight yet efficient SLM designed for effective understanding and reasoning over long-form speech. To address the challenge of aligning high-frame-rate speech features with LLMs, we introduce the Hierarchical Frame Querying <span class="search-hit mathjax">Transformer</span> (HFQ-Former), which compresses frame-level speech features while capturing both local and global context. Furthermore, we present a novel three-stage training strategy that enhances generalization across a wide range of speech-related tasks. Experimental results demonstrate that FastSLM achieves competitive performance compared to existing state-of-the-art models, despite operating with significantly lower FLOPs and parameter counts, while representing speech with only 1.67 tokens per second. The source code and model checkpoints are available at https://huggingface.co/okestro-ai-lab/FastSLM.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06199v1-abstract-full').style.display = 'none'; document.getElementById('2601.06199v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.06140">arXiv:2601.06140</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.06140">pdf</a>, <a href="https://arxiv.org/ps/2601.06140">ps</a>, <a href="https://arxiv.org/format/2601.06140">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Causal and Federated <span class="search-hit mathjax">Multimodal</span> Learning for Cardiovascular Risk Prediction under Heterogeneous Populations
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Kaushik%2C+R">Rohit Kaushik</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kaushik%2C+E">Eva Kaushik</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.06140v1-abstract-short" style="display: inline;">
        &hellip;calling for predictive models that not only handle diverse and high-dimensional biomedical signals but also maintain interpretability and privacy. We create a single <span class="search-hit mathjax">multimodal</span> learning framework that integrates cross modal <span class="search-hit mathjax">transformers</span> with graph neural networks and causal representation learning to measure personali&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06140v1-abstract-full').style.display = 'inline'; document.getElementById('2601.06140v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.06140v1-abstract-full" style="display: none;">
        Cardiovascular disease (CVD) continues to be the major cause of death globally, calling for predictive models that not only handle diverse and high-dimensional biomedical signals but also maintain interpretability and privacy. We create a single <span class="search-hit mathjax">multimodal</span> learning framework that integrates cross modal <span class="search-hit mathjax">transformers</span> with graph neural networks and causal representation learning to measure personalized CVD risk. The model combines genomic variation, cardiac MRI, ECG waveforms, wearable streams, and structured EHR data to predict risk while also implementing causal invariance constraints across different clinical subpopulations.
  To maintain transparency, we employ SHAP based feature attribution, counterfactual explanations and causal latent alignment for understandable risk factors. Besides, we position the design in a federated, privacy, preserving optimization protocol and establish rules for convergence, calibration and uncertainty quantification under distributional shift. Experimental studies based on large-scale biobank and multi institutional datasets reveal state discrimination and robustness, exhibiting fair performance across demographic strata and clinically distinct cohorts. This study paves the way for a principled approach to clinically trustworthy, interpretable and privacy respecting CVD prediction at the population level.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06140v1-abstract-full').style.display = 'none'; document.getElementById('2601.06140v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, 5 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.05508">arXiv:2601.05508</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.05508">pdf</a>, <a href="https://arxiv.org/ps/2601.05508">ps</a>, <a href="https://arxiv.org/format/2601.05508">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Enabling Stroke-Level Structural Analysis of Hieroglyphic Scripts without Language-Specific Priors
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Luo%2C+F">Fuwen Luo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wan%2C+Z">Zihao Wan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Z">Ziyue Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Y">Yaluo Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+P+T+L">Pau Tong Lin Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qiao%2C+X">Xuanjia Qiao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+X">Xiaolong Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+P">Peng Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Y">Yang Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.05508v1-abstract-short" style="display: inline;">
        &hellip;writing systems, encode rich semantic and cultural information within their internal structural composition. Yet, current advanced Large Language Models (LLMs) and <span class="search-hit mathjax">Multimodal</span> LLMs (MLLMs) usually remain structurally blind to this information. LLMs process characters as textual tokens, while MLLMs additionally view them as raw pixel grids. Both fall short to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.05508v1-abstract-full').style.display = 'inline'; document.getElementById('2601.05508v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.05508v1-abstract-full" style="display: none;">
        Hieroglyphs, as logographic writing systems, encode rich semantic and cultural information within their internal structural composition. Yet, current advanced Large Language Models (LLMs) and <span class="search-hit mathjax">Multimodal</span> LLMs (MLLMs) usually remain structurally blind to this information. LLMs process characters as textual tokens, while MLLMs additionally view them as raw pixel grids. Both fall short to model the underlying logic of character strokes. Furthermore, existing structural analysis methods are often script-specific and labor-intensive. In this paper, we propose Hieroglyphic Stroke Analyzer (HieroSA), a novel and generalizable framework that enables MLLMs to automatically derive stroke-level structures from character bitmaps without handcrafted data. It <span class="search-hit mathjax">transforms</span> modern logographic and ancient hieroglyphs character images into explicit, interpretable line-segment representations in a normalized coordinate space, allowing for cross-lingual generalization. Extensive experiments demonstrate that HieroSA effectively captures character-internal structures and semantics, bypassing the need for language-specific priors. Experimental results highlight the potential of our work as a graphematics analysis tool for a deeper understanding of hieroglyphic scripts. View our code at https://github.com/THUNLP-MT/HieroSA.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.05508v1-abstract-full').style.display = 'none'; document.getElementById('2601.05508v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.05470">arXiv:2601.05470</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.05470">pdf</a>, <a href="https://arxiv.org/ps/2601.05470">ps</a>, <a href="https://arxiv.org/format/2601.05470">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ROAP: A Reading-Order and Attention-Prior Pipeline for Optimizing Layout <span class="search-hit mathjax">Transformers</span> in Key Information Extraction
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Xie%2C+T">Tingwei Xie</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+J">Jinxin He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Song%2C+Y">Yonghong Song</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.05470v1-abstract-short" style="display: inline;">
        The efficacy of <span class="search-hit mathjax">Multimodal</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.05470v1-abstract-full').style.display = 'inline'; document.getElementById('2601.05470v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.05470v1-abstract-full" style="display: none;">
        The efficacy of <span class="search-hit mathjax">Multimodal</span> <span class="search-hit mathjax">Transformers</span> in visually-rich document understanding (VrDU) is critically constrained by two inherent limitations: the lack of explicit modeling for logical reading order and the interference of visual tokens that dilutes attention on textual semantics.
  To address these challenges, this paper presents ROAP, a lightweight and architecture-agnostic pipeline designed to optimize attention distributions in Layout <span class="search-hit mathjax">Transformers</span> without altering their pre-trained backbones.
  The proposed pipeline first employs an Adaptive-XY-Gap (AXG-Tree) to robustly extract hierarchical reading sequences from complex layouts. These sequences are then integrated into the attention mechanism via a Reading-Order-Aware Relative Position Bias (RO-RPB). Furthermore, a Textual-Token Sub-block Attention Prior (TT-Prior) is introduced to adaptively suppress visual noise and enhance fine-grained text-text interactions.
  Extensive experiments on the FUNSD and CORD benchmarks demonstrate that ROAP consistently improves the performance of representative backbones, including LayoutLMv3 and GeoLayoutLM.
  These findings confirm that explicitly modeling reading logic and regulating modality interference are critical for robust document understanding, offering a scalable solution for complex layout analysis. The implementation code will be released at https://github.com/KevinYuLei/ROAP.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.05470v1-abstract-full').style.display = 'none'; document.getElementById('2601.05470v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages, 4 figures, 4 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.05353">arXiv:2601.05353</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.05353">pdf</a>, <a href="https://arxiv.org/ps/2601.05353">ps</a>, <a href="https://arxiv.org/format/2601.05353">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        GlyRAG: Context-Aware Retrieval-Augmented Framework for Blood Glucose Forecasting
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Soumma%2C+S+B">Shovito Barua Soumma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ghasemzadeh%2C+H">Hassan Ghasemzadeh</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.05353v1-abstract-short" style="display: inline;">
        &hellip;an LLM as a contextualization agent to generate clinical summaries. These summaries are embedded by a language model and fused with patch-based glucose representations in a <span class="search-hit mathjax">multimodal</span> <span class="search-hit mathjax">transformer</span> architecture with a cross translation loss aligining textual and physiological embeddings. A retrieval module then identifie&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.05353v1-abstract-full').style.display = 'inline'; document.getElementById('2601.05353v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.05353v1-abstract-full" style="display: none;">
        Accurate forecasting of blood glucose from CGM is essential for preventing dysglycemic events, thus enabling proactive diabetes management. However, current forecasting models treat blood glucose readings captured using CGMs as a numerical sequence, either ignoring context or relying on additional sensors/modalities that are difficult to collect and deploy at scale. Recently, LLMs have shown promise for time-series forecasting tasks, yet their role as agentic context extractors in diabetes care remains largely unexplored. To address these limitations, we propose GlyRAG, a context-aware, retrieval-augmented forecasting framework that derives semantic understanding of blood glucose dynamics directly from CGM traces without requiring additional sensor modalities. GlyRAG employs an LLM as a contextualization agent to generate clinical summaries. These summaries are embedded by a language model and fused with patch-based glucose representations in a <span class="search-hit mathjax">multimodal</span> <span class="search-hit mathjax">transformer</span> architecture with a cross translation loss aligining textual and physiological embeddings. A retrieval module then identifies similar historical episodes in the learned embedding space and uses cross-attention to integrate these case-based analogues prior to making a forecasting inference. Extensive evaluations on two T1D cohorts show that GlyRAG consistently outperforms state-of-the art methods, achieving up to 39% lower RMSE and a further 1.7% reduction in RMSE over the baseline. Clinical evaluation shows that GlyRAG places 85% predictions in safe zones and achieves 51% improvement in predicting dysglycemic events across both cohorts. These results indicate that LLM-based contextualization and retrieval over CGM traces can enhance the accuracy and clinical reliability of long-horizon glucose forecasting without the need for extra sensors, thus supporting future agentic decision-support tools for diabetes management.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.05353v1-abstract-full').style.display = 'none'; document.getElementById('2601.05353v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.05269">arXiv:2601.05269</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.05269">pdf</a>, <a href="https://arxiv.org/ps/2601.05269">ps</a>, <a href="https://arxiv.org/format/2601.05269">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Studying Illustrations in Manuscripts: An Efficient Deep-Learning Approach
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Evron%2C+Y">Yoav Evron</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Siegal%2C+M+B">Michal Bar-Asher Siegal</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fire%2C+M">Michael Fire</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.05269v2-abstract-short" style="display: inline;">
        The recent Artificial Intelligence (AI) revolution has opened <span class="search-hit mathjax">transformative</span> possibilities for the humanities, particularly in unlocking the visual-artistic content embedded in historical illuminated manuscripts. While digital archives now offer unprecedented access to these materials, the ability to systematically locate, extract, and analyze illustrations&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.05269v2-abstract-full').style.display = 'inline'; document.getElementById('2601.05269v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.05269v2-abstract-full" style="display: none;">
        The recent Artificial Intelligence (AI) revolution has opened <span class="search-hit mathjax">transformative</span> possibilities for the humanities, particularly in unlocking the visual-artistic content embedded in historical illuminated manuscripts. While digital archives now offer unprecedented access to these materials, the ability to systematically locate, extract, and analyze illustrations at scale remains a major challenge. We present a general and scalable AI-based pipeline for large-scale visual analysis of illuminated manuscripts. The framework integrates modern deep-learning models for page-level illustration detection, illustration extraction, and <span class="search-hit mathjax">multimodal</span> description, enabling scholars to search, cluster, and study visual materials and artistic trends across entire corpora. We demonstrate the applicability of this approach on large heterogeneous collections, including the Vatican Library and richly illuminated manuscripts such as the Bible of Borso d&#39;Este. The system reveals meaningful visual patterns and cross-manuscript relationships by embedding illustrations into a shared representation space and analyzing their similarity structure (see figure 4). By harnessing recent advances in computer vision and vision-language models, our framework enables new forms of large-scale visual scholarship in historical studies, art history, and cultural heritage making it possible to explore iconography, stylistic trends, and cultural connections in ways that were previously impractical.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.05269v2-abstract-full').style.display = 'none'; document.getElementById('2601.05269v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 January, 2026; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 November, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">17 pages, 5 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.04571">arXiv:2601.04571</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.04571">pdf</a>, <a href="https://arxiv.org/ps/2601.04571">ps</a>, <a href="https://arxiv.org/format/2601.04571">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Enhancing <span class="search-hit mathjax">Multimodal</span> Retrieval via Complementary Information Extraction and Alignment
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zeng%2C+D">Delong Zeng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xie%2C+Y">Yuexiang Xie</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Y">Yaliang Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shen%2C+Y">Ying Shen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.04571v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Multimodal</span> retrieval has emerged as a promising yet challenging research direction in recent years. Most existing studies in&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.04571v1-abstract-full').style.display = 'inline'; document.getElementById('2601.04571v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.04571v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Multimodal</span> retrieval has emerged as a promising yet challenging research direction in recent years. Most existing studies in <span class="search-hit mathjax">multimodal</span> retrieval focus on capturing information in <span class="search-hit mathjax">multimodal</span> data that is similar to their paired texts, but often ignores the complementary information contained in <span class="search-hit mathjax">multimodal</span> data. In this study, we propose CIEA, a novel <span class="search-hit mathjax">multimodal</span> retrieval approach that employs Complementary Information Extraction and Alignment, which <span class="search-hit mathjax">transforms</span> both text and images in documents into a unified latent space and features a complementary information extractor designed to identify and preserve differences in the image representations. We optimize CIEA using two complementary contrastive losses to ensure semantic integrity and effectively capture the complementary information contained in images. Extensive experiments demonstrate the effectiveness of CIEA, which achieves significant improvements over both divide-and-conquer models and universal dense retrieval models. We provide an ablation study, further discussions, and case studies to highlight the advancements achieved by CIEA. To promote further research in the community, we have released the source code at https://github.com/zengdlong/CIEA.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.04571v1-abstract-full').style.display = 'none'; document.getElementById('2601.04571v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by ACL&#39;2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.04376">arXiv:2601.04376</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.04376">pdf</a>, <a href="https://arxiv.org/ps/2601.04376">ps</a>, <a href="https://arxiv.org/format/2601.04376">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Combining Facial Videos and Biosignals for Stress Estimation During Driving
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Valergaki%2C+P">Paraskevi Valergaki</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nicodemou%2C+V+C">Vassilis C. Nicodemou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Oikonomidis%2C+I">Iason Oikonomidis</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Argyros%2C+A">Antonis Argyros</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Roussos%2C+A">Anastasios Roussos</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.04376v2-abstract-short" style="display: inline;">
        &hellip;physiological signals such as perinasal perspiration and heart rate, facial activity provides complementary cues that can be captured unobtrusively from video. We propose a <span class="search-hit mathjax">multimodal</span> stress estimation framework that combines facial videos and physiological signals, remaining effective even when biosignal acquisition is challenging. Facial behavior is repres&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.04376v2-abstract-full').style.display = 'inline'; document.getElementById('2601.04376v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.04376v2-abstract-full" style="display: none;">
        Reliable stress recognition is critical in applications such as medical monitoring and safety-critical systems, including real-world driving. While stress is commonly detected using physiological signals such as perinasal perspiration and heart rate, facial activity provides complementary cues that can be captured unobtrusively from video. We propose a <span class="search-hit mathjax">multimodal</span> stress estimation framework that combines facial videos and physiological signals, remaining effective even when biosignal acquisition is challenging. Facial behavior is represented using a dense 3D Morphable Model, yielding a 56-dimensional descriptor that captures subtle expression and head-pose dynamics over time. To study how stress modulates facial motion, we perform extensive experiments alongside established physiological markers. Paired hypothesis tests between baseline and stressor phases show that 38 of 56 facial components exhibit consistent, phase-specific stress responses comparable to physiological markers. Building on these findings, we introduce a <span class="search-hit mathjax">Transformer</span>-based temporal modeling framework and evaluate unimodal, early-fusion, and cross-modal attention strategies. Cross-modal attention fusion of 3D-derived facial features with physiological signals substantially improves performance over physiological signals alone, increasing AUROC from 52.7% and accuracy from 51.0% to 92.0% and 86.7%, respectively. Although evaluated on driving data, the proposed framework and protocol may generalize to other stress estimation settings.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.04376v2-abstract-full').style.display = 'none'; document.getElementById('2601.04376v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 January, 2026; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 7 January, 2026;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Under submission to ICPR 2026</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.04359">arXiv:2601.04359</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.04359">pdf</a>, <a href="https://arxiv.org/ps/2601.04359">ps</a>, <a href="https://arxiv.org/format/2601.04359">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PackCache: A Training-Free Acceleration Method for Unified Autoregressive Video Generation via Compact KV-Cache
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+K">Kunyang Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shah%2C+M">Mubarak Shah</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shang%2C+Y">Yuzhang Shang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.04359v1-abstract-short" style="display: inline;">
        A unified autoregressive model is a <span class="search-hit mathjax">Transformer</span>-based framework that addresses diverse <span class="search-hit mathjax">multimodal</span> tasks (e.g., text, image, video) as a single sequence modeling problem under a shared token space. Such models rely on the KV-cache mechanism to reduce attention computation from O(T^2) to O(T); however, KV-cache size grow&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.04359v1-abstract-full').style.display = 'inline'; document.getElementById('2601.04359v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.04359v1-abstract-full" style="display: none;">
        A unified autoregressive model is a <span class="search-hit mathjax">Transformer</span>-based framework that addresses diverse <span class="search-hit mathjax">multimodal</span> tasks (e.g., text, image, video) as a single sequence modeling problem under a shared token space. Such models rely on the KV-cache mechanism to reduce attention computation from O(T^2) to O(T); however, KV-cache size grows linearly with the number of generated tokens, and it rapidly becomes the dominant bottleneck limiting inference efficiency and generative length. Unified autoregressive video generation inherits this limitation. Our analysis reveals that KV-cache tokens exhibit distinct spatiotemporal properties: (i) text and conditioning-image tokens act as persistent semantic anchors that consistently receive high attention, and (ii) attention to previous frames naturally decays with temporal distance. Leveraging these observations, we introduce PackCache, a training-free KV-cache management method that dynamically compacts the KV cache through three coordinated mechanisms: condition anchoring that preserves semantic references, cross-frame decay modeling that allocates cache budget according to temporal distance, and spatially preserving position embedding that maintains coherent 3D structure under cache removal. In terms of efficiency, PackCache accelerates end-to-end generation by 1.7-2.2x on 48-frame long sequences, showcasing its strong potential for enabling longer-sequence video generation. Notably, the final four frames - the portion most impacted by the progressively expanding KV-cache and thus the most expensive segment of the clip - PackCache delivers a 2.6x and 3.7x acceleration on A40 and H200, respectively, for 48-frame videos.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.04359v1-abstract-full').style.display = 'none'; document.getElementById('2601.04359v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.04299">arXiv:2601.04299</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.04299">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Quantitative Methods">q-bio.QM</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Transformer</span>-Based Multi-Modal Temporal Embeddings for Explainable Metabolic Phenotyping in Type 1 Diabetes
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Khokhar%2C+P+B">Pir Bakhsh Khokhar</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gravino%2C+C">Carmine Gravino</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Palomba%2C+F">Fabio Palomba</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yayilgan%2C+S+Y">Sule Yildrim Yayilgan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shaikh%2C+S">Sarang Shaikh</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.04299v1-abstract-short" style="display: inline;">
        &hellip;hemoglobin (HbA1c). This study proposes an explainable deep learning framework that integrates continuous glucose monitoring (CGM) data with laboratory profiles to learn <span class="search-hit mathjax">multimodal</span> temporal embeddings of individual metabolic status. Temporal dependencies across modalities are modeled using a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.04299v1-abstract-full').style.display = 'inline'; document.getElementById('2601.04299v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.04299v1-abstract-full" style="display: none;">
        Type 1 diabetes (T1D) is a highly metabolically heterogeneous disease that cannot be adequately characterized by conventional biomarkers such as glycated hemoglobin (HbA1c). This study proposes an explainable deep learning framework that integrates continuous glucose monitoring (CGM) data with laboratory profiles to learn <span class="search-hit mathjax">multimodal</span> temporal embeddings of individual metabolic status. Temporal dependencies across modalities are modeled using a <span class="search-hit mathjax">transformer</span> encoder, while latent metabolic phenotypes are identified via Gaussian mixture modeling. Model interpretability is achieved through <span class="search-hit mathjax">transformer</span> attention visualization and SHAP-based feature attribution. Five latent metabolic phenotypes, ranging from metabolic stability to elevated cardiometabolic risk, were identified among 577 individuals with T1D. These phenotypes exhibit distinct biochemical profiles, including differences in glycemic control, lipid metabolism, renal markers, and thyrotropin (TSH) levels. Attention analysis highlights glucose variability as a dominant temporal factor, while SHAP analysis identifies HbA1c, triglycerides, cholesterol, creatinine, and TSH as key contributors to phenotype differentiation. Phenotype membership shows statistically significant, albeit modest, associations with hypertension, myocardial infarction, and heart failure. Overall, this explainable <span class="search-hit mathjax">multimodal</span> temporal embedding framework reveals physiologically coherent metabolic subgroups in T1D and supports risk stratification beyond single biomarkers.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.04299v1-abstract-full').style.display = 'none'; document.getElementById('2601.04299v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.04219">arXiv:2601.04219</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.04219">pdf</a>, <a href="https://arxiv.org/ps/2601.04219">ps</a>, <a href="https://arxiv.org/format/2601.04219">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multiagent Systems">cs.MA</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AgentTutor: Empowering Personalized Learning with Multi-Turn Interactive Teaching in Intelligent Education Systems
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Y">Yuxin Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Song%2C+Z">Zeqing Song</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lou%2C+J">Jiong Lou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+C">Chentao Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+J">Jie Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.04219v1-abstract-short" style="display: inline;">
        The rapid advancement of large-scale language models (LLMs) has shown their potential to <span class="search-hit mathjax">transform</span> intelligent education systems (IESs) through automated teaching and learning support applications. However, current IESs often rely on single-turn static question-answering, which fails to assess learners&#39; cognitive levels, cannot adjust teaching strategies&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.04219v1-abstract-full').style.display = 'inline'; document.getElementById('2601.04219v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.04219v1-abstract-full" style="display: none;">
        The rapid advancement of large-scale language models (LLMs) has shown their potential to <span class="search-hit mathjax">transform</span> intelligent education systems (IESs) through automated teaching and learning support applications. However, current IESs often rely on single-turn static question-answering, which fails to assess learners&#39; cognitive levels, cannot adjust teaching strategies based on real-time feedback, and is limited to providing simple one-off responses. To address these issues, we introduce AgentTutor, a multi-turn interactive intelligent education system to empower personalized learning. It features an LLM-powered generative multi-agent system and a learner-specific personalized learning profile environment that dynamically optimizes and delivers teaching strategies based on learners&#39; learning status, personalized goals, learning preferences, and <span class="search-hit mathjax">multimodal</span> study materials. It includes five key modules: curriculum decomposition, learner assessment, dynamic strategy, teaching reflection, and knowledge &amp; experience memory. We conducted extensive experiments on multiple benchmark datasets, AgentTutor significantly enhances learners&#39; performance while demonstrating strong effectiveness in multi-turn interactions and competitiveness in teaching quality among other baselines.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.04219v1-abstract-full').style.display = 'none'; document.getElementById('2601.04219v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 December, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">AAAI2026 Workshop AI4EDU</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.04100">arXiv:2601.04100</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.04100">pdf</a>, <a href="https://arxiv.org/ps/2601.04100">ps</a>, <a href="https://arxiv.org/format/2601.04100">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Quantifying the Impact of Modules and Their Interactions in the PSO-X Framework
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Camacho-Villal%C3%B3n%2C+C+L">Christian L. Camacho-Villalón</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nikolikj%2C+A">Ana Nikolikj</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dost%2C+K">Katharina Dost</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tuba%2C+E">Eva Tuba</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=D%C5%BEeroski%2C+S">Sašo Džeroski</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Eftimov%2C+T">Tome Eftimov</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.04100v1-abstract-short" style="display: inline;">
        &hellip;in different problem classes. In practice, this allows us to identify which modules have greater influence on PSO-X performance depending on problem features such as <span class="search-hit mathjax">multimodality</span>, mathematical <span class="search-hit mathjax">transformations</span> and varying dimensionality. We then perform a cluster analysis to identify groups of problem classes that shar&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.04100v1-abstract-full').style.display = 'inline'; document.getElementById('2601.04100v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.04100v1-abstract-full" style="display: none;">
        The PSO-X framework incorporates dozens of modules that have been proposed for solving single-objective continuous optimization problems using particle swarm optimization. While modular frameworks enable users to automatically generate and configure algorithms tailored to specific optimization problems, the complexity of this process increases with the number of modules in the framework and the degrees of freedom defined for their interaction. Understanding how modules affect the performance of algorithms for different problems is critical to making the process of finding effective implementations more efficient and identifying promising areas for further investigation. Despite their practical applications and scientific relevance, there is a lack of empirical studies investigating which modules matter most in modular optimization frameworks and how they interact. In this paper, we analyze the performance of 1424 particle swarm optimization algorithms instantiated from the PSO-X framework on the 25 functions in the CEC&#39;05 benchmark suite with 10 and 30 dimensions. We use functional ANOVA to quantify the impact of modules and their combinations on performance in different problem classes. In practice, this allows us to identify which modules have greater influence on PSO-X performance depending on problem features such as <span class="search-hit mathjax">multimodality</span>, mathematical <span class="search-hit mathjax">transformations</span> and varying dimensionality. We then perform a cluster analysis to identify groups of problem classes that share similar module effect patterns. Our results show low variability in the importance of modules in all problem classes, suggesting that particle swarm optimization performance is driven by a few influential modules.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.04100v1-abstract-full').style.display = 'none'; document.getElementById('2601.04100v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.03660">arXiv:2601.03660</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.03660">pdf</a>, <a href="https://arxiv.org/ps/2601.03660">ps</a>, <a href="https://arxiv.org/format/2601.03660">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MGPC: <span class="search-hit mathjax">Multimodal</span> Network for Generalizable Point Cloud Completion With Modality Dropout and Progressive Decoding
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+J">Jiangyuan Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ma%2C+H">Hongxuan Ma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+Y">Yuhao Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Z">Zhe Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+J">Jian Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zou%2C+W">Wei Zou</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.03660v1-abstract-short" style="display: inline;">
        &hellip;from partial observations caused by limited viewpoints and occlusions. Existing learning-based works, including 3D Convolutional Neural Network (CNN)-based, point-based, and <span class="search-hit mathjax">Transformer</span>-based methods, have achieved strong performance on synthetic benchmarks. However, due to the limitations of modality, scalability, and generative capacity, their generalizati&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.03660v1-abstract-full').style.display = 'inline'; document.getElementById('2601.03660v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.03660v1-abstract-full" style="display: none;">
        Point cloud completion aims to recover complete 3D geometry from partial observations caused by limited viewpoints and occlusions. Existing learning-based works, including 3D Convolutional Neural Network (CNN)-based, point-based, and <span class="search-hit mathjax">Transformer</span>-based methods, have achieved strong performance on synthetic benchmarks. However, due to the limitations of modality, scalability, and generative capacity, their generalization to novel objects and real-world scenarios remains challenging. In this paper, we propose MGPC, a generalizable <span class="search-hit mathjax">multimodal</span> point cloud completion framework that integrates point clouds, RGB images, and text within a unified architecture. MGPC introduces an innovative modality dropout strategy, a <span class="search-hit mathjax">Transformer</span>-based fusion module, and a novel progressive generator to improve robustness, scalability, and geometric modeling capability. We further develop an automatic data generation pipeline and construct MGPC-1M, a large-scale benchmark with over 1,000 categories and one million training pairs. Extensive experiments on MGPC-1M and in-the-wild data demonstrate that the proposed method consistently outperforms prior baselines and exhibits strong generalization under real-world conditions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.03660v1-abstract-full').style.display = 'none'; document.getElementById('2601.03660v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Code and dataset are available at https://github.com/L-J-Yuan/MGPC</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.03482">arXiv:2601.03482</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.03482">pdf</a>, <a href="https://arxiv.org/ps/2601.03482">ps</a>, <a href="https://arxiv.org/format/2601.03482">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Applications">stat.AP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Personalization of Large Foundation Models for Health Interventions
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Konigorski%2C+S">Stefan Konigorski</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Vedder%2C+J+E">Johannes E. Vedder</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Owoyele%2C+B+A">Babajide Alamu Owoyele</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=%C3%96zkan%2C+%C4%B0">İbrahim Özkan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.03482v1-abstract-short" style="display: inline;">
        Large foundation models (LFMs) <span class="search-hit mathjax">transform</span> healthcare AI in prevention, diagnostics, and treatment. However, whether LFMs can provide truly personalized treatment recommendations remains an open question. Recent research has revealed multiple challenges for personalization, including the fundamental generalizability paradox: models achieving high accuracy in o&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.03482v1-abstract-full').style.display = 'inline'; document.getElementById('2601.03482v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.03482v1-abstract-full" style="display: none;">
        Large foundation models (LFMs) <span class="search-hit mathjax">transform</span> healthcare AI in prevention, diagnostics, and treatment. However, whether LFMs can provide truly personalized treatment recommendations remains an open question. Recent research has revealed multiple challenges for personalization, including the fundamental generalizability paradox: models achieving high accuracy in one clinical study perform at chance level in others, demonstrating that personalization and external validity exist in tension. This exemplifies broader contradictions in AI-driven healthcare: the privacy-performance paradox, scale-specificity paradox, and the automation-empathy paradox. As another challenge, the degree of causal understanding required for personalized recommendations, as opposed to mere predictive capacities of LFMs, remains an open question. N-of-1 trials -- crossover self-experiments and the gold standard for individual causal inference in personalized medicine -- resolve these tensions by providing within-person causal evidence while preserving privacy through local experimentation. Despite their impressive capabilities, this paper argues that LFMs cannot replace N-of-1 trials. We argue that LFMs and N-of-1 trials are complementary: LFMs excel at rapid hypothesis generation from population patterns using <span class="search-hit mathjax">multimodal</span> data, while N-of-1 trials excel at causal validation for a given individual. We propose a hybrid framework that combines the strengths of both to enable personalization and navigate the identified paradoxes: LFMs generate ranked intervention candidates with uncertainty estimates, which trigger subsequent N-of-1 trials. Clarifying the boundary between prediction and causation and explicitly addressing the paradoxical tensions are essential for responsible AI integration in personalized medicine.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.03482v1-abstract-full').style.display = 'none'; document.getElementById('2601.03482v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to the AAAI 2026 Workshop on Personalization in the Era of Large Foundation Models (PerFM)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.03464">arXiv:2601.03464</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.03464">pdf</a>, <a href="https://arxiv.org/ps/2601.03464">ps</a>, <a href="https://arxiv.org/format/2601.03464">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Prompting Underestimates LLM Capability for Time Series Classification
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Schumacher%2C+D">Dan Schumacher</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nourbakhsh%2C+E">Erfan Nourbakhsh</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Slavin%2C+R">Rocky Slavin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rios%2C+A">Anthony Rios</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.03464v1-abstract-short" style="display: inline;">
        &hellip;to 0.61-0.67, often matching or exceeding specialized time series models. Layer-wise analyses further show that class-discriminative time series information emerges in early <span class="search-hit mathjax">transformer</span> layers and is amplified by visual and <span class="search-hit mathjax">multimodal</span> inputs. Together, these results demonstrate a systematic mismatch between what LLMs i&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.03464v1-abstract-full').style.display = 'inline'; document.getElementById('2601.03464v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.03464v1-abstract-full" style="display: none;">
        Prompt-based evaluations suggest that large language models (LLMs) perform poorly on time series classification, raising doubts about whether they encode meaningful temporal structure. We show that this conclusion reflects limitations of prompt-based generation rather than the model&#39;s representational capacity by directly comparing prompt outputs with linear probes over the same internal representations. While zero-shot prompting performs near chance, linear probes improve average F1 from 0.15-0.26 to 0.61-0.67, often matching or exceeding specialized time series models. Layer-wise analyses further show that class-discriminative time series information emerges in early <span class="search-hit mathjax">transformer</span> layers and is amplified by visual and <span class="search-hit mathjax">multimodal</span> inputs. Together, these results demonstrate a systematic mismatch between what LLMs internally represent and what prompt-based evaluation reveals, leading current evaluations to underestimate their time series understanding.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.03464v1-abstract-full').style.display = 'none'; document.getElementById('2601.03464v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages + Appendix and References, 9 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.03460">arXiv:2601.03460</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.03460">pdf</a>, <a href="https://arxiv.org/ps/2601.03460">ps</a>, <a href="https://arxiv.org/format/2601.03460">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        FROST-Drive: Scalable and Efficient End-to-End Driving with a Frozen Vision Encoder
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+Z">Zeyu Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+Y">Yimin Zhu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+Y">Yu Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+Y">Yu Sun</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.03460v1-abstract-short" style="display: inline;">
        &hellip;frozen, our approach directly transfers the rich, generalized world knowledge from the VLM to the driving task. Our model architecture combines this frozen encoder with a <span class="search-hit mathjax">transformer</span>-based adapter for <span class="search-hit mathjax">multimodal</span> fusion and a GRU-based decoder for smooth waypoint generation. Furthermore, we introduce a custom loss funct&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.03460v1-abstract-full').style.display = 'inline'; document.getElementById('2601.03460v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.03460v1-abstract-full" style="display: none;">
        End-to-end (E2E) models in autonomous driving aim to directly map sensor inputs to control commands, but their ability to generalize to novel and complex scenarios remains a key challenge. The common practice of fully fine-tuning the vision encoder on driving datasets potentially limits its generalization by causing the model to specialize too heavily in the training data. This work challenges the necessity of this training paradigm. We propose FROST-Drive, a novel E2E architecture designed to preserve and leverage the powerful generalization capabilities of a pretrained vision encoder from a Vision-Language Model (VLM). By keeping the encoder&#39;s weights frozen, our approach directly transfers the rich, generalized world knowledge from the VLM to the driving task. Our model architecture combines this frozen encoder with a <span class="search-hit mathjax">transformer</span>-based adapter for <span class="search-hit mathjax">multimodal</span> fusion and a GRU-based decoder for smooth waypoint generation. Furthermore, we introduce a custom loss function designed to directly optimize for Rater Feedback Score (RFS), a metric that prioritizes robust trajectory planning. We conduct extensive experiments on Waymo Open E2E Dataset, a large-scale datasets deliberately curated to capture the long-tail scenarios, demonstrating that our frozen-encoder approach significantly outperforms models that employ full fine-tuning. Our results provide substantial evidence that preserving the broad knowledge of a capable VLM is a more effective strategy for achieving robust, generalizable driving performance than intensive domain-specific adaptation. This offers a new pathway for developing vision-based models that can better handle the complexities of real-world application domains.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.03460v1-abstract-full').style.display = 'none'; document.getElementById('2601.03460v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.03329">arXiv:2601.03329</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.03329">pdf</a>, <a href="https://arxiv.org/ps/2601.03329">ps</a>, <a href="https://arxiv.org/format/2601.03329">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Attention mechanisms in neural networks
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hays%2C+H">Hasi Hays</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.03329v1-abstract-short" style="display: inline;">
        &hellip;foundations, computational properties, and practical implementations in contemporary deep learning systems. Applications in natural language processing, computer vision, and <span class="search-hit mathjax">multimodal</span> learning demonstrate the versatility of attention mechanisms. We examine language modeling with autoregressive&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.03329v1-abstract-full').style.display = 'inline'; document.getElementById('2601.03329v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.03329v1-abstract-full" style="display: none;">
        Attention mechanisms represent a fundamental paradigm shift in neural network architectures, enabling models to selectively focus on relevant portions of input sequences through learned weighting functions. This monograph provides a comprehensive and rigorous mathematical treatment of attention mechanisms, encompassing their theoretical foundations, computational properties, and practical implementations in contemporary deep learning systems. Applications in natural language processing, computer vision, and <span class="search-hit mathjax">multimodal</span> learning demonstrate the versatility of attention mechanisms. We examine language modeling with autoregressive <span class="search-hit mathjax">transformers</span>, bidirectional encoders for representation learning, sequence-to-sequence translation, Vision <span class="search-hit mathjax">Transformers</span> for image classification, and cross-modal attention for vision-language tasks. Empirical analysis reveals training characteristics, scaling laws that relate performance to model size and computation, attention pattern visualizations, and performance benchmarks across standard datasets. We discuss the interpretability of learned attention patterns and their relationship to linguistic and visual structures. The monograph concludes with a critical examination of current limitations, including computational scalability, data efficiency, systematic generalization, and interpretability challenges.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.03329v1-abstract-full').style.display = 'none'; document.getElementById('2601.03329v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.02737">arXiv:2601.02737</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.02737">pdf</a>, <a href="https://arxiv.org/ps/2601.02737">ps</a>, <a href="https://arxiv.org/format/2601.02737">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Unveiling and Bridging the Functional Perception Gap in MLLMs: Atomic Visual Alignment and Hierarchical Evaluation via PET-Bench
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ye%2C+Z">Zanting Ye</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Niu%2C+X">Xiaolong Niu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+X">Xuanbin Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Han%2C+X">Xu Han</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+S">Shengyuan Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hao%2C+J">Jing Hao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Peng%2C+Z">Zhihao Peng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+H">Hao Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lv%2C+J">Jieqin Lv</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+F">Fanghu Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+Y">Yanchao Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+H">Hubing Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yuan%2C+Y">Yixuan Yuan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zaidi%2C+H">Habib Zaidi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rahmim%2C+A">Arman Rahmim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zheng%2C+Y">Yefeng Zheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lu%2C+L">Lijun Lu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.02737v1-abstract-short" style="display: inline;">
        While <span class="search-hit mathjax">Multimodal</span> Large Language Models (MLLMs) have demonstrated remarkable proficiency in tasks such as abnormality detection and report generation for anatomical modalities, their capability in functional imaging remains largely unexplored. In this work, we identify and quantify a fundamental functional perception gap: the inability of current vision encod&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02737v1-abstract-full').style.display = 'inline'; document.getElementById('2601.02737v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.02737v1-abstract-full" style="display: none;">
        While <span class="search-hit mathjax">Multimodal</span> Large Language Models (MLLMs) have demonstrated remarkable proficiency in tasks such as abnormality detection and report generation for anatomical modalities, their capability in functional imaging remains largely unexplored. In this work, we identify and quantify a fundamental functional perception gap: the inability of current vision encoders to decode functional tracer biodistribution independent of morphological priors. Identifying Positron Emission Tomography (PET) as the quintessential modality to investigate this disconnect, we introduce PET-Bench, the first large-scale functional imaging benchmark comprising 52,308 hierarchical QA pairs from 9,732 multi-site, multi-tracer PET studies. Extensive evaluation of 19 state-of-the-art MLLMs reveals a critical safety hazard termed the Chain-of-Thought (CoT) hallucination trap. We observe that standard CoT prompting, widely considered to enhance reasoning, paradoxically decouples linguistic generation from visual evidence in PET, producing clinically fluent but factually ungrounded diagnoses. To resolve this, we propose Atomic Visual Alignment (AVA), a simple fine-tuning strategy that enforces the mastery of low-level functional perception prior to high-level diagnostic reasoning. Our results demonstrate that AVA effectively bridges the perception gap, <span class="search-hit mathjax">transforming</span> CoT from a source of hallucination into a robust inference tool and improving diagnostic accuracy by up to 14.83%. Code and data are available at https://github.com/yezanting/PET-Bench.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02737v1-abstract-full').style.display = 'none'; document.getElementById('2601.02737v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, 6 figures, 6 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.02677">arXiv:2601.02677</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.02677">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Risk Management">q-fin.RM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Statistical Finance">q-fin.ST</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Uni-FinLLM: A Unified <span class="search-hit mathjax">Multimodal</span> Large Language Model with Modular Task Heads for Micro-Level Stock Prediction and Macro-Level Systemic Risk Assessment
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+G">Gongao Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zeng%2C+H">Haijiang Zeng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jiang%2C+L">Lu Jiang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.02677v1-abstract-short" style="display: inline;">
        &hellip;to systemic vulnerabilities. Existing approaches often treat these tasks in isolation, failing to capture cross-scale dependencies. We propose Uni-FinLLM, a unified <span class="search-hit mathjax">multimodal</span> large language model that uses a shared&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02677v1-abstract-full').style.display = 'inline'; document.getElementById('2601.02677v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.02677v1-abstract-full" style="display: none;">
        Financial institutions and regulators require systems that integrate heterogeneous data to assess risks from stock fluctuations to systemic vulnerabilities. Existing approaches often treat these tasks in isolation, failing to capture cross-scale dependencies. We propose Uni-FinLLM, a unified <span class="search-hit mathjax">multimodal</span> large language model that uses a shared <span class="search-hit mathjax">Transformer</span> backbone and modular task heads to jointly process financial text, numerical time series, fundamentals, and visual data. Through cross-modal attention and multi-task optimization, it learns a coherent representation for micro-, meso-, and macro-level predictions. Evaluated on stock forecasting, credit-risk assessment, and systemic-risk detection, Uni-FinLLM significantly outperforms baselines. It raises stock directional accuracy to 67.4% (from 61.7%), credit-risk accuracy to 84.1% (from 79.6%), and macro early-warning accuracy to 82.3%. Results validate that a unified <span class="search-hit mathjax">multimodal</span> LLM can jointly model asset behavior and systemic vulnerabilities, offering a scalable decision-support engine for finance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02677v1-abstract-full').style.display = 'none'; document.getElementById('2601.02677v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.02456">arXiv:2601.02456</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.02456">pdf</a>, <a href="https://arxiv.org/ps/2601.02456">ps</a>, <a href="https://arxiv.org/format/2601.02456">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        InternVLA-A1: Unifying Understanding, Generation and Action for Robotic Manipulation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Cai%2C+J">Junhao Cai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cai%2C+Z">Zetao Cai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cao%2C+J">Jiafei Cao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+Y">Yilun Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+Z">Zeyu He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jiang%2C+L">Lei Jiang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+H">Hang Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+H">Hengjie Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Y">Yang Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Y">Yufei Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lu%2C+Y">Yanan Lu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lv%2C+Q">Qi Lv</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ma%2C+H">Haoxiang Ma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pang%2C+J">Jiangmiao Pang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qiao%2C+Y">Yu Qiao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qiu%2C+Z">Zherui Qiu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shen%2C+Y">Yanqing Shen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+X">Xu Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tian%2C+Y">Yang Tian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+B">Bolun Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+H">Hanqing Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+J">Jiaheng Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+T">Tai Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wei%2C+X">Xueyuan Wei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+C">Chao Wu</a>
      , et al. (17 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.02456v1-abstract-short" style="display: inline;">
        Prevalent Vision-Language-Action (VLA) models are typically built upon <span class="search-hit mathjax">Multimodal</span> Large Language Models (MLLMs) and demonstrate exceptional proficiency in semantic understanding, but they inherently lack the capability to deduce physical world dynamics. Consequently, recent approaches have shifted toward World Models, typically formulated via video predictio&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02456v1-abstract-full').style.display = 'inline'; document.getElementById('2601.02456v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.02456v1-abstract-full" style="display: none;">
        Prevalent Vision-Language-Action (VLA) models are typically built upon <span class="search-hit mathjax">Multimodal</span> Large Language Models (MLLMs) and demonstrate exceptional proficiency in semantic understanding, but they inherently lack the capability to deduce physical world dynamics. Consequently, recent approaches have shifted toward World Models, typically formulated via video prediction; however, these methods often suffer from a lack of semantic grounding and exhibit brittleness when handling prediction errors. To synergize semantic understanding with dynamic predictive capabilities, we present InternVLA-A1. This model employs a unified Mixture-of-<span class="search-hit mathjax">Transformers</span> architecture, coordinating three experts for scene understanding, visual foresight generation, and action execution. These components interact seamlessly through a unified masked self-attention mechanism. Building upon InternVL3 and Qwen3-VL, we instantiate InternVLA-A1 at 2B and 3B parameter scales. We pre-train these models on hybrid synthetic-real datasets spanning InternData-A1 and Agibot-World, covering over 533M frames. This hybrid training strategy effectively harnesses the diversity of synthetic simulation data while minimizing the sim-to-real gap. We evaluated InternVLA-A1 across 12 real-world robotic tasks and simulation benchmark. It significantly outperforms leading models like pi0 and GR00T N1.5, achieving a 14.5\% improvement in daily tasks and a 40\%-73.3\% boost in dynamic settings, such as conveyor belt sorting.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02456v1-abstract-full').style.display = 'none'; document.getElementById('2601.02456v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Homepage: https://internrobotics.github.io/internvla-a1.github.io/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.02358">arXiv:2601.02358</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.02358">pdf</a>, <a href="https://arxiv.org/ps/2601.02358">ps</a>, <a href="https://arxiv.org/format/2601.02358">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        VINO: A Unified Visual Generator with Interleaved OmniModal Context
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+J">Junyi Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+T">Tong He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fu%2C+Z">Zhoujie Fu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wan%2C+P">Pengfei Wan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gai%2C+K">Kun Gai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ye%2C+W">Weicai Ye</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.02358v1-abstract-short" style="display: inline;">
        &hellip;on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a <span class="search-hit mathjax">Multimodal</span> Diffusion <span class="search-hit mathjax">Transformer</span> (MMDiT), where <span class="search-hit mathjax">multimodal</span> inputs are encoded as interleaved conditioning token&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02358v1-abstract-full').style.display = 'inline'; document.getElementById('2601.02358v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.02358v1-abstract-full" style="display: none;">
        We present VINO, a unified visual generator that performs image and video generation and editing within a single framework. Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a <span class="search-hit mathjax">Multimodal</span> Diffusion <span class="search-hit mathjax">Transformer</span> (MMDiT), where <span class="search-hit mathjax">multimodal</span> inputs are encoded as interleaved conditioning tokens, and then used to guide the diffusion process. This design supports multi-reference grounding, long-form instruction following, and coherent identity preservation across static and dynamic content, while avoiding modality-specific architectural components. To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output. Across diverse generation and editing benchmarks, VINO demonstrates strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits. Our results highlight a practical path toward scalable unified visual generation, and the promise of interleaved, in-context computation as a foundation for general-purpose visual creation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02358v1-abstract-full').style.display = 'none'; document.getElementById('2601.02358v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Project page: https://sotamak1r.github.io/VINO-web/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.02356">arXiv:2601.02356</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.02356">pdf</a>, <a href="https://arxiv.org/ps/2601.02356">ps</a>, <a href="https://arxiv.org/format/2601.02356">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric <span class="search-hit mathjax">Transformation</span> in Scenes
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Tan%2C+J">Jing Tan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Z">Zhaoyang Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shen%2C+Y">Yantao Shen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cai%2C+J">Jiarui Cai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+S">Shuo Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jiajun Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xia%2C+W">Wei Xia</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tu%2C+Z">Zhuowen Tu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Soatto%2C+S">Stefano Soatto</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.02356v2-abstract-short" style="display: inline;">
        We introduce Talk2Move, a reinforcement learning (RL) based diffusion framework for text-instructed spatial <span class="search-hit mathjax">transformation</span> of objects within scenes. Spatially manipulating objects in a scene through natural language poses a challenge for&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02356v2-abstract-full').style.display = 'inline'; document.getElementById('2601.02356v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.02356v2-abstract-full" style="display: none;">
        We introduce Talk2Move, a reinforcement learning (RL) based diffusion framework for text-instructed spatial <span class="search-hit mathjax">transformation</span> of objects within scenes. Spatially manipulating objects in a scene through natural language poses a challenge for <span class="search-hit mathjax">multimodal</span> generation systems. While existing text-based manipulation methods can adjust appearance or style, they struggle to perform object-level geometric <span class="search-hit mathjax">transformations</span>-such as translating, rotating, or resizing objects-due to scarce paired supervision and pixel-level optimization limits. Talk2Move employs Group Relative Policy Optimization (GRPO) to explore geometric actions through diverse rollouts generated from input images and lightweight textual variations, removing the need for costly paired data. A spatial reward guided model aligns geometric <span class="search-hit mathjax">transformations</span> with linguistic description, while off-policy step evaluation and active step sampling improve learning efficiency by focusing on informative <span class="search-hit mathjax">transformation</span> stages. Furthermore, we design object-centric spatial rewards that evaluate displacement, rotation, and scaling behaviors directly, enabling interpretable and coherent <span class="search-hit mathjax">transformations</span>. Experiments on curated benchmarks demonstrate that Talk2Move achieves precise, consistent, and semantically faithful object <span class="search-hit mathjax">transformations</span>, outperforming existing text-guided editing approaches in both spatial accuracy and scene coherence.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02356v2-abstract-full').style.display = 'none'; document.getElementById('2601.02356v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 January, 2026; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 January, 2026;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Project page: https://sparkstj.github.io/talk2move</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.02249">arXiv:2601.02249</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.02249">pdf</a>, <a href="https://arxiv.org/ps/2601.02249">ps</a>, <a href="https://arxiv.org/format/2601.02249">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SLGNet: Synergizing Structural Priors and Language-Guided Modulation for <span class="search-hit mathjax">Multimodal</span> Object Detection
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Xiang%2C+X">Xiantai Xiang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+G">Guangyao Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wen%2C+Z">Zixiao Wen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+W">Wenshuai Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Niu%2C+B">Ben Niu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+F">Feng Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+L">Lijia Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Q">Qiantong Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Y">Yuhan Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pan%2C+Z">Zongxu Pan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hu%2C+Y">Yuxin Hu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.02249v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Multimodal</span> object detection leveraging RGB and Infrared (IR) images is pivotal for robust perception in all-weather scenarios. While recent adapter-based approaches efficiently transfer RGB-pretrained foundation models to this task, they often prioritize model efficiency at the expense of cross-modal structural consistency. Consequently, critical structural&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02249v1-abstract-full').style.display = 'inline'; document.getElementById('2601.02249v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.02249v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Multimodal</span> object detection leveraging RGB and Infrared (IR) images is pivotal for robust perception in all-weather scenarios. While recent adapter-based approaches efficiently transfer RGB-pretrained foundation models to this task, they often prioritize model efficiency at the expense of cross-modal structural consistency. Consequently, critical structural cues are frequently lost when significant domain gaps arise, such as in high-contrast or nighttime environments. Moreover, conventional static <span class="search-hit mathjax">multimodal</span> fusion mechanisms typically lack environmental awareness, resulting in suboptimal adaptation and constrained detection performance under complex, dynamic scene variations. To address these limitations, we propose SLGNet, a parameter-efficient framework that synergizes hierarchical structural priors and language-guided modulation within a frozen Vision <span class="search-hit mathjax">Transformer</span> (ViT)-based foundation model. Specifically, we design a Structure-Aware Adapter to extract hierarchical structural representations from both modalities and dynamically inject them into the ViT to compensate for structural degradation inherent in ViT-based backbones. Furthermore, we propose a Language-Guided Modulation module that exploits VLM-driven structured captions to dynamically recalibrate visual features, thereby endowing the model with robust environmental awareness. Extensive experiments on the LLVIP, FLIR, KAIST, and DroneVehicle datasets demonstrate that SLGNet establishes new state-of-the-art performance. Notably, on the LLVIP benchmark, our method achieves an mAP of 66.1, while reducing trainable parameters by approximately 87% compared to traditional full fine-tuning. This confirms SLGNet as a robust and efficient solution for <span class="search-hit mathjax">multimodal</span> perception.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02249v1-abstract-full').style.display = 'none'; document.getElementById('2601.02249v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.02211">arXiv:2601.02211</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.02211">pdf</a>, <a href="https://arxiv.org/ps/2601.02211">ps</a>, <a href="https://arxiv.org/format/2601.02211">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Unraveling MMDiT Blocks: Training-free Analysis and Enhancement of Text-conditioned Diffusion
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+B">Binglei Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+M">Mengping Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tan%2C+Z">Zhiyu Tan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+J">Junping Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+H">Hao Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.02211v1-abstract-short" style="display: inline;">
        Recent breakthroughs of <span class="search-hit mathjax">transformer</span>-based diffusion models, particularly with <span class="search-hit mathjax">Multimodal</span> Diffusion <span class="search-hit mathjax">Transformers</span> (MMDiT) driven models like FLUX and Qwen Image, have facilitated thrilling experiences in text-to-image generation and editing. To understand the internal mechanism of&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02211v1-abstract-full').style.display = 'inline'; document.getElementById('2601.02211v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.02211v1-abstract-full" style="display: none;">
        Recent breakthroughs of <span class="search-hit mathjax">transformer</span>-based diffusion models, particularly with <span class="search-hit mathjax">Multimodal</span> Diffusion <span class="search-hit mathjax">Transformers</span> (MMDiT) driven models like FLUX and Qwen Image, have facilitated thrilling experiences in text-to-image generation and editing. To understand the internal mechanism of MMDiT-based models, existing methods tried to analyze the effect of specific components like positional encoding and attention layers. Yet, a comprehensive understanding of how different blocks and their interactions with textual conditions contribute to the synthesis process remains elusive. In this paper, we first develop a systematic pipeline to comprehensively investigate each block&#39;s functionality by removing, disabling and enhancing textual hidden-states at corresponding blocks. Our analysis reveals that 1) semantic information appears in earlier blocks and finer details are rendered in later blocks, 2) removing specific blocks is usually less disruptive than disabling text conditions, and 3) enhancing textual conditions in selective blocks improves semantic attributes. Building on these observations, we further propose novel training-free strategies for improved text alignment, precise editing, and acceleration. Extensive experiments demonstrated that our method outperforms various baselines and remains flexible across text-to-image generation, image editing, and inference acceleration. Our method improves T2I-Combench++ from 56.92% to 63.00% and GenEval from 66.42% to 71.63% on SD3.5, without sacrificing synthesis quality. These results advance understanding of MMDiT models and provide valuable insights to unlock new possibilities for further improvements.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02211v1-abstract-full').style.display = 'none'; document.getElementById('2601.02211v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">11 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.02204">arXiv:2601.02204</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.02204">pdf</a>, <a href="https://arxiv.org/ps/2601.02204">ps</a>, <a href="https://arxiv.org/format/2601.02204">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        NextFlow: Unified Sequential Modeling Activates <span class="search-hit mathjax">Multimodal</span> Understanding and Generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+H">Huichao Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qu%2C+L">Liao Qu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Y">Yiheng Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+H">Hang Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Song%2C+Y">Yangyang Song</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+Y">Yongsheng Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+S">Shikun Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+X">Xian Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+X">Xu Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jiang%2C+Y">Yi Jiang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ye%2C+H">Hu Ye</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+B">Bo Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gao%2C+Y">Yiming Gao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+P">Peng Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+A">Akide Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+Z">Zhipeng Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Deng%2C+Q">Qili Deng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xing%2C+L">Linjie Xing</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+J">Jiyang Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Z">Zhao Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+Y">Yang Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+M">Mingcong Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Y">Yi Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+Q">Qian He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hu%2C+X">Xiwei Hu</a>
      , et al. (11 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.02204v1-abstract-short" style="display: inline;">
        We present NextFlow, a unified decoder-only autoregressive <span class="search-hit mathjax">transformer</span> trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates <span class="search-hit mathjax">multimodal</span> understanding and generation capabilities, unlocking abi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02204v1-abstract-full').style.display = 'inline'; document.getElementById('2601.02204v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.02204v1-abstract-full" style="display: none;">
        We present NextFlow, a unified decoder-only autoregressive <span class="search-hit mathjax">transformer</span> trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates <span class="search-hit mathjax">multimodal</span> understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02204v1-abstract-full').style.display = 'none'; document.getElementById('2601.02204v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Project page: https://github.com/ByteVisionLab/NextFlow</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.02008">arXiv:2601.02008</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.02008">pdf</a>, <a href="https://arxiv.org/ps/2601.02008">ps</a>, <a href="https://arxiv.org/format/2601.02008">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        XAI-MeD: Explainable Knowledge Guided Neuro-Symbolic Framework for Domain Generalization and Rare Class Detection in Medical Imaging
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Urooj%2C+M">Midhat Urooj</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Banerjee%2C+A">Ayan Banerjee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gupta%2C+S">Sandeep Gupta</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.02008v1-abstract-short" style="display: inline;">
        &hellip;class sensitivity and deliver transparent clinically aligned interpretations The framework encodes clinical expertise as logical connectives over atomic medical propositions <span class="search-hit mathjax">transforming</span> them into machine checkable class specific rules Their diagnostic utility is quantified through weighted feature satisfaction scores enabling a symbolic reasoning branch tha&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02008v1-abstract-full').style.display = 'inline'; document.getElementById('2601.02008v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.02008v1-abstract-full" style="display: none;">
        Explainability domain generalization and rare class reliability are critical challenges in medical AI where deep models often fail under real world distribution shifts and exhibit bias against infrequent clinical conditions This paper introduces XAIMeD an explainable medical AI framework that integrates clinically accurate expert knowledge into deep learning through a unified neuro symbolic architecture XAIMeD is designed to improve robustness under distribution shift enhance rare class sensitivity and deliver transparent clinically aligned interpretations The framework encodes clinical expertise as logical connectives over atomic medical propositions <span class="search-hit mathjax">transforming</span> them into machine checkable class specific rules Their diagnostic utility is quantified through weighted feature satisfaction scores enabling a symbolic reasoning branch that complements neural predictions A confidence weighted fusion integrates symbolic and deep outputs while a Hunt inspired adaptive routing mechanism guided by Entropy Imbalance Gain EIG and Rare Class Gini mitigates class imbalance high intra class variability and uncertainty We evaluate XAIMeD across diverse modalities on four challenging tasks i Seizure Onset Zone SOZ localization from rs fMRI ii Diabetic Retinopathy grading across 6 multicenter datasets demonstrate substantial performance improvements including 6 percent gains in cross domain generalization and a 10 percent improved rare class F1 score far outperforming state of the art deep learning baselines Ablation studies confirm that the clinically grounded symbolic components act as effective regularizers ensuring robustness to distribution shifts XAIMeD thus provides a principled clinically faithful and interpretable approach to <span class="search-hit mathjax">multimodal</span> medical AI.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02008v1-abstract-full').style.display = 'none'; document.getElementById('2601.02008v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at AAAI Bridge Program 2026</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.01593">arXiv:2601.01593</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.01593">pdf</a>, <a href="https://arxiv.org/ps/2601.01593">ps</a>, <a href="https://arxiv.org/format/2601.01593">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Beyond Patches: Global-aware Autoregressive Model for <span class="search-hit mathjax">Multimodal</span> Few-Shot Font Generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Cai%2C+H">Haonan Cai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Luo%2C+Y">Yuxuan Luo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lian%2C+Z">Zhouhui Lian</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.01593v1-abstract-short" style="display: inline;">
        Manual font design is an intricate process that <span class="search-hit mathjax">transforms</span> a stylistic visual concept into a coherent glyph set. This challenge persists in automated Few-shot Font Generation (FFG), where models often struggle to preserve both the structural integrity and stylistic fidelity from limited references. While autoregressive (AR) models have demonstrated impressiv&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.01593v1-abstract-full').style.display = 'inline'; document.getElementById('2601.01593v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.01593v1-abstract-full" style="display: none;">
        Manual font design is an intricate process that <span class="search-hit mathjax">transforms</span> a stylistic visual concept into a coherent glyph set. This challenge persists in automated Few-shot Font Generation (FFG), where models often struggle to preserve both the structural integrity and stylistic fidelity from limited references. While autoregressive (AR) models have demonstrated impressive generative capabilities, their application to FFG is constrained by conventional patch-level tokenization, which neglects global dependencies crucial for coherent font synthesis. Moreover, existing FFG methods remain within the image-to-image paradigm, relying solely on visual references and overlooking the role of language in conveying stylistic intent during font design. To address these limitations, we propose GAR-Font, a novel AR framework for <span class="search-hit mathjax">multimodal</span> few-shot font generation. GAR-Font introduces a global-aware tokenizer that effectively captures both local structures and global stylistic patterns, a <span class="search-hit mathjax">multimodal</span> style encoder offering flexible style control through a lightweight language-style adapter without requiring intensive <span class="search-hit mathjax">multimodal</span> pretraining, and a post-refinement pipeline that further enhances structural fidelity and style coherence. Extensive experiments show that GAR-Font outperforms existing FFG methods, excelling in maintaining global style faithfulness and achieving higher-quality results with textual stylistic guidance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.01593v1-abstract-full').style.display = 'none'; document.getElementById('2601.01593v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">25 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.01322">arXiv:2601.01322</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.01322">pdf</a>, <a href="https://arxiv.org/ps/2601.01322">ps</a>, <a href="https://arxiv.org/format/2601.01322">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        LinMU: <span class="search-hit mathjax">Multimodal</span> Understanding Made Linear
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+H">Hongjie Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jha%2C+N+K">Niraj K. Jha</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.01322v1-abstract-short" style="display: inline;">
        &hellip;and makes their understanding of high-resolution images and long-context videos prohibitively expensive. To address this challenge, we introduce LinMU (Linear-complexity <span class="search-hit mathjax">Multimodal</span> Understanding), a VLM design that achieves linear complexity without using any quadratic-complexity modules while maintaining the performance of global-attention-based VLMs. LinMU&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.01322v1-abstract-full').style.display = 'inline'; document.getElementById('2601.01322v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.01322v1-abstract-full" style="display: none;">
        Modern Vision-Language Models (VLMs) achieve impressive performance but are limited by the quadratic complexity of self-attention, which prevents their deployment on edge devices and makes their understanding of high-resolution images and long-context videos prohibitively expensive. To address this challenge, we introduce LinMU (Linear-complexity <span class="search-hit mathjax">Multimodal</span> Understanding), a VLM design that achieves linear complexity without using any quadratic-complexity modules while maintaining the performance of global-attention-based VLMs. LinMU replaces every self-attention layer in the VLM with the M-MATE block: a dual-branch module that combines a bidirectional state-space model for global context (Flex-MA branch) with localized Swin-style window attention (Local-Swin branch) for adjacent correlations. To <span class="search-hit mathjax">transform</span> a pre-trained VLM into the LinMU architecture, we propose a three-stage distillation framework that (i) initializes both branches with self-attention weights and trains the Flex-MA branch alone, (ii) unfreezes the Local-Swin branch and fine-tunes it jointly with the Flex-MA branch, and (iii) unfreezes the remaining blocks and fine-tunes them using LoRA adapters, while regressing on hidden states and token-level logits of the frozen VLM teacher. On MMMU, TextVQA, LongVideoBench, Video-MME, and other benchmarks, LinMU matches the performance of teacher models, yet reduces Time-To-First-Token (TTFT) by up to 2.7$\times$ and improves token throughput by up to 9.0$\times$ on minute-length videos. Ablations confirm the importance of each distillation stage and the necessity of the two branches of the M-MATE block. The proposed framework demonstrates that state-of-the-art <span class="search-hit mathjax">multimodal</span> reasoning can be achieved without quadratic attention, thus opening up avenues for long-context VLMs that can deal with high-resolution images and long videos.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.01322v1-abstract-full').style.display = 'none'; document.getElementById('2601.01322v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">23 pages, 7 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.00907">arXiv:2601.00907</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.00907">pdf</a>, <a href="https://arxiv.org/ps/2601.00907">ps</a>, <a href="https://arxiv.org/format/2601.00907">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Placenta Accreta Spectrum Detection using <span class="search-hit mathjax">Multimodal</span> Deep Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ali%2C+S">Sumaiya Ali</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Alhothali%2C+A">Areej Alhothali</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Albasri%2C+S">Sameera Albasri</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Alzamzami%2C+O">Ohoud Alzamzami</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Abduljabbar%2C+A">Ahmed Abduljabbar</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Alwazzan%2C+M">Muhammad Alwazzan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.00907v1-abstract-short" style="display: inline;">
        &hellip;maternal and neonatal risks. This study aimed to develop and validate a deep learning framework that enhances PAS detection by integrating multiple imaging modalities. A <span class="search-hit mathjax">multimodal</span> deep learning model was designed using an intermediate feature-level fusion architecture combining 3D Magnetic Resonance Imaging (MRI) and 2D Ultrasound (US) scans. Unimodal featu&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.00907v1-abstract-full').style.display = 'inline'; document.getElementById('2601.00907v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.00907v1-abstract-full" style="display: none;">
        Placenta Accreta Spectrum (PAS) is a life-threatening obstetric complication involving abnormal placental invasion into the uterine wall. Early and accurate prenatal diagnosis is essential to reduce maternal and neonatal risks. This study aimed to develop and validate a deep learning framework that enhances PAS detection by integrating multiple imaging modalities. A <span class="search-hit mathjax">multimodal</span> deep learning model was designed using an intermediate feature-level fusion architecture combining 3D Magnetic Resonance Imaging (MRI) and 2D Ultrasound (US) scans. Unimodal feature extractors, a 3D DenseNet121-Vision <span class="search-hit mathjax">Transformer</span> for MRI and a 2D ResNet50 for US, were selected after systematic comparative analysis. Curated datasets comprising 1,293 MRI and 1,143 US scans were used to train the unimodal models and paired samples of patient-matched MRI-US scans was isolated for <span class="search-hit mathjax">multimodal</span> model development and evaluation. On an independent test set, the <span class="search-hit mathjax">multimodal</span> fusion model achieved superior performance, with an accuracy of 92.5% and an Area Under the Receiver Operating Characteristic Curve (AUC) of 0.927, outperforming the MRI-only (82.5%, AUC 0.825) and US-only (87.5%, AUC 0.879) models. Integrating MRI and US features provides complementary diagnostic information, demonstrating strong potential to enhance prenatal risk assessment and improve patient outcomes.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.00907v1-abstract-full').style.display = 'none'; document.getElementById('2601.00907v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 December, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.00670">arXiv:2601.00670</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.00670">pdf</a>, <a href="https://arxiv.org/ps/2601.00670">ps</a>, <a href="https://arxiv.org/format/2601.00670">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Wave2Word: A <span class="search-hit mathjax">Multimodal</span> <span class="search-hit mathjax">Transformer</span> Framework for Joint EEG-Text Alignment and Multi-Task Representation Learning in Neurocritical Care
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Samanta%2C+A+K">Argha Kamal Samanta</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mewada%2C+D">Deepak Mewada</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sarma%2C+M">Monalisa Sarma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Samanta%2C+D">Debasis Samanta</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.00670v1-abstract-short" style="display: inline;">
        &hellip;exhibits overlapping patterns, graded expert agreement, and temporal persistence, which are not well captured by classification objectives alone. This work proposes a <span class="search-hit mathjax">multimodal</span> EEG representation learning framework that integrates signal-domain modeling with structured clinical language supervision. First, raw EEG is&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.00670v1-abstract-full').style.display = 'inline'; document.getElementById('2601.00670v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.00670v1-abstract-full" style="display: none;">
        Continuous electroencephalography (EEG) is routinely used in neurocritical care to monitor seizures and other harmful brain activity, including rhythmic and periodic patterns that are clinically significant. Although deep learning methods have achieved high accuracy in seizure detection, most existing approaches remain seizure-centric, rely on discrete-label supervision, and are primarily evaluated using accuracy-based metrics. A central limitation of current EEG modeling practice is the weak correspondence between learned representations and how EEG findings are interpreted and summarized in clinical workflows. Harmful EEG activity exhibits overlapping patterns, graded expert agreement, and temporal persistence, which are not well captured by classification objectives alone. This work proposes a <span class="search-hit mathjax">multimodal</span> EEG representation learning framework that integrates signal-domain modeling with structured clinical language supervision. First, raw EEG is <span class="search-hit mathjax">transformed</span> into a longitudinal bipolar montage and time-frequency representations. Second, dual <span class="search-hit mathjax">transformer</span>-based encoders model complementary temporal and frequency-centric dependencies and are fused using an adaptive gating mechanism. Third, EEG embeddings are aligned with structured expert consensus descriptions through a contrastive objective. Finally, an EEG-conditioned text reconstruction loss is introduced as a representation-level constraint alongside standard classification loss. Experimental evaluation using a controlled train-validation-test split achieves a six-class test accuracy of 0.9797. Ablation analyses show that removing contrastive alignment reduces cross-modal retrieval performance from Recall@10 of 0.3390 to 0.0045, despite minimal change in classification accuracy. These findings demonstrate that discriminative accuracy does not reliably reflect representation quality for clinically meaningful EEG modeling.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.00670v1-abstract-full').style.display = 'none'; document.getElementById('2601.00670v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2512.24645">arXiv:2512.24645</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2512.24645">pdf</a>, <a href="https://arxiv.org/ps/2512.24645">ps</a>, <a href="https://arxiv.org/format/2512.24645">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3746027.3756869">10.1145/3746027.3756869 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AudioFab: Building A General and Intelligent Audio Factory through Tool Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+C">Cheng Zhu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Han%2C+J">Jing Han</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xue%2C+Q">Qianshuai Xue</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+K">Kehan Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+H">Huan Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Z">Zixing Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2512.24645v1-abstract-short" style="display: inline;">
        Currently, artificial intelligence is profoundly <span class="search-hit mathjax">transforming</span> the audio domain; however, numerous advanced algorithms and tools remain fragmented, lacking a unified and efficient framework to unlock their full potential. Existing audio agent frameworks often suffer from complex environment configurations and inefficient tool collaboration. To address these l&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.24645v1-abstract-full').style.display = 'inline'; document.getElementById('2512.24645v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2512.24645v1-abstract-full" style="display: none;">
        Currently, artificial intelligence is profoundly <span class="search-hit mathjax">transforming</span> the audio domain; however, numerous advanced algorithms and tools remain fragmented, lacking a unified and efficient framework to unlock their full potential. Existing audio agent frameworks often suffer from complex environment configurations and inefficient tool collaboration. To address these limitations, we introduce AudioFab, an open-source agent framework aimed at establishing an open and intelligent audio-processing ecosystem. Compared to existing solutions, AudioFab&#39;s modular design resolves dependency conflicts, simplifying tool integration and extension. It also optimizes tool learning through intelligent selection and few-shot learning, improving efficiency and accuracy in complex audio tasks. Furthermore, AudioFab provides a user-friendly natural language interface tailored for non-expert users. As a foundational framework, AudioFab&#39;s core contribution lies in offering a stable and extensible platform for future research and development in audio and <span class="search-hit mathjax">multimodal</span> AI. The code is available at https://github.com/SmileHnu/AudioFab.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.24645v1-abstract-full').style.display = 'none'; document.getElementById('2512.24645v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 December, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2025.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        ACM Multimedia 2025
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2512.24271">arXiv:2512.24271</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2512.24271">pdf</a>, <a href="https://arxiv.org/ps/2512.24271">ps</a>, <a href="https://arxiv.org/format/2512.24271">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Taming Hallucinations: Boosting MLLMs&#39; Video Understanding via Counterfactual Video Generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+Z">Zhe Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wen%2C+H">Hao Wen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hao%2C+A">Aiming Hao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Song%2C+B">Bingze Song</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+M">Meiqi Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jiahong Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chu%2C+X">Xiangxiang Chu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lu%2C+S">Sheng Lu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+H">Haoqian Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2512.24271v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Multimodal</span> Large Language Models (MLLMs) have made remarkable progress in video understanding. However, they suffer from a critical vulnerability: an over-reliance on language priors, which can lead to visual ungrounded hallucinations, especially when processing counterfactual videos that defy common sense. This limitation, stemming from the intrinsic data i&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.24271v1-abstract-full').style.display = 'inline'; document.getElementById('2512.24271v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2512.24271v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Multimodal</span> Large Language Models (MLLMs) have made remarkable progress in video understanding. However, they suffer from a critical vulnerability: an over-reliance on language priors, which can lead to visual ungrounded hallucinations, especially when processing counterfactual videos that defy common sense. This limitation, stemming from the intrinsic data imbalance between text and video, is challenging to address due to the substantial cost of collecting and annotating counterfactual data. To address this, we introduce DualityForge, a novel counterfactual data synthesis framework that employs controllable, diffusion-based video editing to <span class="search-hit mathjax">transform</span> real-world videos into counterfactual scenarios. By embedding structured contextual information into the video editing and QA generation processes, the framework automatically produces high-quality QA pairs together with original-edited video pairs for contrastive training. Based on this, we build DualityVidQA, a large-scale video dataset designed to reduce MLLM hallucinations. In addition, to fully exploit the contrastive nature of our paired data, we propose Duality-Normalized Advantage Training (DNA-Train), a two-stage SFT-RL training regime where the RL phase applies pair-wise $\ell_1$ advantage normalization, thereby enabling a more stable and efficient policy optimization. Experiments on DualityVidQA-Test demonstrate that our method substantially reduces model hallucinations on counterfactual videos, yielding a relative improvement of 24.0% over the Qwen2.5-VL-7B baseline. Moreover, our approach achieves significant gains across both hallucination and general-purpose benchmarks, indicating strong generalization capability. We will open-source our dataset and code.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.24271v1-abstract-full').style.display = 'none'; document.getElementById('2512.24271v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 December, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">18 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2512.24243">arXiv:2512.24243</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2512.24243">pdf</a>, <a href="https://arxiv.org/ps/2512.24243">ps</a>, <a href="https://arxiv.org/format/2512.24243">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MambaSeg: Harnessing Mamba for Accurate and Efficient Image-Event Semantic Segmentation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Gu%2C+F">Fuqiang Gu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Y">Yuanke Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Long%2C+X">Xianlei Long</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ji%2C+K">Kangping Ji</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+C">Chao Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gu%2C+Q">Qingyi Gu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ni%2C+Z">Zhenliang Ni</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2512.24243v1-abstract-short" style="display: inline;">
        &hellip;task in computer vision with wide-ranging applications, including autonomous driving and robotics. While RGB-based methods have achieved strong performance with CNNs and <span class="search-hit mathjax">Transformers</span>, their effectiveness degrades under fast motion, low-light, or high dynamic range conditions due to limitations of frame cameras. Event cameras offer complementary advantages su&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.24243v1-abstract-full').style.display = 'inline'; document.getElementById('2512.24243v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2512.24243v1-abstract-full" style="display: none;">
        Semantic segmentation is a fundamental task in computer vision with wide-ranging applications, including autonomous driving and robotics. While RGB-based methods have achieved strong performance with CNNs and <span class="search-hit mathjax">Transformers</span>, their effectiveness degrades under fast motion, low-light, or high dynamic range conditions due to limitations of frame cameras. Event cameras offer complementary advantages such as high temporal resolution and low latency, yet lack color and texture, making them insufficient on their own. To address this, recent research has explored <span class="search-hit mathjax">multimodal</span> fusion of RGB and event data; however, many existing approaches are computationally expensive and focus primarily on spatial fusion, neglecting the temporal dynamics inherent in event streams. In this work, we propose MambaSeg, a novel dual-branch semantic segmentation framework that employs parallel Mamba encoders to efficiently model RGB images and event streams. To reduce cross-modal ambiguity, we introduce the Dual-Dimensional Interaction Module (DDIM), comprising a Cross-Spatial Interaction Module (CSIM) and a Cross-Temporal Interaction Module (CTIM), which jointly perform fine-grained fusion along both spatial and temporal dimensions. This design improves cross-modal alignment, reduces ambiguity, and leverages the complementary properties of each modality. Extensive experiments on the DDD17 and DSEC datasets demonstrate that MambaSeg achieves state-of-the-art segmentation performance while significantly reducing computational cost, showcasing its promise for efficient, scalable, and robust <span class="search-hit mathjax">multimodal</span> perception.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.24243v1-abstract-full').style.display = 'none'; document.getElementById('2512.24243v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 December, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by AAAI 2026</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2512.23906">arXiv:2512.23906</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2512.23906">pdf</a>, <a href="https://arxiv.org/ps/2512.23906">ps</a>, <a href="https://arxiv.org/format/2512.23906">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A <span class="search-hit mathjax">multimodal</span> <span class="search-hit mathjax">Transformer</span> for InSAR-based ground deformation forecasting with cross-site generalization across Europe
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Yao%2C+W">Wendong Yao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+B">Binhua Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dev%2C+S">Soumyabrata Dev</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2512.23906v1-abstract-short" style="display: inline;">
        &hellip;of long-term trends, seasonal cycles, and occasional abrupt discontinuities (e.g., co-seismic steps), together with strong spatial heterogeneity. In this study we propose a <span class="search-hit mathjax">multimodal</span> patch-based&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.23906v1-abstract-full').style.display = 'inline'; document.getElementById('2512.23906v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2512.23906v1-abstract-full" style="display: none;">
        Near-real-time regional-scale monitoring of ground deformation is increasingly required to support urban planning, critical infrastructure management, and natural hazard mitigation. While Interferometric Synthetic Aperture Radar (InSAR) and continental-scale services such as the European Ground Motion Service (EGMS) provide dense observations of past motion, predicting the next observation remains challenging due to the superposition of long-term trends, seasonal cycles, and occasional abrupt discontinuities (e.g., co-seismic steps), together with strong spatial heterogeneity. In this study we propose a <span class="search-hit mathjax">multimodal</span> patch-based <span class="search-hit mathjax">Transformer</span> for single-step, fixed-interval next-epoch nowcasting of displacement maps from EGMS time series (resampled to a 64x64 grid over 100 km x 100 km tiles). The model ingests recent displacement snapshots together with (i) static kinematic indicators (mean velocity, acceleration, seasonal amplitude) computed in a leakage-safe manner from the training window only, and (ii) harmonic day-of-year encodings. On the eastern Ireland tile (E32N34), the STGCN is strongest in the displacement-only setting, whereas the <span class="search-hit mathjax">multimodal</span> <span class="search-hit mathjax">Transformer</span> clearly outperforms CNN-LSTM, CNN-LSTM+Attn, and <span class="search-hit mathjax">multimodal</span> STGCN when all models receive the same <span class="search-hit mathjax">multimodal</span> inputs, achieving RMSE = 0.90 mm and $R^2$ = 0.97 on the test set with the best threshold accuracies.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.23906v1-abstract-full').style.display = 'none'; document.getElementById('2512.23906v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 December, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">submitted to ISPRS Journal of Photogrammetry and Remote Sensing for review</span>
    </p>
    

    
      <p class="comments is-size-7">
        
          <span class="has-text-black-bis has-text-weight-semibold">Report number:</span>
          PHOTO-D-25-03411
        

        

        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2512.23903">arXiv:2512.23903</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2512.23903">pdf</a>, <a href="https://arxiv.org/ps/2512.23903">ps</a>, <a href="https://arxiv.org/format/2512.23903">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Scaling Remote Sensing Foundation Models: Data Domain Tradeoffs at the Peta-Scale
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wickrema%2C+C">Charith Wickrema</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mace%2C+E">Eliza Mace</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Brown%2C+H">Hunter Brown</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cabrera%2C+H">Heidys Cabrera</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Krall%2C+N">Nick Krall</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=O%27Neill%2C+M">Matthew O&#39;Neill</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sarkar%2C+S">Shivangi Sarkar</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Weissman%2C+L">Lowell Weissman</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hughes%2C+E">Eric Hughes</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zarrella%2C+G">Guido Zarrella</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2512.23903v1-abstract-short" style="display: inline;">
        &hellip;techniques for training foundation models on high-resolution electro-optical (EO) datasets that exceed the current state-of-the-art scale by orders of magnitude. Modern <span class="search-hit mathjax">multimodal</span> machine learning (ML) applications, such as generative artificial intelligence (GenAI) systems for image captioning, search, and reasoning, depend on robust, domain-specialized enc&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.23903v1-abstract-full').style.display = 'inline'; document.getElementById('2512.23903v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2512.23903v1-abstract-full" style="display: none;">
        We explore the scaling behaviors of artificial intelligence to establish practical techniques for training foundation models on high-resolution electro-optical (EO) datasets that exceed the current state-of-the-art scale by orders of magnitude. Modern <span class="search-hit mathjax">multimodal</span> machine learning (ML) applications, such as generative artificial intelligence (GenAI) systems for image captioning, search, and reasoning, depend on robust, domain-specialized encoders for non-text modalities. In natural-image domains where internet-scale data is plentiful, well-established scaling laws help optimize the joint scaling of model capacity, training compute, and dataset size. Unfortunately, these relationships are much less well-understood in high-value domains like remote sensing (RS). Using over a quadrillion pixels of commercial satellite EO data and the MITRE Federal AI Sandbox, we train progressively larger vision <span class="search-hit mathjax">transformer</span> (ViT) backbones, report success and failure modes observed at petascale, and analyze implications for bridging domain gaps across additional RS modalities. We observe that even at this scale, performance is consistent with a data limited regime rather than a model parameter-limited one. These practical insights are intended to inform data-collection strategies, compute budgets, and optimization schedules that advance the future development of frontier-scale RS foundation models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.23903v1-abstract-full').style.display = 'none'; document.getElementById('2512.23903v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 December, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2512.23597">arXiv:2512.23597</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2512.23597">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Scalable Residual Feature Aggregation Framework with Hybrid Metaheuristic Optimization for Robust Early Pancreatic Neoplasm Detection in <span class="search-hit mathjax">Multimodal</span> CT Imaging
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Thiruvengadam%2C+J+A">Janani Annur Thiruvengadam</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nabigaru%2C+K+M">Kiran Mayee Nabigaru</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kovi%2C+A">Anusha Kovi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2512.23597v1-abstract-short" style="display: inline;">
        &hellip;be addressed with an effective and scalable system that can assist in enhancing the salience of the subtle visual cues and provide a high level of the generalization on the <span class="search-hit mathjax">multimodal</span> imaging data. A Scalable Residual Feature Aggregation (SRFA) framework is proposed to be used to meet these conditions in this study. The framework integrates a pipeline of pre&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.23597v1-abstract-full').style.display = 'inline'; document.getElementById('2512.23597v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2512.23597v1-abstract-full" style="display: none;">
        The early detection of pancreatic neoplasm is a major clinical dilemma, and it is predominantly so because tumors are likely to occur with minimal contrast margins and a large spread anatomy-wide variation amongst patients on a CT scan. These complexities require to be addressed with an effective and scalable system that can assist in enhancing the salience of the subtle visual cues and provide a high level of the generalization on the <span class="search-hit mathjax">multimodal</span> imaging data. A Scalable Residual Feature Aggregation (SRFA) framework is proposed to be used to meet these conditions in this study. The framework integrates a pipeline of preprocessing followed by the segmentation using the MAGRes-UNet that is effective in making the pancreatic structures and isolating regions of interest more visible. DenseNet-121 performed with residual feature storage is used to extract features to allow deep hierarchical features to be aggregated without properties loss. To go further, hybrid HHO-BA metaheuristic feature selection strategy is used, which guarantees the best feature subset refinement. To be classified, the system is trained based on a new hybrid model that integrates the ability to pay attention on the world, which is the Vision <span class="search-hit mathjax">Transformer</span> (ViT) with the high representational efficiency of EfficientNet-B3. A dual optimization mechanism incorporating SSA and GWO is used to fine-tune hyperparameters to enhance greater robustness and less overfitting. Experimental results support the significant improvement in performance, with the suggested model reaching 96.23% accuracy, 95.58% F1-score and 94.83% specificity, the model is significantly better than the traditional CNNs and contemporary <span class="search-hit mathjax">transformer</span>-based models. Such results highlight the possibility of the SRFA framework as a useful instrument in the early detection of pancreatic tumors.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.23597v1-abstract-full').style.display = 'none'; document.getElementById('2512.23597v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 December, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2512.23568">arXiv:2512.23568</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2512.23568">pdf</a>, <a href="https://arxiv.org/ps/2512.23568">ps</a>, <a href="https://arxiv.org/format/2512.23568">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ThinkGen: Generalized Thinking for Visual Generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Jiao%2C+S">Siyu Jiao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+Y">Yiheng Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhong%2C+Y">Yujie Zhong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=She%2C+Q">Qi She</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+W">Wei Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lan%2C+X">Xiaohan Lan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+Z">Zilong Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+F">Fei Yu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+Y">Yingchen Yu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+Y">Yunqing Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+Y">Yao Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wei%2C+Y">Yunchao Wei</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2512.23568v1-abstract-short" style="display: inline;">
        Recent progress in <span class="search-hit mathjax">Multimodal</span> Large Language Models (MLLMs) demonstrates that Chain-of-Thought (CoT) reasoning enables systematic solutions to complex understanding tasks. However, its extension to generation tasks remains nascent and limited by scenario-specific mechanisms that hinder generalization and adaptation. In this work, we present ThinkGen, the fir&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.23568v1-abstract-full').style.display = 'inline'; document.getElementById('2512.23568v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2512.23568v1-abstract-full" style="display: none;">
        Recent progress in <span class="search-hit mathjax">Multimodal</span> Large Language Models (MLLMs) demonstrates that Chain-of-Thought (CoT) reasoning enables systematic solutions to complex understanding tasks. However, its extension to generation tasks remains nascent and limited by scenario-specific mechanisms that hinder generalization and adaptation. In this work, we present ThinkGen, the first think-driven visual generation framework that explicitly leverages MLLM&#39;s CoT reasoning in various generation scenarios. ThinkGen employs a decoupled architecture comprising a pretrained MLLM and a Diffusion <span class="search-hit mathjax">Transformer</span> (DiT), wherein the MLLM generates tailored instructions based on user intent, and DiT produces high-quality images guided by these instructions. We further propose a separable GRPO-based training paradigm (SepGRPO), alternating reinforcement learning between the MLLM and DiT modules. This flexible design enables joint training across diverse datasets, facilitating effective CoT reasoning for a wide range of generative scenarios. Extensive experiments demonstrate that ThinkGen achieves robust, state-of-the-art performance across multiple generation benchmarks. Code is available: https://github.com/jiaosiyuu/ThinkGen
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.23568v1-abstract-full').style.display = 'none'; document.getElementById('2512.23568v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 December, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2512.23380">arXiv:2512.23380</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2512.23380">pdf</a>, <a href="https://arxiv.org/ps/2512.23380">ps</a>, <a href="https://arxiv.org/format/2512.23380">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Operating Systems">cs.OS</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1038/s41598-025-27693-4">10.1038/s41598-025-27693-4 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A unified framework for detecting point and collective anomalies in operating system logs via collaborative <span class="search-hit mathjax">transformers</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Nasirzadeh%2C+M">Mohammad Nasirzadeh</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tahmoresnezhad%2C+J">Jafar Tahmoresnezhad</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rashidi-Khazaee%2C+P">Parviz Rashidi-Khazaee</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2512.23380v1-abstract-short" style="display: inline;">
        &hellip;in logs that can be considered log modalities. In light of this intuition, unimodal methods often struggle by ignoring the different modalities of log data. Meanwhile, <span class="search-hit mathjax">multimodal</span> methods fail to handle the interactions between these modalities. Applying&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.23380v1-abstract-full').style.display = 'inline'; document.getElementById('2512.23380v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2512.23380v1-abstract-full" style="display: none;">
        Log anomaly detection is crucial for preserving the security of operating systems. Depending on the source of log data collection, various information is recorded in logs that can be considered log modalities. In light of this intuition, unimodal methods often struggle by ignoring the different modalities of log data. Meanwhile, <span class="search-hit mathjax">multimodal</span> methods fail to handle the interactions between these modalities. Applying <span class="search-hit mathjax">multimodal</span> sentiment analysis to log anomaly detection, we propose CoLog, a framework that collaboratively encodes logs utilizing various modalities. CoLog utilizes collaborative <span class="search-hit mathjax">transformers</span> and multi-head impressed attention to learn interactions among several modalities, ensuring comprehensive anomaly detection. To handle the heterogeneity caused by these interactions, CoLog incorporates a modality adaptation layer, which adapts the representations from different log modalities. This methodology enables CoLog to learn nuanced patterns and dependencies within the data, enhancing its anomaly detection capabilities. Extensive experiments demonstrate CoLog&#39;s superiority over existing state-of-the-art methods. Furthermore, in detecting both point and collective anomalies, CoLog achieves a mean precision of 99.63%, a mean recall of 99.59%, and a mean F1 score of 99.61% across seven benchmark datasets for log-based anomaly detection. The comprehensive detection capabilities of CoLog make it highly suitable for cybersecurity, system monitoring, and operational efficiency. CoLog represents a significant advancement in log anomaly detection, providing a sophisticated and effective solution to point and collective anomaly detection through a unified framework and a solution to the complex challenges automatic log data analysis poses. We also provide the implementation of CoLog at https://github.com/NasirzadehMoh/CoLog.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.23380v1-abstract-full').style.display = 'none'; document.getElementById('2512.23380v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 December, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">72 pages, 19 figures, 19 tables, accepted in scientific reports on 5 November 2025</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Scientific Reports 15, 45698 (2025)
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2512.23304">arXiv:2512.23304</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2512.23304">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MedGemma vs GPT-4: Open-Source and Proprietary Zero-shot Medical Disease Classification from Images
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Prottasha%2C+M+S+I">Md. Sazzadul Islam Prottasha</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rafi%2C+N+W">Nabil Walid Rafi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2512.23304v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Multimodal</span> Large Language Models (LLMs) introduce an emerging paradigm for medical imaging by interpreting scans through the lens of extensive clinical knowledge, offering a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.23304v1-abstract-full').style.display = 'inline'; document.getElementById('2512.23304v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2512.23304v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Multimodal</span> Large Language Models (LLMs) introduce an emerging paradigm for medical imaging by interpreting scans through the lens of extensive clinical knowledge, offering a <span class="search-hit mathjax">transformative</span> approach to disease classification. This study presents a critical comparison between two fundamentally different AI architectures: the specialized open-source agent MedGemma and the proprietary large <span class="search-hit mathjax">multimodal</span> model GPT-4 for diagnosing six different diseases. The MedGemma-4b-it model, fine-tuned using Low-Rank Adaptation (LoRA), demonstrated superior diagnostic capability by achieving a mean test accuracy of 80.37% compared to 69.58% for the untuned GPT-4. Furthermore, MedGemma exhibited notably higher sensitivity in high-stakes clinical tasks, such as cancer and pneumonia detection. Quantitative analysis via confusion matrices and classification reports provides comprehensive insights into model performance across all categories. These results emphasize that domain-specific fine-tuning is essential for minimizing hallucinations in clinical implementation, positioning MedGemma as a sophisticated tool for complex, evidence-based medical reasoning.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.23304v1-abstract-full').style.display = 'none'; document.getElementById('2512.23304v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 December, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted for publication in the Journal of Machine Learning and Deep Learning (JMLDL). 9 pages, 9 figures, 10 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2512.23028">arXiv:2512.23028</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2512.23028">pdf</a>, <a href="https://arxiv.org/ps/2512.23028">ps</a>, <a href="https://arxiv.org/format/2512.23028">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An Architecture-Led Hybrid Report on Body Language Detection Project
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Tong%2C+T">Thomson Tong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Darooneh%2C+D">Diba Darooneh</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2512.23028v1-abstract-short" style="display: inline;">
        &hellip;attributes (emotion by default), validates output structure using a predefined schema, and optionally renders an annotated video. We first summarize the shared <span class="search-hit mathjax">multimodal</span> foundation (visual tokenization, <span class="search-hit mathjax">Transformer</span> attention, and instruction following), then describe each architecture at a level sufficient to justify&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.23028v1-abstract-full').style.display = 'inline'; document.getElementById('2512.23028v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2512.23028v1-abstract-full" style="display: none;">
        This report provides an architecture-led analysis of two modern vision-language models (VLMs), Qwen2.5-VL-7B-Instruct and Llama-4-Scout-17B-16E-Instruct, and explains how their architectural properties map to a practical video-to-artifact pipeline implemented in the BodyLanguageDetection repository [1]. The system samples video frames, prompts a VLM to detect visible people and generate pixel-space bounding boxes with prompt-conditioned attributes (emotion by default), validates output structure using a predefined schema, and optionally renders an annotated video. We first summarize the shared <span class="search-hit mathjax">multimodal</span> foundation (visual tokenization, <span class="search-hit mathjax">Transformer</span> attention, and instruction following), then describe each architecture at a level sufficient to justify engineering choices without speculative internals. Finally, we connect model behavior to system constraints: structured outputs can be syntactically valid while semantically incorrect, schema validation is structural (not geometric correctness), person identifiers are frame-local in the current prompting contract, and interactive single-frame analysis returns free-form text rather than schema-enforced JSON. These distinctions are critical for writing defensible claims, designing robust interfaces, and planning evaluation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.23028v1-abstract-full').style.display = 'none'; document.getElementById('2512.23028v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 December, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2512.23025">arXiv:2512.23025</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2512.23025">pdf</a>, <a href="https://arxiv.org/ps/2512.23025">ps</a>, <a href="https://arxiv.org/format/2512.23025">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        LENS: LLM-Enabled Narrative Synthesis for Mental Health by Aligning <span class="search-hit mathjax">Multimodal</span> Sensing with Language Models
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+W">Wenxuan Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pillai%2C+A">Arvind Pillai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nepal%2C+S">Subigya Nepal</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Collins%2C+A+C">Amanda C Collins</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mackin%2C+D+M">Daniel M Mackin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Heinz%2C+M+V">Michael V Heinz</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Griffin%2C+T+Z">Tess Z Griffin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jacobson%2C+N+C">Nicholas C Jacobson</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Campbell%2C+A">Andrew Campbell</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2512.23025v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Multimodal</span> health sensing offers rich behavioral signals for assessing mental health, yet translating these numerical time-series measurements into natural language remains challenging. Current LLMs cannot natively ingest long-duration sensor streams, and paired sensor-text datasets are scarce. To address these challenges, we introduce LENS, a framework that&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.23025v1-abstract-full').style.display = 'inline'; document.getElementById('2512.23025v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2512.23025v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Multimodal</span> health sensing offers rich behavioral signals for assessing mental health, yet translating these numerical time-series measurements into natural language remains challenging. Current LLMs cannot natively ingest long-duration sensor streams, and paired sensor-text datasets are scarce. To address these challenges, we introduce LENS, a framework that aligns <span class="search-hit mathjax">multimodal</span> sensing data with language models to generate clinically grounded mental-health narratives. LENS first constructs a large-scale dataset by <span class="search-hit mathjax">transforming</span> Ecological Momentary Assessment (EMA) responses related to depression and anxiety symptoms into natural-language descriptions, yielding over 100,000 sensor-text QA pairs from 258 participants. To enable native time-series integration, we train a patch-level encoder that projects raw sensor signals directly into an LLM&#39;s representation space. Our results show that LENS outperforms strong baselines on standard NLP metrics and task-specific measures of symptom-severity accuracy. A user study with 13 mental-health professionals further indicates that LENS-produced narratives are comprehensive and clinically meaningful. Ultimately, our approach advances LLMs as interfaces for health sensing, providing a scalable path toward models that can reason over raw behavioral signals and support downstream clinical decision-making.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.23025v1-abstract-full').style.display = 'none'; document.getElementById('2512.23025v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 December, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">22 pages, 9 figures, under review</span>
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                     
          
          <li>
            <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=50"
              class="pagination-link "
              aria-label="Page 2"
              aria-current="page">2
            </a>
          </li>
          
          <li>
            <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=100"
              class="pagination-link "
              aria-label="Page 3"
              aria-current="page">3
            </a>
          </li>
          
          <li>
            <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=150"
              class="pagination-link "
              aria-label="Page 4"
              aria-current="page">4
            </a>
          </li>
          
          <li>
            <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=200"
              class="pagination-link "
              aria-label="Page 5"
              aria-current="page">5
            </a>
          </li>
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  

  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  <script src="https://static.arxiv.org/static/base/1.0.0a5/js/member_acknowledgement.js"></script>
  </body>
</html>

<!-- ===== END SEARCH ===== -->

<!-- ===== ABS URL: https://arxiv.org/abs/2601.07581 | HTTP 200 ===== -->

<!DOCTYPE html>
<html lang="en">

<head>  <title>[2601.07581] BenchSeg: A Large-Scale Dataset and Benchmark for Multi-View Food Video Segmentation</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="apple-touch-icon" sizes="180x180" href="/static/browse/0.3.4/images/icons/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/static/browse/0.3.4/images/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/static/browse/0.3.4/images/icons/favicon-16x16.png">
  <link rel="manifest" href="/static/browse/0.3.4/images/icons/site.webmanifest">
  <link rel="mask-icon" href="/static/browse/0.3.4/images/icons/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">
  <link rel="stylesheet" type="text/css" media="screen" href="/static/browse/0.3.4/css/arXiv.css?v=20241206" />
  <link rel="stylesheet" type="text/css" media="print" href="/static/browse/0.3.4/css/arXiv-print.css?v=20200611" />
  <link rel="stylesheet" type="text/css" media="screen" href="/static/browse/0.3.4/css/browse_search.css" />
  <script language="javascript" src="/static/browse/0.3.4/js/accordion.js" ></script>
  <script language="javascript" src="/static/browse/0.3.4/js/optin-modal.js?v=20250819"></script>
  
  <link rel="canonical" href="https://arxiv.org/abs/2601.07581"/>
  <meta name="description" content="Abstract page for arXiv paper 2601.07581: BenchSeg: A Large-Scale Dataset and Benchmark for Multi-View Food Video Segmentation"><meta property="og:type" content="website" />
<meta property="og:site_name" content="arXiv.org" />
<meta property="og:title" content="BenchSeg: A Large-Scale Dataset and Benchmark for Multi-View Food Video Segmentation" />
<meta property="og:url" content="https://arxiv.org/abs/2601.07581v1" />
<meta property="og:image" content="/static/browse/0.3.4/images/arxiv-logo-fb.png" />
<meta property="og:image:secure_url" content="/static/browse/0.3.4/images/arxiv-logo-fb.png" />
<meta property="og:image:width" content="1200" />
<meta property="og:image:height" content="700" />
<meta property="og:image:alt" content="arXiv logo"/>
<meta property="og:description" content="Food image segmentation is a critical task for dietary analysis, enabling accurate estimation of food volume and nutrients. However, current methods suffer from limited multi-view data and poor generalization to new viewpoints. We introduce BenchSeg, a novel multi-view food video segmentation dataset and benchmark. BenchSeg aggregates 55 dish scenes (from Nutrition5k, Vegetables &amp; Fruits, MetaFood3D, and FoodKit) with 25,284 meticulously annotated frames, capturing each dish under free 360° camera motion. We evaluate a diverse set of 20 state-of-the-art segmentation models (e.g., SAM-based, transformer, CNN, and large multimodal) on the existing FoodSeg103 dataset and evaluate them (alone and combined with video-memory modules) on BenchSeg. Quantitative and qualitative results demonstrate that while standard image segmenters degrade sharply under novel viewpoints, memory-augmented methods maintain temporal consistency across frames. Our best model based on a combination of SeTR-MLA+XMem2 outperforms prior work (e.g., improving over FoodMem by ~2.63% mAP), offering new insights into food segmentation and tracking for dietary analysis. We release BenchSeg to foster future research. The project page including the dataset annotations and the food segmentation models can be found at https://amughrabi.github.io/benchseg."/>
<meta name="twitter:site" content="@arxiv"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="BenchSeg: A Large-Scale Dataset and Benchmark for Multi-View Food..."/>
<meta name="twitter:description" content="Food image segmentation is a critical task for dietary analysis, enabling accurate estimation of food volume and nutrients. However, current methods suffer from limited multi-view data and poor..."/>
<meta name="twitter:image" content="https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png"/>
<meta name="twitter:image:alt" content="arXiv logo"/>
  <link rel="stylesheet" media="screen" type="text/css" href="/static/browse/0.3.4/css/tooltip.css"/><link rel="stylesheet" media="screen" type="text/css" href="https://static.arxiv.org/js/bibex-dev/bibex.css?20200709"/>  <script src="/static/browse/0.3.4/js/mathjaxToggle.min.js" type="text/javascript"></script>  <script src="//code.jquery.com/jquery-latest.min.js" type="text/javascript"></script>
  <script src="//cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js" type="text/javascript"></script>
  <script src="//cdn.jsdelivr.net/npm/dompurify@2.3.5/dist/purify.min.js"></script>
  <script src="/static/browse/0.3.4/js/toggle-labs.js?20241022" type="text/javascript"></script>
  <script src="/static/browse/0.3.4/js/cite.js" type="text/javascript"></script><meta name="citation_title" content="BenchSeg: A Large-Scale Dataset and Benchmark for Multi-View Food Video Segmentation" /><meta name="citation_author" content="AlMughrabi, Ahmad" /><meta name="citation_author" content="Rivo, Guillermo" /><meta name="citation_author" content="Jiménez-Farfán, Carlos" /><meta name="citation_author" content="Haroon, Umair" /><meta name="citation_author" content="Al-Areqi, Farid" /><meta name="citation_author" content="Jung, Hyunjun" /><meta name="citation_author" content="Busam, Benjamin" /><meta name="citation_author" content="Marques, Ricardo" /><meta name="citation_author" content="Radeva, Petia" /><meta name="citation_date" content="2026/01/12" /><meta name="citation_online_date" content="2026/01/12" /><meta name="citation_pdf_url" content="https://arxiv.org/pdf/2601.07581" /><meta name="citation_arxiv_id" content="2601.07581" /><meta name="citation_abstract" content="Food image segmentation is a critical task for dietary analysis, enabling accurate estimation of food volume and nutrients. However, current methods suffer from limited multi-view data and poor generalization to new viewpoints. We introduce BenchSeg, a novel multi-view food video segmentation dataset and benchmark. BenchSeg aggregates 55 dish scenes (from Nutrition5k, Vegetables &amp; Fruits, MetaFood3D, and FoodKit) with 25,284 meticulously annotated frames, capturing each dish under free 360{\deg} camera motion. We evaluate a diverse set of 20 state-of-the-art segmentation models (e.g., SAM-based, transformer, CNN, and large multimodal) on the existing FoodSeg103 dataset and evaluate them (alone and combined with video-memory modules) on BenchSeg. Quantitative and qualitative results demonstrate that while standard image segmenters degrade sharply under novel viewpoints, memory-augmented methods maintain temporal consistency across frames. Our best model based on a combination of SeTR-MLA+XMem2 outperforms prior work (e.g., improving over FoodMem by ~2.63% mAP), offering new insights into food segmentation and tracking for dietary analysis. We release BenchSeg to foster future research. The project page including the dataset annotations and the food segmentation models can be found at https://amughrabi.github.io/benchseg." />
</head>

<body  class="with-cu-identity">
  
  
  <div class="flex-wrap-footer">
    <header>
      <a href="#content" class="is-sr-only">Skip to main content</a>
      <!-- start desktop header -->
      <div class="columns is-vcentered is-hidden-mobile" id="cu-identity">
        <div class="column" id="cu-logo">
          <a href="https://www.cornell.edu/"><img src="/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" /></a>
        </div><!-- /from April 7 at 1:00 AM to May 29 at 21:40 --><!-- /from May 2 at 1:00 AM to May 5 at 9:45 AM --><div class="column" id="support-ack">
          <span id="support-ack-url">We gratefully acknowledge support from the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors.</span>
          <a href="https://info.arxiv.org/about/donate.html" class="btn-header-donate">Donate</a>
        </div>
      </div>

      <div id="header" class="is-hidden-mobile">
<a aria-hidden="true" tabindex="-1" href="/IgnoreMe"></a>
  <div class="header-breadcrumbs is-hidden-mobile">
    <a href="/"><img src="/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg" alt="arxiv logo" style="height:40px;"/></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a> <span>&gt;</span> arXiv:2601.07581
  </div>

        <div class="columns is-vcentered is-mobile" style="justify-content: flex-end;">
        </div>

          <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
     </div><!-- /end desktop header -->

      <div class="mobile-header">
        <div class="columns is-mobile">
          <div class="column logo-arxiv"><a href="https://arxiv.org/"><img src="/static/browse/0.3.4/images/arxiv-logomark-small-white.svg" alt="arXiv logo" style="height:60px;" /></a></div>
          <div class="column logo-cornell"><a href="https://www.cornell.edu/">
            <picture>
              <source media="(min-width: 501px)"
                srcset="/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg  400w"
                sizes="400w" />
              <source srcset="/static/browse/0.3.4/images/icons/cu/cornell_seal_simple_black.svg 2x" />
              <img src="/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University Logo" />
            </picture>
          </a></div>
          <div class="column nav" id="toggle-container" role="menubar">
            <button class="toggle-control"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-white"><title>open search</title><path d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></button>
            <div class="mobile-toggle-block toggle-target">
              <form class="mobile-search-form" method="GET" action="https://arxiv.org/search">
                <div class="field has-addons">
                  <input class="input" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
                  <input type="hidden" name="source" value="header">
                  <input type="hidden" name="searchtype" value="all">
                  <button class="button">GO</button>
                </div>
              </form>
            </div>

            <button class="toggle-control"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-white" role="menu"><title>open navigation menu</title><path d="M16 132h416c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163 0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"/ ></svg></button>
            <div class="mobile-toggle-block toggle-target">
              <nav class="mobile-menu" aria-labelledby="mobilemenulabel">
                <h2 id="mobilemenulabel">quick links</h2>
                <ul>
                    <li><a href="https://arxiv.org/login">Login</a></li>
                    <li><a href="https://info.arxiv.org/help">Help Pages</a></li>
                    <li><a href="https://info.arxiv.org/about">About</a></li>
                </ul>
              </nav>
            </div>
          </div>
        </div>
      </div><!-- /end mobile-header -->
    </header>

    <main>
      <div id="content">
<!--
rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
         xmlns:dc="http://purl.org/dc/elements/1.1/"
         xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
    <rdf:Description
        rdf:about="/abs/2601.07581"
        dc:identifier="/abs/2601.07581"
        dc:title="BenchSeg: A Large-Scale Dataset and Benchmark for Multi-View Food Video Segmentation"
        trackback:ping="/trackback/2601.07581" />
    </rdf:RDF>
--><div id="abs-outer">

  <div class="leftcolumn">
    <div class="subheader">
      <h1>Computer Science > Computer Vision and Pattern Recognition</h1>
    </div>

    <div class="header-breadcrumbs-mobile">
      <strong>arXiv:2601.07581</strong> (cs)
    </div>
<link rel="stylesheet" type="text/css" href="/static/base/1.0.1/css/abs.css">
<div id="content-inner">
  <div id="abs">
    <div class="dateline">
  [Submitted on 12 Jan 2026]</div>
    <h1 class="title mathjax"><span class="descriptor">Title:</span>BenchSeg: A Large-Scale Dataset and Benchmark for Multi-View Food Video Segmentation</h1>
    <div class="authors"><span class="descriptor">Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=AlMughrabi,+A" rel="nofollow">Ahmad AlMughrabi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Rivo,+G" rel="nofollow">Guillermo Rivo</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jim%C3%A9nez-Farf%C3%A1n,+C" rel="nofollow">Carlos Jiménez-Farfán</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Haroon,+U" rel="nofollow">Umair Haroon</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Al-Areqi,+F" rel="nofollow">Farid Al-Areqi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jung,+H" rel="nofollow">Hyunjun Jung</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Busam,+B" rel="nofollow">Benjamin Busam</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Marques,+R" rel="nofollow">Ricardo Marques</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Radeva,+P" rel="nofollow">Petia Radeva</a></div>            <div id="download-button-info" hidden>View a PDF of the paper titled BenchSeg: A Large-Scale Dataset and Benchmark for Multi-View Food Video Segmentation, by Ahmad AlMughrabi and 8 other authors</div>
    <a class="mobile-submission-download" href="/pdf/2601.07581">View PDF</a>



    <blockquote class="abstract mathjax">
            <span class="descriptor">Abstract:</span>Food image segmentation is a critical task for dietary analysis, enabling accurate estimation of food volume and nutrients. However, current methods suffer from limited multi-view data and poor generalization to new viewpoints. We introduce BenchSeg, a novel multi-view food video segmentation dataset and benchmark. BenchSeg aggregates 55 dish scenes (from Nutrition5k, Vegetables &amp; Fruits, MetaFood3D, and FoodKit) with 25,284 meticulously annotated frames, capturing each dish under free 360° camera motion. We evaluate a diverse set of 20 state-of-the-art segmentation models (e.g., SAM-based, transformer, CNN, and large multimodal) on the existing FoodSeg103 dataset and evaluate them (alone and combined with video-memory modules) on BenchSeg. Quantitative and qualitative results demonstrate that while standard image segmenters degrade sharply under novel viewpoints, memory-augmented methods maintain temporal consistency across frames. Our best model based on a combination of SeTR-MLA+XMem2 outperforms prior work (e.g., improving over FoodMem by ~2.63% mAP), offering new insights into food segmentation and tracking for dietary analysis. We release BenchSeg to foster future research. The project page including the dataset annotations and the food segmentation models can be found at <a href="https://amughrabi.github.io/benchseg" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
    </blockquote>

    <!--CONTEXT-->
    <div class="metatable">
      <table summary="Additional metadata"><tr>
          <td class="tablecell label">Subjects:</td>
          <td class="tablecell subjects">
            <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span></td>
        </tr><tr>
          <td class="tablecell label">Cite as:</td>
          <td class="tablecell arxivid"><span class="arxivid"><a href="https://arxiv.org/abs/2601.07581">arXiv:2601.07581</a> [cs.CV]</span></td>
        </tr>
        <tr>
          <td class="tablecell label">&nbsp;</td>
          <td class="tablecell arxividv">(or <span class="arxivid">
              <a href="https://arxiv.org/abs/2601.07581v1">arXiv:2601.07581v1</a> [cs.CV]</span> for this version)
          </td>
        </tr>
        <tr>
          <td class="tablecell label">&nbsp;</td>
          <td class="tablecell arxivdoi">              <a href="https://doi.org/10.48550/arXiv.2601.07581" id="arxiv-doi-link">https://doi.org/10.48550/arXiv.2601.07581</a><div class="button-and-tooltip">
              <button class="more-info" aria-describedby="more-info-desc-1">
                <svg height="15" role="presentation" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z" class=""></path></svg>
                <span class="visually-hidden">Focus to learn more</span>
              </button>
              <!-- tooltip description -->
              <div role="tooltip" id="more-info-desc-1">
                <span class="left-corner"></span>                  arXiv-issued DOI via DataCite (pending registration)</div>
            </div>
          </td>
        </tr></table>
    </div>
  </div>
</div>
    <div class="submission-history">
      <h2>Submission history</h2> From: Ahmad AlMughrabi [<a href="/show-email/8625b244/2601.07581" rel="nofollow">view email</a>]      <br/>    <strong>[v1]</strong>
        Mon, 12 Jan 2026 14:32:51 UTC (42,668 KB)<br/>
</div>
  </div>
  <!--end leftcolumn-->
<div class="extra-services">    <div class="full-text">
      <a name="other"></a>
      <span class="descriptor">Full-text links:</span>
      <h2>Access Paper:</h2>
      <ul>
  <div id="download-button-info" hidden>
View a PDF of the paper titled BenchSeg: A Large-Scale Dataset and Benchmark for Multi-View Food Video Segmentation, by Ahmad AlMughrabi and 8 other authors</div><li><a href="/pdf/2601.07581" aria-describedby="download-button-info" accesskey="f" class="abs-button download-pdf">View PDF</a></li><li><a href="/src/2601.07581" class="abs-button download-eprint">TeX Source
 </a></li></ul>
      <div class="abs-license"><a href="http://creativecommons.org/licenses/by-nc-nd/4.0/" title="Rights to this article" class="has_license">
          <img alt="license icon" role="presentation" src="https://arxiv.org/icons/licenses/by-nc-nd-4.0.png"/>
          <span>view license</span>
        </a></div>
    </div>
    <!--end full-text-->    <div class="browse">
    Current browse context: <div class="current">cs.CV</div>

  <div class="prevnext">
<span class="arrow">
      <a class="abs-button prev-url" href="/prevnext?id=2601.07581&amp;function=prev&amp;context=cs.CV"
         accesskey="p" title="previous in cs.CV (accesskey p)" rel="nofollow">&lt;&nbsp;prev</a>
    </span>
    <span class="is-hidden-mobile">&nbsp; | &nbsp;</span>    <span class="arrow">
      <a class="abs-button next-url" href="/prevnext?id=2601.07581&amp;function=next&amp;context=cs.CV" accesskey="n"
         title="next in cs.CV (accesskey n)"  rel="nofollow">next&nbsp;&gt;</a>
    </span><br/>
  </div><div class="list">
    <a class="abs-button abs-button-grey abs-button-small context-new" href="/list/cs.CV/new"  rel="nofollow">new</a>
    <span class="is-hidden-mobile"> | </span>
    <a class="abs-button abs-button-grey abs-button-small context-recent" href="/list/cs.CV/recent" rel="nofollow">recent</a>
    <span class="is-hidden-mobile"> | </span><a class="abs-button abs-button-grey abs-button-small context-id" href="/list/cs.CV/2026-01" rel="nofollow">2026-01</a>
  </div><div class="abs-switch-cat">
    Change to browse by:
    <div class="switch context-change">
        <a href="/abs/2601.07581?context=cs" rel="nofollow">cs</a><br class="is-hidden-mobile">
    </div>
  </div>

    </div>
      <div class="extra-ref-cite">
        <h3>References &amp; Citations</h3>
        <ul>
          <li><a  class="abs-button abs-button-small cite-ads" href="https://ui.adsabs.harvard.edu/abs/arXiv:2601.07581">NASA ADS</a></li><li><a  class="abs-button abs-button-small cite-google-scholar" href="https://scholar.google.com/scholar_lookup?arxiv_id=2601.07581" target="_blank" rel="noopener">Google Scholar</a></li>
          <li><a  class="abs-button abs-button-small cite-semantic-scholar" href="https://api.semanticscholar.org/arXiv:2601.07581" target="_blank" rel="noopener">Semantic Scholar</a></li>
        </ul>
        <div style="clear:both;"></div>
      </div>

<div class='extra-ref-cite'>
    <span id='bib-cite-trigger' class="bib-cite-button abs-button">export BibTeX citation</span>
    <span id='bib-cite-loading' hidden='true'>Loading...</span>
</div>

<div id='bib-cite-modal' class='bib-modal' hidden='true'>
    <div class='bib-modal-content'>
        <div class='bib-modal-title'>
            <h2>BibTeX formatted citation</h2>
            <span class='bib-modal-close' >&times;</span>
        </div>
        <div>
            <textarea id='bib-cite-target' class="bib-citation-content" aria-label="loading the citation">loading...</textarea>
        </div>
        <div>
            <span>Data provided by: </span>
            <a id='bib-cite-source-api'></a>
        </div>
    </div>
</div><div class="bookmarks">
  <div><h3>Bookmark</h3></div><a class="abs-button abs-button-grey abs-button-small" href="http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2601.07581&amp;description=BenchSeg: A Large-Scale Dataset and Benchmark for Multi-View Food Video Segmentation"
     title="Bookmark on BibSonomy">
    <img src="/static/browse/0.3.4/images/icons/social/bibsonomy.png"
         alt="BibSonomy logo"/>
  </a>
  <a class="abs-button abs-button-grey abs-button-small" href="https://reddit.com/submit?url=https://arxiv.org/abs/2601.07581&amp;title=BenchSeg: A Large-Scale Dataset and Benchmark for Multi-View Food Video Segmentation"
     title="Bookmark on Reddit">
    <img src="/static/browse/0.3.4/images/icons/social/reddit.png"
         alt="Reddit logo"/>
  </a>
</div>  </div>
  <!--end extra-services-->
<!-- LABS AREA -->
<div id="labstabs">
  <div class="labstabs"><input type="radio" name="tabs" id="tabone"checked="checked">
    <label for="tabone">Bibliographic Tools</label>
    <div class="tab labs-display-bib">
      <h1>Bibliographic and Citation Tools</h1>
      <div class="toggle">
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input id="bibex-toggle" type="checkbox" class="lab-toggle"
                     data-script-url="/static/browse/0.3.4/bibex/bibex.js?20241202">
              <span class="slider"></span>
              <span class="is-sr-only">Bibliographic Explorer Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-bibex">Bibliographic Explorer</span> <em>(<a href="https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer">What is the Explorer?</a>)</em>
          </div>
        </div>
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="connectedpapers-toggle"
                type="checkbox"
                class="lab-toggle"
                data-script-url="/static/browse/0.3.4/js/connectedpapers.js"
                aria-labelledby="label-for-connected-papers">
              <span class="slider"></span>
              <span class="is-sr-only">Connected Papers Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-connected-papers">Connected Papers</span> <em>(<a href="https://www.connectedpapers.com/about" target="_blank">What is Connected Papers?</a>)</em>
          </div>
        </div><div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="litmaps-toggle"
                type="checkbox"
                class="lab-toggle"
                data-script-url="/static/browse/0.3.4/js/litmaps.js?20210617"
                aria-labelledby="label-for-litmaps">
              <span class="slider"></span>
              <span class="is-sr-only">Litmaps Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-litmaps">Litmaps</span> <em>(<a href="https://www.litmaps.co/" target="_blank">What is Litmaps?</a>)</em>
          </div>
        </div>
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="scite-toggle"
                type="checkbox"
                class="lab-toggle"
                data-script-url="/static/browse/0.3.4/js/scite.js?20210617"
                aria-labelledby="label-for-scite">
              <span class="slider"></span>
              <span class="is-sr-only">scite.ai Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-scite">scite Smart Citations</span> <em>(<a href="https://www.scite.ai/" target="_blank">What are Smart Citations?</a>)</em>
          </div>
        </div>
      </div>
        <div class="labs-content-placeholder labs-display" style="display: none;"></div>
        <div style="min-height: 15px" id="connectedpapers-output"></div>
        <div style="min-height: 15px" id="litmaps-open-in"></div>
        <div style="min-height: 15px" id="scite-open-in"></div>
    </div>


    <input type="radio" name="tabs" id="tabtwo">
    <label for="tabtwo">Code, Data, Media</label>
    <div class="tab">
      <h1>Code, Data and Media Associated with this Article</h1>
      <div class="toggle">
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="alphaxiv-toggle"
                data-script-url="/static/browse/0.3.4/js/alphaxiv.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-alphaxiv">
              <span class="slider"></span>
              <span class="is-sr-only">alphaXiv Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-alphaxiv">alphaXiv</span> <em>(<a href="https://alphaxiv.org/" target="_blank">What is alphaXiv?</a>)</em>
          </div>
        </div>

        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input        
                id="catalyzex-toggle"
                data-script-url="/static/browse/0.3.4/js/catalyzex.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-cx">
              <span class="slider"></span>
              <span class="is-sr-only">Links to Code Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-cx">CatalyzeX Code Finder for Papers</span> <em>(<a href="https://www.catalyzex.com" target="_blank">What is CatalyzeX?</a>)</em>
          </div>
        </div>

        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="dagshub-toggle"
                data-script-url="/static/browse/0.3.4/js/dagshub.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-dagshub">
              <span class="slider"></span>
              <span class="is-sr-only">DagsHub Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-dagshub">DagsHub</span> <em>(<a href="https://dagshub.com/" target="_blank">What is DagsHub?</a>)</em>
          </div>
        </div>
  
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="gotitpub-toggle"
                data-script-url="/static/browse/0.3.4/js/gotitpub.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-gotitpub">
              <span class="slider"></span>
              <span class="is-sr-only">GotitPub Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-gotitpub">Gotit.pub</span> <em>(<a href="http://gotit.pub/faq" target="_blank">What is GotitPub?</a>)</em>
          </div>
        </div>

        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="huggingface-toggle"
                data-script-url="/static/browse/0.3.4/js/huggingface.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-huggingface">
              <span class="slider"></span>
              <span class="is-sr-only">Huggingface Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-huggingface">Hugging Face</span> <em>(<a href="https://huggingface.co/huggingface" target="_blank">What is Huggingface?</a>)</em>
          </div>
        </div>

        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="paperwithcode-toggle"
                data-script-url="/static/browse/0.3.4/js/paperswithcode.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-pwc">
              <span class="slider"></span>
              <span class="is-sr-only">Links to Code Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-pwc">Papers with Code</span> <em>(<a href="https://paperswithcode.com/" target="_blank">What is Papers with Code?</a>)</em>
          </div>
        </div>


        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="sciencecast-toggle"
                data-script-url="/static/browse/0.3.4/js/sciencecast.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-sciencecast">
              <span class="slider"></span>
              <span class="is-sr-only">ScienceCast Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-sciencecast">ScienceCast</span> <em>(<a href="https://sciencecast.org/welcome" target="_blank">What is ScienceCast?</a>)</em>
          </div>
        </div>
      </div>

      <div id="alphaxiv-output" style="display:none"></div>
      <div id="catalyzex-output" style="display:none"></div>
      <div id="dagshub-output" style="display:none"></div>
      <div id="gotitpub-output" style="display:none"></div>
      <div id="pwc-output" style="display:none"></div>
      <div id="pwc-data-output" style="display:none"></div>
      <div id="sciencecast-output" style="display:none"></div>
      <div id="huggingface-output" style="display:none"></div>
    </div>


      <input type="radio" name="tabs" id="labstabs-demos-input">
      <label for="labstabs-demos-input" id="labstabs-demos-label">Demos</label>
      <div class="tab">
        <h1>Demos</h1>
        <div class="toggle">
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input
                  id="replicate-toggle"
                  data-script-url="/static/browse/0.3.4/js/replicate.js"
                  type="checkbox" class="lab-toggle" aria-labelledby="label-for-replicate">
                <span class="slider"></span>
                <span class="is-sr-only">Replicate Toggle</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-replicate">Replicate</span> <em>(<a href="https://replicate.com/docs/arxiv/about" target="_blank">What is Replicate?</a>)</em>
            </div>
          </div>
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input
                  id="spaces-toggle"
                  data-script-url="/static/browse/0.3.4/js/spaces.js"
                  type="checkbox" class="lab-toggle" aria-labelledby="label-for-spaces">
                <span class="slider"></span>
                <span class="is-sr-only">Spaces Toggle</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-spaces">Hugging Face Spaces</span> <em>(<a href="https://huggingface.co/docs/hub/spaces" target="_blank">What is Spaces?</a>)</em>
            </div>
          </div>
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input
                  id="txyz-toggle"
                  data-script-url="/static/browse/0.3.4/js/txyz.js"
                  type="checkbox" class="lab-toggle" aria-labelledby="label-for-txyz">
                <span class="slider"></span>
                <span class="is-sr-only">Spaces Toggle</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-txyz">TXYZ.AI</span> <em>(<a href="https://txyz.ai" target="_blank">What is TXYZ.AI?</a>)</em>
            </div>
          </div>
        </div>
        <div id="replicate-output"></div>
        <div id="spaces-output"></div>
        <div id="txyz-output"></div>
      </div>
      <input type="radio" name="tabs" id="tabfour">
      <label for="tabfour">Related Papers</label>
      <div class="tab">
        <h1>Recommenders and Search Tools</h1>
        <div class="toggle">
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input id="influenceflower-toggle"
                data-script-url="/static/browse/0.3.4/js/influenceflower.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-influenceflower">
                <span class="slider"></span>
                <span class="is-sr-only">Link to Influence Flower</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-influenceflower">Influence Flower</span> <em>(<a href="https://influencemap.cmlab.dev/" target="_blank">What are Influence Flowers?</a>)</em>
            </div>
          </div>
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input id="core-recommender-toggle" type="checkbox" class="lab-toggle" aria-labelledby="label-for-core">
                <span class="slider"></span>
                <span class="is-sr-only">Core recommender toggle</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-core">CORE Recommender</span> <em>(<a href="https://core.ac.uk/services/recommender">What is CORE?</a>)</em>
            </div>
          </div></div>
        <div id="influenceflower-output"></div>
        <div id="influenceflower-output-graph" style="display:none">
          <ul class="flower-tabs">
            <li class="active"><a class="btn tab-btn" onclick="openTab(event, 'tab-author')">Author</a></li>
            <li><a class="btn tab-btn" onclick="openTab(event, 'tab-venue')">Venue</a></li>
            <li><a class="btn tab-btn" onclick="openTab(event, 'tab-inst')">Institution</a></li>
            <li><a class="btn tab-btn" onclick="openTab(event, 'tab-topic')">Topic</a></li>
          </ul>
          <div class="flower-tab-content">
            <div class="tab-flower active" id="tab-author"><svg id="flower-graph-author"></svg></div>
            <div class="tab-flower" id="tab-venue"><svg id="flower-graph-venue"></svg></div>
            <div class="tab-flower" id="tab-inst"><svg id="flower-graph-inst"></svg></div>
            <div class="tab-flower" id="tab-topic"><svg id="flower-graph-topic"></svg></div>
          </div>
        </div>
        <div id="coreRecommenderOutput"></div>
        <div id="iarxivOutput"></div>
      </div>

      <input type="radio" name="tabs" id="tabfive">
      <label for="tabfive">
        About arXivLabs
      </label>
      <div class="tab">
        <div class="columns">
          <div class="column">
            <h1>arXivLabs: experimental projects with community collaborators</h1>
            <p>arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.</p>
            <p>Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.</p>
            <p>Have an idea for a project that will add value for arXiv's community? <a href="https://info.arxiv.org/labs/index.html"><strong>Learn more about arXivLabs</strong></a>.</p>
          </div>
          <div class="column is-narrow is-full-mobile">
            <p class="icon-labs"><svg xmlns="http://www.w3.org/2000/svg" role="presentation" viewBox="0 0 635.572 811"><path d="M175.6 676v27h-27v-27zm-54 27v27h27v-27zm-27 27v27h27v-27zm396-54v27h-27v-27zm0 27v27h27v-27zm27 27v27h27v-27zm-27-414h27v27h-27zm27 0h27v-27h-27zm27-27h27v-27h-27zm-396 45h-27v-27h27zm-27-54h-27v27h27zm-27-27h-27v27h27z"/><path d="M94.6 730v27h-27v-27zm477 0v27h-27v-27zm-27-495h27v27h-27zm-450 18h-27v-27h27zm477 9h27v27h-27zm-54 495h27v27h-27zm-423 0h27v27h-27zm-54-504h27v27h-27z" fill="#666"/><path d="M67.6 730v27h-27v-27zm54 54v27h-27v-27zm0-108v27h27v-27zm-27 27v27h27v-27zm-81 0v27h27v-27zm585 27v27h-27v-27zm-108-54v27h27v-27zm27 27v27h27v-27zm81 0v27h27v-27zm-54-495h27v27h-27zm-54 108h27v-27h-27zm27-27h27v-27h-27zm0-81h27v-27h-27zm-423 18h-27v-27h27zm54 54h-27v27h27zm-27-27h-27v27h27zm0-81h-27v27h27zm423 612v27h-27v-27zm81-522v27h-27v-27zm-585-9v27h-27v-27z" fill="#999"/><path d="M94.6 784v27h-27v-27zm-27-27v27h27v-27zm-27-54v27h27v-27zm27 0v27h27v-27zm0-27v27h27v-27zm27 0v27h27v-27zm0-27v27h27v-27zm27 0v27h27v-27zm-108 81v27h27v-27zm558 54v27h-27v-27zm-27-27v27h27v-27zm27-54v27h27v-27zm-27 0v27h27v-27zm0-27v27h27v-27zm-27 0v27h27v-27zm0-27v27h27v-27zm-27 0v27h27v-27zm108 81v27h27v-27zm0-495h27v27h-27zm-27 27h27v-27h-27zm-54-27h27v-27h-27zm0 27h27v-27h-27zm-27 0h27v-27h-27zm0 27h27v-27h-27zm-27 0h27v-27h-27zm0 27h27v-27h-27zm81-108h27v-27h-27zm-504 45h-27v-27h27zm27-27h-27v27h27zm54-27h-27v27h27zm0 27h-27v27h27zm27 0h-27v27h27zm0 27h-27v27h27zm27 0h-27v27h27zm0 27h-27v27h27zm-81-108h-27v27h27z" fill="#ccc"/><path d="M598.6 665.1H41.5C-76.5 667 176 280.2 176 280.2h53a46.5 46.5 0 0162.8-56.3 29.2 29.2 0 1128.5 35.9h-1a46.5 46.5 0 01-1.5 20.3l142.5-.1s255.3 387 138.3 385.1zM291 181a29.3 29.3 0 10-29.2-29.3A29.3 29.3 0 00291 181zm65.4-66.8a22.4 22.4 0 10-22.5-22.4 22.4 22.4 0 0022.5 22.4z" fill="#fc0"/><path d="M245.5 172V10h153v162s324 495 198 495h-558c-126 0 207-495 207-495zm126 54h56m-13 72h56m-9 72h56m-20 72h56m-22 72h56m-29 72h56m-457-45c20.8 41.7 87.3 81 160.7 81 72.1 0 142.1-38.2 163.4-81" fill="none" stroke="#000" stroke-miterlimit="10" stroke-width="20"/><path d="M273.3 421.7c0 31-9.8 56.3-21.9 56.3s-21.8-25.2-21.8-56.3 9.8-56.3 21.8-56.3 21.9 25.2 21.9 56.3zm114.4-56.3c-12 0-21.8 25.2-21.8 56.3s9.7 56.3 21.8 56.3 21.9-25.2 21.9-56.3-9.8-56.3-21.9-56.3zM150.1 526.6c-18.2 6.7-27.5 22.9-23.2 30.2s14.8-5.5 33-12.2 37.4-4.9 33-12.2-24.5-12.6-42.8-5.8zm296 5.8c-4.2 7.3 14.9 5.5 33.1 12.2s28.7 19.5 33 12.2-5-23.5-23.2-30.2-38.5-1.5-42.8 5.8z"/></svg></p>
          </div>
        </div>
      </div>

    </div>
</div>
<!-- END LABS AREA -->
  <div class="endorsers">
    <a href="/auth/show-endorsers/2601.07581" class="endorser-who" rel="nofollow">Which authors of this paper are endorsers?</a> |
    <a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="https://info.arxiv.org/help/mathjax.html">What is MathJax?</a>)
    <span class="help" style="font-style: normal; float: right; margin-top: 0; margin-right: 1em;"></span>
  </div>
  <script type="text/javascript" language="javascript">mathjaxToggle();</script>
</div>
      </div>
    </main>

    <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://info.arxiv.org/about">About</a></li>
                <li><a href="https://info.arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
                <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
  </div>

  <script src="/static/base/1.0.1/js/member_acknowledgement.js"></script>

</body>

</html>

<!-- ===== END ABS ===== -->

<!-- ===== HTML URL: https://arxiv.org/html/2601.07581 | HTTP 404 ===== -->

<!DOCTYPE html>
<html lang="en">
  <head>
<meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="/static/base/1.0.1/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/static/base/1.0.1/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/static/base/1.0.1/images/icons/favicon-16x16.png">
<link rel="manifest" href="/static/base/1.0.1/images/icons/site.webmanifest">
<link rel="mask-icon" href="/static/base/1.0.1/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="/static/base/1.0.1/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title> | arXiv e-print repository</title>
<script defer src="/static/base/1.0.1/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="/static/base/1.0.1/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="/static/base/1.0.1/js/notification.js"></script>
  </head>
  <body>
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
<!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="/static/base/1.0.1/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="/static/base/1.0.1/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
    <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
<a href="https://arxiv.org/login">Login</a>    </div>
</div>  </header>
  <main class="container" id="main-container">
<h1>No HTML for '2601.07581'</h1>
<p>HTML is not available for the source.</p>
<p>This could be due to the source files not being HTML, LaTeX, or a conversion failure.</p>
<p>If you are an author, learn how you can help <a href="https://info.arxiv.org/about/accessibility_html_error_messages.html">HTML conversions for your papers</a>.</p>  </main>
  <footer>
<div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>  </footer>
  <script src="/static/base/1.0.1/js/member_acknowledgement.js"></script>
  </body>
</html>

<!-- ===== END HTML ===== -->

<!-- ===== ABS URL: https://arxiv.org/abs/2601.07516 | HTTP 200 ===== -->

<!DOCTYPE html>
<html lang="en">

<head>  <title>[2601.07516] Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="apple-touch-icon" sizes="180x180" href="/static/browse/0.3.4/images/icons/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/static/browse/0.3.4/images/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/static/browse/0.3.4/images/icons/favicon-16x16.png">
  <link rel="manifest" href="/static/browse/0.3.4/images/icons/site.webmanifest">
  <link rel="mask-icon" href="/static/browse/0.3.4/images/icons/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">
  <link rel="stylesheet" type="text/css" media="screen" href="/static/browse/0.3.4/css/arXiv.css?v=20241206" />
  <link rel="stylesheet" type="text/css" media="print" href="/static/browse/0.3.4/css/arXiv-print.css?v=20200611" />
  <link rel="stylesheet" type="text/css" media="screen" href="/static/browse/0.3.4/css/browse_search.css" />
  <script language="javascript" src="/static/browse/0.3.4/js/accordion.js" ></script>
  <script language="javascript" src="/static/browse/0.3.4/js/optin-modal.js?v=20250819"></script>
  
  <link rel="canonical" href="https://arxiv.org/abs/2601.07516"/>
  <meta name="description" content="Abstract page for arXiv paper 2601.07516: Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><meta property="og:type" content="website" />
<meta property="og:site_name" content="arXiv.org" />
<meta property="og:title" content="Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions" />
<meta property="og:url" content="https://arxiv.org/abs/2601.07516v1" />
<meta property="og:image" content="/static/browse/0.3.4/images/arxiv-logo-fb.png" />
<meta property="og:image:secure_url" content="/static/browse/0.3.4/images/arxiv-logo-fb.png" />
<meta property="og:image:width" content="1200" />
<meta property="og:image:height" content="700" />
<meta property="og:image:alt" content="arXiv logo"/>
<meta property="og:description" content="Vision-language models are increasingly employed as multimodal conversational agents (MCAs) for diverse conversational tasks. Recently, reinforcement learning (RL) has been widely explored for adapting MCAs to various human-AI interaction scenarios. Despite showing great enhancement in generalization performance, fine-tuning MCAs via RL still faces challenges in handling the extremely large text token space. To address this, we learn a compact latent action space for RL fine-tuning instead. Specifically, we adopt the learning from observation mechanism to construct the codebook for the latent action space, where future observations are leveraged to estimate current latent actions that could further be used to reconstruct future observations. However, the scarcity of paired image-text data hinders learning a codebook with sufficient coverage. Thus, we leverage both paired image-text data and text-only data to construct the latent action space, using a cross-modal projector for transforming text embeddings into image-text embeddings. We initialize the cross-modal projector on paired image-text data, and further train it on massive text-only data with a novel cycle consistency loss to enhance its robustness. We show that our latent action based method outperforms competitive baselines on two conversation tasks across various RL algorithms."/>
<meta name="twitter:site" content="@arxiv"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Controlling Multimodal Conversational Agents with..."/>
<meta name="twitter:description" content="Vision-language models are increasingly employed as multimodal conversational agents (MCAs) for diverse conversational tasks. Recently, reinforcement learning (RL) has been widely explored for..."/>
<meta name="twitter:image" content="https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png"/>
<meta name="twitter:image:alt" content="arXiv logo"/>
  <link rel="stylesheet" media="screen" type="text/css" href="/static/browse/0.3.4/css/tooltip.css"/><link rel="stylesheet" media="screen" type="text/css" href="https://static.arxiv.org/js/bibex-dev/bibex.css?20200709"/>  <script src="/static/browse/0.3.4/js/mathjaxToggle.min.js" type="text/javascript"></script>  <script src="//code.jquery.com/jquery-latest.min.js" type="text/javascript"></script>
  <script src="//cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js" type="text/javascript"></script>
  <script src="//cdn.jsdelivr.net/npm/dompurify@2.3.5/dist/purify.min.js"></script>
  <script src="/static/browse/0.3.4/js/toggle-labs.js?20241022" type="text/javascript"></script>
  <script src="/static/browse/0.3.4/js/cite.js" type="text/javascript"></script><meta name="citation_title" content="Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions" /><meta name="citation_author" content="Li, Yongqi" /><meta name="citation_author" content="Lang, Hao" /><meta name="citation_author" content="Qian, Tieyun" /><meta name="citation_author" content="Li, Yongbin" /><meta name="citation_date" content="2026/01/12" /><meta name="citation_online_date" content="2026/01/12" /><meta name="citation_pdf_url" content="https://arxiv.org/pdf/2601.07516" /><meta name="citation_arxiv_id" content="2601.07516" /><meta name="citation_abstract" content="Vision-language models are increasingly employed as multimodal conversational agents (MCAs) for diverse conversational tasks. Recently, reinforcement learning (RL) has been widely explored for adapting MCAs to various human-AI interaction scenarios. Despite showing great enhancement in generalization performance, fine-tuning MCAs via RL still faces challenges in handling the extremely large text token space. To address this, we learn a compact latent action space for RL fine-tuning instead. Specifically, we adopt the learning from observation mechanism to construct the codebook for the latent action space, where future observations are leveraged to estimate current latent actions that could further be used to reconstruct future observations. However, the scarcity of paired image-text data hinders learning a codebook with sufficient coverage. Thus, we leverage both paired image-text data and text-only data to construct the latent action space, using a cross-modal projector for transforming text embeddings into image-text embeddings. We initialize the cross-modal projector on paired image-text data, and further train it on massive text-only data with a novel cycle consistency loss to enhance its robustness. We show that our latent action based method outperforms competitive baselines on two conversation tasks across various RL algorithms." />
</head>

<body  class="with-cu-identity">
  
  
  <div class="flex-wrap-footer">
    <header>
      <a href="#content" class="is-sr-only">Skip to main content</a>
      <!-- start desktop header -->
      <div class="columns is-vcentered is-hidden-mobile" id="cu-identity">
        <div class="column" id="cu-logo">
          <a href="https://www.cornell.edu/"><img src="/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" /></a>
        </div><!-- /from April 7 at 1:00 AM to May 29 at 21:40 --><!-- /from May 2 at 1:00 AM to May 5 at 9:45 AM --><div class="column" id="support-ack">
          <span id="support-ack-url">We gratefully acknowledge support from the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors.</span>
          <a href="https://info.arxiv.org/about/donate.html" class="btn-header-donate">Donate</a>
        </div>
      </div>

      <div id="header" class="is-hidden-mobile">
<a aria-hidden="true" tabindex="-1" href="/IgnoreMe"></a>
  <div class="header-breadcrumbs is-hidden-mobile">
    <a href="/"><img src="/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg" alt="arxiv logo" style="height:40px;"/></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a> <span>&gt;</span> arXiv:2601.07516
  </div>

        <div class="columns is-vcentered is-mobile" style="justify-content: flex-end;">
        </div>

          <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
     </div><!-- /end desktop header -->

      <div class="mobile-header">
        <div class="columns is-mobile">
          <div class="column logo-arxiv"><a href="https://arxiv.org/"><img src="/static/browse/0.3.4/images/arxiv-logomark-small-white.svg" alt="arXiv logo" style="height:60px;" /></a></div>
          <div class="column logo-cornell"><a href="https://www.cornell.edu/">
            <picture>
              <source media="(min-width: 501px)"
                srcset="/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg  400w"
                sizes="400w" />
              <source srcset="/static/browse/0.3.4/images/icons/cu/cornell_seal_simple_black.svg 2x" />
              <img src="/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University Logo" />
            </picture>
          </a></div>
          <div class="column nav" id="toggle-container" role="menubar">
            <button class="toggle-control"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-white"><title>open search</title><path d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></button>
            <div class="mobile-toggle-block toggle-target">
              <form class="mobile-search-form" method="GET" action="https://arxiv.org/search">
                <div class="field has-addons">
                  <input class="input" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
                  <input type="hidden" name="source" value="header">
                  <input type="hidden" name="searchtype" value="all">
                  <button class="button">GO</button>
                </div>
              </form>
            </div>

            <button class="toggle-control"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-white" role="menu"><title>open navigation menu</title><path d="M16 132h416c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163 0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"/ ></svg></button>
            <div class="mobile-toggle-block toggle-target">
              <nav class="mobile-menu" aria-labelledby="mobilemenulabel">
                <h2 id="mobilemenulabel">quick links</h2>
                <ul>
                    <li><a href="https://arxiv.org/login">Login</a></li>
                    <li><a href="https://info.arxiv.org/help">Help Pages</a></li>
                    <li><a href="https://info.arxiv.org/about">About</a></li>
                </ul>
              </nav>
            </div>
          </div>
        </div>
      </div><!-- /end mobile-header -->
    </header>

    <main>
      <div id="content">
<!--
rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
         xmlns:dc="http://purl.org/dc/elements/1.1/"
         xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
    <rdf:Description
        rdf:about="/abs/2601.07516"
        dc:identifier="/abs/2601.07516"
        dc:title="Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"
        trackback:ping="/trackback/2601.07516" />
    </rdf:RDF>
--><div id="abs-outer">

  <div class="leftcolumn">
    <div class="subheader">
      <h1>Computer Science > Computation and Language</h1>
    </div>

    <div class="header-breadcrumbs-mobile">
      <strong>arXiv:2601.07516</strong> (cs)
    </div>
<link rel="stylesheet" type="text/css" href="/static/base/1.0.1/css/abs.css">
<div id="content-inner">
  <div id="abs">
    <div class="dateline">
  [Submitted on 12 Jan 2026]</div>
    <h1 class="title mathjax"><span class="descriptor">Title:</span>Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions</h1>
    <div class="authors"><span class="descriptor">Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+Y" rel="nofollow">Yongqi Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lang,+H" rel="nofollow">Hao Lang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Qian,+T" rel="nofollow">Tieyun Qian</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+Y" rel="nofollow">Yongbin Li</a></div>            <div id="download-button-info" hidden>View a PDF of the paper titled Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions, by Yongqi Li and 3 other authors</div>
    <a class="mobile-submission-download" href="/pdf/2601.07516">View PDF</a>
    <a class="mobile-submission-download" href="https://arxiv.org/html/2601.07516v1">HTML (experimental)</a>



    <blockquote class="abstract mathjax">
            <span class="descriptor">Abstract:</span>Vision-language models are increasingly employed as multimodal conversational agents (MCAs) for diverse conversational tasks. Recently, reinforcement learning (RL) has been widely explored for adapting MCAs to various human-AI interaction scenarios. Despite showing great enhancement in generalization performance, fine-tuning MCAs via RL still faces challenges in handling the extremely large text token space. To address this, we learn a compact latent action space for RL fine-tuning instead. Specifically, we adopt the learning from observation mechanism to construct the codebook for the latent action space, where future observations are leveraged to estimate current latent actions that could further be used to reconstruct future observations. However, the scarcity of paired image-text data hinders learning a codebook with sufficient coverage. Thus, we leverage both paired image-text data and text-only data to construct the latent action space, using a cross-modal projector for transforming text embeddings into image-text embeddings. We initialize the cross-modal projector on paired image-text data, and further train it on massive text-only data with a novel cycle consistency loss to enhance its robustness. We show that our latent action based method outperforms competitive baselines on two conversation tasks across various RL algorithms.
    </blockquote>

    <!--CONTEXT-->
    <div class="metatable">
      <table summary="Additional metadata"><tr>
          <td class="tablecell label">Subjects:</td>
          <td class="tablecell subjects">
            <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)</td>
        </tr><tr>
          <td class="tablecell label">Cite as:</td>
          <td class="tablecell arxivid"><span class="arxivid"><a href="https://arxiv.org/abs/2601.07516">arXiv:2601.07516</a> [cs.CL]</span></td>
        </tr>
        <tr>
          <td class="tablecell label">&nbsp;</td>
          <td class="tablecell arxividv">(or <span class="arxivid">
              <a href="https://arxiv.org/abs/2601.07516v1">arXiv:2601.07516v1</a> [cs.CL]</span> for this version)
          </td>
        </tr>
        <tr>
          <td class="tablecell label">&nbsp;</td>
          <td class="tablecell arxivdoi">              <a href="https://doi.org/10.48550/arXiv.2601.07516" id="arxiv-doi-link">https://doi.org/10.48550/arXiv.2601.07516</a><div class="button-and-tooltip">
              <button class="more-info" aria-describedby="more-info-desc-1">
                <svg height="15" role="presentation" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z" class=""></path></svg>
                <span class="visually-hidden">Focus to learn more</span>
              </button>
              <!-- tooltip description -->
              <div role="tooltip" id="more-info-desc-1">
                <span class="left-corner"></span>                  arXiv-issued DOI via DataCite (pending registration)</div>
            </div>
          </td>
        </tr></table>
    </div>
  </div>
</div>
    <div class="submission-history">
      <h2>Submission history</h2> From: Yongqi Li [<a href="/show-email/1d498d5a/2601.07516" rel="nofollow">view email</a>]      <br/>    <strong>[v1]</strong>
        Mon, 12 Jan 2026 13:13:24 UTC (679 KB)<br/>
</div>
  </div>
  <!--end leftcolumn-->
<div class="extra-services">    <div class="full-text">
      <a name="other"></a>
      <span class="descriptor">Full-text links:</span>
      <h2>Access Paper:</h2>
      <ul>
  <div id="download-button-info" hidden>
View a PDF of the paper titled Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions, by Yongqi Li and 3 other authors</div><li><a href="/pdf/2601.07516" aria-describedby="download-button-info" accesskey="f" class="abs-button download-pdf">View PDF</a></li><li><a href="https://arxiv.org/html/2601.07516v1" class="abs-button" id="latexml-download-link">HTML (experimental)</a></li><li><a href="/src/2601.07516" class="abs-button download-eprint">TeX Source
 </a></li></ul>
      <div class="abs-license"><a href="http://arxiv.org/licenses/nonexclusive-distrib/1.0/" title="Rights to this article">view license</a></div>
    </div>
    <!--end full-text-->    <div class="browse">
    Current browse context: <div class="current">cs.CL</div>

  <div class="prevnext">
<span class="arrow">
      <a class="abs-button prev-url" href="/prevnext?id=2601.07516&amp;function=prev&amp;context=cs.CL"
         accesskey="p" title="previous in cs.CL (accesskey p)" rel="nofollow">&lt;&nbsp;prev</a>
    </span>
    <span class="is-hidden-mobile">&nbsp; | &nbsp;</span>    <span class="arrow">
      <a class="abs-button next-url" href="/prevnext?id=2601.07516&amp;function=next&amp;context=cs.CL" accesskey="n"
         title="next in cs.CL (accesskey n)"  rel="nofollow">next&nbsp;&gt;</a>
    </span><br/>
  </div><div class="list">
    <a class="abs-button abs-button-grey abs-button-small context-new" href="/list/cs.CL/new"  rel="nofollow">new</a>
    <span class="is-hidden-mobile"> | </span>
    <a class="abs-button abs-button-grey abs-button-small context-recent" href="/list/cs.CL/recent" rel="nofollow">recent</a>
    <span class="is-hidden-mobile"> | </span><a class="abs-button abs-button-grey abs-button-small context-id" href="/list/cs.CL/2026-01" rel="nofollow">2026-01</a>
  </div><div class="abs-switch-cat">
    Change to browse by:
    <div class="switch context-change">
        <a href="/abs/2601.07516?context=cs" rel="nofollow">cs</a><br class="is-hidden-mobile">
        <a class="subclass" href="/abs/2601.07516?context=cs.AI" rel="nofollow">cs.AI</a><br class="is-hidden-mobile">
        <a class="subclass" href="/abs/2601.07516?context=cs.LG" rel="nofollow">cs.LG</a><br class="is-hidden-mobile">
    </div>
  </div>

    </div>
      <div class="extra-ref-cite">
        <h3>References &amp; Citations</h3>
        <ul>
          <li><a  class="abs-button abs-button-small cite-ads" href="https://ui.adsabs.harvard.edu/abs/arXiv:2601.07516">NASA ADS</a></li><li><a  class="abs-button abs-button-small cite-google-scholar" href="https://scholar.google.com/scholar_lookup?arxiv_id=2601.07516" target="_blank" rel="noopener">Google Scholar</a></li>
          <li><a  class="abs-button abs-button-small cite-semantic-scholar" href="https://api.semanticscholar.org/arXiv:2601.07516" target="_blank" rel="noopener">Semantic Scholar</a></li>
        </ul>
        <div style="clear:both;"></div>
      </div>

<div class='extra-ref-cite'>
    <span id='bib-cite-trigger' class="bib-cite-button abs-button">export BibTeX citation</span>
    <span id='bib-cite-loading' hidden='true'>Loading...</span>
</div>

<div id='bib-cite-modal' class='bib-modal' hidden='true'>
    <div class='bib-modal-content'>
        <div class='bib-modal-title'>
            <h2>BibTeX formatted citation</h2>
            <span class='bib-modal-close' >&times;</span>
        </div>
        <div>
            <textarea id='bib-cite-target' class="bib-citation-content" aria-label="loading the citation">loading...</textarea>
        </div>
        <div>
            <span>Data provided by: </span>
            <a id='bib-cite-source-api'></a>
        </div>
    </div>
</div><div class="bookmarks">
  <div><h3>Bookmark</h3></div><a class="abs-button abs-button-grey abs-button-small" href="http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2601.07516&amp;description=Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"
     title="Bookmark on BibSonomy">
    <img src="/static/browse/0.3.4/images/icons/social/bibsonomy.png"
         alt="BibSonomy logo"/>
  </a>
  <a class="abs-button abs-button-grey abs-button-small" href="https://reddit.com/submit?url=https://arxiv.org/abs/2601.07516&amp;title=Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"
     title="Bookmark on Reddit">
    <img src="/static/browse/0.3.4/images/icons/social/reddit.png"
         alt="Reddit logo"/>
  </a>
</div>  </div>
  <!--end extra-services-->
<!-- LABS AREA -->
<div id="labstabs">
  <div class="labstabs"><input type="radio" name="tabs" id="tabone"checked="checked">
    <label for="tabone">Bibliographic Tools</label>
    <div class="tab labs-display-bib">
      <h1>Bibliographic and Citation Tools</h1>
      <div class="toggle">
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input id="bibex-toggle" type="checkbox" class="lab-toggle"
                     data-script-url="/static/browse/0.3.4/bibex/bibex.js?20241202">
              <span class="slider"></span>
              <span class="is-sr-only">Bibliographic Explorer Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-bibex">Bibliographic Explorer</span> <em>(<a href="https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer">What is the Explorer?</a>)</em>
          </div>
        </div>
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="connectedpapers-toggle"
                type="checkbox"
                class="lab-toggle"
                data-script-url="/static/browse/0.3.4/js/connectedpapers.js"
                aria-labelledby="label-for-connected-papers">
              <span class="slider"></span>
              <span class="is-sr-only">Connected Papers Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-connected-papers">Connected Papers</span> <em>(<a href="https://www.connectedpapers.com/about" target="_blank">What is Connected Papers?</a>)</em>
          </div>
        </div><div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="litmaps-toggle"
                type="checkbox"
                class="lab-toggle"
                data-script-url="/static/browse/0.3.4/js/litmaps.js?20210617"
                aria-labelledby="label-for-litmaps">
              <span class="slider"></span>
              <span class="is-sr-only">Litmaps Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-litmaps">Litmaps</span> <em>(<a href="https://www.litmaps.co/" target="_blank">What is Litmaps?</a>)</em>
          </div>
        </div>
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="scite-toggle"
                type="checkbox"
                class="lab-toggle"
                data-script-url="/static/browse/0.3.4/js/scite.js?20210617"
                aria-labelledby="label-for-scite">
              <span class="slider"></span>
              <span class="is-sr-only">scite.ai Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-scite">scite Smart Citations</span> <em>(<a href="https://www.scite.ai/" target="_blank">What are Smart Citations?</a>)</em>
          </div>
        </div>
      </div>
        <div class="labs-content-placeholder labs-display" style="display: none;"></div>
        <div style="min-height: 15px" id="connectedpapers-output"></div>
        <div style="min-height: 15px" id="litmaps-open-in"></div>
        <div style="min-height: 15px" id="scite-open-in"></div>
    </div>


    <input type="radio" name="tabs" id="tabtwo">
    <label for="tabtwo">Code, Data, Media</label>
    <div class="tab">
      <h1>Code, Data and Media Associated with this Article</h1>
      <div class="toggle">
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="alphaxiv-toggle"
                data-script-url="/static/browse/0.3.4/js/alphaxiv.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-alphaxiv">
              <span class="slider"></span>
              <span class="is-sr-only">alphaXiv Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-alphaxiv">alphaXiv</span> <em>(<a href="https://alphaxiv.org/" target="_blank">What is alphaXiv?</a>)</em>
          </div>
        </div>

        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input        
                id="catalyzex-toggle"
                data-script-url="/static/browse/0.3.4/js/catalyzex.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-cx">
              <span class="slider"></span>
              <span class="is-sr-only">Links to Code Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-cx">CatalyzeX Code Finder for Papers</span> <em>(<a href="https://www.catalyzex.com" target="_blank">What is CatalyzeX?</a>)</em>
          </div>
        </div>

        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="dagshub-toggle"
                data-script-url="/static/browse/0.3.4/js/dagshub.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-dagshub">
              <span class="slider"></span>
              <span class="is-sr-only">DagsHub Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-dagshub">DagsHub</span> <em>(<a href="https://dagshub.com/" target="_blank">What is DagsHub?</a>)</em>
          </div>
        </div>
  
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="gotitpub-toggle"
                data-script-url="/static/browse/0.3.4/js/gotitpub.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-gotitpub">
              <span class="slider"></span>
              <span class="is-sr-only">GotitPub Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-gotitpub">Gotit.pub</span> <em>(<a href="http://gotit.pub/faq" target="_blank">What is GotitPub?</a>)</em>
          </div>
        </div>

        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="huggingface-toggle"
                data-script-url="/static/browse/0.3.4/js/huggingface.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-huggingface">
              <span class="slider"></span>
              <span class="is-sr-only">Huggingface Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-huggingface">Hugging Face</span> <em>(<a href="https://huggingface.co/huggingface" target="_blank">What is Huggingface?</a>)</em>
          </div>
        </div>

        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="paperwithcode-toggle"
                data-script-url="/static/browse/0.3.4/js/paperswithcode.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-pwc">
              <span class="slider"></span>
              <span class="is-sr-only">Links to Code Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-pwc">Papers with Code</span> <em>(<a href="https://paperswithcode.com/" target="_blank">What is Papers with Code?</a>)</em>
          </div>
        </div>


        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="sciencecast-toggle"
                data-script-url="/static/browse/0.3.4/js/sciencecast.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-sciencecast">
              <span class="slider"></span>
              <span class="is-sr-only">ScienceCast Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-sciencecast">ScienceCast</span> <em>(<a href="https://sciencecast.org/welcome" target="_blank">What is ScienceCast?</a>)</em>
          </div>
        </div>
      </div>

      <div id="alphaxiv-output" style="display:none"></div>
      <div id="catalyzex-output" style="display:none"></div>
      <div id="dagshub-output" style="display:none"></div>
      <div id="gotitpub-output" style="display:none"></div>
      <div id="pwc-output" style="display:none"></div>
      <div id="pwc-data-output" style="display:none"></div>
      <div id="sciencecast-output" style="display:none"></div>
      <div id="huggingface-output" style="display:none"></div>
    </div>


      <input type="radio" name="tabs" id="labstabs-demos-input">
      <label for="labstabs-demos-input" id="labstabs-demos-label">Demos</label>
      <div class="tab">
        <h1>Demos</h1>
        <div class="toggle">
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input
                  id="replicate-toggle"
                  data-script-url="/static/browse/0.3.4/js/replicate.js"
                  type="checkbox" class="lab-toggle" aria-labelledby="label-for-replicate">
                <span class="slider"></span>
                <span class="is-sr-only">Replicate Toggle</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-replicate">Replicate</span> <em>(<a href="https://replicate.com/docs/arxiv/about" target="_blank">What is Replicate?</a>)</em>
            </div>
          </div>
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input
                  id="spaces-toggle"
                  data-script-url="/static/browse/0.3.4/js/spaces.js"
                  type="checkbox" class="lab-toggle" aria-labelledby="label-for-spaces">
                <span class="slider"></span>
                <span class="is-sr-only">Spaces Toggle</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-spaces">Hugging Face Spaces</span> <em>(<a href="https://huggingface.co/docs/hub/spaces" target="_blank">What is Spaces?</a>)</em>
            </div>
          </div>
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input
                  id="txyz-toggle"
                  data-script-url="/static/browse/0.3.4/js/txyz.js"
                  type="checkbox" class="lab-toggle" aria-labelledby="label-for-txyz">
                <span class="slider"></span>
                <span class="is-sr-only">Spaces Toggle</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-txyz">TXYZ.AI</span> <em>(<a href="https://txyz.ai" target="_blank">What is TXYZ.AI?</a>)</em>
            </div>
          </div>
        </div>
        <div id="replicate-output"></div>
        <div id="spaces-output"></div>
        <div id="txyz-output"></div>
      </div>
      <input type="radio" name="tabs" id="tabfour">
      <label for="tabfour">Related Papers</label>
      <div class="tab">
        <h1>Recommenders and Search Tools</h1>
        <div class="toggle">
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input id="influenceflower-toggle"
                data-script-url="/static/browse/0.3.4/js/influenceflower.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-influenceflower">
                <span class="slider"></span>
                <span class="is-sr-only">Link to Influence Flower</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-influenceflower">Influence Flower</span> <em>(<a href="https://influencemap.cmlab.dev/" target="_blank">What are Influence Flowers?</a>)</em>
            </div>
          </div>
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input id="core-recommender-toggle" type="checkbox" class="lab-toggle" aria-labelledby="label-for-core">
                <span class="slider"></span>
                <span class="is-sr-only">Core recommender toggle</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-core">CORE Recommender</span> <em>(<a href="https://core.ac.uk/services/recommender">What is CORE?</a>)</em>
            </div>
          </div></div>
        <div id="influenceflower-output"></div>
        <div id="influenceflower-output-graph" style="display:none">
          <ul class="flower-tabs">
            <li class="active"><a class="btn tab-btn" onclick="openTab(event, 'tab-author')">Author</a></li>
            <li><a class="btn tab-btn" onclick="openTab(event, 'tab-venue')">Venue</a></li>
            <li><a class="btn tab-btn" onclick="openTab(event, 'tab-inst')">Institution</a></li>
            <li><a class="btn tab-btn" onclick="openTab(event, 'tab-topic')">Topic</a></li>
          </ul>
          <div class="flower-tab-content">
            <div class="tab-flower active" id="tab-author"><svg id="flower-graph-author"></svg></div>
            <div class="tab-flower" id="tab-venue"><svg id="flower-graph-venue"></svg></div>
            <div class="tab-flower" id="tab-inst"><svg id="flower-graph-inst"></svg></div>
            <div class="tab-flower" id="tab-topic"><svg id="flower-graph-topic"></svg></div>
          </div>
        </div>
        <div id="coreRecommenderOutput"></div>
        <div id="iarxivOutput"></div>
      </div>

      <input type="radio" name="tabs" id="tabfive">
      <label for="tabfive">
        About arXivLabs
      </label>
      <div class="tab">
        <div class="columns">
          <div class="column">
            <h1>arXivLabs: experimental projects with community collaborators</h1>
            <p>arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.</p>
            <p>Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.</p>
            <p>Have an idea for a project that will add value for arXiv's community? <a href="https://info.arxiv.org/labs/index.html"><strong>Learn more about arXivLabs</strong></a>.</p>
          </div>
          <div class="column is-narrow is-full-mobile">
            <p class="icon-labs"><svg xmlns="http://www.w3.org/2000/svg" role="presentation" viewBox="0 0 635.572 811"><path d="M175.6 676v27h-27v-27zm-54 27v27h27v-27zm-27 27v27h27v-27zm396-54v27h-27v-27zm0 27v27h27v-27zm27 27v27h27v-27zm-27-414h27v27h-27zm27 0h27v-27h-27zm27-27h27v-27h-27zm-396 45h-27v-27h27zm-27-54h-27v27h27zm-27-27h-27v27h27z"/><path d="M94.6 730v27h-27v-27zm477 0v27h-27v-27zm-27-495h27v27h-27zm-450 18h-27v-27h27zm477 9h27v27h-27zm-54 495h27v27h-27zm-423 0h27v27h-27zm-54-504h27v27h-27z" fill="#666"/><path d="M67.6 730v27h-27v-27zm54 54v27h-27v-27zm0-108v27h27v-27zm-27 27v27h27v-27zm-81 0v27h27v-27zm585 27v27h-27v-27zm-108-54v27h27v-27zm27 27v27h27v-27zm81 0v27h27v-27zm-54-495h27v27h-27zm-54 108h27v-27h-27zm27-27h27v-27h-27zm0-81h27v-27h-27zm-423 18h-27v-27h27zm54 54h-27v27h27zm-27-27h-27v27h27zm0-81h-27v27h27zm423 612v27h-27v-27zm81-522v27h-27v-27zm-585-9v27h-27v-27z" fill="#999"/><path d="M94.6 784v27h-27v-27zm-27-27v27h27v-27zm-27-54v27h27v-27zm27 0v27h27v-27zm0-27v27h27v-27zm27 0v27h27v-27zm0-27v27h27v-27zm27 0v27h27v-27zm-108 81v27h27v-27zm558 54v27h-27v-27zm-27-27v27h27v-27zm27-54v27h27v-27zm-27 0v27h27v-27zm0-27v27h27v-27zm-27 0v27h27v-27zm0-27v27h27v-27zm-27 0v27h27v-27zm108 81v27h27v-27zm0-495h27v27h-27zm-27 27h27v-27h-27zm-54-27h27v-27h-27zm0 27h27v-27h-27zm-27 0h27v-27h-27zm0 27h27v-27h-27zm-27 0h27v-27h-27zm0 27h27v-27h-27zm81-108h27v-27h-27zm-504 45h-27v-27h27zm27-27h-27v27h27zm54-27h-27v27h27zm0 27h-27v27h27zm27 0h-27v27h27zm0 27h-27v27h27zm27 0h-27v27h27zm0 27h-27v27h27zm-81-108h-27v27h27z" fill="#ccc"/><path d="M598.6 665.1H41.5C-76.5 667 176 280.2 176 280.2h53a46.5 46.5 0 0162.8-56.3 29.2 29.2 0 1128.5 35.9h-1a46.5 46.5 0 01-1.5 20.3l142.5-.1s255.3 387 138.3 385.1zM291 181a29.3 29.3 0 10-29.2-29.3A29.3 29.3 0 00291 181zm65.4-66.8a22.4 22.4 0 10-22.5-22.4 22.4 22.4 0 0022.5 22.4z" fill="#fc0"/><path d="M245.5 172V10h153v162s324 495 198 495h-558c-126 0 207-495 207-495zm126 54h56m-13 72h56m-9 72h56m-20 72h56m-22 72h56m-29 72h56m-457-45c20.8 41.7 87.3 81 160.7 81 72.1 0 142.1-38.2 163.4-81" fill="none" stroke="#000" stroke-miterlimit="10" stroke-width="20"/><path d="M273.3 421.7c0 31-9.8 56.3-21.9 56.3s-21.8-25.2-21.8-56.3 9.8-56.3 21.8-56.3 21.9 25.2 21.9 56.3zm114.4-56.3c-12 0-21.8 25.2-21.8 56.3s9.7 56.3 21.8 56.3 21.9-25.2 21.9-56.3-9.8-56.3-21.9-56.3zM150.1 526.6c-18.2 6.7-27.5 22.9-23.2 30.2s14.8-5.5 33-12.2 37.4-4.9 33-12.2-24.5-12.6-42.8-5.8zm296 5.8c-4.2 7.3 14.9 5.5 33.1 12.2s28.7 19.5 33 12.2-5-23.5-23.2-30.2-38.5-1.5-42.8 5.8z"/></svg></p>
          </div>
        </div>
      </div>

    </div>
</div>
<!-- END LABS AREA -->
  <div class="endorsers">
    <a href="/auth/show-endorsers/2601.07516" class="endorser-who" rel="nofollow">Which authors of this paper are endorsers?</a> |
    <a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="https://info.arxiv.org/help/mathjax.html">What is MathJax?</a>)
    <span class="help" style="font-style: normal; float: right; margin-top: 0; margin-right: 1em;"></span>
  </div>
  <script type="text/javascript" language="javascript">mathjaxToggle();</script>
</div>
      </div>
    </main>

    <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://info.arxiv.org/about">About</a></li>
                <li><a href="https://info.arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
                <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
  </div>

  <script src="/static/base/1.0.1/js/member_acknowledgement.js"></script>

</body>

</html>

<!-- ===== END ABS ===== -->

<!-- ===== HTML URL: https://arxiv.org/html/2601.07516v1 | HTTP 200 ===== -->

<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions</title>
<!--Generated on Tue Jan  6 08:11:14 2026 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"/>
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="/static/browse/0.3.4/css/arxiv-html-papers-20250916.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2601.07516v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S1" title="In Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S2" title="In Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Preliminary</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S2.SS0.SSS0.Px1" title="In 2 Preliminary ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title">Reinforcement Learning for VLM Agents</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S2.SS0.SSS0.Px2" title="In 2 Preliminary ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title">Latent Actions for Reinforcement Learning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S3" title="In Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S3.SS1" title="In 3 Methodology ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Model Design</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S3.SS1.SSS0.Px1" title="In 3.1 Model Design ‣ 3 Methodology ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title">Language World Model <math alttext="f_{\text{world}}" class="ltx_Math" display="inline" intent=":literal"><semantics><msub><mi>f</mi><mtext>world</mtext></msub><annotation encoding="application/x-tex">f_{\text{world}}</annotation></semantics></math></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S3.SS1.SSS0.Px2" title="In 3.1 Model Design ‣ 3 Methodology ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title">Inverse Dynamics Model <math alttext="f_{\text{inverse}}" class="ltx_Math" display="inline" intent=":literal"><semantics><msub><mi>f</mi><mtext>inverse</mtext></msub><annotation encoding="application/x-tex">f_{\text{inverse}}</annotation></semantics></math></span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S3.SS1.SSS0.Px3" title="In 3.1 Model Design ‣ 3 Methodology ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title">Policy Model <math alttext="\pi_{\theta}" class="ltx_Math" display="inline" intent=":literal"><semantics><msub><mi>π</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">\pi_{\theta}</annotation></semantics></math></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S3.SS2" title="In 3 Methodology ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Latent Action Space Learning</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S3.SS2.SSS1" title="In 3.2 Latent Action Space Learning ‣ 3 Methodology ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Inverse Dynamics Learning</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S3.SS2.SSS1.Px1" title="In 3.2.1 Inverse Dynamics Learning ‣ 3.2 Latent Action Space Learning ‣ 3 Methodology ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title">Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S3.SS2.SSS1.Px2" title="In 3.2.1 Inverse Dynamics Learning ‣ 3.2 Latent Action Space Learning ‣ 3 Methodology ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title">Cross-modal Projector Training</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S3.SS2.SSS2" title="In 3.2 Latent Action Space Learning ‣ 3 Methodology ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Policy Behavior Cloning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S3.SS3" title="In 3 Methodology ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Latent Action Reinforcement Learning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S4" title="In Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S4.SS1" title="In 4 Experiments ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S4.SS1.SSS0.Px1" title="In 4.1 Experimental Setup ‣ 4 Experiments ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title">Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S4.SS1.SSS0.Px2" title="In 4.1 Experimental Setup ‣ 4 Experiments ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title">Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S4.SS1.SSS0.Px3" title="In 4.1 Experimental Setup ‣ 4 Experiments ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title">Evaluation Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S4.SS1.SSS0.Px4" title="In 4.1 Experimental Setup ‣ 4 Experiments ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title">Baselines</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S4.SS2" title="In 4 Experiments ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Main Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S4.SS2.SSS0.Px1" title="In 4.2 Main Results ‣ 4 Experiments ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title">Overall Performance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S4.SS2.SSS0.Px2" title="In 4.2 Main Results ‣ 4 Experiments ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title">Performance on Fine-grained Dimensions</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S4.SS3" title="In 4 Experiments ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Ablation Study</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S4.SS4" title="In 4 Experiments ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S4.SS4.SSS0.Px1" title="In 4.4 Analysis ‣ 4 Experiments ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title">Rollout Diversity with Latent Actions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S4.SS4.SSS0.Px2" title="In 4.4 Analysis ‣ 4 Experiments ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title">Computational Budget</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S5" title="In Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S5.SS0.SSS0.Px1" title="In 5 Related Work ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title">Multimodal Conversational Agents</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S5.SS0.SSS0.Px2" title="In 5 Related Work ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title">Reinforcement Learning with Latent Actions</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S6" title="In Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A1" title="In Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Details on Model Design</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A1.SS1" title="In Appendix A Details on Model Design ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Language World Model</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A1.SS1.SSS0.Px1" title="In A.1 Language World Model ‣ Appendix A Details on Model Design ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title">Encode Module</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A1.SS1.SSS0.Px2" title="In A.1 Language World Model ‣ Appendix A Details on Model Design ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title">Merge Module</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A1.SS2" title="In Appendix A Details on Model Design ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Inverse Dynamics Model</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A1.SS2.SSS0.Px1" title="In A.2 Inverse Dynamics Model ‣ Appendix A Details on Model Design ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title">Encode Module</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A1.SS2.SSS0.Px2" title="In A.2 Inverse Dynamics Model ‣ Appendix A Details on Model Design ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title">Inverse Transformer Layers</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A1.SS2.SSS0.Px3" title="In A.2 Inverse Dynamics Model ‣ Appendix A Details on Model Design ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title">Inverse Action Head</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A1.SS3" title="In Appendix A Details on Model Design ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Policy Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A1.SS4" title="In Appendix A Details on Model Design ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4 </span>Codebook for the Latent Action Space</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A1.SS5" title="In Appendix A Details on Model Design ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.5 </span>Cross-modal Projector</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A2" title="In Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Experimental Details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A2.SS1" title="In Appendix B Experimental Details ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Details on Datasets</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A2.SS1.SSS0.Px1" title="In B.1 Details on Datasets ‣ Appendix B Experimental Details ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title">Corpora for Constructing the Latent Action Space</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A2.SS2" title="In Appendix B Experimental Details ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.2 </span>Details on Evaluation Metric</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A2.SS3" title="In Appendix B Experimental Details ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.3 </span>Training Details</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A2.SS3.SSS0.Px1" title="In B.3 Training Details ‣ Appendix B Experimental Details ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title">Baseline Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A2.SS3.SSS0.Px2" title="In B.3 Training Details ‣ Appendix B Experimental Details ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title">Latent Action Space Learning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A2.SS3.SSS0.Px3" title="In B.3 Training Details ‣ Appendix B Experimental Details ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title">Latent Action RL</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A2.SS3.SSS0.Px4" title="In B.3 Training Details ‣ Appendix B Experimental Details ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title">Reward Function</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A2.SS3.SSS0.Px5" title="In B.3 Training Details ‣ Appendix B Experimental Details ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title">Implementation Details</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A2.SS4" title="In Appendix B Experimental Details ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.4 </span>Inference Details</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A3" title="In Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Additional Empirical Results</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A3.SS1" title="In Appendix C Additional Empirical Results ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.1 </span>Analysis on Data Exposure</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A3.SS2" title="In Appendix C Additional Empirical Results ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2 </span>Detailed Results on Fine-grained Dimensions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A3.SS3" title="In Appendix C Additional Empirical Results ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.3 </span>Case Study</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Controlling Multimodal Conversational Agents 
<br class="ltx_break"/>with Coverage-Enhanced Latent Actions</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span class="ltx_text ltx_font_bold" id="id1.1.1">Yongqi Li<sup class="ltx_sup" id="id1.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id1.1.1.1.1">1,2,∗,†</span></sup></span>,
<span class="ltx_text ltx_font_bold" id="id2.2.2">Hao Lang<sup class="ltx_sup" id="id2.2.2.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id2.2.2.1.1">2,†</span></sup></span>,
<span class="ltx_text ltx_font_bold" id="id3.3.3">Tieyun Qian<sup class="ltx_sup" id="id3.3.3.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id3.3.3.1.1">1,3,‡</span></sup></span>,
<span class="ltx_text ltx_font_bold" id="id5.5.5">Yongbin Li<sup class="ltx_sup" id="id5.5.5.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id5.5.5.1.1">2,‡</span></sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id5.5.5.2"><span class="ltx_text ltx_font_medium" id="id5.5.5.2.1">1</span></sup></span> School of Computer Science, Wuhan University, <sup class="ltx_sup" id="id8.8.id1">2</sup> Tongyi Lab
<br class="ltx_break"/><sup class="ltx_sup" id="id9.9.id2">3</sup> Zhongguancun Academy
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id10.10.id3">{liyongqi,qty}@whu.edu.cn</span>, <span class="ltx_text ltx_font_typewriter" id="id11.11.id4">{hao.lang,shuide.lyb}@alibaba-inc.com</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id12.id1">Vision-language models are increasingly employed as multimodal conversational agents (MCAs) for diverse conversational tasks.
Recently, reinforcement learning (RL) has been widely explored for adapting MCAs to various human-AI interaction scenarios. Despite showing great enhancement in generalization performance, fine-tuning MCAs via RL still faces challenges in handling the extremely large text token space.
To address this, we learn a compact latent action space for RL fine-tuning instead.
Specifically, we adopt the learning from observation mechanism to construct the codebook for the latent action space, where future observations are leveraged to estimate current latent actions that could further be used to reconstruct future observations.
However, the scarcity of paired image-text data hinders learning a codebook with sufficient coverage.
Thus, we leverage both paired image-text data and text-only data to construct the latent action space, using a cross-modal projector for transforming text embeddings into image-text embeddings.
We initialize the cross-modal projector on paired image-text data, and further train it on massive text-only data with a novel cycle consistency loss to enhance its robustness.
We show that our latent action based method outperforms competitive baselines on two conversation tasks across various RL algorithms.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.7">
<p class="ltx_p ltx_align_center" id="p1.7.8"><span class="ltx_text ltx_font_bold" id="p1.7.8.1">Controlling Multimodal Conversational Agents 
<br class="ltx_break"/>with Coverage-Enhanced Latent Actions</span></p>
<p class="ltx_p ltx_align_center" id="p1.7.7" style="width:345.0pt;"><span class="ltx_text ltx_inline-block" id="p1.7.7.7" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.7.7.7.7">
<span class="ltx_tr" id="p1.4.4.4.4.4">
<span class="ltx_td ltx_align_center" id="p1.4.4.4.4.4.4"><span class="ltx_text ltx_font_bold" id="p1.4.4.4.4.4.4.4">
Yongqi Li<sup class="ltx_sup" id="p1.4.4.4.4.4.4.4.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.4.4.4.4.4.4.4.1.1">1,2,∗,†</span></sup>,
Hao Lang<sup class="ltx_sup" id="p1.4.4.4.4.4.4.4.2"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.4.4.4.4.4.4.4.2.1">2,†</span></sup>,
Tieyun Qian<sup class="ltx_sup" id="p1.4.4.4.4.4.4.4.3"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.4.4.4.4.4.4.4.3.1">1,3,‡</span></sup>,
Yongbin Li<sup class="ltx_sup" id="p1.4.4.4.4.4.4.4.4"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.4.4.4.4.4.4.4.4.1">2,‡</span></sup></span></span></span>
<span class="ltx_tr" id="p1.6.6.6.6.6">
<span class="ltx_td ltx_align_center" id="p1.6.6.6.6.6.2"><sup class="ltx_sup" id="p1.6.6.6.6.6.2.1">1</sup> School of Computer Science, Wuhan University, <sup class="ltx_sup" id="p1.6.6.6.6.6.2.2">2</sup> Tongyi Lab</span></span>
<span class="ltx_tr" id="p1.7.7.7.7.7">
<span class="ltx_td ltx_align_center" id="p1.7.7.7.7.7.1"><sup class="ltx_sup" id="p1.7.7.7.7.7.1.1">3</sup> Zhongguancun Academy</span></span>
<span class="ltx_tr" id="p1.7.7.7.7.8">
<span class="ltx_td ltx_align_center" id="p1.7.7.7.7.8.1"><span class="ltx_text ltx_font_typewriter" id="p1.7.7.7.7.8.1.1">{liyongqi,qty}@whu.edu.cn</span>, <span class="ltx_text ltx_font_typewriter" id="p1.7.7.7.7.8.1.2">{hao.lang,shuide.lyb}@alibaba-inc.com</span></span></span>
</span></span></p>
</div>
</div>
<span class="ltx_note ltx_role_footnotetext" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotetext: </span>Work done while the author was interning at Tongyi Lab.</span></span></span><span class="ltx_note ltx_role_footnotetext" id="footnotex2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">footnotetext: </span>Equal contributions.</span></span></span><span class="ltx_note ltx_role_footnotetext" id="footnotex3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">footnotetext: </span>Corresponding authors.</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Vision-language models (VLMs) <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">yin-2024-MLLMsurvey</span></cite> like Qwen-VL <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">bai-2025-qwen3vl</span></cite> and GPT-4o <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">hurst-2024-gpt4ocard</span></cite> are increasingly employed as multimodal conversational agents (MCAs) for various conversation tasks <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">yao-2025-MLLMAgentsurvey</span></cite>.
MCAs enable emotionally rich and contextually grounded dialogues based on understanding both input images and texts, and thus become particularly valuable in fields like entertainment <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">mehta-2022-exploring</span></cite>, online education <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">griol-2014-developing</span></cite>, and personalized assistants <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">nguyen-2024-yo</span></cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.3">Recently, reinforcement learning (RL) <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">sutton-1998-reinforcement</span></cite> has been widely explored for adapting MCAs to diverse real-world human-AI interaction scenarios <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">zhou-2025-reinforcedMLLM</span></cite>.
Generally, RL algorithms frame response token generation in MCAs as a sequential decision-making process <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">chen-2021-decision</span></cite>, which optimize the policy to maximize cumulative rewards through interacting with environments.
Despite showing great enhancement in generalization performance <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">chu-2025-sftRL</span></cite>, fine-tuning MCAs via RL still faces challenges in dealing with large exploration spaces.
For instance, with token vocabulary size <math alttext="|\mathcal{V}|" class="ltx_Math" display="inline" id="S1.p2.1.m1" intent=":literal"><semantics><mrow><mo stretchy="false">|</mo><mi class="ltx_font_mathcaligraphic">𝒱</mi><mo stretchy="false">|</mo></mrow><annotation encoding="application/x-tex">|\mathcal{V}|</annotation></semantics></math> and maximum response length <math alttext="m" class="ltx_Math" display="inline" id="S1.p2.2.m2" intent=":literal"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>, the sampling space for RL scales exponentially as <math alttext="|\mathcal{V}|^{m}" class="ltx_Math" display="inline" id="S1.p2.3.m3" intent=":literal"><semantics><msup><mrow><mo stretchy="false">|</mo><mi class="ltx_font_mathcaligraphic">𝒱</mi><mo stretchy="false">|</mo></mrow><mi>m</mi></msup><annotation encoding="application/x-tex">|\mathcal{V}|^{m}</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.2">To address the challenge of large text token space, we learn a compact latent action space for RL fine-tuning instead, following previous works <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">jia-2025-controlling</span></cite>.
Specifically, we adopt the learning from observation mechanism <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">jiang-2023-efficient</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">ye-2025-latent</span></cite> to construct the codebook for the latent action space, where future observations are leveraged to estimate current latent actions that could be further used to reconstruct future observations.
As a result, the action sampling space at each step is reduced from the token vocabulary size <math alttext="|\mathcal{V}|" class="ltx_Math" display="inline" id="S1.p3.1.m1" intent=":literal"><semantics><mrow><mo stretchy="false">|</mo><mi class="ltx_font_mathcaligraphic">𝒱</mi><mo stretchy="false">|</mo></mrow><annotation encoding="application/x-tex">|\mathcal{V}|</annotation></semantics></math> (e.g., 152K for Qwen2.5-VL <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">bai-2025-qwen25vl</span></cite>) to the latent action codebook size <math alttext="|\mathcal{C}|" class="ltx_Math" display="inline" id="S1.p3.2.m2" intent=":literal"><semantics><mrow><mo stretchy="false">|</mo><mi class="ltx_font_mathcaligraphic">𝒞</mi><mo stretchy="false">|</mo></mrow><annotation encoding="application/x-tex">|\mathcal{C}|</annotation></semantics></math> (e.g., 128).</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Generally, the codebook has to be learned from diverse data with sufficient coverage, which is a prerequisite for effective RL exploration in latent spaces <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">chen-2025-coverage</span></cite>.
Note that VLMs in MCAs are typically pre-trained on paired image-text corpora <math alttext="(V,T)" class="ltx_Math" display="inline" id="S1.p4.1.m1" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mi>V</mi><mo>,</mo><mi>T</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(V,T)</annotation></semantics></math>, which implicitly convey complementary and partially redundant information between visual and textual modalities <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">radford-2021-clip</span></cite>.
Unfortunately, while unpaired image collections and text corpora are abundant on the web, curating them into aligned image-text corpora remains prohibitively costly <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">gupta-2025-better</span></cite>, posing a dilemma in constructing latent spaces.
On one hand, using limited paired data and abundant unpaired data would introduce <span class="ltx_text ltx_font_italic" id="S1.p4.1.1">unimodal bias</span> <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">zhang-2024-UniModalBias</span></cite>, where a model would overly rely on one modality and ignore others.
On the other hand, training the codebook solely on limited paired data may result in insufficient coverage, thereby impairing the agent’s generalization ability when handling diverse unseen conversation scenarios.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.6">In this paper, we leverage both paired image-text data <math alttext="(V,T)" class="ltx_Math" display="inline" id="S1.p5.1.m1" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mi>V</mi><mo>,</mo><mi>T</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(V,T)</annotation></semantics></math> and unpaired text-only data <math alttext="T" class="ltx_Math" display="inline" id="S1.p5.2.m2" intent=":literal"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math> to learn the coodbook for the latent space.
To improve the coverage of latent actions while avoiding potentially unimodal bias, we attempt to construct pseudo paired data <math alttext="(V^{\prime},T)" class="ltx_Math" display="inline" id="S1.p5.3.m3" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>V</mi><mo>′</mo></msup><mo>,</mo><mi>T</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(V^{\prime},T)</annotation></semantics></math> based on text-only data <math alttext="T" class="ltx_Math" display="inline" id="S1.p5.4.m4" intent=":literal"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math>, and use the pseudo data <math alttext="(V^{\prime},T)" class="ltx_Math" display="inline" id="S1.p5.5.m5" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>V</mi><mo>′</mo></msup><mo>,</mo><mi>T</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(V^{\prime},T)</annotation></semantics></math> and the collected data <math alttext="(V,T)" class="ltx_Math" display="inline" id="S1.p5.6.m6" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mi>V</mi><mo>,</mo><mi>T</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(V,T)</annotation></semantics></math> to learn the cookbook.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.16">However, training a conditional image generator <math alttext="G(V|T)" class="ltx_Math" display="inline" id="S1.p6.1.m1" intent=":literal"><semantics><mrow><mi>G</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>V</mi><mo fence="false">|</mo><mi>T</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">G(V|T)</annotation></semantics></math> for this purpose is computationally expensive due to the high dimension nature of images <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">pope-2021-dimension</span></cite>.
Thus, we learn a cross-modal projector <math alttext="P" class="ltx_Math" display="inline" id="S1.p6.2.m2" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> instead, which transforms an input text <math alttext="e^{T}" class="ltx_Math" display="inline" id="S1.p6.3.m3" intent=":literal"><semantics><msup><mi>e</mi><mi>T</mi></msup><annotation encoding="application/x-tex">e^{T}</annotation></semantics></math> to an image-text pair <math alttext="e^{V,T}" class="ltx_Math" display="inline" id="S1.p6.4.m4" intent=":literal"><semantics><msup><mi>e</mi><mrow><mi>V</mi><mo>,</mo><mi>T</mi></mrow></msup><annotation encoding="application/x-tex">e^{V,T}</annotation></semantics></math> in the embedding space, based on the cross-modal redundancy assumption <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">radford-2021-clip</span></cite>.
Concretely, for each item in the paired image-text data <math alttext="(V,T)" class="ltx_Math" display="inline" id="S1.p6.5.m5" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mi>V</mi><mo>,</mo><mi>T</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(V,T)</annotation></semantics></math>, we compute the text embedding <math alttext="e^{T}" class="ltx_Math" display="inline" id="S1.p6.6.m6" intent=":literal"><semantics><msup><mi>e</mi><mi>T</mi></msup><annotation encoding="application/x-tex">e^{T}</annotation></semantics></math> and image-text embedding <math alttext="e^{V,T}" class="ltx_Math" display="inline" id="S1.p6.7.m7" intent=":literal"><semantics><msup><mi>e</mi><mrow><mi>V</mi><mo>,</mo><mi>T</mi></mrow></msup><annotation encoding="application/x-tex">e^{V,T}</annotation></semantics></math> using an existing encoder, and train the projector <math alttext="P" class="ltx_Math" display="inline" id="S1.p6.8.m8" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> to imitate the projection between these two kinds of embeddings.
To enhance the robustness of the projector <math alttext="P" class="ltx_Math" display="inline" id="S1.p6.9.m9" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>, we further train it on massive text-only data <math alttext="T" class="ltx_Math" display="inline" id="S1.p6.10.m10" intent=":literal"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math> using a cycle consistency loss <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">zhu-2017-unpaired</span></cite>.
We introduce an additional projector <math alttext="P^{\prime}" class="ltx_Math" display="inline" id="S1.p6.11.m11" intent=":literal"><semantics><msup><mi>P</mi><mo>′</mo></msup><annotation encoding="application/x-tex">P^{\prime}</annotation></semantics></math> that can transform image-text embedding <math alttext="e^{V,T}" class="ltx_Math" display="inline" id="S1.p6.12.m12" intent=":literal"><semantics><msup><mi>e</mi><mrow><mi>V</mi><mo>,</mo><mi>T</mi></mrow></msup><annotation encoding="application/x-tex">e^{V,T}</annotation></semantics></math> back to text embedding <math alttext="e^{T}" class="ltx_Math" display="inline" id="S1.p6.13.m13" intent=":literal"><semantics><msup><mi>e</mi><mi>T</mi></msup><annotation encoding="application/x-tex">e^{T}</annotation></semantics></math>.
In this way, we can optimize the projector <math alttext="P" class="ltx_Math" display="inline" id="S1.p6.14.m14" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> by enforcing cycle consistency on text-only data <math alttext="T" class="ltx_Math" display="inline" id="S1.p6.15.m15" intent=":literal"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math> such that <math alttext="P^{\prime}(P(e^{T}))\approx e^{T}" class="ltx_Math" display="inline" id="S1.p6.16.m16" intent=":literal"><semantics><mrow><mrow><msup><mi>P</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>P</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>e</mi><mi>T</mi></msup><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>≈</mo><msup><mi>e</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">P^{\prime}(P(e^{T}))\approx e^{T}</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.3">We evaluate our method on two conversation tasks, namely multimodal role-playing conversation <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">dai-2025-mmrole</span></cite> and multimodal personalized conversation <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">li-2025-aligning</span></cite>.
To evaluate the generalizability of latent actions, we conduct experiments using various RL algorithms, such as GRPO <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">shao-2024-GRPO</span></cite> and Dr.GRPO <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">liu-2025-DRGRPO</span></cite>.
We construct the latent action space using paired image-text data <math alttext="(V,T)" class="ltx_Math" display="inline" id="S1.p7.1.m1" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mi>V</mi><mo>,</mo><mi>T</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(V,T)</annotation></semantics></math> and text-only data <math alttext="T" class="ltx_Math" display="inline" id="S1.p7.2.m2" intent=":literal"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math>.
The <math alttext="(V,T)" class="ltx_Math" display="inline" id="S1.p7.3.m3" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mi>V</mi><mo>,</mo><mi>T</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(V,T)</annotation></semantics></math> data are comprised of image-caption pairs, multimodal news articles, and multimodal Wikipedia pages, totaling 14M images and 1B text tokens.
The text-only data are mainly derived from SlimPajama <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">cerebras-2023-slimpajama</span></cite>, which contains 627B text tokens.
Experimental results show that our method outperforms competitive baselines.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">In summary, our work makes the following three key contributions.
1) We are the first to introduce latent actions for fine-tuning multimodal conversational agents via RL, which significantly reduces the exploration space.
2) We construct the latent action space with both paired image-text data and text-only data, using a cross-modal projector trained with a novel cycle consistency loss.
3) We evaluate our latent action based method on two multimodal conversation tasks and demonstrate that our method outperforms competitive baselines, and further show that the cross-modal projector is critical for improving the coverage of latent actions.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Preliminary</h2>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Reinforcement Learning for VLM Agents</h5>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.16">In reinforcement learning (RL), problems are framed by a Markov Decision Process (MDP) <math alttext="\mathcal{M}=\langle\mathcal{S},\mathcal{A},\mathcal{T},\mathcal{R}\rangle" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.1.m1" intent=":literal"><semantics><mrow><mi class="ltx_font_mathcaligraphic">ℳ</mi><mo>=</mo><mrow><mo stretchy="false">⟨</mo><mi class="ltx_font_mathcaligraphic">𝒮</mi><mo>,</mo><mi class="ltx_font_mathcaligraphic">𝒜</mi><mo>,</mo><mi class="ltx_font_mathcaligraphic">𝒯</mi><mo>,</mo><mi class="ltx_font_mathcaligraphic">ℛ</mi><mo stretchy="false">⟩</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{M}=\langle\mathcal{S},\mathcal{A},\mathcal{T},\mathcal{R}\rangle</annotation></semantics></math>.
For VLMs, the state at step <math alttext="t" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.2.m2" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> is the contextual information <math alttext="s_{t}=(x^{V},x^{T_{1:t}})\in\mathcal{S}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.3.m3" intent=":literal"><semantics><mrow><msub><mi>s</mi><mi>t</mi></msub><mo>=</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mi>V</mi></msup><mo>,</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub></msup><mo stretchy="false">)</mo></mrow><mo>∈</mo><mi class="ltx_font_mathcaligraphic">𝒮</mi></mrow><annotation encoding="application/x-tex">s_{t}=(x^{V},x^{T_{1:t}})\in\mathcal{S}</annotation></semantics></math>, which includes the input image <math alttext="x^{V}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.4.m4" intent=":literal"><semantics><msup><mi>x</mi><mi>V</mi></msup><annotation encoding="application/x-tex">x^{V}</annotation></semantics></math> and the current token sequence <math alttext="x^{T_{1:t}}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.5.m5" intent=":literal"><semantics><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub></msup><annotation encoding="application/x-tex">x^{T_{1:t}}</annotation></semantics></math>.
<math alttext="\mathcal{A}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.6.m6" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒜</mi><annotation encoding="application/x-tex">\mathcal{A}</annotation></semantics></math> is the action space containing all possible actions <math alttext="a_{t}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.7.m7" intent=":literal"><semantics><msub><mi>a</mi><mi>t</mi></msub><annotation encoding="application/x-tex">a_{t}</annotation></semantics></math> at each step.
<math alttext="\mathcal{T}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.8.m8" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒯</mi><annotation encoding="application/x-tex">\mathcal{T}</annotation></semantics></math> is the state transition function, governing the transition from <math alttext="s_{t}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.9.m9" intent=":literal"><semantics><msub><mi>s</mi><mi>t</mi></msub><annotation encoding="application/x-tex">s_{t}</annotation></semantics></math> to <math alttext="s_{t+1}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.10.m10" intent=":literal"><semantics><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">s_{t+1}</annotation></semantics></math>, i.e., <math alttext="P\big(s_{t+1}\mid s_{t},a_{t}\big)" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.11.m11" intent=":literal"><semantics><mrow><mi>P</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="1.200em" minsize="1.200em">(</mo><mrow><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>∣</mo><mrow><msub><mi>s</mi><mi>t</mi></msub><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub></mrow></mrow><mo maxsize="1.200em" minsize="1.200em">)</mo></mrow></mrow><annotation encoding="application/x-tex">P\big(s_{t+1}\mid s_{t},a_{t}\big)</annotation></semantics></math>.
The reward function <math alttext="\mathcal{R}\big(x^{T_{p+1:m}}\big)" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.12.m12" intent=":literal"><semantics><mrow><mi class="ltx_font_mathcaligraphic">ℛ</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="1.200em" minsize="1.200em">(</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mrow><mi>p</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>m</mi></mrow></msub></msup><mo maxsize="1.200em" minsize="1.200em">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{R}\big(x^{T_{p+1:m}}\big)</annotation></semantics></math> assigns a scalar reward to the response <math alttext="x^{T_{p+1:m}}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.13.m13" intent=":literal"><semantics><msup><mi>x</mi><msub><mi>T</mi><mrow><mrow><mi>p</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>m</mi></mrow></msub></msup><annotation encoding="application/x-tex">x^{T_{p+1:m}}</annotation></semantics></math>, conditioned on the input <math alttext="(x^{V},x^{T_{1:p}})" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.14.m14" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mi>V</mi></msup><mo>,</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>p</mi></mrow></msub></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x^{V},x^{T_{1:p}})</annotation></semantics></math>, with prompt length <math alttext="p" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.15.m15" intent=":literal"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> and maximum sequence length <math alttext="m" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.16.m16" intent=":literal"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>, following common practice in RL for VLMs <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">shen-2025-vlmR1</span></cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Latent Actions for Reinforcement Learning</h5>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.10">In traditional token-level RL, each action <math alttext="a_{t}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.1.m1" intent=":literal"><semantics><msub><mi>a</mi><mi>t</mi></msub><annotation encoding="application/x-tex">a_{t}</annotation></semantics></math> corresponds to selecting the next text token <math alttext="x^{T_{t+1}}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.2.m2" intent=":literal"><semantics><msup><mi>x</mi><msub><mi>T</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></msup><annotation encoding="application/x-tex">x^{T_{t+1}}</annotation></semantics></math> from the token vocabulary <math alttext="\mathcal{V}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.3.m3" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒱</mi><annotation encoding="application/x-tex">\mathcal{V}</annotation></semantics></math>, i.e., <math alttext="\mathcal{A}=\mathcal{V}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.4.m4" intent=":literal"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒜</mi><mo>=</mo><mi class="ltx_font_mathcaligraphic">𝒱</mi></mrow><annotation encoding="application/x-tex">\mathcal{A}=\mathcal{V}</annotation></semantics></math>. While in latent action RL, at each step <math alttext="t" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.5.m5" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>, the policy <math alttext="\pi_{\theta}(a_{t}|x^{V},x^{T_{1:t}})" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.6.m6" intent=":literal"><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>a</mi><mi>t</mi></msub><mo fence="false">|</mo><mrow><msup><mi>x</mi><mi>V</mi></msup><mo>,</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\pi_{\theta}(a_{t}|x^{V},x^{T_{1:t}})</annotation></semantics></math> selects a latent action <math alttext="a_{t}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.7.m7" intent=":literal"><semantics><msub><mi>a</mi><mi>t</mi></msub><annotation encoding="application/x-tex">a_{t}</annotation></semantics></math> from a compact codebook <math alttext="\mathcal{C}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.8.m8" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒞</mi><annotation encoding="application/x-tex">\mathcal{C}</annotation></semantics></math>, i.e., <math alttext="\mathcal{A}=\mathcal{C}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.9.m9" intent=":literal"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒜</mi><mo>=</mo><mi class="ltx_font_mathcaligraphic">𝒞</mi></mrow><annotation encoding="application/x-tex">\mathcal{A}=\mathcal{C}</annotation></semantics></math>.
During RL exploration, the latent action policy samples a latent action at each step, ultimately yielding the terminal state <math alttext="s_{m}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.10.m10" intent=":literal"><semantics><msub><mi>s</mi><mi>m</mi></msub><annotation encoding="application/x-tex">s_{m}</annotation></semantics></math>.
During exploitation, the latent action policy is refined to maximize expected rewards using RL algorithms such as GRPO <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">shao-2024-GRPO</span></cite>.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="261" id="S2.F1.g1" src="x1.png" width="297"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Illustrations of integrating latent actions with vision-language models.</figcaption>
</figure>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="335" id="S2.F2.g1" src="x2.png" width="595"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Pipeline for constructing the latent action space. (a) <span class="ltx_text ltx_font_bold" id="S2.F2.5.1">Inverse dynamics learning</span>: Given future observations, the inverse dynamics model infers a discrete latent action from a learnable codebook; the language world model then uses this latent action and current observations to reconstruct the next token <math alttext="x^{T_{t+1}}" class="ltx_Math" display="inline" id="S2.F2.2.m1" intent=":literal"><semantics><msup><mi>x</mi><msub><mi>T</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></msup><annotation encoding="application/x-tex">x^{T_{t+1}}</annotation></semantics></math>. The language world model, inverse dynamics model, and codebook are jointly trained. (b) <span class="ltx_text ltx_font_bold" id="S2.F2.6.2">Policy behavior cloning</span>: A policy model is trained to predict the same latent actions as those inferred by the inverse dynamics model, using only current observations.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we first describe the overall model design for incorporating latent actions into VLMs (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S3.SS1" title="3.1 Model Design ‣ 3 Methodology ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">3.1</span></a>).
Next, we detail the unsupervised construction of the latent action space (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S3.SS2" title="3.2 Latent Action Space Learning ‣ 3 Methodology ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">3.2</span></a>).
Finally, we introduce the procedure of latent action based RL fine-tuning (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S3.SS3" title="3.3 Latent Action Reinforcement Learning ‣ 3 Methodology ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">3.3</span></a>).</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Model Design</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">To fine-tune MCAs via latent action RL, we introduce three new modules, as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S2.F1" title="Figure 1 ‣ Latent Actions for Reinforcement Learning ‣ 2 Preliminary ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">1</span></a>. These modules are designed to share a base VLM while adding a small number of additional parameters, thereby introducing only marginal computational overhead. For further details on the model design, please refer to the Appendix <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A1" title="Appendix A Details on Model Design ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Language World Model <math alttext="f_{\text{world}}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.1.m1" intent=":literal"><semantics><msub><mi>f</mi><mtext>world</mtext></msub><annotation encoding="application/x-tex">f_{\text{world}}</annotation></semantics></math>
</h5>
<div class="ltx_para" id="S3.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.7">The language world model <math alttext="f_{\text{world}}(x^{T_{t+1}}|x^{V},x^{T_{1:t}},a_{t})" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.1.m1" intent=":literal"><semantics><mrow><msub><mi>f</mi><mtext>world</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>x</mi><msub><mi>T</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></msup><mo fence="false">|</mo><mrow><msup><mi>x</mi><mi>V</mi></msup><mo>,</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub></msup><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{\text{world}}(x^{T_{t+1}}|x^{V},x^{T_{1:t}},a_{t})</annotation></semantics></math> takes current observations <math alttext="(x^{V},x^{T_{1:t}})" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.2.m2" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mi>V</mi></msup><mo>,</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x^{V},x^{T_{1:t}})</annotation></semantics></math> and a latent action <math alttext="a_{t}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.3.m3" intent=":literal"><semantics><msub><mi>a</mi><mi>t</mi></msub><annotation encoding="application/x-tex">a_{t}</annotation></semantics></math> as input, and auto-regressively outputs the next token <math alttext="x^{T_{t+1}}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.4.m4" intent=":literal"><semantics><msup><mi>x</mi><msub><mi>T</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></msup><annotation encoding="application/x-tex">x^{T_{t+1}}</annotation></semantics></math>.
The latent action <math alttext="a_{t}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.5.m5" intent=":literal"><semantics><msub><mi>a</mi><mi>t</mi></msub><annotation encoding="application/x-tex">a_{t}</annotation></semantics></math> is provided by the inverse dynamics model <math alttext="f_{\text{inverse}}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.6.m6" intent=":literal"><semantics><msub><mi>f</mi><mtext>inverse</mtext></msub><annotation encoding="application/x-tex">f_{\text{inverse}}</annotation></semantics></math> during constructing the latent action space, and by the policy <math alttext="\pi_{\theta}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.7.m7" intent=":literal"><semantics><msub><mi>π</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">\pi_{\theta}</annotation></semantics></math> during inference and RL phases.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Inverse Dynamics Model <math alttext="f_{\text{inverse}}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.1.m1" intent=":literal"><semantics><msub><mi>f</mi><mtext>inverse</mtext></msub><annotation encoding="application/x-tex">f_{\text{inverse}}</annotation></semantics></math>
</h5>
<div class="ltx_para" id="S3.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p1.8">The inverse dynamics model <math alttext="f_{\text{inverse}}(a_{t}|x^{V},x^{T_{1:t+1}})" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p1.1.m1" intent=":literal"><semantics><mrow><msub><mi>f</mi><mtext>inverse</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>a</mi><mi>t</mi></msub><mo fence="false">|</mo><mrow><msup><mi>x</mi><mi>V</mi></msup><mo>,</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></mrow></msub></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{\text{inverse}}(a_{t}|x^{V},x^{T_{1:t+1}})</annotation></semantics></math> takes future observations <math alttext="(x^{V},x^{T_{1:t+1}})" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p1.2.m2" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mi>V</mi></msup><mo>,</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></mrow></msub></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x^{V},x^{T_{1:t+1}})</annotation></semantics></math> as input, and outputs a discrete latent action index <math alttext="a_{t}\in\{1,\dots,|\mathcal{C}|\}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p1.3.m3" intent=":literal"><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><mo stretchy="false">|</mo><mi class="ltx_font_mathcaligraphic">𝒞</mi><mo stretchy="false">|</mo></mrow><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">a_{t}\in\{1,\dots,|\mathcal{C}|\}</annotation></semantics></math> for the current step.
The corresponding latent action embedding <math alttext="c_{a_{t}}=\mathcal{C}[a_{t}]\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p1.4.m4" intent=":literal"><semantics><mrow><msub><mi>c</mi><msub><mi>a</mi><mi>t</mi></msub></msub><mo>=</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒞</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">[</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false">]</mo></mrow></mrow><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">c_{a_{t}}=\mathcal{C}[a_{t}]\in\mathbb{R}^{d}</annotation></semantics></math> is then retrieved from the trainable codebook <math alttext="\mathcal{C}\in\mathbb{R}^{|\mathcal{C}|\times d}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p1.5.m5" intent=":literal"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒞</mi><mo>∈</mo><msup><mi>ℝ</mi><mrow><mrow><mo stretchy="false">|</mo><mi class="ltx_font_mathcaligraphic">𝒞</mi><mo rspace="0.055em" stretchy="false">|</mo></mrow><mo rspace="0.222em">×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathcal{C}\in\mathbb{R}^{|\mathcal{C}|\times d}</annotation></semantics></math> and used by <math alttext="f_{\text{world}}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p1.6.m6" intent=":literal"><semantics><msub><mi>f</mi><mtext>world</mtext></msub><annotation encoding="application/x-tex">f_{\text{world}}</annotation></semantics></math> to reconstruct the next token <math alttext="x^{T_{t+1}}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p1.7.m7" intent=":literal"><semantics><msup><mi>x</mi><msub><mi>T</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></msup><annotation encoding="application/x-tex">x^{T_{t+1}}</annotation></semantics></math>.
Note that <math alttext="f_{\text{inverse}}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p1.8.m8" intent=":literal"><semantics><msub><mi>f</mi><mtext>inverse</mtext></msub><annotation encoding="application/x-tex">f_{\text{inverse}}</annotation></semantics></math> only assists training and does not serve for the inference phase.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Policy Model <math alttext="\pi_{\theta}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.1.m1" intent=":literal"><semantics><msub><mi>π</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">\pi_{\theta}</annotation></semantics></math>
</h5>
<div class="ltx_para" id="S3.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px3.p1.6">The latent action policy model <math alttext="\pi_{\theta}(a_{t}|x^{V},x^{T_{1:t}})" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.1.m1" intent=":literal"><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>a</mi><mi>t</mi></msub><mo fence="false">|</mo><mrow><msup><mi>x</mi><mi>V</mi></msup><mo>,</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\pi_{\theta}(a_{t}|x^{V},x^{T_{1:t}})</annotation></semantics></math> takes the current observations <math alttext="(x^{V},x^{T_{1:t}})" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.2.m2" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mi>V</mi></msup><mo>,</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x^{V},x^{T_{1:t}})</annotation></semantics></math> as input, and predicts latent action <math alttext="a_{t}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.3.m3" intent=":literal"><semantics><msub><mi>a</mi><mi>t</mi></msub><annotation encoding="application/x-tex">a_{t}</annotation></semantics></math> for the current step.
Since the language world model <math alttext="f_{\text{world}}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.4.m4" intent=":literal"><semantics><msub><mi>f</mi><mtext>world</mtext></msub><annotation encoding="application/x-tex">f_{\text{world}}</annotation></semantics></math> is controlled by latent actions, we can optimize the latent action distribution of <math alttext="\pi_{\theta}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.5.m5" intent=":literal"><semantics><msub><mi>π</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">\pi_{\theta}</annotation></semantics></math> for steering <math alttext="f_{\text{world}}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.6.m6" intent=":literal"><semantics><msub><mi>f</mi><mtext>world</mtext></msub><annotation encoding="application/x-tex">f_{\text{world}}</annotation></semantics></math> to generate responses toward higher rewards.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Latent Action Space Learning</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.6">Following <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">jia-2025-controlling</span></cite>, we construct the latent action space using large-scale corpora in two steps. 1) <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.6.1">inverse dynamics learning</span>, which trains the <math alttext="f_{\text{world}}" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1" intent=":literal"><semantics><msub><mi>f</mi><mtext>world</mtext></msub><annotation encoding="application/x-tex">f_{\text{world}}</annotation></semantics></math>, <math alttext="f_{\text{inverse}}" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2" intent=":literal"><semantics><msub><mi>f</mi><mtext>inverse</mtext></msub><annotation encoding="application/x-tex">f_{\text{inverse}}</annotation></semantics></math>, and <math alttext="\mathcal{C}" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒞</mi><annotation encoding="application/x-tex">\mathcal{C}</annotation></semantics></math> in an unsupervised manner (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S2.F2" title="Figure 2 ‣ Latent Actions for Reinforcement Learning ‣ 2 Preliminary ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">2</span></a> (a)); 2) <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.6.2">policy behavior cloning</span>, which trains the policy model <math alttext="\pi_{\theta}" class="ltx_Math" display="inline" id="S3.SS2.p1.4.m4" intent=":literal"><semantics><msub><mi>π</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">\pi_{\theta}</annotation></semantics></math> to mimic the latent action <math alttext="a_{t}" class="ltx_Math" display="inline" id="S3.SS2.p1.5.m5" intent=":literal"><semantics><msub><mi>a</mi><mi>t</mi></msub><annotation encoding="application/x-tex">a_{t}</annotation></semantics></math> inferred by <math alttext="f_{\text{inverse}}" class="ltx_Math" display="inline" id="S3.SS2.p1.6.m6" intent=":literal"><semantics><msub><mi>f</mi><mtext>inverse</mtext></msub><annotation encoding="application/x-tex">f_{\text{inverse}}</annotation></semantics></math> (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S2.F2" title="Figure 2 ‣ Latent Actions for Reinforcement Learning ‣ 2 Preliminary ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">2</span></a> (b)).</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Inverse Dynamics Learning</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">We first outline the overall objective of inverse dynamics learning, followed by the training procedure of the introduced cross-modal projector.</p>
</div>
<section class="ltx_paragraph" id="S3.SS2.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">Overview</h5>
<div class="ltx_para" id="S3.SS2.SSS1.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.Px1.p1.4">As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S2.F2" title="Figure 2 ‣ Latent Actions for Reinforcement Learning ‣ 2 Preliminary ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">2</span></a> (a), we jointly train the inverse dynamics model <math alttext="f_{\text{inverse}}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px1.p1.1.m1" intent=":literal"><semantics><msub><mi>f</mi><mtext>inverse</mtext></msub><annotation encoding="application/x-tex">f_{\text{inverse}}</annotation></semantics></math>, language world model <math alttext="f_{\text{world}}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px1.p1.2.m2" intent=":literal"><semantics><msub><mi>f</mi><mtext>world</mtext></msub><annotation encoding="application/x-tex">f_{\text{world}}</annotation></semantics></math>, and the latent action codebook <math alttext="\mathcal{C}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px1.p1.3.m3" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒞</mi><annotation encoding="application/x-tex">\mathcal{C}</annotation></semantics></math>, using the mixed corpus <math alttext="\mathcal{D}^{VT}\cup\mathcal{D}^{T}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px1.p1.4.m4" intent=":literal"><semantics><mrow><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mrow><mi>V</mi><mo lspace="0em" rspace="0em">​</mo><mi>T</mi></mrow></msup><mo>∪</mo><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">\mathcal{D}^{VT}\cup\mathcal{D}^{T}</annotation></semantics></math> (paired image-text data and text-only data).
The loss is as:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\text{inverse}}=\mathbb{E}_{\mathcal{D}^{VT}\cup\mathcal{D}^{T}}\left[-\sum_{t=1}^{m-1}\log f_{\text{world}}\big(x^{T_{t+1}}|e^{V,T}_{t},a_{t}\big)\right]," class="ltx_Math" display="block" id="S3.E1.m1" intent=":literal"><semantics><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>inverse</mtext></msub><mo>=</mo><mrow><msub><mi>𝔼</mi><mrow><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mrow><mi>V</mi><mo lspace="0em" rspace="0em">​</mo><mi>T</mi></mrow></msup><mo>∪</mo><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>T</mi></msup></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><mo>−</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>m</mi><mo>−</mo><mn>1</mn></mrow></munderover><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mi>f</mi><mtext>world</mtext></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="1.200em" minsize="1.200em">(</mo><mrow><msup><mi>x</mi><msub><mi>T</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></msup><mo fence="false">|</mo><mrow><msubsup><mi>e</mi><mi>t</mi><mrow><mi>V</mi><mo>,</mo><mi>T</mi></mrow></msubsup><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub></mrow></mrow><mo maxsize="1.200em" minsize="1.200em">)</mo></mrow></mrow></mrow></mrow><mo>]</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\text{inverse}}=\mathbb{E}_{\mathcal{D}^{VT}\cup\mathcal{D}^{T}}\left[-\sum_{t=1}^{m-1}\log f_{\text{world}}\big(x^{T_{t+1}}|e^{V,T}_{t},a_{t}\big)\right],</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS1.Px1.p1.8">where the expectation is taken over sequences <math alttext="(x^{V},x^{T_{1:m}})" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px1.p1.5.m1" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mi>V</mi></msup><mo>,</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>m</mi></mrow></msub></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x^{V},x^{T_{1:m}})</annotation></semantics></math> sampled from the mixed corpus <math alttext="\mathcal{D}^{VT}\cup\mathcal{D}^{T}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px1.p1.6.m2" intent=":literal"><semantics><mrow><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mrow><mi>V</mi><mo lspace="0em" rspace="0em">​</mo><mi>T</mi></mrow></msup><mo>∪</mo><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">\mathcal{D}^{VT}\cup\mathcal{D}^{T}</annotation></semantics></math>, with <math alttext="a_{t}=f_{\text{inverse}}(e^{V,T}_{t+1})\in\{1,...,|\mathcal{C}|\}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px1.p1.7.m3" intent=":literal"><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>=</mo><mrow><msub><mi>f</mi><mtext>inverse</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>e</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>V</mi><mo>,</mo><mi>T</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><mo stretchy="false">|</mo><mi class="ltx_font_mathcaligraphic">𝒞</mi><mo stretchy="false">|</mo></mrow><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">a_{t}=f_{\text{inverse}}(e^{V,T}_{t+1})\in\{1,...,|\mathcal{C}|\}</annotation></semantics></math>.
The embedding <math alttext="e^{V,T}_{t}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px1.p1.8.m4" intent=":literal"><semantics><msubsup><mi>e</mi><mi>t</mi><mrow><mi>V</mi><mo>,</mo><mi>T</mi></mrow></msubsup><annotation encoding="application/x-tex">e^{V,T}_{t}</annotation></semantics></math> is obtained via:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="e^{V,T}_{t}=\begin{cases}f_{\text{VLM}}(x^{V},x^{T_{1:t}}),&amp;\text{if }x^{V}\neq\emptyset\quad(\text{from }\mathcal{D}^{VT});\\
P\big(f_{\text{VLM}}(x^{T_{1:t}})\big),&amp;\text{if }x^{V}=\emptyset\quad(\text{from }\mathcal{D}^{T}),\end{cases}" class="ltx_Math" display="block" id="S3.E2.m1" intent=":literal"><semantics><mrow><msubsup><mi>e</mi><mi>t</mi><mrow><mi>V</mi><mo>,</mo><mi>T</mi></mrow></msubsup><mo>=</mo><mrow><mo>{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mtr><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><msub><mi>f</mi><mtext>VLM</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mi>V</mi></msup><mo>,</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub></msup><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><mrow><mtext>if </mtext><mo lspace="0em" rspace="0em">​</mo><msup><mi>x</mi><mi>V</mi></msup></mrow><mo>≠</mo><mrow><mi mathvariant="normal">∅</mi><mspace style="width:1em;" width="1em"></mspace><mrow><mo stretchy="false">(</mo><mrow><mtext>from </mtext><mo lspace="0em" rspace="0em">​</mo><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mrow><mi>V</mi><mo lspace="0em" rspace="0em">​</mo><mi>T</mi></mrow></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>;</mo></mrow></mtd></mtr><mtr><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><mi>P</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="1.200em" minsize="1.200em">(</mo><mrow><msub><mi>f</mi><mtext>VLM</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub></msup><mo stretchy="false">)</mo></mrow></mrow><mo maxsize="1.200em" minsize="1.200em">)</mo></mrow></mrow><mo>,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><mrow><mtext>if </mtext><mo lspace="0em" rspace="0em">​</mo><msup><mi>x</mi><mi>V</mi></msup></mrow><mo>=</mo><mrow><mi mathvariant="normal">∅</mi><mspace style="width:1em;" width="1em"></mspace><mrow><mo stretchy="false">(</mo><mrow><mtext>from </mtext><mo lspace="0em" rspace="0em">​</mo><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>T</mi></msup></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">e^{V,T}_{t}=\begin{cases}f_{\text{VLM}}(x^{V},x^{T_{1:t}}),&amp;\text{if }x^{V}\neq\emptyset\quad(\text{from }\mathcal{D}^{VT});\\
P\big(f_{\text{VLM}}(x^{T_{1:t}})\big),&amp;\text{if }x^{V}=\emptyset\quad(\text{from }\mathcal{D}^{T}),\end{cases}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS1.Px1.p1.10">where <math alttext="f_{\text{VLM}}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px1.p1.9.m1" intent=":literal"><semantics><msub><mi>f</mi><mtext>VLM</mtext></msub><annotation encoding="application/x-tex">f_{\text{VLM}}</annotation></semantics></math> denotes the encoding module based on VLMs.
<math alttext="P" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px1.p1.10.m2" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> denotes the cross-modal projector for transforming text embeddings into image-text embeddings, and its training procedure is as follows.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">Cross-modal Projector Training</h5>
<div class="ltx_para" id="S3.SS2.SSS1.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS1.Px2.p1.6">Let <math alttext="P" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px2.p1.1.m1" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> denote the forward cross-modal projector, which maps text embeddings <math alttext="e^{T}_{t}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px2.p1.2.m2" intent=":literal"><semantics><msubsup><mi>e</mi><mi>t</mi><mi>T</mi></msubsup><annotation encoding="application/x-tex">e^{T}_{t}</annotation></semantics></math> to the parameters of a diagonal Gaussian distribution over the image-text embedding space, i.e., <math alttext="(\mu_{t},\sigma_{t})=P(e^{T}_{t})" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px2.p1.3.m3" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><msub><mi>μ</mi><mi>t</mi></msub><mo>,</mo><msub><mi>σ</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><mo>=</mo><mrow><mi>P</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>e</mi><mi>t</mi><mi>T</mi></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">(\mu_{t},\sigma_{t})=P(e^{T}_{t})</annotation></semantics></math>.
Let <math alttext="P^{\prime}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px2.p1.4.m4" intent=":literal"><semantics><msup><mi>P</mi><mo>′</mo></msup><annotation encoding="application/x-tex">P^{\prime}</annotation></semantics></math> denote the reverse projector, which maps image-text embeddings back to the text embedding space.
We train <math alttext="P" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px2.p1.5.m5" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> and <math alttext="P^{\prime}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px2.p1.6.m6" intent=":literal"><semantics><msup><mi>P</mi><mo>′</mo></msup><annotation encoding="application/x-tex">P^{\prime}</annotation></semantics></math> in the following two steps.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.Px2.p2">
<p class="ltx_p" id="S3.SS2.SSS1.Px2.p2.2"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS2.SSS1.Px2.p2.2.1">Step 1: Initialization on paired image-text data.</span>
We first train the forward projector <math alttext="P" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px2.p2.1.m1" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> on paired image-text data <math alttext="\mathcal{D}^{VT}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px2.p2.2.m2" intent=":literal"><semantics><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mrow><mi>V</mi><mo lspace="0em" rspace="0em">​</mo><mi>T</mi></mrow></msup><annotation encoding="application/x-tex">\mathcal{D}^{VT}</annotation></semantics></math>, where the loss is defined as:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\text{t2vt}}=\mathbb{E}_{\mathcal{D}^{VT}}\left[\sum_{t=1}^{m-1}\frac{1}{2}\left(\left\|\frac{e^{V,T}_{t}-\mu_{t}}{\sigma_{t}}\right\|^{2}+\|\log\sigma_{t}^{2}\|_{1}\right)\right]," class="ltx_Math" display="block" id="S3.E3.m1" intent=":literal"><semantics><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>t2vt</mtext></msub><mo>=</mo><mrow><msub><mi>𝔼</mi><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mrow><mi>V</mi><mo lspace="0em" rspace="0em">​</mo><mi>T</mi></mrow></msup></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><munderover><mo lspace="0em" movablelimits="false">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>m</mi><mo>−</mo><mn>1</mn></mrow></munderover><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><msup><mrow><mo>‖</mo><mfrac><mrow><msubsup><mi>e</mi><mi>t</mi><mrow><mi>V</mi><mo>,</mo><mi>T</mi></mrow></msubsup><mo>−</mo><msub><mi>μ</mi><mi>t</mi></msub></mrow><msub><mi>σ</mi><mi>t</mi></msub></mfrac><mo>‖</mo></mrow><mn>2</mn></msup><mo>+</mo><msub><mrow><mo stretchy="false">‖</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msubsup><mi>σ</mi><mi>t</mi><mn>2</mn></msubsup></mrow><mo stretchy="false">‖</mo></mrow><mn>1</mn></msub></mrow><mo>)</mo></mrow></mrow></mrow><mo>]</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\text{t2vt}}=\mathbb{E}_{\mathcal{D}^{VT}}\left[\sum_{t=1}^{m-1}\frac{1}{2}\left(\left\|\frac{e^{V,T}_{t}-\mu_{t}}{\sigma_{t}}\right\|^{2}+\|\log\sigma_{t}^{2}\|_{1}\right)\right],</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS1.Px2.p2.5">where the expectation is taken over sequences <math alttext="(x^{V},x^{T_{1:m}})\sim\mathcal{D}^{VT}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px2.p2.3.m1" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mi>V</mi></msup><mo>,</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>m</mi></mrow></msub></msup><mo stretchy="false">)</mo></mrow><mo>∼</mo><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mrow><mi>V</mi><mo lspace="0em" rspace="0em">​</mo><mi>T</mi></mrow></msup></mrow><annotation encoding="application/x-tex">(x^{V},x^{T_{1:m}})\sim\mathcal{D}^{VT}</annotation></semantics></math>,
and <math alttext="e^{V,T}_{t}=f_{\text{VLM}}(x^{V},x^{T_{1:t}})" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px2.p2.4.m2" intent=":literal"><semantics><mrow><msubsup><mi>e</mi><mi>t</mi><mrow><mi>V</mi><mo>,</mo><mi>T</mi></mrow></msubsup><mo>=</mo><mrow><msub><mi>f</mi><mtext>VLM</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mi>V</mi></msup><mo>,</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">e^{V,T}_{t}=f_{\text{VLM}}(x^{V},x^{T_{1:t}})</annotation></semantics></math>, and <math alttext="(\mu_{t},\sigma_{t})=P(e^{T}_{t}=f_{\text{VLM}}(x^{T_{1:t}}))" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px2.p2.5.m3" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><msub><mi>μ</mi><mi>t</mi></msub><mo>,</mo><msub><mi>σ</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><mo>=</mo><mrow><mi>P</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>e</mi><mi>t</mi><mi>T</mi></msubsup><mo>=</mo><mrow><msub><mi>f</mi><mtext>VLM</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">(\mu_{t},\sigma_{t})=P(e^{T}_{t}=f_{\text{VLM}}(x^{T_{1:t}}))</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.Px2.p3">
<p class="ltx_p" id="S3.SS2.SSS1.Px2.p3.3">Similarly, <math alttext="P^{\prime}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px2.p3.1.m1" intent=":literal"><semantics><msup><mi>P</mi><mo>′</mo></msup><annotation encoding="application/x-tex">P^{\prime}</annotation></semantics></math> is trained on <math alttext="\mathcal{D}^{VT}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px2.p3.2.m2" intent=":literal"><semantics><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mrow><mi>V</mi><mo lspace="0em" rspace="0em">​</mo><mi>T</mi></mrow></msup><annotation encoding="application/x-tex">\mathcal{D}^{VT}</annotation></semantics></math> using the symmetric loss <math alttext="\mathcal{L}_{\text{vt2t}}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px2.p3.3.m3" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>vt2t</mtext></msub><annotation encoding="application/x-tex">\mathcal{L}_{\text{vt2t}}</annotation></semantics></math>, defined as:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\text{vt2t}}=\mathbb{E}_{\mathcal{D}^{VT}}\left[\sum_{t=1}^{m-1}\frac{1}{2}\left(\left\|\frac{e^{T}_{t}-\nu_{t}}{\tau_{t}}\right\|^{2}+\|\log\tau_{t}^{2}\|_{1}\right)\right]," class="ltx_Math" display="block" id="S3.E4.m1" intent=":literal"><semantics><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>vt2t</mtext></msub><mo>=</mo><mrow><msub><mi>𝔼</mi><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mrow><mi>V</mi><mo lspace="0em" rspace="0em">​</mo><mi>T</mi></mrow></msup></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><munderover><mo lspace="0em" movablelimits="false">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>m</mi><mo>−</mo><mn>1</mn></mrow></munderover><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><msup><mrow><mo>‖</mo><mfrac><mrow><msubsup><mi>e</mi><mi>t</mi><mi>T</mi></msubsup><mo>−</mo><msub><mi>ν</mi><mi>t</mi></msub></mrow><msub><mi>τ</mi><mi>t</mi></msub></mfrac><mo>‖</mo></mrow><mn>2</mn></msup><mo>+</mo><msub><mrow><mo stretchy="false">‖</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msubsup><mi>τ</mi><mi>t</mi><mn>2</mn></msubsup></mrow><mo stretchy="false">‖</mo></mrow><mn>1</mn></msub></mrow><mo>)</mo></mrow></mrow></mrow><mo>]</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\text{vt2t}}=\mathbb{E}_{\mathcal{D}^{VT}}\left[\sum_{t=1}^{m-1}\frac{1}{2}\left(\left\|\frac{e^{T}_{t}-\nu_{t}}{\tau_{t}}\right\|^{2}+\|\log\tau_{t}^{2}\|_{1}\right)\right],</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS1.Px2.p3.6">where the expectation is taken over sequences <math alttext="(x^{V},x^{T_{1:m}})\sim\mathcal{D}^{VT}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px2.p3.4.m1" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mi>V</mi></msup><mo>,</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>m</mi></mrow></msub></msup><mo stretchy="false">)</mo></mrow><mo>∼</mo><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mrow><mi>V</mi><mo lspace="0em" rspace="0em">​</mo><mi>T</mi></mrow></msup></mrow><annotation encoding="application/x-tex">(x^{V},x^{T_{1:m}})\sim\mathcal{D}^{VT}</annotation></semantics></math>,
<math alttext="e^{T}_{t}=f_{\text{VLM}}(x^{T_{1:t}})" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px2.p3.5.m2" intent=":literal"><semantics><mrow><msubsup><mi>e</mi><mi>t</mi><mi>T</mi></msubsup><mo>=</mo><mrow><msub><mi>f</mi><mtext>VLM</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">e^{T}_{t}=f_{\text{VLM}}(x^{T_{1:t}})</annotation></semantics></math> denotes the text embedding,
and <math alttext="(\nu_{t},\tau_{t})=P^{\prime}(e^{V,T}_{t}=f_{\text{VLM}}(x^{V},x^{T_{1:t}}))" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px2.p3.6.m3" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><msub><mi>ν</mi><mi>t</mi></msub><mo>,</mo><msub><mi>τ</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><mo>=</mo><mrow><msup><mi>P</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msubsup><mi>e</mi><mi>t</mi><mrow><mi>V</mi><mo>,</mo><mi>T</mi></mrow></msubsup><mo>=</mo><mrow><msub><mi>f</mi><mtext>VLM</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mi>V</mi></msup><mo>,</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">(\nu_{t},\tau_{t})=P^{\prime}(e^{V,T}_{t}=f_{\text{VLM}}(x^{V},x^{T_{1:t}}))</annotation></semantics></math>.
The total loss for <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.Px2.p3.6.1">Step 1</span> is:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\text{proj}_{\text{1}}}=\mathcal{L}_{\text{t2vt}}+\mathcal{L}_{\text{vt2t}}." class="ltx_Math" display="block" id="S3.E5.m1" intent=":literal"><semantics><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><msub><mtext>proj</mtext><mtext>1</mtext></msub></msub><mo>=</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>t2vt</mtext></msub><mo>+</mo><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>vt2t</mtext></msub></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\text{proj}_{\text{1}}}=\mathcal{L}_{\text{t2vt}}+\mathcal{L}_{\text{vt2t}}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.Px2.p4">
<p class="ltx_p" id="S3.SS2.SSS1.Px2.p4.4"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS2.SSS1.Px2.p4.4.1">Step 2: Jointly training on paired image-text data and text-only data</span>
We now jointly train <math alttext="P" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px2.p4.1.m1" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> and <math alttext="P^{\prime}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px2.p4.2.m2" intent=":literal"><semantics><msup><mi>P</mi><mo>′</mo></msup><annotation encoding="application/x-tex">P^{\prime}</annotation></semantics></math> on paired data <math alttext="\mathcal{D}^{VT}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px2.p4.3.m3" intent=":literal"><semantics><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mrow><mi>V</mi><mo lspace="0em" rspace="0em">​</mo><mi>T</mi></mrow></msup><annotation encoding="application/x-tex">\mathcal{D}^{VT}</annotation></semantics></math> and text-only data <math alttext="\mathcal{D}^{T}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px2.p4.4.m4" intent=":literal"><semantics><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>T</mi></msup><annotation encoding="application/x-tex">\mathcal{D}^{T}</annotation></semantics></math>. The total objective is:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\text{proj}_{\text{2}}}=\mathcal{L}_{\text{t2vt}}+\mathcal{L}_{\text{vt2t}}+\mathcal{L}_{\text{cycle}}" class="ltx_Math" display="block" id="S3.E6.m1" intent=":literal"><semantics><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><msub><mtext>proj</mtext><mtext>2</mtext></msub></msub><mo>=</mo><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>t2vt</mtext></msub><mo>+</mo><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>vt2t</mtext></msub><mo>+</mo><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>cycle</mtext></msub></mrow></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\text{proj}_{\text{2}}}=\mathcal{L}_{\text{t2vt}}+\mathcal{L}_{\text{vt2t}}+\mathcal{L}_{\text{cycle}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS1.Px2.p4.9">where <math alttext="\mathcal{L}_{\text{t2vt}}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px2.p4.5.m1" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>t2vt</mtext></msub><annotation encoding="application/x-tex">\mathcal{L}_{\text{t2vt}}</annotation></semantics></math> (Eq. <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S3.E3" title="Equation 3 ‣ Cross-modal Projector Training ‣ 3.2.1 Inverse Dynamics Learning ‣ 3.2 Latent Action Space Learning ‣ 3 Methodology ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">3</span></a>) and <math alttext="\mathcal{L}_{\text{vt2t}}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px2.p4.6.m2" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>vt2t</mtext></msub><annotation encoding="application/x-tex">\mathcal{L}_{\text{vt2t}}</annotation></semantics></math> (Eq. <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S3.E4" title="Equation 4 ‣ Cross-modal Projector Training ‣ 3.2.1 Inverse Dynamics Learning ‣ 3.2 Latent Action Space Learning ‣ 3 Methodology ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">4</span></a>) are computed over <math alttext="\mathcal{D}^{VT}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px2.p4.7.m3" intent=":literal"><semantics><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mrow><mi>V</mi><mo lspace="0em" rspace="0em">​</mo><mi>T</mi></mrow></msup><annotation encoding="application/x-tex">\mathcal{D}^{VT}</annotation></semantics></math>, and <math alttext="\mathcal{L}_{\text{cycle}}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px2.p4.8.m4" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>cycle</mtext></msub><annotation encoding="application/x-tex">\mathcal{L}_{\text{cycle}}</annotation></semantics></math> denotes a novel cycle consistency loss computed on text-only data <math alttext="\mathcal{D}^{T}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px2.p4.9.m5" intent=":literal"><semantics><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>T</mi></msup><annotation encoding="application/x-tex">\mathcal{D}^{T}</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.Px2.p5">
<p class="ltx_p" id="S3.SS2.SSS1.Px2.p5.1">The cycle consistency loss <math alttext="\mathcal{L}_{\text{cycle}}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px2.p5.1.m1" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>cycle</mtext></msub><annotation encoding="application/x-tex">\mathcal{L}_{\text{cycle}}</annotation></semantics></math> is defined as:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\text{cycle}}=\mathbb{E}_{\mathcal{D}^{T}}\left[\sum_{t=1}^{m-1}\frac{1}{2}\left(\left\|\frac{e^{T}_{t}-\nu_{t}}{\tau_{t}}\right\|^{2}+\|\log\tau_{t}^{2}\|_{1}\right)\right]," class="ltx_Math" display="block" id="S3.E7.m1" intent=":literal"><semantics><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>cycle</mtext></msub><mo>=</mo><mrow><msub><mi>𝔼</mi><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>T</mi></msup></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><munderover><mo lspace="0em" movablelimits="false">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>m</mi><mo>−</mo><mn>1</mn></mrow></munderover><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><msup><mrow><mo>‖</mo><mfrac><mrow><msubsup><mi>e</mi><mi>t</mi><mi>T</mi></msubsup><mo>−</mo><msub><mi>ν</mi><mi>t</mi></msub></mrow><msub><mi>τ</mi><mi>t</mi></msub></mfrac><mo>‖</mo></mrow><mn>2</mn></msup><mo>+</mo><msub><mrow><mo stretchy="false">‖</mo><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msubsup><mi>τ</mi><mi>t</mi><mn>2</mn></msubsup></mrow><mo stretchy="false">‖</mo></mrow><mn>1</mn></msub></mrow><mo>)</mo></mrow></mrow></mrow><mo>]</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\text{cycle}}=\mathbb{E}_{\mathcal{D}^{T}}\left[\sum_{t=1}^{m-1}\frac{1}{2}\left(\left\|\frac{e^{T}_{t}-\nu_{t}}{\tau_{t}}\right\|^{2}+\|\log\tau_{t}^{2}\|_{1}\right)\right],</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS1.Px2.p5.5">where the expectation is taken over text-only sequences <math alttext="x^{T_{1:m}}\sim\mathcal{D}^{T}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px2.p5.2.m1" intent=":literal"><semantics><mrow><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>m</mi></mrow></msub></msup><mo>∼</mo><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">x^{T_{1:m}}\sim\mathcal{D}^{T}</annotation></semantics></math>,
<math alttext="e^{T}_{t}=f_{\text{VLM}}(x^{T_{1:t}})" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px2.p5.3.m2" intent=":literal"><semantics><mrow><msubsup><mi>e</mi><mi>t</mi><mi>T</mi></msubsup><mo>=</mo><mrow><msub><mi>f</mi><mtext>VLM</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">e^{T}_{t}=f_{\text{VLM}}(x^{T_{1:t}})</annotation></semantics></math>, and <math alttext="(\mu_{t},\sigma_{t})=P(e^{T}_{t})" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px2.p5.4.m3" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><msub><mi>μ</mi><mi>t</mi></msub><mo>,</mo><msub><mi>σ</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><mo>=</mo><mrow><mi>P</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>e</mi><mi>t</mi><mi>T</mi></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">(\mu_{t},\sigma_{t})=P(e^{T}_{t})</annotation></semantics></math>, and <math alttext="(\nu_{t},\tau_{t})=P^{\prime}(\mu_{t})" class="ltx_Math" display="inline" id="S3.SS2.SSS1.Px2.p5.5.m4" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><msub><mi>ν</mi><mi>t</mi></msub><mo>,</mo><msub><mi>τ</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><mo>=</mo><mrow><msup><mi>P</mi><mo>′</mo></msup><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>μ</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">(\nu_{t},\tau_{t})=P^{\prime}(\mu_{t})</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Policy Behavior Cloning</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.4">During RL exploration and inference, future observations are unavailable, making the inverse dynamics model <math alttext="f_{\text{inverse}}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p1.1.m1" intent=":literal"><semantics><msub><mi>f</mi><mtext>inverse</mtext></msub><annotation encoding="application/x-tex">f_{\text{inverse}}</annotation></semantics></math> inapplicable.
Thus, we train a policy model <math alttext="\pi_{\theta}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p1.2.m2" intent=":literal"><semantics><msub><mi>π</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">\pi_{\theta}</annotation></semantics></math> via behavior cloning to mimic latent actions inferred by <math alttext="f_{\text{inverse}}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p1.3.m3" intent=":literal"><semantics><msub><mi>f</mi><mtext>inverse</mtext></msub><annotation encoding="application/x-tex">f_{\text{inverse}}</annotation></semantics></math> (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S2.F2" title="Figure 2 ‣ Latent Actions for Reinforcement Learning ‣ 2 Preliminary ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">2</span></a> (b)).
Specifically, for samples from the mixed corpus <math alttext="\mathcal{D}^{\text{mix}}=\mathcal{D}^{VT}\cup\mathcal{D}^{T}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p1.4.m4" intent=":literal"><semantics><mrow><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mtext>mix</mtext></msup><mo>=</mo><mrow><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mrow><mi>V</mi><mo lspace="0em" rspace="0em">​</mo><mi>T</mi></mrow></msup><mo>∪</mo><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>T</mi></msup></mrow></mrow><annotation encoding="application/x-tex">\mathcal{D}^{\text{mix}}=\mathcal{D}^{VT}\cup\mathcal{D}^{T}</annotation></semantics></math>,
we compute the loss as:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\text{bc}}=\mathbb{E}_{\mathcal{D}^{\text{mix}}}\left[-\sum_{t=1}^{m-1}\log\pi_{\theta}\big(a_{t}^{\ast}=f_{\text{inverse}}(e^{V,T}_{t+1})\mid e^{V,T}_{t}\big)\right]," class="ltx_Math" display="block" id="S3.E8.m1" intent=":literal"><semantics><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>bc</mtext></msub><mo>=</mo><mrow><msub><mi>𝔼</mi><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mtext>mix</mtext></msup></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><mo>−</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>m</mi><mo>−</mo><mn>1</mn></mrow></munderover><mrow><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mi>π</mi><mi>θ</mi></msub></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="1.200em" minsize="1.200em">(</mo><mrow><msubsup><mi>a</mi><mi>t</mi><mo>∗</mo></msubsup><mo>=</mo><mrow><mrow><msub><mi>f</mi><mtext>inverse</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>e</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>V</mi><mo>,</mo><mi>T</mi></mrow></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo>∣</mo><msubsup><mi>e</mi><mi>t</mi><mrow><mi>V</mi><mo>,</mo><mi>T</mi></mrow></msubsup></mrow></mrow><mo maxsize="1.200em" minsize="1.200em">)</mo></mrow></mrow></mrow></mrow><mo>]</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\text{bc}}=\mathbb{E}_{\mathcal{D}^{\text{mix}}}\left[-\sum_{t=1}^{m-1}\log\pi_{\theta}\big(a_{t}^{\ast}=f_{\text{inverse}}(e^{V,T}_{t+1})\mid e^{V,T}_{t}\big)\right],</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS2.p1.6">where the expectation is taken over sequences <math alttext="(x^{V},x^{T_{1:m}})\sim\mathcal{D}^{\text{mix}}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p1.5.m1" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mi>V</mi></msup><mo>,</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>m</mi></mrow></msub></msup><mo stretchy="false">)</mo></mrow><mo>∼</mo><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mtext>mix</mtext></msup></mrow><annotation encoding="application/x-tex">(x^{V},x^{T_{1:m}})\sim\mathcal{D}^{\text{mix}}</annotation></semantics></math>,
with <math alttext="e^{V,T}_{t}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p1.6.m2" intent=":literal"><semantics><msubsup><mi>e</mi><mi>t</mi><mrow><mi>V</mi><mo>,</mo><mi>T</mi></mrow></msubsup><annotation encoding="application/x-tex">e^{V,T}_{t}</annotation></semantics></math> defined as in Eq. <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S3.E2" title="Equation 2 ‣ Overview ‣ 3.2.1 Inverse Dynamics Learning ‣ 3.2 Latent Action Space Learning ‣ 3 Methodology ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Latent Action Reinforcement Learning</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.9">On downstream multimodal conversational tasks, we perform reinforcement learning at the policy model level, as illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S3.F3" title="Figure 3 ‣ 3.3 Latent Action Reinforcement Learning ‣ 3 Methodology ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">3</span></a>.
For each prompt <math alttext="(x^{V},x^{T_{1:p}})\sim\mathcal{D}_{\text{rl}}" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mi>V</mi></msup><mo>,</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>p</mi></mrow></msub></msup><mo stretchy="false">)</mo></mrow><mo>∼</mo><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mtext>rl</mtext></msub></mrow><annotation encoding="application/x-tex">(x^{V},x^{T_{1:p}})\sim\mathcal{D}_{\text{rl}}</annotation></semantics></math> with the prompt length <math alttext="p" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2" intent=":literal"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>, the policy <math alttext="\pi_{\theta}" class="ltx_Math" display="inline" id="S3.SS3.p1.3.m3" intent=":literal"><semantics><msub><mi>π</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">\pi_{\theta}</annotation></semantics></math> and the world model <math alttext="f_{\text{world}}" class="ltx_Math" display="inline" id="S3.SS3.p1.4.m4" intent=":literal"><semantics><msub><mi>f</mi><mtext>world</mtext></msub><annotation encoding="application/x-tex">f_{\text{world}}</annotation></semantics></math> jointly generate response <math alttext="x^{T_{p+1:m}}" class="ltx_Math" display="inline" id="S3.SS3.p1.5.m5" intent=":literal"><semantics><msup><mi>x</mi><msub><mi>T</mi><mrow><mrow><mi>p</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>m</mi></mrow></msub></msup><annotation encoding="application/x-tex">x^{T_{p+1:m}}</annotation></semantics></math> auto-regressively, i.e., at each step <math alttext="t=p,\dots,m-1" class="ltx_Math" display="inline" id="S3.SS3.p1.6.m6" intent=":literal"><semantics><mrow><mi>t</mi><mo>=</mo><mrow><mi>p</mi><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><mi>m</mi><mo>−</mo><mn>1</mn></mrow></mrow></mrow><annotation encoding="application/x-tex">t=p,\dots,m-1</annotation></semantics></math>, <math alttext="a_{t}\sim\pi_{\theta}(\cdot|x^{V},x^{T_{1:t}}),x^{T_{t+1}}=f_{\text{world}}(x^{V},x^{T_{1:t}},a_{t})" class="ltx_math_unparsed" display="inline" id="S3.SS3.p1.7.m7" intent=":literal"><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>∼</mo><msub><mi>π</mi><mi>θ</mi></msub><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><msup><mi>x</mi><mi>V</mi></msup><mo>,</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub></msup><mo stretchy="false">)</mo></mrow><mo>,</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></msup><mo>=</mo><msub><mi>f</mi><mtext>world</mtext></msub><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mi>V</mi></msup><mo>,</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub></msup><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">a_{t}\sim\pi_{\theta}(\cdot|x^{V},x^{T_{1:t}}),x^{T_{t+1}}=f_{\text{world}}(x^{V},x^{T_{1:t}},a_{t})</annotation></semantics></math>, with maximum length <math alttext="m" class="ltx_Math" display="inline" id="S3.SS3.p1.8.m8" intent=":literal"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>.
We optimize <math alttext="\pi_{\theta}" class="ltx_Math" display="inline" id="S3.SS3.p1.9.m9" intent=":literal"><semantics><msub><mi>π</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">\pi_{\theta}</annotation></semantics></math> by maximizing the expected rewards:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{J}(\theta)=\mathbb{E}_{(x^{V},x^{T_{1:p}})\sim\mathcal{D}_{\text{rl}}}\left[R\big(x^{T_{p+1:m}}\big)\right]," class="ltx_Math" display="block" id="S3.E9.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi class="ltx_font_mathcaligraphic">𝒥</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>𝔼</mi><mrow><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mi>V</mi></msup><mo>,</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>p</mi></mrow></msub></msup><mo stretchy="false">)</mo></mrow><mo>∼</mo><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mtext>rl</mtext></msub></mrow></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo>[</mo><mrow><mi>R</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="1.200em" minsize="1.200em">(</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mrow><mi>p</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>m</mi></mrow></msub></msup><mo maxsize="1.200em" minsize="1.200em">)</mo></mrow></mrow><mo>]</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathcal{J}(\theta)=\mathbb{E}_{(x^{V},x^{T_{1:p}})\sim\mathcal{D}_{\text{rl}}}\left[R\big(x^{T_{p+1:m}}\big)\right],</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.p1.10">where <math alttext="R(\cdot)" class="ltx_Math" display="inline" id="S3.SS3.p1.10.m1" intent=":literal"><semantics><mrow><mi>R</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">R(\cdot)</annotation></semantics></math> denotes the reward function.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="133" id="S3.F3.g1" src="x3.png" width="316"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Illustrations of latent action RL. The language world model is frozen, while the policy model is optimized to select latent actions from the codebook that steer the generated responses toward higher rewards.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">We summarize our framework in Algorithm <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#alg1" title="Algorithm 1 ‣ 3.3 Latent Action Reinforcement Learning ‣ 3 Methodology ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold" id="alg1.2.1.1">Algorithm 1</span> </span> Latent Action Space Learning and Latent Action RL</figcaption>
<div class="ltx_listing ltx_listing" id="alg1.3">
<div class="ltx_listingline" id="alg1.l1">
<span class="ltx_tag ltx_tag_listingline">1:</span><span class="ltx_text ltx_font_bold" id="alg1.l1.1">Stage 1: Latent Action Space Learning</span>
</div>
<div class="ltx_listingline" id="alg1.l2">
<span class="ltx_tag ltx_tag_listingline">2:</span>Initialize <math alttext="f_{\text{world}},f_{\text{inverse}},\mathcal{C}" class="ltx_Math" display="inline" id="alg1.l2.m1" intent=":literal"><semantics><mrow><msub><mi>f</mi><mtext>world</mtext></msub><mo>,</mo><msub><mi>f</mi><mtext>inverse</mtext></msub><mo>,</mo><mi class="ltx_font_mathcaligraphic">𝒞</mi></mrow><annotation encoding="application/x-tex">f_{\text{world}},f_{\text{inverse}},\mathcal{C}</annotation></semantics></math> by minimizing <math alttext="\mathcal{L}_{\text{inverse}}" class="ltx_Math" display="inline" id="alg1.l2.m2" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>inverse</mtext></msub><annotation encoding="application/x-tex">\mathcal{L}_{\text{inverse}}</annotation></semantics></math> (Eq. <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S3.E1" title="Equation 1 ‣ Overview ‣ 3.2.1 Inverse Dynamics Learning ‣ 3.2 Latent Action Space Learning ‣ 3 Methodology ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">1</span></a>) on <math alttext="\mathcal{D}^{VT}" class="ltx_Math" display="inline" id="alg1.l2.m3" intent=":literal"><semantics><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mrow><mi>V</mi><mo lspace="0em" rspace="0em">​</mo><mi>T</mi></mrow></msup><annotation encoding="application/x-tex">\mathcal{D}^{VT}</annotation></semantics></math>.

</div>
<div class="ltx_listingline" id="alg1.l3">
<span class="ltx_tag ltx_tag_listingline">3:</span>Initialize the cross-modal projectors <math alttext="P,P^{\prime}" class="ltx_Math" display="inline" id="alg1.l3.m1" intent=":literal"><semantics><mrow><mi>P</mi><mo>,</mo><msup><mi>P</mi><mo>′</mo></msup></mrow><annotation encoding="application/x-tex">P,P^{\prime}</annotation></semantics></math> by minimizing <math alttext="\mathcal{L}_{\text{proj}_{1}}" class="ltx_Math" display="inline" id="alg1.l3.m2" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><msub><mtext>proj</mtext><mn>1</mn></msub></msub><annotation encoding="application/x-tex">\mathcal{L}_{\text{proj}_{1}}</annotation></semantics></math> (Eq. <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S3.E5" title="Equation 5 ‣ Cross-modal Projector Training ‣ 3.2.1 Inverse Dynamics Learning ‣ 3.2 Latent Action Space Learning ‣ 3 Methodology ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">5</span></a>) on <math alttext="\mathcal{D}^{VT}" class="ltx_Math" display="inline" id="alg1.l3.m3" intent=":literal"><semantics><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mrow><mi>V</mi><mo lspace="0em" rspace="0em">​</mo><mi>T</mi></mrow></msup><annotation encoding="application/x-tex">\mathcal{D}^{VT}</annotation></semantics></math>.

</div>
<div class="ltx_listingline" id="alg1.l4">
<span class="ltx_tag ltx_tag_listingline">4:</span>Jointly optimize <math alttext="f_{\text{world}},f_{\text{inverse}},\mathcal{C},P,P^{\prime}" class="ltx_Math" display="inline" id="alg1.l4.m1" intent=":literal"><semantics><mrow><msub><mi>f</mi><mtext>world</mtext></msub><mo>,</mo><msub><mi>f</mi><mtext>inverse</mtext></msub><mo>,</mo><mi class="ltx_font_mathcaligraphic">𝒞</mi><mo>,</mo><mi>P</mi><mo>,</mo><msup><mi>P</mi><mo>′</mo></msup></mrow><annotation encoding="application/x-tex">f_{\text{world}},f_{\text{inverse}},\mathcal{C},P,P^{\prime}</annotation></semantics></math> by minimizing <math alttext="\mathcal{L}_{\text{inverse}}" class="ltx_Math" display="inline" id="alg1.l4.m2" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>inverse</mtext></msub><annotation encoding="application/x-tex">\mathcal{L}_{\text{inverse}}</annotation></semantics></math> (Eq. <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S3.E1" title="Equation 1 ‣ Overview ‣ 3.2.1 Inverse Dynamics Learning ‣ 3.2 Latent Action Space Learning ‣ 3 Methodology ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">1</span></a>) and <math alttext="\mathcal{L}_{\text{proj}_{2}}" class="ltx_Math" display="inline" id="alg1.l4.m3" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><msub><mtext>proj</mtext><mn>2</mn></msub></msub><annotation encoding="application/x-tex">\mathcal{L}_{\text{proj}_{2}}</annotation></semantics></math> (Eq. <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S3.E6" title="Equation 6 ‣ Cross-modal Projector Training ‣ 3.2.1 Inverse Dynamics Learning ‣ 3.2 Latent Action Space Learning ‣ 3 Methodology ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">6</span></a>) on <math alttext="\mathcal{D}^{VT}\cup\mathcal{D}^{T}" class="ltx_Math" display="inline" id="alg1.l4.m4" intent=":literal"><semantics><mrow><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mrow><mi>V</mi><mo lspace="0em" rspace="0em">​</mo><mi>T</mi></mrow></msup><mo>∪</mo><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">\mathcal{D}^{VT}\cup\mathcal{D}^{T}</annotation></semantics></math>.

</div>
<div class="ltx_listingline" id="alg1.l5">
<span class="ltx_tag ltx_tag_listingline">5:</span>Initialize the policy model <math alttext="\pi_{\theta}" class="ltx_Math" display="inline" id="alg1.l5.m1" intent=":literal"><semantics><msub><mi>π</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">\pi_{\theta}</annotation></semantics></math> by minimizing <math alttext="\mathcal{L}_{\text{bc}}" class="ltx_Math" display="inline" id="alg1.l5.m2" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>bc</mtext></msub><annotation encoding="application/x-tex">\mathcal{L}_{\text{bc}}</annotation></semantics></math> (Eq. <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S3.E8" title="Equation 8 ‣ 3.2.2 Policy Behavior Cloning ‣ 3.2 Latent Action Space Learning ‣ 3 Methodology ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">8</span></a>) on <math alttext="\mathcal{D}^{VT}\cup\mathcal{D}^{T}" class="ltx_Math" display="inline" id="alg1.l5.m3" intent=":literal"><semantics><mrow><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mrow><mi>V</mi><mo lspace="0em" rspace="0em">​</mo><mi>T</mi></mrow></msup><mo>∪</mo><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">\mathcal{D}^{VT}\cup\mathcal{D}^{T}</annotation></semantics></math>.

</div>
<div class="ltx_listingline" id="alg1.l6">
<span class="ltx_tag ltx_tag_listingline">6:</span><span class="ltx_text ltx_font_bold" id="alg1.l6.1">Stage 2: Latent Action RL</span>
</div>
<div class="ltx_listingline" id="alg1.l7">
<span class="ltx_tag ltx_tag_listingline">7:</span>Sample <math alttext="(x^{V},x^{T_{1:p}})\sim\mathcal{D}_{\text{rl}}" class="ltx_Math" display="inline" id="alg1.l7.m1" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mi>V</mi></msup><mo>,</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>p</mi></mrow></msub></msup><mo stretchy="false">)</mo></mrow><mo>∼</mo><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mtext>rl</mtext></msub></mrow><annotation encoding="application/x-tex">(x^{V},x^{T_{1:p}})\sim\mathcal{D}_{\text{rl}}</annotation></semantics></math>:

</div>
<div class="ltx_listingline" id="alg1.l8">
<span class="ltx_tag ltx_tag_listingline">8:</span> Roll out <math alttext="x^{T_{p+1:m}}" class="ltx_Math" display="inline" id="alg1.l8.m1" intent=":literal"><semantics><msup><mi>x</mi><msub><mi>T</mi><mrow><mrow><mi>p</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>m</mi></mrow></msub></msup><annotation encoding="application/x-tex">x^{T_{p+1:m}}</annotation></semantics></math> via <math alttext="a_{t}\sim\pi_{\theta}(\cdot|x^{V},x^{T_{1:t}})" class="ltx_math_unparsed" display="inline" id="alg1.l8.m2" intent=":literal"><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>∼</mo><msub><mi>π</mi><mi>θ</mi></msub><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo fence="false" rspace="0.167em" stretchy="false">|</mo><msup><mi>x</mi><mi>V</mi></msup><mo>,</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub></msup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">a_{t}\sim\pi_{\theta}(\cdot|x^{V},x^{T_{1:t}})</annotation></semantics></math>, <math alttext="x^{T_{t+1}}=f_{\text{world}}(x^{V},x^{T_{1:t}},a_{t})" class="ltx_Math" display="inline" id="alg1.l8.m3" intent=":literal"><semantics><mrow><msup><mi>x</mi><msub><mi>T</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></msup><mo>=</mo><mrow><msub><mi>f</mi><mtext>world</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mi>V</mi></msup><mo>,</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub></msup><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">x^{T_{t+1}}=f_{\text{world}}(x^{V},x^{T_{1:t}},a_{t})</annotation></semantics></math>, <math alttext="t=p,...,m-1" class="ltx_Math" display="inline" id="alg1.l8.m4" intent=":literal"><semantics><mrow><mi>t</mi><mo>=</mo><mrow><mi>p</mi><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><mi>m</mi><mo>−</mo><mn>1</mn></mrow></mrow></mrow><annotation encoding="application/x-tex">t=p,...,m-1</annotation></semantics></math>.

</div>
<div class="ltx_listingline" id="alg1.l9">
<span class="ltx_tag ltx_tag_listingline">9:</span> Compute reward <math alttext="R(x^{T_{p+1:m}})" class="ltx_Math" display="inline" id="alg1.l9.m1" intent=":literal"><semantics><mrow><mi>R</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mrow><mi>p</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mi>m</mi></mrow></msub></msup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">R(x^{T_{p+1:m}})</annotation></semantics></math>.

</div>
<div class="ltx_listingline" id="alg1.l10">
<span class="ltx_tag ltx_tag_listingline">10:</span> Optimize <math alttext="\pi_{\theta}" class="ltx_Math" display="inline" id="alg1.l10.m1" intent=":literal"><semantics><msub><mi>π</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">\pi_{\theta}</annotation></semantics></math> by maximizing <math alttext="\mathcal{J}(\theta)" class="ltx_Math" display="inline" id="alg1.l10.m2" intent=":literal"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒥</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{J}(\theta)</annotation></semantics></math> (Eq. <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S3.E9" title="Equation 9 ‣ 3.3 Latent Action Reinforcement Learning ‣ 3 Methodology ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">9</span></a>).

</div>
</div>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setup</h3>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Models</h5>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.1">We build the language world model, inverse dynamics model, and policy model upon the same foundation vision-language model. Specifically, we use <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS0.Px1.p1.1.1">Qwen2.5-VL-3B-Instruct</span> and <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS0.Px1.p1.1.2">Qwen2.5-VL-7B-Instruct</span> <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">bai-2025-qwen25vl</span></cite> for main experiments.
The latent action space is implemented as a codebook with size <math alttext="|\mathcal{C}|=128" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.1.m1" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">|</mo><mi class="ltx_font_mathcaligraphic">𝒞</mi><mo stretchy="false">|</mo></mrow><mo>=</mo><mn>128</mn></mrow><annotation encoding="application/x-tex">|\mathcal{C}|=128</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Datasets</h5>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p1.4">During the latent action space construction stage (Section <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S3.SS2" title="3.2 Latent Action Space Learning ‣ 3 Methodology ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">3.2</span></a>), we use a mixture of paired image-text corpora <math alttext="\mathcal{D}^{VT}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.1.m1" intent=":literal"><semantics><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mrow><mi>V</mi><mo lspace="0em" rspace="0em">​</mo><mi>T</mi></mrow></msup><annotation encoding="application/x-tex">\mathcal{D}^{VT}</annotation></semantics></math> and text-only corpora <math alttext="\mathcal{D}^{T}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.2.m2" intent=":literal"><semantics><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>T</mi></msup><annotation encoding="application/x-tex">\mathcal{D}^{T}</annotation></semantics></math>.
For <math alttext="\mathcal{D}^{VT}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.3.m3" intent=":literal"><semantics><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mrow><mi>V</mi><mo lspace="0em" rspace="0em">​</mo><mi>T</mi></mrow></msup><annotation encoding="application/x-tex">\mathcal{D}^{VT}</annotation></semantics></math>, we collect image-caption pairs from <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS0.Px2.p1.4.1">Conceptual-12M</span> <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">changpinyo-2021-Conceptual</span></cite>, multimodal news articles from <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS0.Px2.p1.4.2">N24News</span> <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">wang-2022-N24News</span></cite>, and multimodal Wikipedia data from <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS0.Px2.p1.4.3">WikiWeb2M</span> <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">burns-2023-wiki</span></cite>, totaling 14 million images and 1 billion text tokens.
For <math alttext="\mathcal{D}^{T}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.4.m4" intent=":literal"><semantics><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>T</mi></msup><annotation encoding="application/x-tex">\mathcal{D}^{T}</annotation></semantics></math>, we collect text-only data mainly from the <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS0.Px2.p1.4.4">SlimPajama-627B</span> dataset <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">cerebras-2023-slimpajama</span></cite>, which contains 627 billion text tokens.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p2">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p2.1">For latent action RL (Section <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S3.SS3" title="3.3 Latent Action Reinforcement Learning ‣ 3 Methodology ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">3.3</span></a>), we evaluate our method on two downstream tasks: 1) multimodal role-playing conversation on <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS0.Px2.p2.1.1">MMRole</span> <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">dai-2025-mmrole</span></cite>, where we focus on the challenging <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS0.Px2.p2.1.2">Comment</span> subset; we train on the in-distribution (ID) split and evaluate on ID and out-of-distribution (OOD) test sets; 2) multimodal personalized conversation on <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS0.Px2.p2.1.3">PCogAlignBench</span> <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">li-2025-aligning</span></cite>, where we train the agent on the <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS0.Px2.p2.1.4">LS1</span> set and evaluate on <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS0.Px2.p2.1.5">LS1</span> and <span class="ltx_text ltx_font_typewriter" id="S4.SS1.SSS0.Px2.p2.1.6">LS2</span> test sets.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Evaluation Metrics</h5>
<div class="ltx_para" id="S4.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px3.p1.1">We adopt the <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS0.Px3.p1.1.1">LLM-as-a-Judge</span> metric to evaluate model performance, using prompt templates validated by <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">dai-2025-mmrole</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">li-2025-aligning</span></cite>, which show high correlation with human judgments.
For each sample, the LLM judge scores both the model and ground-truth responses across benchmark-specific dimensions, with scores ranging 1-10.
Then, following <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">dai-2025-mmrole</span></cite>, we report the ratio of the model’s average score to the ground-truth response’s average score across all evaluation dimensions.
We report the mean and standard deviation across three evaluation runs.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T1.100" style="width:433.6pt;height:230.8pt;vertical-align:-113.3pt;"><span class="ltx_transformed_inner" style="transform:translate(-36.4pt,19.4pt) scale(0.856098586125005,0.856098586125005) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.100.100">
<tr class="ltx_tr" id="S4.T1.100.100.101">
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T1.100.100.101.1" rowspan="2" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.100.100.101.1.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T1.100.100.101.2" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.100.100.101.2.1">MMRole</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T1.100.100.101.3" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.100.100.101.3.1">PCogAlignBench</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.100.100.101.4" rowspan="2" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.100.100.101.4.1">Average</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.100.100.102">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.100.100.102.1" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_italic" id="S4.T1.100.100.102.1.1">ID</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.100.100.102.2" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_italic" id="S4.T1.100.100.102.2.1">OOD</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.100.100.102.3" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_italic" id="S4.T1.100.100.102.3.1">LS1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.100.100.102.4" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_italic" id="S4.T1.100.100.102.4.1">LS2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.5.5">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.5.5.6" rowspan="10" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text" id="S4.T1.5.5.5.6.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T1.5.5.5.6.1.1" style="width:8.8pt;height:108.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:108.8pt;transform:translate(-50.0pt,-50.0pt) rotate(-90deg) ;">
<span class="ltx_p" id="S4.T1.5.5.5.6.1.1.1"><span class="ltx_text ltx_font_italic" id="S4.T1.5.5.5.6.1.1.1.1">Qwen2.5-VL-3B-Instruct</span></span>
</span></span></span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.5.5.5.7" style="padding-left:7.4pt;padding-right:7.4pt;">Prompt</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.1.1" style="padding-left:7.4pt;padding-right:7.4pt;">0.728<sub class="ltx_sub" id="S4.T1.1.1.1.1.1"><span class="ltx_text ltx_font_italic" id="S4.T1.1.1.1.1.1.1">±0.005</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.2.2" style="padding-left:7.4pt;padding-right:7.4pt;">0.687<sub class="ltx_sub" id="S4.T1.2.2.2.2.1"><span class="ltx_text ltx_font_italic" id="S4.T1.2.2.2.2.1.1">±0.025</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.3.3.3" style="padding-left:7.4pt;padding-right:7.4pt;">0.678<sub class="ltx_sub" id="S4.T1.3.3.3.3.1"><span class="ltx_text ltx_font_italic" id="S4.T1.3.3.3.3.1.1">±0.003</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.4.4" style="padding-left:7.4pt;padding-right:7.4pt;">0.676<sub class="ltx_sub" id="S4.T1.4.4.4.4.1"><span class="ltx_text ltx_font_italic" id="S4.T1.4.4.4.4.1.1">±0.002</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.5.5.5.5" style="padding-left:7.4pt;padding-right:7.4pt;">0.692<sub class="ltx_sub" id="S4.T1.5.5.5.5.1"><span class="ltx_text ltx_font_italic" id="S4.T1.5.5.5.5.1.1">±0.009</span></sub>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.10.10.10">
<td class="ltx_td ltx_align_left" id="S4.T1.10.10.10.6" style="padding-left:7.4pt;padding-right:7.4pt;">SFT</td>
<td class="ltx_td ltx_align_center" id="S4.T1.6.6.6.1" style="padding-left:7.4pt;padding-right:7.4pt;">0.843<sub class="ltx_sub" id="S4.T1.6.6.6.1.1"><span class="ltx_text ltx_font_italic" id="S4.T1.6.6.6.1.1.1">±0.002</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.7.7.7.2" style="padding-left:7.4pt;padding-right:7.4pt;">0.809<sub class="ltx_sub" id="S4.T1.7.7.7.2.1"><span class="ltx_text ltx_font_italic" id="S4.T1.7.7.7.2.1.1">±0.012</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.8.8.8.3" style="padding-left:7.4pt;padding-right:7.4pt;">0.808<sub class="ltx_sub" id="S4.T1.8.8.8.3.1"><span class="ltx_text ltx_font_italic" id="S4.T1.8.8.8.3.1.1">±0.009</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.9.9.9.4" style="padding-left:7.4pt;padding-right:7.4pt;">0.810<sub class="ltx_sub" id="S4.T1.9.9.9.4.1"><span class="ltx_text ltx_font_italic" id="S4.T1.9.9.9.4.1.1">±0.005</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.10.10.10.5" style="padding-left:7.4pt;padding-right:7.4pt;">0.817<sub class="ltx_sub" id="S4.T1.10.10.10.5.1"><span class="ltx_text ltx_font_italic" id="S4.T1.10.10.10.5.1.1">±0.007</span></sub>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.15.15.15">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.15.15.15.6" style="padding-left:7.4pt;padding-right:7.4pt;">GRPO (Token)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.11.11.11.1" style="padding-left:7.4pt;padding-right:7.4pt;">0.838<sub class="ltx_sub" id="S4.T1.11.11.11.1.1"><span class="ltx_text ltx_font_italic" id="S4.T1.11.11.11.1.1.1">±0.017</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.12.12.12.2" style="padding-left:7.4pt;padding-right:7.4pt;">0.796<sub class="ltx_sub" id="S4.T1.12.12.12.2.1"><span class="ltx_text ltx_font_italic" id="S4.T1.12.12.12.2.1.1">±0.027</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.13.13.13.3" style="padding-left:7.4pt;padding-right:7.4pt;">0.845<sub class="ltx_sub" id="S4.T1.13.13.13.3.1"><span class="ltx_text ltx_font_italic" id="S4.T1.13.13.13.3.1.1">±0.007</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.14.14.14.4" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.14.14.14.4.1">0.845<sub class="ltx_sub" id="S4.T1.14.14.14.4.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.14.14.14.4.1.1.1">±0.004</span></sub></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.15.15.15.5" style="padding-left:7.4pt;padding-right:7.4pt;">0.831<sub class="ltx_sub" id="S4.T1.15.15.15.5.1"><span class="ltx_text ltx_font_italic" id="S4.T1.15.15.15.5.1.1">±0.014</span></sub>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.20.20.20">
<td class="ltx_td ltx_align_left" id="S4.T1.20.20.20.6" style="padding-left:7.4pt;padding-right:7.4pt;">GRPO (Latent Action)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.16.16.16.1" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.16.16.16.1.1">0.949<sub class="ltx_sub" id="S4.T1.16.16.16.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.16.16.16.1.1.1.1">±0.007</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.17.17.17.2" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.17.17.17.2.1">0.915<sub class="ltx_sub" id="S4.T1.17.17.17.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.17.17.17.2.1.1.1">±0.065</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.18.18.18.3" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.18.18.18.3.1">0.871<sub class="ltx_sub" id="S4.T1.18.18.18.3.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.18.18.18.3.1.1.1">±0.011</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.19.19.19.4" style="padding-left:7.4pt;padding-right:7.4pt;">0.837<sub class="ltx_sub" id="S4.T1.19.19.19.4.1"><span class="ltx_text ltx_font_italic" id="S4.T1.19.19.19.4.1.1">±0.010</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.20.20.20.5" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.20.20.20.5.1">0.893<sub class="ltx_sub" id="S4.T1.20.20.20.5.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.20.20.20.5.1.1.1">±0.023</span></sub></span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.25.25.25">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.25.25.25.6" style="padding-left:7.4pt;padding-right:7.4pt;">Dr.GRPO (Token)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.21.21.21.1" style="padding-left:7.4pt;padding-right:7.4pt;">0.867<sub class="ltx_sub" id="S4.T1.21.21.21.1.1"><span class="ltx_text ltx_font_italic" id="S4.T1.21.21.21.1.1.1">±0.011</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.22.22.22.2" style="padding-left:7.4pt;padding-right:7.4pt;">0.823<sub class="ltx_sub" id="S4.T1.22.22.22.2.1"><span class="ltx_text ltx_font_italic" id="S4.T1.22.22.22.2.1.1">±0.002</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.23.23.23.3" style="padding-left:7.4pt;padding-right:7.4pt;">0.835<sub class="ltx_sub" id="S4.T1.23.23.23.3.1"><span class="ltx_text ltx_font_italic" id="S4.T1.23.23.23.3.1.1">±0.008</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.24.24.24.4" style="padding-left:7.4pt;padding-right:7.4pt;">0.834<sub class="ltx_sub" id="S4.T1.24.24.24.4.1"><span class="ltx_text ltx_font_italic" id="S4.T1.24.24.24.4.1.1">±0.012</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.25.25.25.5" style="padding-left:7.4pt;padding-right:7.4pt;">0.840<sub class="ltx_sub" id="S4.T1.25.25.25.5.1"><span class="ltx_text ltx_font_italic" id="S4.T1.25.25.25.5.1.1">±0.008</span></sub>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.30.30.30">
<td class="ltx_td ltx_align_left" id="S4.T1.30.30.30.6" style="padding-left:7.4pt;padding-right:7.4pt;">Dr.GRPO (Latent Action)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.26.26.26.1" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.26.26.26.1.1">0.953<sub class="ltx_sub" id="S4.T1.26.26.26.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.26.26.26.1.1.1.1">±0.016</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.27.27.27.2" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.27.27.27.2.1">0.916<sub class="ltx_sub" id="S4.T1.27.27.27.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.27.27.27.2.1.1.1">±0.038</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.28.28.28.3" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.28.28.28.3.1">0.874<sub class="ltx_sub" id="S4.T1.28.28.28.3.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.28.28.28.3.1.1.1">±0.009</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.29.29.29.4" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.29.29.29.4.1">0.840<sub class="ltx_sub" id="S4.T1.29.29.29.4.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.29.29.29.4.1.1.1">±0.009</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.30.30.30.5" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.30.30.30.5.1">0.896<sub class="ltx_sub" id="S4.T1.30.30.30.5.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.30.30.30.5.1.1.1">±0.018</span></sub></span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.35.35.35">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.35.35.35.6" style="padding-left:7.4pt;padding-right:7.4pt;">DAPO (Token)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.31.31.31.1" style="padding-left:7.4pt;padding-right:7.4pt;">0.856<sub class="ltx_sub" id="S4.T1.31.31.31.1.1"><span class="ltx_text ltx_font_italic" id="S4.T1.31.31.31.1.1.1">±0.003</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.32.32.32.2" style="padding-left:7.4pt;padding-right:7.4pt;">0.805<sub class="ltx_sub" id="S4.T1.32.32.32.2.1"><span class="ltx_text ltx_font_italic" id="S4.T1.32.32.32.2.1.1">±0.033</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.33.33.33.3" style="padding-left:7.4pt;padding-right:7.4pt;">0.835<sub class="ltx_sub" id="S4.T1.33.33.33.3.1"><span class="ltx_text ltx_font_italic" id="S4.T1.33.33.33.3.1.1">±0.008</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.34.34.34.4" style="padding-left:7.4pt;padding-right:7.4pt;">0.828<sub class="ltx_sub" id="S4.T1.34.34.34.4.1"><span class="ltx_text ltx_font_italic" id="S4.T1.34.34.34.4.1.1">±0.008</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.35.35.35.5" style="padding-left:7.4pt;padding-right:7.4pt;">0.831<sub class="ltx_sub" id="S4.T1.35.35.35.5.1"><span class="ltx_text ltx_font_italic" id="S4.T1.35.35.35.5.1.1">±0.013</span></sub>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.40.40.40">
<td class="ltx_td ltx_align_left" id="S4.T1.40.40.40.6" style="padding-left:7.4pt;padding-right:7.4pt;">DAPO (Latent Action)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.36.36.36.1" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.36.36.36.1.1">0.941<sub class="ltx_sub" id="S4.T1.36.36.36.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.36.36.36.1.1.1.1">±0.016</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.37.37.37.2" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.37.37.37.2.1">0.889<sub class="ltx_sub" id="S4.T1.37.37.37.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.37.37.37.2.1.1.1">±0.009</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.38.38.38.3" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.38.38.38.3.1">0.879<sub class="ltx_sub" id="S4.T1.38.38.38.3.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.38.38.38.3.1.1.1">±0.011</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.39.39.39.4" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.39.39.39.4.1">0.835<sub class="ltx_sub" id="S4.T1.39.39.39.4.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.39.39.39.4.1.1.1">±0.006</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.40.40.40.5" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.40.40.40.5.1">0.886<sub class="ltx_sub" id="S4.T1.40.40.40.5.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.40.40.40.5.1.1.1">±0.010</span></sub></span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.45.45.45">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.45.45.45.6" style="padding-left:7.4pt;padding-right:7.4pt;">BNPO (Token)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.41.41.41.1" style="padding-left:7.4pt;padding-right:7.4pt;">0.860<sub class="ltx_sub" id="S4.T1.41.41.41.1.1"><span class="ltx_text ltx_font_italic" id="S4.T1.41.41.41.1.1.1">±0.012</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.42.42.42.2" style="padding-left:7.4pt;padding-right:7.4pt;">0.801<sub class="ltx_sub" id="S4.T1.42.42.42.2.1"><span class="ltx_text ltx_font_italic" id="S4.T1.42.42.42.2.1.1">±0.038</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.43.43.43.3" style="padding-left:7.4pt;padding-right:7.4pt;">0.849<sub class="ltx_sub" id="S4.T1.43.43.43.3.1"><span class="ltx_text ltx_font_italic" id="S4.T1.43.43.43.3.1.1">±0.008</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.44.44.44.4" style="padding-left:7.4pt;padding-right:7.4pt;">0.836<sub class="ltx_sub" id="S4.T1.44.44.44.4.1"><span class="ltx_text ltx_font_italic" id="S4.T1.44.44.44.4.1.1">±0.007</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.45.45.45.5" style="padding-left:7.4pt;padding-right:7.4pt;">0.836<sub class="ltx_sub" id="S4.T1.45.45.45.5.1"><span class="ltx_text ltx_font_italic" id="S4.T1.45.45.45.5.1.1">±0.016</span></sub>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.50.50.50">
<td class="ltx_td ltx_align_left" id="S4.T1.50.50.50.6" style="padding-left:7.4pt;padding-right:7.4pt;">BNPO (Latent Action)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.46.46.46.1" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.46.46.46.1.1">0.940<sub class="ltx_sub" id="S4.T1.46.46.46.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.46.46.46.1.1.1.1">±0.004</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.47.47.47.2" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.47.47.47.2.1">0.901<sub class="ltx_sub" id="S4.T1.47.47.47.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.47.47.47.2.1.1.1">±0.014</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.48.48.48.3" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.48.48.48.3.1">0.872<sub class="ltx_sub" id="S4.T1.48.48.48.3.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.48.48.48.3.1.1.1">±0.007</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.49.49.49.4" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.49.49.49.4.1">0.836<sub class="ltx_sub" id="S4.T1.49.49.49.4.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.49.49.49.4.1.1.1">±0.008</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.50.50.50.5" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.50.50.50.5.1">0.887<sub class="ltx_sub" id="S4.T1.50.50.50.5.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.50.50.50.5.1.1.1">±0.008</span></sub></span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.55.55.55">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_tt" id="S4.T1.55.55.55.6" rowspan="10" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text" id="S4.T1.55.55.55.6.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T1.55.55.55.6.1.1" style="width:8.8pt;height:108.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:108.8pt;transform:translate(-50.0pt,-50.0pt) rotate(-90deg) ;">
<span class="ltx_p" id="S4.T1.55.55.55.6.1.1.1"><span class="ltx_text ltx_font_italic" id="S4.T1.55.55.55.6.1.1.1.1">Qwen2.5-VL-7B-Instruct</span></span>
</span></span></span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T1.55.55.55.7" style="padding-left:7.4pt;padding-right:7.4pt;">Prompt</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.51.51.51.1" style="padding-left:7.4pt;padding-right:7.4pt;">0.839<sub class="ltx_sub" id="S4.T1.51.51.51.1.1"><span class="ltx_text ltx_font_italic" id="S4.T1.51.51.51.1.1.1">±0.006</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.52.52.52.2" style="padding-left:7.4pt;padding-right:7.4pt;">0.821<sub class="ltx_sub" id="S4.T1.52.52.52.2.1"><span class="ltx_text ltx_font_italic" id="S4.T1.52.52.52.2.1.1">±0.024</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.53.53.53.3" style="padding-left:7.4pt;padding-right:7.4pt;">0.721<sub class="ltx_sub" id="S4.T1.53.53.53.3.1"><span class="ltx_text ltx_font_italic" id="S4.T1.53.53.53.3.1.1">±0.003</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.54.54.54.4" style="padding-left:7.4pt;padding-right:7.4pt;">0.710<sub class="ltx_sub" id="S4.T1.54.54.54.4.1"><span class="ltx_text ltx_font_italic" id="S4.T1.54.54.54.4.1.1">±0.003</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.55.55.55.5" style="padding-left:7.4pt;padding-right:7.4pt;">0.773<sub class="ltx_sub" id="S4.T1.55.55.55.5.1"><span class="ltx_text ltx_font_italic" id="S4.T1.55.55.55.5.1.1">±0.009</span></sub>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.60.60.60">
<td class="ltx_td ltx_align_left" id="S4.T1.60.60.60.6" style="padding-left:7.4pt;padding-right:7.4pt;">SFT</td>
<td class="ltx_td ltx_align_center" id="S4.T1.56.56.56.1" style="padding-left:7.4pt;padding-right:7.4pt;">0.885<sub class="ltx_sub" id="S4.T1.56.56.56.1.1"><span class="ltx_text ltx_font_italic" id="S4.T1.56.56.56.1.1.1">±0.003</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.57.57.57.2" style="padding-left:7.4pt;padding-right:7.4pt;">0.856<sub class="ltx_sub" id="S4.T1.57.57.57.2.1"><span class="ltx_text ltx_font_italic" id="S4.T1.57.57.57.2.1.1">±0.013</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.58.58.58.3" style="padding-left:7.4pt;padding-right:7.4pt;">0.808<sub class="ltx_sub" id="S4.T1.58.58.58.3.1"><span class="ltx_text ltx_font_italic" id="S4.T1.58.58.58.3.1.1">±0.005</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.59.59.59.4" style="padding-left:7.4pt;padding-right:7.4pt;">0.799<sub class="ltx_sub" id="S4.T1.59.59.59.4.1"><span class="ltx_text ltx_font_italic" id="S4.T1.59.59.59.4.1.1">±0.004</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.60.60.60.5" style="padding-left:7.4pt;padding-right:7.4pt;">0.837<sub class="ltx_sub" id="S4.T1.60.60.60.5.1"><span class="ltx_text ltx_font_italic" id="S4.T1.60.60.60.5.1.1">±0.006</span></sub>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.65.65.65">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.65.65.65.6" style="padding-left:7.4pt;padding-right:7.4pt;">GRPO (Token)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.61.61.61.1" style="padding-left:7.4pt;padding-right:7.4pt;">0.892<sub class="ltx_sub" id="S4.T1.61.61.61.1.1"><span class="ltx_text ltx_font_italic" id="S4.T1.61.61.61.1.1.1">±0.004</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.62.62.62.2" style="padding-left:7.4pt;padding-right:7.4pt;">0.840<sub class="ltx_sub" id="S4.T1.62.62.62.2.1"><span class="ltx_text ltx_font_italic" id="S4.T1.62.62.62.2.1.1">±0.014</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.63.63.63.3" style="padding-left:7.4pt;padding-right:7.4pt;">0.870<sub class="ltx_sub" id="S4.T1.63.63.63.3.1"><span class="ltx_text ltx_font_italic" id="S4.T1.63.63.63.3.1.1">±0.016</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.64.64.64.4" style="padding-left:7.4pt;padding-right:7.4pt;">0.851<sub class="ltx_sub" id="S4.T1.64.64.64.4.1"><span class="ltx_text ltx_font_italic" id="S4.T1.64.64.64.4.1.1">±0.012</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.65.65.65.5" style="padding-left:7.4pt;padding-right:7.4pt;">0.863<sub class="ltx_sub" id="S4.T1.65.65.65.5.1"><span class="ltx_text ltx_font_italic" id="S4.T1.65.65.65.5.1.1">±0.011</span></sub>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.70.70.70">
<td class="ltx_td ltx_align_left" id="S4.T1.70.70.70.6" style="padding-left:7.4pt;padding-right:7.4pt;">GRPO (Latent Action)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.66.66.66.1" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.66.66.66.1.1">0.920<sub class="ltx_sub" id="S4.T1.66.66.66.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.66.66.66.1.1.1.1">±0.005</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.67.67.67.2" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.67.67.67.2.1">0.872<sub class="ltx_sub" id="S4.T1.67.67.67.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.67.67.67.2.1.1.1">±0.016</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.68.68.68.3" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.68.68.68.3.1">0.898<sub class="ltx_sub" id="S4.T1.68.68.68.3.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.68.68.68.3.1.1.1">±0.009</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.69.69.69.4" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.69.69.69.4.1">0.852<sub class="ltx_sub" id="S4.T1.69.69.69.4.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.69.69.69.4.1.1.1">±0.010</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.70.70.70.5" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.70.70.70.5.1">0.885<sub class="ltx_sub" id="S4.T1.70.70.70.5.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.70.70.70.5.1.1.1">±0.010</span></sub></span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.75.75.75">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.75.75.75.6" style="padding-left:7.4pt;padding-right:7.4pt;">Dr.GRPO (Token)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.71.71.71.1" style="padding-left:7.4pt;padding-right:7.4pt;">0.892<sub class="ltx_sub" id="S4.T1.71.71.71.1.1"><span class="ltx_text ltx_font_italic" id="S4.T1.71.71.71.1.1.1">±0.006</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.72.72.72.2" style="padding-left:7.4pt;padding-right:7.4pt;">0.854<sub class="ltx_sub" id="S4.T1.72.72.72.2.1"><span class="ltx_text ltx_font_italic" id="S4.T1.72.72.72.2.1.1">±0.009</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.73.73.73.3" style="padding-left:7.4pt;padding-right:7.4pt;">0.854<sub class="ltx_sub" id="S4.T1.73.73.73.3.1"><span class="ltx_text ltx_font_italic" id="S4.T1.73.73.73.3.1.1">±0.006</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.74.74.74.4" style="padding-left:7.4pt;padding-right:7.4pt;">0.839<sub class="ltx_sub" id="S4.T1.74.74.74.4.1"><span class="ltx_text ltx_font_italic" id="S4.T1.74.74.74.4.1.1">±0.004</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.75.75.75.5" style="padding-left:7.4pt;padding-right:7.4pt;">0.860<sub class="ltx_sub" id="S4.T1.75.75.75.5.1"><span class="ltx_text ltx_font_italic" id="S4.T1.75.75.75.5.1.1">±0.006</span></sub>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.80.80.80">
<td class="ltx_td ltx_align_left" id="S4.T1.80.80.80.6" style="padding-left:7.4pt;padding-right:7.4pt;">Dr.GRPO (Latent Action)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.76.76.76.1" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.76.76.76.1.1">0.916<sub class="ltx_sub" id="S4.T1.76.76.76.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.76.76.76.1.1.1.1">±0.010</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.77.77.77.2" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.77.77.77.2.1">0.864<sub class="ltx_sub" id="S4.T1.77.77.77.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.77.77.77.2.1.1.1">±0.020</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.78.78.78.3" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.78.78.78.3.1">0.897<sub class="ltx_sub" id="S4.T1.78.78.78.3.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.78.78.78.3.1.1.1">±0.008</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.79.79.79.4" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.79.79.79.4.1">0.851<sub class="ltx_sub" id="S4.T1.79.79.79.4.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.79.79.79.4.1.1.1">±0.015</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.80.80.80.5" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.80.80.80.5.1">0.882<sub class="ltx_sub" id="S4.T1.80.80.80.5.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.80.80.80.5.1.1.1">±0.013</span></sub></span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.85.85.85">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.85.85.85.6" style="padding-left:7.4pt;padding-right:7.4pt;">DAPO (Token)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.81.81.81.1" style="padding-left:7.4pt;padding-right:7.4pt;">0.892<sub class="ltx_sub" id="S4.T1.81.81.81.1.1"><span class="ltx_text ltx_font_italic" id="S4.T1.81.81.81.1.1.1">±0.004</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.82.82.82.2" style="padding-left:7.4pt;padding-right:7.4pt;">0.842<sub class="ltx_sub" id="S4.T1.82.82.82.2.1"><span class="ltx_text ltx_font_italic" id="S4.T1.82.82.82.2.1.1">±0.025</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.83.83.83.3" style="padding-left:7.4pt;padding-right:7.4pt;">0.844<sub class="ltx_sub" id="S4.T1.83.83.83.3.1"><span class="ltx_text ltx_font_italic" id="S4.T1.83.83.83.3.1.1">±0.013</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.84.84.84.4" style="padding-left:7.4pt;padding-right:7.4pt;">0.828<sub class="ltx_sub" id="S4.T1.84.84.84.4.1"><span class="ltx_text ltx_font_italic" id="S4.T1.84.84.84.4.1.1">±0.007</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.85.85.85.5" style="padding-left:7.4pt;padding-right:7.4pt;">0.852<sub class="ltx_sub" id="S4.T1.85.85.85.5.1"><span class="ltx_text ltx_font_italic" id="S4.T1.85.85.85.5.1.1">±0.012</span></sub>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.90.90.90">
<td class="ltx_td ltx_align_left" id="S4.T1.90.90.90.6" style="padding-left:7.4pt;padding-right:7.4pt;">DAPO (Latent Action)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.86.86.86.1" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.86.86.86.1.1">0.920<sub class="ltx_sub" id="S4.T1.86.86.86.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.86.86.86.1.1.1.1">±0.009</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.87.87.87.2" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.87.87.87.2.1">0.863<sub class="ltx_sub" id="S4.T1.87.87.87.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.87.87.87.2.1.1.1">±0.017</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.88.88.88.3" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.88.88.88.3.1">0.903<sub class="ltx_sub" id="S4.T1.88.88.88.3.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.88.88.88.3.1.1.1">±0.012</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.89.89.89.4" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.89.89.89.4.1">0.850<sub class="ltx_sub" id="S4.T1.89.89.89.4.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.89.89.89.4.1.1.1">±0.005</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.90.90.90.5" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.90.90.90.5.1">0.884<sub class="ltx_sub" id="S4.T1.90.90.90.5.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.90.90.90.5.1.1.1">±0.011</span></sub></span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.95.95.95">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.95.95.95.6" style="padding-left:7.4pt;padding-right:7.4pt;">BNPO (Token)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.91.91.91.1" style="padding-left:7.4pt;padding-right:7.4pt;">0.894<sub class="ltx_sub" id="S4.T1.91.91.91.1.1"><span class="ltx_text ltx_font_italic" id="S4.T1.91.91.91.1.1.1">±0.004</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.92.92.92.2" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.92.92.92.2.1">0.859<sub class="ltx_sub" id="S4.T1.92.92.92.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.92.92.92.2.1.1.1">±0.029</span></sub></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.93.93.93.3" style="padding-left:7.4pt;padding-right:7.4pt;">0.850<sub class="ltx_sub" id="S4.T1.93.93.93.3.1"><span class="ltx_text ltx_font_italic" id="S4.T1.93.93.93.3.1.1">±0.007</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.94.94.94.4" style="padding-left:7.4pt;padding-right:7.4pt;">0.836<sub class="ltx_sub" id="S4.T1.94.94.94.4.1"><span class="ltx_text ltx_font_italic" id="S4.T1.94.94.94.4.1.1">±0.004</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.95.95.95.5" style="padding-left:7.4pt;padding-right:7.4pt;">0.860<sub class="ltx_sub" id="S4.T1.95.95.95.5.1"><span class="ltx_text ltx_font_italic" id="S4.T1.95.95.95.5.1.1">±0.011</span></sub>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.100.100.100">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T1.100.100.100.6" style="padding-left:7.4pt;padding-right:7.4pt;">BNPO (Latent Action)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.96.96.96.1" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.96.96.96.1.1">0.916<sub class="ltx_sub" id="S4.T1.96.96.96.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.96.96.96.1.1.1.1">±0.006</span></sub></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.97.97.97.2" style="padding-left:7.4pt;padding-right:7.4pt;">0.842<sub class="ltx_sub" id="S4.T1.97.97.97.2.1"><span class="ltx_text ltx_font_italic" id="S4.T1.97.97.97.2.1.1">±0.018</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.98.98.98.3" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.98.98.98.3.1">0.901<sub class="ltx_sub" id="S4.T1.98.98.98.3.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.98.98.98.3.1.1.1">±0.009</span></sub></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.99.99.99.4" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.99.99.99.4.1">0.852<sub class="ltx_sub" id="S4.T1.99.99.99.4.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.99.99.99.4.1.1.1">±0.012</span></sub></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.100.100.100.5" style="padding-left:7.4pt;padding-right:7.4pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.100.100.100.5.1">0.878<sub class="ltx_sub" id="S4.T1.100.100.100.5.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T1.100.100.100.5.1.1.1">±0.011</span></sub></span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Performance comparison on MMRole and PCogAlignBench, using the <span class="ltx_text ltx_font_italic" id="S4.T1.103.1">LLM-as-a-Judge</span> metric. Results are averaged over three runs. We conduct experiments using various VLMs, including Qwen2.5-VL-3B-Instruct and Qwen2.5-VL-7B-Instruct. Best results are in <span class="ltx_text ltx_font_bold" id="S4.T1.104.2">bold</span> on each RL algorithm.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Baselines</h5>
<div class="ltx_para" id="S4.SS1.SSS0.Px4.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px4.p1.1">We consider two categories of baselines: 1) Non-RL baselines: the naive <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px4.p1.1.1">Prompt</span> and supervised fine-tuning (<span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px4.p1.1.2">SFT</span>); 2) RL-based methods, where we compare two optimization strategies, token-level and latent action RL, using four algorithms: a) Group Relative Policy Optimization (<span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px4.p1.1.3">GRPO</span>) <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">shao-2024-GRPO</span></cite>, b) <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px4.p1.1.4">Dr. GRPO</span> <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">liu-2025-DRGRPO</span></cite>, c) Decoupled Clip and Dynamic Sampling Policy Optimization (<span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px4.p1.1.5">DAPO</span>) <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">yu-2025-dapo</span></cite>, and d) Beta Normalization Policy Optimization (<span class="ltx_text ltx_font_bold" id="S4.SS1.SSS0.Px4.p1.1.6">BNPO</span>) <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">xiao-2025-BNPO</span></cite>. The reward functions are kept the same for methods. Please refer to the Appendix <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A2" title="Appendix B Experimental Details ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">B</span></a> for more experimental details.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Main Results</h3>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Overall Performance</h5>
<div class="ltx_para" id="S4.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px1.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S4.T1" title="Table 1 ‣ Evaluation Metrics ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">1</span></a> reports the experimental results of token-level baselines and our proposed latent action level RL. Based on these results, we have made the following observations.
1) Our method achieves superior performance across diverse tasks and datasets. On average, it outperforms token-level RL by 4% (averaged over all settings).
2) Our latent action framework is RL-agnostic and readily compatible with diverse policy optimization algorithms, including GRPO, Dr. GRPO, DAPO, and BNPO, yielding consistent gains over baselines.
3) The improvements brought by latent actions are consistently observed in both 3B and 7B models, demonstrating the scalability of our approach.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Performance on Fine-grained Dimensions</h5>
<div class="ltx_para" id="S4.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p1.1">To thoroughly evaluate the performance of multimodal conversational agents trained with latent actions across various fine-grained conversational dimensions, following prior work <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">dai-2025-mmrole</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">li-2025-aligning</span></cite>, we assess eight dimensions on <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSS0.Px2.p1.1.1">MMRole</span>: 1) Instruction Adherence (IA), 2) Fluency (Flu), 3) Coherency (Coh), 4) Image-Text Relevance (ITR), 5) Response Accuracy (RA), 6) Personality Consistency (OC), 7) Knowledge Consistency (KC), and 8) Tone Consistency (TC).
On <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSS0.Px2.p1.1.2">PCogAlignBench</span>, we evaluate: 1) Role-Set Awareness (RSA), 2) Body Behavior Awareness (BBA), 3) Mind Feelings Awareness (MFA), 4) Contextual Awareness (CA), and 5) Conversational Flow (CF).
We present the comparison results in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S4.F4" title="Figure 4 ‣ Performance on Fine-grained Dimensions ‣ 4.2 Main Results ‣ 4 Experiments ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">4</span></a>, with detailed results provided in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A3.SS2" title="C.2 Detailed Results on Fine-grained Dimensions ‣ Appendix C Additional Empirical Results ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">C.2</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS0.Px2.p2">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p2.1">As shown in Figure 4, we make the following observations:
1) Overall, our methods outperform token-level baselines across all evaluated dimensions.
2) While both our method and the baselines achieve strong performance on basic conversational capabilities, such as Fluency (Flu) and Conversational Flow (CF), our approach demonstrates substantially more pronounced improvements on more challenging personalized dimensions, such as Tone Consistency (TC) on <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSS0.Px2.p2.1.1">MMRole</span>.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="205" id="S4.F4.g1" src="x4.png" width="324"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Fine-grained performance comparison on (a) MMRole and (b) PCogAlignBench. Results using latent actions are shown with dashed lines, while results using token-level RL are plotted with solid lines.</figcaption>
</figure>
<figure class="ltx_table" id="S4.T2">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T2.20" style="width:433.6pt;height:65.3pt;vertical-align:-30.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-27.5pt,4.1pt) scale(0.887279404627093,0.887279404627093) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.20.20">
<tr class="ltx_tr" id="S4.T2.20.20.21">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.20.20.21.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T2.20.20.21.1.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T2.20.20.21.2"><span class="ltx_text ltx_font_bold" id="S4.T2.20.20.21.2.1">MMRole</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T2.20.20.21.3"><span class="ltx_text ltx_font_bold" id="S4.T2.20.20.21.3.1">PCogAlignBench</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.20.20.21.4" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T2.20.20.21.4.1">Avg.</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.20.20.22">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.20.20.22.1"><span class="ltx_text ltx_font_italic" id="S4.T2.20.20.22.1.1">ID</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.20.20.22.2"><span class="ltx_text ltx_font_italic" id="S4.T2.20.20.22.2.1">OOD</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.20.20.22.3"><span class="ltx_text ltx_font_italic" id="S4.T2.20.20.22.3.1">LS1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.20.20.22.4"><span class="ltx_text ltx_font_italic" id="S4.T2.20.20.22.4.1">LS2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.5.5.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.5.5.5.6"><span class="ltx_text ltx_font_bold" id="S4.T2.5.5.5.6.1">Ours</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.1">0.949<sub class="ltx_sub" id="S4.T2.1.1.1.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T2.1.1.1.1.1.1.1">±0.007</span></sub></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.2.2.2.2"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.2.2.1">0.915<sub class="ltx_sub" id="S4.T2.2.2.2.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T2.2.2.2.2.1.1.1">±0.065</span></sub></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.3.3.3.3"><span class="ltx_text ltx_font_bold" id="S4.T2.3.3.3.3.1">0.871<sub class="ltx_sub" id="S4.T2.3.3.3.3.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T2.3.3.3.3.1.1.1">±0.011</span></sub></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.4.4.4"><span class="ltx_text ltx_font_bold" id="S4.T2.4.4.4.4.1">0.837<sub class="ltx_sub" id="S4.T2.4.4.4.4.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T2.4.4.4.4.1.1.1">±0.010</span></sub></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.5.5.5.5"><span class="ltx_text ltx_font_bold" id="S4.T2.5.5.5.5.1">0.893<sub class="ltx_sub" id="S4.T2.5.5.5.5.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T2.5.5.5.5.1.1.1">±0.023</span></sub></span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.10.10.10">
<td class="ltx_td ltx_align_left" id="S4.T2.10.10.10.6">Ours <span class="ltx_text ltx_font_italic" id="S4.T2.10.10.10.6.1">w/o cycle consistency</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.6.6.6.1">0.921<sub class="ltx_sub" id="S4.T2.6.6.6.1.1"><span class="ltx_text ltx_font_italic" id="S4.T2.6.6.6.1.1.1">±0.005</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.7.7.7.2">0.878<sub class="ltx_sub" id="S4.T2.7.7.7.2.1"><span class="ltx_text ltx_font_italic" id="S4.T2.7.7.7.2.1.1">±0.023</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.8.8.8.3">0.858<sub class="ltx_sub" id="S4.T2.8.8.8.3.1"><span class="ltx_text ltx_font_italic" id="S4.T2.8.8.8.3.1.1">±0.007</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.9.9.9.4">0.825<sub class="ltx_sub" id="S4.T2.9.9.9.4.1"><span class="ltx_text ltx_font_italic" id="S4.T2.9.9.9.4.1.1">±0.013</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.10.10.10.5">0.870<sub class="ltx_sub" id="S4.T2.10.10.10.5.1"><span class="ltx_text ltx_font_italic" id="S4.T2.10.10.10.5.1.1">±0.012</span></sub>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.15.15.15">
<td class="ltx_td ltx_align_left" id="S4.T2.15.15.15.6">Ours <span class="ltx_text ltx_font_italic" id="S4.T2.15.15.15.6.1">w/o cross-modal projector</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.11.11.11.1">0.944<sub class="ltx_sub" id="S4.T2.11.11.11.1.1"><span class="ltx_text ltx_font_italic" id="S4.T2.11.11.11.1.1.1">±0.014</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.12.12.12.2">0.901<sub class="ltx_sub" id="S4.T2.12.12.12.2.1"><span class="ltx_text ltx_font_italic" id="S4.T2.12.12.12.2.1.1">±0.014</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.13.13.13.3">0.858<sub class="ltx_sub" id="S4.T2.13.13.13.3.1"><span class="ltx_text ltx_font_italic" id="S4.T2.13.13.13.3.1.1">±0.010</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.14.14.14.4">0.819<sub class="ltx_sub" id="S4.T2.14.14.14.4.1"><span class="ltx_text ltx_font_italic" id="S4.T2.14.14.14.4.1.1">±0.013</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.15.15.15.5">0.880<sub class="ltx_sub" id="S4.T2.15.15.15.5.1"><span class="ltx_text ltx_font_italic" id="S4.T2.15.15.15.5.1.1">±0.013</span></sub>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.20.20.20">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T2.20.20.20.6">Ours <span class="ltx_text ltx_font_italic" id="S4.T2.20.20.20.6.1">w/o text-only data</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.16.16.16.1">0.932<sub class="ltx_sub" id="S4.T2.16.16.16.1.1"><span class="ltx_text ltx_font_italic" id="S4.T2.16.16.16.1.1.1">±0.010</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.17.17.17.2">0.861<sub class="ltx_sub" id="S4.T2.17.17.17.2.1"><span class="ltx_text ltx_font_italic" id="S4.T2.17.17.17.2.1.1">±0.036</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.18.18.18.3">0.851<sub class="ltx_sub" id="S4.T2.18.18.18.3.1"><span class="ltx_text ltx_font_italic" id="S4.T2.18.18.18.3.1.1">±0.007</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.19.19.19.4">0.817<sub class="ltx_sub" id="S4.T2.19.19.19.4.1"><span class="ltx_text ltx_font_italic" id="S4.T2.19.19.19.4.1.1">±0.006</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.20.20.20.5">0.865<sub class="ltx_sub" id="S4.T2.20.20.20.5.1"><span class="ltx_text ltx_font_italic" id="S4.T2.20.20.20.5.1.1">±0.015</span></sub>
</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Ablation study on main components of our method. We evaluate on MMRole and PCogAlignBench using the <span class="ltx_text ltx_font_italic" id="S4.T2.23.1">LLM-as-a-Judge</span> metric. Results are averaged over three runs. All variants are fine-tuned with GRPO based on Qwen2.5-VL-3B-Instruct. Best results are in <span class="ltx_text ltx_font_bold" id="S4.T2.24.2">bold</span>.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Ablation Study</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.2">To assess the contribution of main components in our method, we conduct ablation study using three variants. 1) <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.2.1">Ours w/o cycle consistency</span>: We remove the cycle consistency loss during cross-modal projector training, and instead directly apply the projector trained only on paired image-text data, i.e., removing <math alttext="\mathcal{L}_{\text{cycle}}" class="ltx_Math" display="inline" id="S4.SS3.p1.1.m1" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>cycle</mtext></msub><annotation encoding="application/x-tex">\mathcal{L}_{\text{cycle}}</annotation></semantics></math> in Eq. <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S3.E6" title="Equation 6 ‣ Cross-modal Projector Training ‣ 3.2.1 Inverse Dynamics Learning ‣ 3.2 Latent Action Space Learning ‣ 3 Methodology ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">6</span></a>; 2) <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.2.2">Ours w/o cross-modal projector</span>: We remove the cross-modal projector entirely, and learn the latent action codebook directly from text-only representations <math alttext="e^{T}" class="ltx_Math" display="inline" id="S4.SS3.p1.2.m2" intent=":literal"><semantics><msup><mi>e</mi><mi>T</mi></msup><annotation encoding="application/x-tex">e^{T}</annotation></semantics></math>; 3) <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.2.3">Ours w/o text-only data</span>: We construct the latent action space using only the limited paired multimodal corpus, excluding all text-only data. The results of ablation study are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S4.T2" title="Table 2 ‣ Performance on Fine-grained Dimensions ‣ 4.2 Main Results ‣ 4 Experiments ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">From Table <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S4.T2" title="Table 2 ‣ Performance on Fine-grained Dimensions ‣ 4.2 Main Results ‣ 4 Experiments ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">2</span></a>, we can make the following observations.
1) Removing the cycle consistency loss leads to an average performance drop of 2.3%, indicating that fine-tuning the projector on large-scale text-only data via cycle consistency loss is crucial for improving its robustness.
2) Eliminating the cross-modal projector causes a noticeable decline in performance. This suggests that directly learning the latent action space from text-only embeddings may introduce a unimodal bias, i.e., the trained latent action policy model overly relies textual representations and fail to effectively handle multimodal scenarios.
3) Solely leveraging paired multimodal data results in the largest performance degradation, particularly in out-of-distribution settings (e.g., <span class="ltx_text ltx_font_typewriter" id="S4.SS3.p2.1.1">OOD</span> on <span class="ltx_text ltx_font_typewriter" id="S4.SS3.p2.1.2">MMRole</span> and <span class="ltx_text ltx_font_typewriter" id="S4.SS3.p2.1.3">LS2</span> on <span class="ltx_text ltx_font_typewriter" id="S4.SS3.p2.1.4">PCogAlignBench</span>). This highlights that the limited diversity and coverage of paired image-text corpora constrain the generalization capability of latent action policy models.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Analysis</h3>
<section class="ltx_paragraph" id="S4.SS4.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Rollout Diversity with Latent Actions</h5>
<div class="ltx_para" id="S4.SS4.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS4.SSS0.Px1.p1.1">Benefiting from the reduced action space, the constructed latent action space is expected to improve the agent’s rollout diversity during RL exploration, i.e., generating more diverse responses. Prior work has shown that such diversity is critical for improving the upper bound of RL performance <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">li-2025-preserving</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">yu-2025-dapo</span></cite>.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS0.Px1.p2">
<p class="ltx_p" id="S4.SS4.SSS0.Px1.p2.6">Following <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">jia-2025-controlling</span></cite>, we quantify rollout diversity via <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS0.Px1.p2.6.1">semantic diversity</span>, as it reflects both linguistic diversity and response quality.
Concretely, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S3.F3" title="Figure 3 ‣ 3.3 Latent Action Reinforcement Learning ‣ 3 Methodology ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">3</span></a>, for each prompt <math alttext="(x^{T},x^{T_{1:p}})" class="ltx_Math" display="inline" id="S4.SS4.SSS0.Px1.p2.1.m1" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mi>T</mi></msup><mo>,</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>p</mi></mrow></msub></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x^{T},x^{T_{1:p}})</annotation></semantics></math> in the RL training set <math alttext="\mathcal{D}_{\text{RL}}" class="ltx_Math" display="inline" id="S4.SS4.SSS0.Px1.p2.2.m2" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒟</mi><mtext>RL</mtext></msub><annotation encoding="application/x-tex">\mathcal{D}_{\text{RL}}</annotation></semantics></math>, the agent generates <math alttext="G" class="ltx_Math" display="inline" id="S4.SS4.SSS0.Px1.p2.3.m3" intent=":literal"><semantics><mi>G</mi><annotation encoding="application/x-tex">G</annotation></semantics></math> responses <math alttext="\{x^{T_{p+1:m,i}}\}_{i=1}^{G}" class="ltx_Math" display="inline" id="S4.SS4.SSS0.Px1.p2.4.m4" intent=":literal"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mrow><mi>p</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>m</mi><mo>,</mo><mi>i</mi></mrow></mrow></msub></msup><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>G</mi></msubsup><annotation encoding="application/x-tex">\{x^{T_{p+1:m,i}}\}_{i=1}^{G}</annotation></semantics></math>, with <math alttext="p" class="ltx_Math" display="inline" id="S4.SS4.SSS0.Px1.p2.5.m5" intent=":literal"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math> as the prompt length and <math alttext="m" class="ltx_Math" display="inline" id="S4.SS4.SSS0.Px1.p2.6.m6" intent=":literal"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math> as the maximum length.
We calculate the semantic diversity as:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E10">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\frac{G(G-1)}{\sum_{i=1}^{G}\sum_{\begin{subarray}{c}j=1,j\neq i\end{subarray}}^{G}\text{Sim}(x^{T_{p+1:m,i}},x^{T_{p+1:m,j}})}," class="ltx_Math" display="block" id="S4.E10.m1" intent=":literal"><semantics><mrow><mfrac><mrow><mi>G</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>G</mi><mo>−</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>G</mi></msubsup><mrow><msubsup><mo lspace="0.167em">∑</mo><mtable><mtr><mtd><mrow><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mo>,</mo><mrow><mi>j</mi><mo>≠</mo><mi>i</mi></mrow></mrow></mtd></mtr></mtable><mi>G</mi></msubsup><mrow><mtext>Sim</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mrow><mi>p</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>m</mi><mo>,</mo><mi>i</mi></mrow></mrow></msub></msup><mo>,</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mrow><mi>p</mi><mo>+</mo><mn>1</mn></mrow><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>m</mi><mo>,</mo><mi>j</mi></mrow></mrow></msub></msup><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow></mfrac><mo>,</mo></mrow><annotation encoding="application/x-tex">\frac{G(G-1)}{\sum_{i=1}^{G}\sum_{\begin{subarray}{c}j=1,j\neq i\end{subarray}}^{G}\text{Sim}(x^{T_{p+1:m,i}},x^{T_{p+1:m,j}})},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(10)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS4.SSS0.Px1.p2.7">where <math alttext="\text{Sim}(\cdot,\cdot)" class="ltx_Math" display="inline" id="S4.SS4.SSS0.Px1.p2.7.m1" intent=":literal"><semantics><mrow><mtext>Sim</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo rspace="0em">,</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{Sim}(\cdot,\cdot)</annotation></semantics></math> denotes the embedding similarity between two responses and we adopt BGE-M3 <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">chen-2024-m3</span></cite> as the embedding model.
We report the average semantic diversity over all samples in the RL training set.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS0.Px1.p3">
<p class="ltx_p" id="S4.SS4.SSS0.Px1.p3.1">In Table <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S4.T3" title="Table 3 ‣ Rollout Diversity with Latent Actions ‣ 4.4 Analysis ‣ 4 Experiments ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">3</span></a>, we compare the rollout diversity of token based and latent action based RL algorithms. From Table <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S4.T3" title="Table 3 ‣ Rollout Diversity with Latent Actions ‣ 4.4 Analysis ‣ 4 Experiments ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">3</span></a>, we observe that latent action RL consistently and significantly outperforms token-level RL in rollout diversity, demonstrating the superior exploration efficiency.
We also provide a case study in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A3.SS3" title="C.3 Case Study ‣ Appendix C Additional Empirical Results ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">C.3</span></a> to illustrate the improvements in rollout diversity intuitively.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T3.16" style="width:433.6pt;height:161.6pt;vertical-align:-77.1pt;"><span class="ltx_transformed_inner" style="transform:translate(68.7pt,-25.6pt) scale(1.4638691507116,1.4638691507116) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.16.16">
<tr class="ltx_tr" id="S4.T3.16.16.17">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T3.16.16.17.1"><span class="ltx_text ltx_font_bold" id="S4.T3.16.16.17.1.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.16.16.17.2"><span class="ltx_text ltx_font_bold" id="S4.T3.16.16.17.2.1">MMRole</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.16.16.17.3"><span class="ltx_text ltx_font_bold" id="S4.T3.16.16.17.3.1">PCogAlignBench</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.2.2.2.3">GRPO (Token)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.1.1">1.079<sub class="ltx_sub" id="S4.T3.1.1.1.1.1"><span class="ltx_text ltx_font_italic" id="S4.T3.1.1.1.1.1.1">±0.230</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.2.2.2.2">1.043<sub class="ltx_sub" id="S4.T3.2.2.2.2.1"><span class="ltx_text ltx_font_italic" id="S4.T3.2.2.2.2.1.1">±0.197</span></sub>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.4.4">
<td class="ltx_td ltx_align_left" id="S4.T3.4.4.4.3">GRPO (Latent Action)</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.3.3.1"><span class="ltx_text ltx_font_bold" id="S4.T3.3.3.3.1.1">1.256<sub class="ltx_sub" id="S4.T3.3.3.3.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T3.3.3.3.1.1.1.1">±0.302</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.4.4.2"><span class="ltx_text ltx_font_bold" id="S4.T3.4.4.4.2.1">1.200<sub class="ltx_sub" id="S4.T3.4.4.4.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T3.4.4.4.2.1.1.1">±0.326</span></sub></span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.6.6.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.6.6.6.3">Dr.GRPO (Token)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.5.5.5.1">1.074<sub class="ltx_sub" id="S4.T3.5.5.5.1.1"><span class="ltx_text ltx_font_italic" id="S4.T3.5.5.5.1.1.1">±0.224</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.6.6.6.2">1.259<sub class="ltx_sub" id="S4.T3.6.6.6.2.1"><span class="ltx_text ltx_font_italic" id="S4.T3.6.6.6.2.1.1">±0.249</span></sub>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.8.8.8">
<td class="ltx_td ltx_align_left" id="S4.T3.8.8.8.3">Dr.GRPO (Latent Action)</td>
<td class="ltx_td ltx_align_center" id="S4.T3.7.7.7.1"><span class="ltx_text ltx_font_bold" id="S4.T3.7.7.7.1.1">1.252<sub class="ltx_sub" id="S4.T3.7.7.7.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T3.7.7.7.1.1.1.1">±0.297</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.8.8.8.2"><span class="ltx_text ltx_font_bold" id="S4.T3.8.8.8.2.1">1.323<sub class="ltx_sub" id="S4.T3.8.8.8.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T3.8.8.8.2.1.1.1">±0.278</span></sub></span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.10.10.10">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.10.10.10.3">DAPO (Token)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.9.9.9.1">1.075<sub class="ltx_sub" id="S4.T3.9.9.9.1.1"><span class="ltx_text ltx_font_italic" id="S4.T3.9.9.9.1.1.1">±0.230</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.10.10.10.2">1.040<sub class="ltx_sub" id="S4.T3.10.10.10.2.1"><span class="ltx_text ltx_font_italic" id="S4.T3.10.10.10.2.1.1">±0.182</span></sub>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.12.12.12">
<td class="ltx_td ltx_align_left" id="S4.T3.12.12.12.3">DAPO (Latent Action)</td>
<td class="ltx_td ltx_align_center" id="S4.T3.11.11.11.1"><span class="ltx_text ltx_font_bold" id="S4.T3.11.11.11.1.1">1.254<sub class="ltx_sub" id="S4.T3.11.11.11.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T3.11.11.11.1.1.1.1">±0.302</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.12.12.12.2"><span class="ltx_text ltx_font_bold" id="S4.T3.12.12.12.2.1">1.131<sub class="ltx_sub" id="S4.T3.12.12.12.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T3.12.12.12.2.1.1.1">±0.253</span></sub></span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.14.14.14">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.14.14.14.3">BNPO (Token)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.13.13.13.1">1.078<sub class="ltx_sub" id="S4.T3.13.13.13.1.1"><span class="ltx_text ltx_font_italic" id="S4.T3.13.13.13.1.1.1">±0.224</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.14.14.14.2">1.261<sub class="ltx_sub" id="S4.T3.14.14.14.2.1"><span class="ltx_text ltx_font_italic" id="S4.T3.14.14.14.2.1.1">±0.257</span></sub>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.16.16.16">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T3.16.16.16.3">BNPO (Latent Action)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.15.15.15.1"><span class="ltx_text ltx_font_bold" id="S4.T3.15.15.15.1.1">1.297<sub class="ltx_sub" id="S4.T3.15.15.15.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T3.15.15.15.1.1.1.1">±0.303</span></sub></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.16.16.16.2"><span class="ltx_text ltx_font_bold" id="S4.T3.16.16.16.2.1">1.321<sub class="ltx_sub" id="S4.T3.16.16.16.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="S4.T3.16.16.16.2.1.1.1">±0.286</span></sub></span></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Rollout diversity during RL exploration. Higher values indicate better rollout diversity. Best results are in <span class="ltx_text ltx_font_bold" id="S4.T3.18.1">bold</span>.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S4.SS4.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Computational Budget</h5>
<div class="ltx_para" id="S4.SS4.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS4.SSS0.Px2.p1.1">To assess the computational overhead introduced by our latent action framework, we analyze the time cost during RL training.
Specifically, we consider the time cost in two stages: 1) <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS0.Px2.p1.1.1">Rollout</span>: generating multiple candidate responses per prompt; 2) <span class="ltx_text ltx_font_italic" id="S4.SS4.SSS0.Px2.p1.1.2">Policy update</span>: updating the policy model using the computed rewards.
We present the time cost per RL step of our method and the baseline in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S4.F5" title="Figure 5 ‣ Computational Budget ‣ 4.4 Analysis ‣ 4 Experiments ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">5</span></a>, using GRPO as an example with a rollout batch size of 8.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS0.Px2.p2">
<p class="ltx_p" id="S4.SS4.SSS0.Px2.p2.1">As illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S4.F5" title="Figure 5 ‣ Computational Budget ‣ 4.4 Analysis ‣ 4 Experiments ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">5</span></a>, our latent action based method incurs a 1.13× slowdown in rollout time, due to the additional latent action prediction step.
However, policy updates in latent action RL require only 0.86× the time of the baseline, as the optimization involves adjusting the policy’s output distribution over a compact latent action space, rather than the full token vocabulary.
Overall, the total RL training time is only 1.08× that of token-level RL.</p>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="155" id="S4.F5.g1" src="x5.png" width="317"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Time cost per step during RL training, including rollout, policy update, and total time.</figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Related Work</h2>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Multimodal Conversational Agents</h5>
<div class="ltx_para" id="S5.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px1.p1.1">Recent advances in vision-language models (VLMs) <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">bai-2025-qwen25vl</span></cite> have enabled increasingly capable multimodal conversational agents (MCAs) <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">yao-2025-MLLMAgentsurvey</span></cite>, such as multimodal role-playing agents <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">dai-2025-mmrole</span></cite> and personalized assistants <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">nguyen-2024-yo</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">li-2025-aligning</span></cite>, which hold significant promise in fields like entertainment <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">mehta-2022-exploring</span></cite> and personalized education <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">griol-2014-developing</span></cite>.
Initial efforts to build MCAs primarily rely on supervised fine-tuning <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">lillava-2024-TMLR</span></cite>, but often suffer from poor generalization.
Recently, RL has been widely explored for fine-tuning MCAs and has demonstrated strong generalization performance <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">zhou-2025-reinforcedMLLM</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">chu-2025-sftRL</span></cite>.
However, fine-tuning MCAs via RL faces challenges in handling the extremely large text token space.
To address this, we propose constructing a compact latent action space for RL fine-tuning, which enables efficient policy learning.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Reinforcement Learning with Latent Actions</h5>
<div class="ltx_para" id="S5.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px2.p1.1">In many real-world scenarios, only observation-only data are available, such as expert demonstration videos of robots where explicit action labels are missing <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">Torabi-2019-RecentImitation</span></cite>.
To address this challenge, prior works leverage the learning from observation mechanism <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">seo-2022-reinforcement</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">baker-2022-video</span></cite> to infer latent actions from observation-only data, which are then used for RL fine-tuning of agents.
For instance, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">zhang-2024-whale</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">gao-2025-adaworld</span></cite> learn latent actions from videos to control video generation, while <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">ye-2025-latent</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">bu-2025-univla</span></cite> extract latent actions from robot manipulation videos and use them for robot policy learning.
These constructed latent actions not only enhance controllability <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">bruce-2024-genie</span></cite> but also enable better transferability across different tasks due to their higher-level nature <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">jang-2025-dreamgen</span></cite>.</p>
</div>
<div class="ltx_para" id="S5.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="S5.SS0.SSS0.Px2.p2.1">The most relevant work to ours is CoLA <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">jia-2025-controlling</span></cite>, which introduces latent actions into RL fine-tuning of LLMs.
However, when constructing the latent action space for multimodal conversational agents, the scarcity of paired image-text data hinders learning a latent space with sufficient coverage.
To overcome this, we leverage both paired image-text data and massive text-only data to construct the latent space, using a cross-modal projector trained with a novel cycle-consistency loss.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this work, we propose to learn a compact latent action space for reinforcement learning (RL) fine-tuning of multimodal conversational agents (MCAs).
To construct this latent space, we leverage both paired image-text data and abundant text-only data, using a cross-modal projector trained with a novel cycle-consistency loss, which improves the coverage of latent actions while avoiding potentially unimodal bias.
We evaluate our approach on two tasks, including multimodal role-playing and multimodal personalized conversation, and demonstrate significant improvements over competitive baselines across various RL algorithms.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Limitations</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">We acknowledge the following limitations in our work.
First, the additional latent action prediction step increases RL training time by 1.08× and inference latency by 1.13×.
Second, due to constraints of computational resources, we evaluate our latent action based approach on multimodal conversational tasks, and leave validation on more tasks to future work, such as visual mathematical reasoning.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Ethics Considerations</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">Our work is entirely at the methodological level, which means that there will not be any negative social impacts.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para" id="Sx2.p2">
<p class="ltx_p" id="Sx2.p2.1"></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Details on Model Design</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Language World Model</h3>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.4">The language world model <math alttext="f_{\text{world}}(x^{T_{t+1}}\mid x^{V},x^{T_{1:t}},a_{t})" class="ltx_Math" display="inline" id="A1.SS1.p1.1.m1" intent=":literal"><semantics><mrow><msub><mi>f</mi><mtext>world</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>x</mi><msub><mi>T</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></msup><mo>∣</mo><mrow><msup><mi>x</mi><mi>V</mi></msup><mo>,</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub></msup><mo>,</mo><msub><mi>a</mi><mi>t</mi></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">f_{\text{world}}(x^{T_{t+1}}\mid x^{V},x^{T_{1:t}},a_{t})</annotation></semantics></math> predicts the next token <math alttext="x^{T_{t+1}}" class="ltx_Math" display="inline" id="A1.SS1.p1.2.m2" intent=":literal"><semantics><msup><mi>x</mi><msub><mi>T</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></msup><annotation encoding="application/x-tex">x^{T_{t+1}}</annotation></semantics></math> autoregressively given the current multimodal context <math alttext="(x^{V},x^{T_{1:t}})" class="ltx_Math" display="inline" id="A1.SS1.p1.3.m3" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mi>V</mi></msup><mo>,</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x^{V},x^{T_{1:t}})</annotation></semantics></math> and a latent action <math alttext="a_{t}" class="ltx_Math" display="inline" id="A1.SS1.p1.4.m4" intent=":literal"><semantics><msub><mi>a</mi><mi>t</mi></msub><annotation encoding="application/x-tex">a_{t}</annotation></semantics></math> predicted by the inverse dynamics model (during the latent action space learning) or the policy model (during latent action RL and inference). It consists of two core modules, reusing some components from the original VLM:</p>
</div>
<section class="ltx_paragraph" id="A1.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Encode Module</h5>
<div class="ltx_para" id="A1.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="A1.SS1.SSS0.Px1.p1.2">This module encodes the input <math alttext="(x^{V},x^{T_{1:t}})" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px1.p1.1.m1" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mi>V</mi></msup><mo>,</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x^{V},x^{T_{1:t}})</annotation></semantics></math> into a context embedding <math alttext="e^{V,T}_{t}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px1.p1.2.m2" intent=":literal"><semantics><mrow><msubsup><mi>e</mi><mi>t</mi><mrow><mi>V</mi><mo>,</mo><mi>T</mi></mrow></msubsup><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">e^{V,T}_{t}\in\mathbb{R}^{d}</annotation></semantics></math>, using the transformer blocks of the original VLM.</p>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Merge Module</h5>
<div class="ltx_para" id="A1.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="A1.SS1.SSS0.Px2.p1.12">This module fuses the context embedding <math alttext="e^{V,T}_{t}" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px2.p1.1.m1" intent=":literal"><semantics><msubsup><mi>e</mi><mi>t</mi><mrow><mi>V</mi><mo>,</mo><mi>T</mi></mrow></msubsup><annotation encoding="application/x-tex">e^{V,T}_{t}</annotation></semantics></math> and the latent action embedding <math alttext="c_{a_{t}}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px2.p1.2.m2" intent=":literal"><semantics><mrow><msub><mi>c</mi><msub><mi>a</mi><mi>t</mi></msub></msub><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">c_{a_{t}}\in\mathbb{R}^{d}</annotation></semantics></math> (where <math alttext="c_{a_{t}}" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px2.p1.3.m3" intent=":literal"><semantics><msub><mi>c</mi><msub><mi>a</mi><mi>t</mi></msub></msub><annotation encoding="application/x-tex">c_{a_{t}}</annotation></semantics></math> is the code vector in <math alttext="\mathcal{C}" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px2.p1.4.m4" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒞</mi><annotation encoding="application/x-tex">\mathcal{C}</annotation></semantics></math> corresponding to the latent action <math alttext="a_{t}" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px2.p1.5.m5" intent=":literal"><semantics><msub><mi>a</mi><mi>t</mi></msub><annotation encoding="application/x-tex">a_{t}</annotation></semantics></math>) to produce the next-token prediction. Specifically, a two-layer MLP <math alttext="f_{\text{mlp}}:\mathbb{R}^{2d}\to\mathbb{R}^{d}" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px2.p1.6.m6" intent=":literal"><semantics><mrow><msub><mi>f</mi><mtext>mlp</mtext></msub><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><msup><mi>ℝ</mi><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>d</mi></mrow></msup><mo stretchy="false">→</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow></mrow><annotation encoding="application/x-tex">f_{\text{mlp}}:\mathbb{R}^{2d}\to\mathbb{R}^{d}</annotation></semantics></math> takes the concatenation <math alttext="[e^{V,T}_{t};c_{a_{t}}]" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px2.p1.7.m7" intent=":literal"><semantics><mrow><mo stretchy="false">[</mo><msubsup><mi>e</mi><mi>t</mi><mrow><mi>V</mi><mo>,</mo><mi>T</mi></mrow></msubsup><mo>;</mo><msub><mi>c</mi><msub><mi>a</mi><mi>t</mi></msub></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[e^{V,T}_{t};c_{a_{t}}]</annotation></semantics></math> as input and outputs a merged representation <math alttext="e^{\text{mlp}}_{t}=f_{\text{mlp}}([e^{V,T}_{t};c_{a_{t}}])" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px2.p1.8.m8" intent=":literal"><semantics><mrow><msubsup><mi>e</mi><mi>t</mi><mtext>mlp</mtext></msubsup><mo>=</mo><mrow><msub><mi>f</mi><mtext>mlp</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mo stretchy="false">[</mo><msubsup><mi>e</mi><mi>t</mi><mrow><mi>V</mi><mo>,</mo><mi>T</mi></mrow></msubsup><mo>;</mo><msub><mi>c</mi><msub><mi>a</mi><mi>t</mi></msub></msub><mo stretchy="false">]</mo></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">e^{\text{mlp}}_{t}=f_{\text{mlp}}([e^{V,T}_{t};c_{a_{t}}])</annotation></semantics></math>. Then, the merged vector <math alttext="e^{\text{mlp}}_{t}" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px2.p1.9.m9" intent=":literal"><semantics><msubsup><mi>e</mi><mi>t</mi><mtext>mlp</mtext></msubsup><annotation encoding="application/x-tex">e^{\text{mlp}}_{t}</annotation></semantics></math> is fed into the original VLM’s language modeling head <math alttext="f_{\text{head}}" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px2.p1.10.m10" intent=":literal"><semantics><msub><mi>f</mi><mtext>head</mtext></msub><annotation encoding="application/x-tex">f_{\text{head}}</annotation></semantics></math>, yielding the token prediction distribution <math alttext="p(x^{T_{t+1}}\mid\cdot)=f_{\text{head}}(e^{\text{mlp}}_{t})" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px2.p1.11.m11" intent=":literal"><semantics><mrow><mrow><mi>p</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msup><mi>x</mi><msub><mi>T</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></msup><mo rspace="0em">∣</mo><mo lspace="0em" rspace="0em">⋅</mo></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><msub><mi>f</mi><mtext>head</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>e</mi><mi>t</mi><mtext>mlp</mtext></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">p(x^{T_{t+1}}\mid\cdot)=f_{\text{head}}(e^{\text{mlp}}_{t})</annotation></semantics></math>. The next token <math alttext="x^{T_{t+1}}" class="ltx_Math" display="inline" id="A1.SS1.SSS0.Px2.p1.12.m12" intent=":literal"><semantics><msup><mi>x</mi><msub><mi>T</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></msup><annotation encoding="application/x-tex">x^{T_{t+1}}</annotation></semantics></math> is selected from this distribution.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Inverse Dynamics Model</h3>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.4">The inverse dynamics model <math alttext="f_{\text{inverse}}(a_{t}|x^{V},x^{T_{1:t+1}}))" class="ltx_math_unparsed" display="inline" id="A1.SS2.p1.1.m1" intent=":literal"><semantics><mrow><msub><mi>f</mi><mtext>inverse</mtext></msub><mrow><mo stretchy="false">(</mo><msub><mi>a</mi><mi>t</mi></msub><mo fence="false" rspace="0.167em" stretchy="false">|</mo><msup><mi>x</mi><mi>V</mi></msup><mo>,</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></mrow></msub></msup><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f_{\text{inverse}}(a_{t}|x^{V},x^{T_{1:t+1}}))</annotation></semantics></math> is designed to take future observations <math alttext="(x^{V},x^{T_{1:t+1}})" class="ltx_Math" display="inline" id="A1.SS2.p1.2.m2" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mi>V</mi></msup><mo>,</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></mrow></msub></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x^{V},x^{T_{1:t+1}})</annotation></semantics></math> as input, and extracts the latent action <math alttext="a_{t}" class="ltx_Math" display="inline" id="A1.SS2.p1.3.m3" intent=":literal"><semantics><msub><mi>a</mi><mi>t</mi></msub><annotation encoding="application/x-tex">a_{t}</annotation></semantics></math> for the current step <math alttext="t" class="ltx_Math" display="inline" id="A1.SS2.p1.4.m4" intent=":literal"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>.
It consists of three core modules.</p>
</div>
<section class="ltx_paragraph" id="A1.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Encode Module</h5>
<div class="ltx_para" id="A1.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="A1.SS2.SSS0.Px1.p1.6">The input <math alttext="(x^{V},x^{T_{1:t+1}})" class="ltx_Math" display="inline" id="A1.SS2.SSS0.Px1.p1.1.m1" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mi>V</mi></msup><mo>,</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></mrow></msub></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x^{V},x^{T_{1:t+1}})</annotation></semantics></math> is encoded into <math alttext="e^{V,T}_{t+1}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="A1.SS2.SSS0.Px1.p1.2.m2" intent=":literal"><semantics><mrow><msubsup><mi>e</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>V</mi><mo>,</mo><mi>T</mi></mrow></msubsup><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">e^{V,T}_{t+1}\in\mathbb{R}^{d}</annotation></semantics></math> using the transformer blocks of the original VLM.
When <math alttext="x^{V}=\emptyset" class="ltx_Math" display="inline" id="A1.SS2.SSS0.Px1.p1.3.m3" intent=":literal"><semantics><mrow><msup><mi>x</mi><mi>V</mi></msup><mo>=</mo><mi mathvariant="normal">∅</mi></mrow><annotation encoding="application/x-tex">x^{V}=\emptyset</annotation></semantics></math> (text-only sequences), the text embedding <math alttext="e^{T}_{t+1}=f_{\text{VLM}}(x^{T_{1:t+1}})" class="ltx_Math" display="inline" id="A1.SS2.SSS0.Px1.p1.4.m4" intent=":literal"><semantics><mrow><msubsup><mi>e</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mi>T</mi></msubsup><mo>=</mo><mrow><msub><mi>f</mi><mtext>VLM</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></mrow></msub></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">e^{T}_{t+1}=f_{\text{VLM}}(x^{T_{1:t+1}})</annotation></semantics></math> is projected to the image-text embedding via the cross-modal projector <math alttext="P" class="ltx_Math" display="inline" id="A1.SS2.SSS0.Px1.p1.5.m5" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>, i.e., <math alttext="\hat{e}^{V,T}_{t+1}=P(e^{T}_{t+1})" class="ltx_Math" display="inline" id="A1.SS2.SSS0.Px1.p1.6.m6" intent=":literal"><semantics><mrow><msubsup><mover accent="true"><mi>e</mi><mo>^</mo></mover><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>V</mi><mo>,</mo><mi>T</mi></mrow></msubsup><mo>=</mo><mrow><mi>P</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>e</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mi>T</mi></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\hat{e}^{V,T}_{t+1}=P(e^{T}_{t+1})</annotation></semantics></math>, as illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S2.F2" title="Figure 2 ‣ Latent Actions for Reinforcement Learning ‣ 2 Preliminary ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Inverse Transformer Layers</h5>
<div class="ltx_para" id="A1.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="A1.SS2.SSS0.Px2.p1.2">To adapt the VLM embedding to the latent action space, the obtained embedding <math alttext="e^{V,T}_{t+1}" class="ltx_Math" display="inline" id="A1.SS2.SSS0.Px2.p1.1.m1" intent=":literal"><semantics><msubsup><mi>e</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>V</mi><mo>,</mo><mi>T</mi></mrow></msubsup><annotation encoding="application/x-tex">e^{V,T}_{t+1}</annotation></semantics></math> is processed by 4-layer Transformer blocks, yielding a representation <math alttext="\tilde{e}^{V,T}_{t+1}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="A1.SS2.SSS0.Px2.p1.2.m2" intent=":literal"><semantics><mrow><msubsup><mover accent="true"><mi>e</mi><mo>~</mo></mover><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>V</mi><mo>,</mo><mi>T</mi></mrow></msubsup><mo>∈</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\tilde{e}^{V,T}_{t+1}\in\mathbb{R}^{d}</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS2.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Inverse Action Head</h5>
<div class="ltx_para" id="A1.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="A1.SS2.SSS0.Px3.p1.2">Following <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">jia-2025-controlling</span></cite>, we adopt a <span class="ltx_text ltx_font_italic" id="A1.SS2.SSS0.Px3.p1.2.1">direct code assignment</span> strategy to avoid code collapse. Specifically, a linear head (inverse action head) maps <math alttext="\tilde{e}^{V,T}_{t+1}" class="ltx_Math" display="inline" id="A1.SS2.SSS0.Px3.p1.1.m1" intent=":literal"><semantics><msubsup><mover accent="true"><mi>e</mi><mo>~</mo></mover><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mrow><mi>V</mi><mo>,</mo><mi>T</mi></mrow></msubsup><annotation encoding="application/x-tex">\tilde{e}^{V,T}_{t+1}</annotation></semantics></math> to logits <math alttext="\mathbf{l}_{t}\in\mathbb{R}^{|\mathcal{C}|}" class="ltx_Math" display="inline" id="A1.SS2.SSS0.Px3.p1.2.m2" intent=":literal"><semantics><mrow><msub><mi>𝐥</mi><mi>t</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><mo stretchy="false">|</mo><mi class="ltx_font_mathcaligraphic">𝒞</mi><mo stretchy="false">|</mo></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{l}_{t}\in\mathbb{R}^{|\mathcal{C}|}</annotation></semantics></math> over the codebook indices. During inverse dynamics learning, we apply the Gumbel-Softmax and a reparameterization trick to obtain a differentiable soft assignment:</p>
<table class="ltx_equation ltx_eqn_table" id="A1.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{g}_{t}=\text{GumbelSoftmax}(\mathbf{l}_{t}),\quad\hat{\mathbf{o}}_{t}=(\mathbf{o}_{t}-\mathbf{g}_{t})_{\text{sg}}+\mathbf{g}_{t}," class="ltx_Math" display="block" id="A1.Ex1.m1" intent=":literal"><semantics><mrow><mrow><mrow><msub><mi>𝐠</mi><mi>t</mi></msub><mo>=</mo><mrow><mtext>GumbelSoftmax</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>𝐥</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><msub><mover accent="true"><mi>𝐨</mi><mo>^</mo></mover><mi>t</mi></msub><mo>=</mo><mrow><msub><mrow><mo stretchy="false">(</mo><mrow><msub><mi>𝐨</mi><mi>t</mi></msub><mo>−</mo><msub><mi>𝐠</mi><mi>t</mi></msub></mrow><mo stretchy="false">)</mo></mrow><mtext>sg</mtext></msub><mo>+</mo><msub><mi>𝐠</mi><mi>t</mi></msub></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathbf{g}_{t}=\text{GumbelSoftmax}(\mathbf{l}_{t}),\quad\hat{\mathbf{o}}_{t}=(\mathbf{o}_{t}-\mathbf{g}_{t})_{\text{sg}}+\mathbf{g}_{t},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="A1.SS2.SSS0.Px3.p1.7">where <math alttext="\mathbf{o}_{t}" class="ltx_Math" display="inline" id="A1.SS2.SSS0.Px3.p1.3.m1" intent=":literal"><semantics><msub><mi>𝐨</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\mathbf{o}_{t}</annotation></semantics></math> is the hard one-hot vector (<math alttext="\arg\max" class="ltx_Math" display="inline" id="A1.SS2.SSS0.Px3.p1.4.m2" intent=":literal"><semantics><mrow><mi>arg</mi><mo lspace="0.167em">⁡</mo><mi>max</mi></mrow><annotation encoding="application/x-tex">\arg\max</annotation></semantics></math> of <math alttext="\mathbf{l}_{t}" class="ltx_Math" display="inline" id="A1.SS2.SSS0.Px3.p1.5.m3" intent=":literal"><semantics><msub><mi>𝐥</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\mathbf{l}_{t}</annotation></semantics></math>), and <math alttext="(\cdot)_{\text{sg}}" class="ltx_Math" display="inline" id="A1.SS2.SSS0.Px3.p1.6.m4" intent=":literal"><semantics><msub><mrow><mo stretchy="false">(</mo><mo lspace="0em" rspace="0em">⋅</mo><mo stretchy="false">)</mo></mrow><mtext>sg</mtext></msub><annotation encoding="application/x-tex">(\cdot)_{\text{sg}}</annotation></semantics></math> denotes stop-gradient. The final latent action embedding is <math alttext="c_{a_{t}}=\hat{\mathbf{o}}_{t}^{\top}\mathcal{C}" class="ltx_Math" display="inline" id="A1.SS2.SSS0.Px3.p1.7.m5" intent=":literal"><semantics><mrow><msub><mi>c</mi><msub><mi>a</mi><mi>t</mi></msub></msub><mo>=</mo><mrow><msubsup><mover accent="true"><mi>𝐨</mi><mo>^</mo></mover><mi>t</mi><mo>⊤</mo></msubsup><mo lspace="0em" rspace="0em">​</mo><mi class="ltx_font_mathcaligraphic">𝒞</mi></mrow></mrow><annotation encoding="application/x-tex">c_{a_{t}}=\hat{\mathbf{o}}_{t}^{\top}\mathcal{C}</annotation></semantics></math>, which is then used by the language world model.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Policy Model</h3>
<div class="ltx_para" id="A1.SS3.p1">
<p class="ltx_p" id="A1.SS3.p1.4">The policy <math alttext="\pi_{\theta}(a_{t}\mid x^{V},x^{T_{1:t}})" class="ltx_Math" display="inline" id="A1.SS3.p1.1.m1" intent=":literal"><semantics><mrow><msub><mi>π</mi><mi>θ</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>∣</mo><mrow><msup><mi>x</mi><mi>V</mi></msup><mo>,</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub></msup></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\pi_{\theta}(a_{t}\mid x^{V},x^{T_{1:t}})</annotation></semantics></math> predicts the latent action <math alttext="a_{t}" class="ltx_Math" display="inline" id="A1.SS3.p1.2.m2" intent=":literal"><semantics><msub><mi>a</mi><mi>t</mi></msub><annotation encoding="application/x-tex">a_{t}</annotation></semantics></math> from the current context <math alttext="(x^{V},x^{T_{1:t}})" class="ltx_Math" display="inline" id="A1.SS3.p1.3.m3" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>x</mi><mi>V</mi></msup><mo>,</mo><msup><mi>x</mi><msub><mi>T</mi><mrow><mn>1</mn><mo lspace="0.278em" rspace="0.278em">:</mo><mi>t</mi></mrow></msub></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(x^{V},x^{T_{1:t}})</annotation></semantics></math>.
Its architecture mirrors <math alttext="f_{\text{inverse}}" class="ltx_Math" display="inline" id="A1.SS3.p1.4.m4" intent=":literal"><semantics><msub><mi>f</mi><mtext>inverse</mtext></msub><annotation encoding="application/x-tex">f_{\text{inverse}}</annotation></semantics></math>, which includes: 1) the encode module, 2) policy transformer layers (8-layer), and 3) policy action head.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>Codebook for the Latent Action Space</h3>
<div class="ltx_para" id="A1.SS4.p1">
<p class="ltx_p" id="A1.SS4.p1.5">The latent action space is defined by a codebook <math alttext="\mathcal{C}=\{c_{1},\dots,c_{K}\}\subset\mathbb{R}^{d}" class="ltx_Math" display="inline" id="A1.SS4.p1.1.m1" intent=":literal"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒞</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><msub><mi>c</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>c</mi><mi>K</mi></msub><mo stretchy="false">}</mo></mrow><mo>⊂</mo><msup><mi>ℝ</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\mathcal{C}=\{c_{1},\dots,c_{K}\}\subset\mathbb{R}^{d}</annotation></semantics></math> with <math alttext="K=128" class="ltx_Math" display="inline" id="A1.SS4.p1.2.m2" intent=":literal"><semantics><mrow><mi>K</mi><mo>=</mo><mn>128</mn></mrow><annotation encoding="application/x-tex">K=128</annotation></semantics></math>. Each code vector <math alttext="c_{k}" class="ltx_Math" display="inline" id="A1.SS4.p1.3.m3" intent=":literal"><semantics><msub><mi>c</mi><mi>k</mi></msub><annotation encoding="application/x-tex">c_{k}</annotation></semantics></math> is initialized independently via Kaiming uniform initialization <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">he-2015-delving</span></cite>. Given a latent action index <math alttext="a_{t}\in\{1,\dots,K\}" class="ltx_Math" display="inline" id="A1.SS4.p1.4.m4" intent=":literal"><semantics><mrow><msub><mi>a</mi><mi>t</mi></msub><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>K</mi><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">a_{t}\in\{1,\dots,K\}</annotation></semantics></math>, the corresponding latent action embedding is retrieved as <math alttext="c_{a_{t}}\in\mathcal{C}" class="ltx_Math" display="inline" id="A1.SS4.p1.5.m5" intent=":literal"><semantics><mrow><msub><mi>c</mi><msub><mi>a</mi><mi>t</mi></msub></msub><mo>∈</mo><mi class="ltx_font_mathcaligraphic">𝒞</mi></mrow><annotation encoding="application/x-tex">c_{a_{t}}\in\mathcal{C}</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.5 </span>Cross-modal Projector</h3>
<div class="ltx_para" id="A1.SS5.p1">
<p class="ltx_p" id="A1.SS5.p1.5">The cross-modal projector <math alttext="P" class="ltx_Math" display="inline" id="A1.SS5.p1.1.m1" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> is implemented as a dual-MLP module: given a text embedding <math alttext="e^{T}_{t}" class="ltx_Math" display="inline" id="A1.SS5.p1.2.m2" intent=":literal"><semantics><msubsup><mi>e</mi><mi>t</mi><mi>T</mi></msubsup><annotation encoding="application/x-tex">e^{T}_{t}</annotation></semantics></math>, the first MLP outputs the mean vector <math alttext="\mu_{t}" class="ltx_Math" display="inline" id="A1.SS5.p1.3.m3" intent=":literal"><semantics><msub><mi>μ</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\mu_{t}</annotation></semantics></math>, and the second MLP outputs the log standard deviation vector <math alttext="\log\sigma_{t}" class="ltx_Math" display="inline" id="A1.SS5.p1.4.m4" intent=":literal"><semantics><mrow><mi>log</mi><mo lspace="0.167em">⁡</mo><msub><mi>σ</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\log\sigma_{t}</annotation></semantics></math> (for numerical stability), forming a diagonal Gaussian distribution <math alttext="\mathcal{N}(\mu_{t},\mathrm{diag}(\sigma_{t}^{2}))" class="ltx_Math" display="inline" id="A1.SS5.p1.5.m5" intent=":literal"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒩</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>μ</mi><mi>t</mi></msub><mo>,</mo><mrow><mi>diag</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>σ</mi><mi>t</mi><mn>2</mn></msubsup><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{N}(\mu_{t},\mathrm{diag}(\sigma_{t}^{2}))</annotation></semantics></math> in the image-text embedding space.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Experimental Details</h2>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Details on Datasets</h3>
<section class="ltx_paragraph" id="A2.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Corpora for Constructing the Latent Action Space</h5>
<div class="ltx_para" id="A2.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="A2.SS1.SSS0.Px1.p1.1">To construct the latent action space in an unsupervised manner, we collect large-scale paired image-text and text-only corpora.
For paired image-text data, we use: (1) image-caption pairs from <span class="ltx_text ltx_font_typewriter" id="A2.SS1.SSS0.Px1.p1.1.1">Conceptual-12M</span> <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">changpinyo-2021-Conceptual</span></cite>; (2) multimodal news articles from <span class="ltx_text ltx_font_typewriter" id="A2.SS1.SSS0.Px1.p1.1.2">N24News</span> <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">wang-2022-N24News</span></cite>; and (3) multimodal Wikipedia articles from <span class="ltx_text ltx_font_typewriter" id="A2.SS1.SSS0.Px1.p1.1.3">WikiWeb2M</span> <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">burns-2023-wiki</span></cite>, comprising 14M images and 1B text tokens in total.
For text-only data, we primarily sample 500K sequences from <span class="ltx_text ltx_font_typewriter" id="A2.SS1.SSS0.Px1.p1.1.4">SlimPajama-627B</span> <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">cerebras-2023-slimpajama</span></cite> due to computational constraints, and additionally include 40K alignment corpora from <span class="ltx_text ltx_font_typewriter" id="A2.SS1.SSS0.Px1.p1.1.5">HelpSteer3</span> <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">wang-2025-helpsteer3</span></cite> to preserve the original VLM’s safety and preference alignment during latent space learning.
To ensure fair comparison, we analyze data exposure in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A3.SS1" title="C.1 Analysis on Data Exposure ‣ Appendix C Additional Empirical Results ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">C.1</span></a> and find that downstream task performance does not benefit from the above corpora, confirming that observed improvements stem from methodological advances.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Details on Evaluation Metric</h3>
<div class="ltx_para" id="A2.SS2.p1">
<p class="ltx_p" id="A2.SS2.p1.1">We adopt <span class="ltx_text ltx_font_italic" id="A2.SS2.p1.1.1">LLM-as-a-Judge</span> metrics to evaluate model performance, using prompt templates validated by <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">dai-2025-mmrole</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">li-2025-aligning</span></cite>, which show high correlation with human judgments.
The evaluation prompt templates used on <span class="ltx_text ltx_font_typewriter" id="A2.SS2.p1.1.2">MMRole</span> and <span class="ltx_text ltx_font_typewriter" id="A2.SS2.p1.1.3">PCogAlignBench</span> are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A2.T4" title="Table 4 ‣ B.4 Inference Details ‣ Appendix B Experimental Details ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">4</span></a>. We adopt the <span class="ltx_text ltx_font_typewriter" id="A2.SS2.p1.1.4">Qwen3-235B-A22B</span> by the Qwen3 API platform as the judge model.</p>
</div>
</section>
<section class="ltx_subsection" id="A2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.3 </span>Training Details</h3>
<section class="ltx_paragraph" id="A2.SS3.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Baseline Methods</h5>
<div class="ltx_para" id="A2.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="A2.SS3.SSS0.Px1.p1.2">For the SFT baseline, we fine-tune the VLM with a learning rate of <math alttext="5\times 10^{-6}" class="ltx_Math" display="inline" id="A2.SS3.SSS0.Px1.p1.1.m1" intent=":literal"><semantics><mrow><mn>5</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>6</mn></mrow></msup></mrow><annotation encoding="application/x-tex">5\times 10^{-6}</annotation></semantics></math> for 2 epochs.
For token-level RL baselines, we use a rollout size of 8, a per-step batch size of 32, and train for 100 RL steps with a constant learning rate of <math alttext="1\times 10^{-6}" class="ltx_Math" display="inline" id="A2.SS3.SSS0.Px1.p1.2.m2" intent=":literal"><semantics><mrow><mn>1</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>6</mn></mrow></msup></mrow><annotation encoding="application/x-tex">1\times 10^{-6}</annotation></semantics></math>. For all RL methods, we use 50% of the training data to initialize the model via SFT, followed by RL fine-tuning on the remaining 50%.
During RL rollouts, we set the sampling temperature to 1.0 for all methods.</p>
</div>
</section>
<section class="ltx_paragraph" id="A2.SS3.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Latent Action Space Learning</h5>
<div class="ltx_para" id="A2.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="A2.SS3.SSS0.Px2.p1.1">As outlined in Algorithm <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#alg1" title="Algorithm 1 ‣ 3.3 Latent Action Reinforcement Learning ‣ 3 Methodology ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">1</span></a>, the latent action space learning procedure consists of the following four stages:</p>
<ol class="ltx_enumerate" id="A2.I1">
<li class="ltx_item" id="A2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="A2.I1.i1.p1">
<p class="ltx_p" id="A2.I1.i1.p1.5">Initialize <math alttext="f_{\text{world}},f_{\text{inverse}},\mathcal{C}" class="ltx_Math" display="inline" id="A2.I1.i1.p1.1.m1" intent=":literal"><semantics><mrow><msub><mi>f</mi><mtext>world</mtext></msub><mo>,</mo><msub><mi>f</mi><mtext>inverse</mtext></msub><mo>,</mo><mi class="ltx_font_mathcaligraphic">𝒞</mi></mrow><annotation encoding="application/x-tex">f_{\text{world}},f_{\text{inverse}},\mathcal{C}</annotation></semantics></math> by minimizing <math alttext="\mathcal{L}_{\text{inverse}}" class="ltx_Math" display="inline" id="A2.I1.i1.p1.2.m2" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>inverse</mtext></msub><annotation encoding="application/x-tex">\mathcal{L}_{\text{inverse}}</annotation></semantics></math> (Eq. <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S3.E1" title="Equation 1 ‣ Overview ‣ 3.2.1 Inverse Dynamics Learning ‣ 3.2 Latent Action Space Learning ‣ 3 Methodology ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">1</span></a>) on <math alttext="\mathcal{D}^{VT}" class="ltx_Math" display="inline" id="A2.I1.i1.p1.3.m3" intent=":literal"><semantics><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mrow><mi>V</mi><mo lspace="0em" rspace="0em">​</mo><mi>T</mi></mrow></msup><annotation encoding="application/x-tex">\mathcal{D}^{VT}</annotation></semantics></math>. <span class="ltx_text ltx_font_italic" id="A2.I1.i1.p1.5.1">Training details</span>: learning rate = <math alttext="1\!\times\!10^{-4}" class="ltx_Math" display="inline" id="A2.I1.i1.p1.4.m4" intent=":literal"><semantics><mrow><mn>1</mn><mo lspace="0.052em" rspace="0.052em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup></mrow><annotation encoding="application/x-tex">1\!\times\!10^{-4}</annotation></semantics></math>, cosine decay with minimum learning rate <math alttext="1\!\times\!10^{-5}" class="ltx_Math" display="inline" id="A2.I1.i1.p1.5.m5" intent=":literal"><semantics><mrow><mn>1</mn><mo lspace="0.052em" rspace="0.052em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>5</mn></mrow></msup></mrow><annotation encoding="application/x-tex">1\!\times\!10^{-5}</annotation></semantics></math>, batch size = 16, max sequence length = 2048, 1 epoch.</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="A2.I1.i2.p1">
<p class="ltx_p" id="A2.I1.i2.p1.4">Initialize the cross-modal projectors <math alttext="P,P^{\prime}" class="ltx_Math" display="inline" id="A2.I1.i2.p1.1.m1" intent=":literal"><semantics><mrow><mi>P</mi><mo>,</mo><msup><mi>P</mi><mo>′</mo></msup></mrow><annotation encoding="application/x-tex">P,P^{\prime}</annotation></semantics></math> by minimizing <math alttext="\mathcal{L}_{\text{proj}_{1}}" class="ltx_Math" display="inline" id="A2.I1.i2.p1.2.m2" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><msub><mtext>proj</mtext><mn>1</mn></msub></msub><annotation encoding="application/x-tex">\mathcal{L}_{\text{proj}_{1}}</annotation></semantics></math> (Eq. <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S3.E5" title="Equation 5 ‣ Cross-modal Projector Training ‣ 3.2.1 Inverse Dynamics Learning ‣ 3.2 Latent Action Space Learning ‣ 3 Methodology ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">5</span></a>) on <math alttext="\mathcal{D}^{VT}" class="ltx_Math" display="inline" id="A2.I1.i2.p1.3.m3" intent=":literal"><semantics><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mrow><mi>V</mi><mo lspace="0em" rspace="0em">​</mo><mi>T</mi></mrow></msup><annotation encoding="application/x-tex">\mathcal{D}^{VT}</annotation></semantics></math>. <span class="ltx_text ltx_font_italic" id="A2.I1.i2.p1.4.1">Training details</span>: learning rate = <math alttext="1\!\times\!10^{-3}" class="ltx_Math" display="inline" id="A2.I1.i2.p1.4.m4" intent=":literal"><semantics><mrow><mn>1</mn><mo lspace="0.052em" rspace="0.052em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>3</mn></mrow></msup></mrow><annotation encoding="application/x-tex">1\!\times\!10^{-3}</annotation></semantics></math>, cosine decay, batch size = 16, 1 epoch.</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="A2.I1.i3.p1">
<p class="ltx_p" id="A2.I1.i3.p1.6">Jointly optimize <math alttext="f_{\text{world}},f_{\text{inverse}},\mathcal{C},P,P^{\prime}" class="ltx_Math" display="inline" id="A2.I1.i3.p1.1.m1" intent=":literal"><semantics><mrow><msub><mi>f</mi><mtext>world</mtext></msub><mo>,</mo><msub><mi>f</mi><mtext>inverse</mtext></msub><mo>,</mo><mi class="ltx_font_mathcaligraphic">𝒞</mi><mo>,</mo><mi>P</mi><mo>,</mo><msup><mi>P</mi><mo>′</mo></msup></mrow><annotation encoding="application/x-tex">f_{\text{world}},f_{\text{inverse}},\mathcal{C},P,P^{\prime}</annotation></semantics></math> by minimizing <math alttext="\mathcal{L}_{\text{inverse}}" class="ltx_Math" display="inline" id="A2.I1.i3.p1.2.m2" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>inverse</mtext></msub><annotation encoding="application/x-tex">\mathcal{L}_{\text{inverse}}</annotation></semantics></math> (Eq. <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S3.E1" title="Equation 1 ‣ Overview ‣ 3.2.1 Inverse Dynamics Learning ‣ 3.2 Latent Action Space Learning ‣ 3 Methodology ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">1</span></a>) and <math alttext="\mathcal{L}_{\text{proj}_{2}}" class="ltx_Math" display="inline" id="A2.I1.i3.p1.3.m3" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><msub><mtext>proj</mtext><mn>2</mn></msub></msub><annotation encoding="application/x-tex">\mathcal{L}_{\text{proj}_{2}}</annotation></semantics></math> (Eq. <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S3.E6" title="Equation 6 ‣ Cross-modal Projector Training ‣ 3.2.1 Inverse Dynamics Learning ‣ 3.2 Latent Action Space Learning ‣ 3 Methodology ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">6</span></a>) on <math alttext="\mathcal{D}^{VT}\cup\mathcal{D}^{T}" class="ltx_Math" display="inline" id="A2.I1.i3.p1.4.m4" intent=":literal"><semantics><mrow><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mrow><mi>V</mi><mo lspace="0em" rspace="0em">​</mo><mi>T</mi></mrow></msup><mo>∪</mo><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">\mathcal{D}^{VT}\cup\mathcal{D}^{T}</annotation></semantics></math>. <span class="ltx_text ltx_font_italic" id="A2.I1.i3.p1.6.1">Training details</span>: learning rate = <math alttext="1\!\times\!10^{-4}" class="ltx_Math" display="inline" id="A2.I1.i3.p1.5.m5" intent=":literal"><semantics><mrow><mn>1</mn><mo lspace="0.052em" rspace="0.052em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup></mrow><annotation encoding="application/x-tex">1\!\times\!10^{-4}</annotation></semantics></math>, cosine decay with minimum learning rate <math alttext="1\!\times\!10^{-5}" class="ltx_Math" display="inline" id="A2.I1.i3.p1.6.m6" intent=":literal"><semantics><mrow><mn>1</mn><mo lspace="0.052em" rspace="0.052em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>5</mn></mrow></msup></mrow><annotation encoding="application/x-tex">1\!\times\!10^{-5}</annotation></semantics></math>, batch size = 16, max sequence length = 2048, 1 epoch.</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="A2.I1.i4.p1">
<p class="ltx_p" id="A2.I1.i4.p1.4">Initialize the policy model <math alttext="\pi_{\theta}" class="ltx_Math" display="inline" id="A2.I1.i4.p1.1.m1" intent=":literal"><semantics><msub><mi>π</mi><mi>θ</mi></msub><annotation encoding="application/x-tex">\pi_{\theta}</annotation></semantics></math> by minimizing <math alttext="\mathcal{L}_{\text{bc}}" class="ltx_Math" display="inline" id="A2.I1.i4.p1.2.m2" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>bc</mtext></msub><annotation encoding="application/x-tex">\mathcal{L}_{\text{bc}}</annotation></semantics></math> (Eq. <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S3.E8" title="Equation 8 ‣ 3.2.2 Policy Behavior Cloning ‣ 3.2 Latent Action Space Learning ‣ 3 Methodology ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">8</span></a>) on <math alttext="\mathcal{D}^{VT}\cup\mathcal{D}^{T}" class="ltx_Math" display="inline" id="A2.I1.i4.p1.3.m3" intent=":literal"><semantics><mrow><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mrow><mi>V</mi><mo lspace="0em" rspace="0em">​</mo><mi>T</mi></mrow></msup><mo>∪</mo><msup><mi class="ltx_font_mathcaligraphic">𝒟</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">\mathcal{D}^{VT}\cup\mathcal{D}^{T}</annotation></semantics></math>. <span class="ltx_text ltx_font_italic" id="A2.I1.i4.p1.4.1">Training details</span>: learning rate = <math alttext="1\!\times\!10^{-4}" class="ltx_Math" display="inline" id="A2.I1.i4.p1.4.m4" intent=":literal"><semantics><mrow><mn>1</mn><mo lspace="0.052em" rspace="0.052em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup></mrow><annotation encoding="application/x-tex">1\!\times\!10^{-4}</annotation></semantics></math>, cosine decay, batch size = 16, max sequence length = 2048, 1 epoch.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_paragraph" id="A2.SS3.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Latent Action RL</h5>
<div class="ltx_para" id="A2.SS3.SSS0.Px3.p1">
<p class="ltx_p" id="A2.SS3.SSS0.Px3.p1.1">We adopt the same RL hyperparameters as the token-level baselines: rollout size of 8, per-step batch size of 32, 100 RL steps, and constant learning rate of <math alttext="1\times 10^{-6}" class="ltx_Math" display="inline" id="A2.SS3.SSS0.Px3.p1.1.m1" intent=":literal"><semantics><mrow><mn>1</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>6</mn></mrow></msup></mrow><annotation encoding="application/x-tex">1\times 10^{-6}</annotation></semantics></math>.
To prevent code collapse and excessive deviation from the initial policy, we incorporate a KL regularization term between the current policy’s action distribution and its initialization, with a coefficient of 0.01.
During RL fine-tuning, only the policy transformer layers and the policy head in the policy model (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A1.SS3" title="A.3 Policy Model ‣ Appendix A Details on Model Design ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">A.3</span></a>) are optimized.</p>
</div>
<div class="ltx_para" id="A2.SS3.SSS0.Px3.p2">
<p class="ltx_p" id="A2.SS3.SSS0.Px3.p2.1">Since all token-level RL methods build upon an SFT-initialized model, for fair comparison, we also perform SFT before latent action RL. Specifically, we fine-tune the transformer blocks in VLMs (shared by the policy model and the language world model) and the language modeling head in VLMs (used by the language world model) using the same SFT data as the baselines.
During RL rollouts, we set the sampling temperature for the latent action level policy model as 1.0.</p>
</div>
</section>
<section class="ltx_paragraph" id="A2.SS3.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Reward Function</h5>
<div class="ltx_para" id="A2.SS3.SSS0.Px4.p1">
<p class="ltx_p" id="A2.SS3.SSS0.Px4.p1.1">For all methods, we employ a generative reward model for fair comparison, where responses are scored by <span class="ltx_text ltx_font_typewriter" id="A2.SS3.SSS0.Px4.p1.1.1">Qwen3-235B-A22B</span> using the evaluation prompt templates in Table <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A2.T4" title="Table 4 ‣ B.4 Inference Details ‣ Appendix B Experimental Details ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</section>
<section class="ltx_paragraph" id="A2.SS3.SSS0.Px5">
<h5 class="ltx_title ltx_title_paragraph">Implementation Details</h5>
<div class="ltx_para" id="A2.SS3.SSS0.Px5.p1">
<p class="ltx_p" id="A2.SS3.SSS0.Px5.p1.1">All experiments are conducted on a single machine equipped with 4 Nvidia A100-80G GPU.
For the baseline SFT and RL algorithms, as well as our newly proposed latent action RL methods, we adapt the framework based on the TRL library <cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_ref ltx_missing_citation ltx_ref_self">vonwerra-2022-trl</span></cite>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.4 </span>Inference Details</h3>
<div class="ltx_para" id="A2.SS4.p1">
<p class="ltx_p" id="A2.SS4.p1.1">For all methods, we use a sampling temperature of 0.1 during inference, i.e., for token-based baselines, this temperature is applied to the token logits; for our latent action based methods, it is applied to the latent action logits.
Additionally, following <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">jia-2025-controlling</span></cite>, for our latent action based methods, token generation by the language world model is deterministic, i.e., tokens are selected via <span class="ltx_text ltx_font_typewriter" id="A2.SS4.p1.1.1">argmax</span> over the output token logits.</p>
</div>
<figure class="ltx_table" id="A2.T4">
<p class="ltx_p" id="A2.T4.1">
<span class="ltx_inline-block ltx_minipage ltx_align_middle ltx_framed ltx_framed_rectangle" id="A2.T4.1.1" style="width:345.0pt;">
<span class="ltx_p" id="A2.T4.1.1.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="A2.T4.1.1.1.1">Prompt Template for Evaluation on MMRole</span></span><span class="ltx_rule" style="width:433.6pt;height:0.4pt;--ltx-bg-color:black;display:inline-block;"> </span>
<span class="ltx_p" id="A2.T4.1.1.2">## [Question Start]
{question}
## [Question End]</span>
<span class="ltx_p" id="A2.T4.1.1.3">## [Model A’s Response Start]
{evaluated_answer}
## [Model A’s Response End]</span>
<span class="ltx_p" id="A2.T4.1.1.4">## [Model B’s Response Start]
{groundtruth_answer}
## [Model B’s Response End]</span>
<span class="ltx_p" id="A2.T4.1.1.5">## [Instruction]
The task instruction of the two models is to directly role-play as {role_name} and talk with a curious human about the given image using the distinctive tone, manner and vocabulary of {role_name}.</span>
<span class="ltx_p" id="A2.T4.1.1.6">Here is the detailed character information about {role_name}:
{role_info}</span>
<span class="ltx_p" id="A2.T4.1.1.7">Please evaluate the following aspects of each model’s response:
1. Instruction Adherence: Do the responses accurately adhere to the task instruction, directly role-playing as {role_name} and only including words that {role_name} should say, without any additional explanatory prefixes or suffixes?
2. Fluency: Are the responses grammatically correct and smoothly articulated?
3. Coherency: Do the responses maintain a coherent thread of dialogue without contradicting earlier parts of the conversation or previously established facts?
4. Image-Text Relevance: Are the responses closely related to the visual content of the image?
5. Response Accuracy: Do the responses accurately answer the curious human’s words or appropriately initiate a conversation based on the image?
6. Personality Consistency: Do the responses accurately and sufficiently reflect the personality of {role_name}?
7. Knowledge Consistency: Are the responses consistent with the factual knowledge that {role_name} should possess, including experiences, abilities, and relationships?
8. Tone Consistency: Do the responses maintain a consistent tone that aligns with {role_name}’s typical manner of speaking and catchphrases, rather than resembling the style of AI assistants?</span>
<span class="ltx_p" id="A2.T4.1.1.8">For each aspect, provide a brief qualitative evaluation for the relative performance of the two models, followed by paired quantitative scores from 1 to 10, where 1 indicates poor performance and 10 indicates excellent performance.</span>
<span class="ltx_p" id="A2.T4.1.1.9">The output should be in the following format:
1. Instruction Adherence: {{Qualitative Evaluation}}, [Scores]: ({{the score of Model A}}, {{the score of Model B}})
2. Fluency: {{Qualitative Evaluation}}, [Scores]: ({{the score of Model A}}, {{the score of Model B}})
etc.</span>
<span class="ltx_p" id="A2.T4.1.1.10">Please ensure that your evaluations are unbiased and that the order in which the responses were presented does not affect your judgment.
Format requirement: Please ensure that your evaluations only include 8 score pairs, which means that there can only be eight pairs of [Scores]: () in your output text.</span><span class="ltx_rule" style="width:433.6pt;height:0.4pt;--ltx-bg-color:black;display:inline-block;"> </span>
<span class="ltx_p" id="A2.T4.1.1.11"><span class="ltx_text ltx_font_bold ltx_font_italic" id="A2.T4.1.1.11.1">Prompt Template for Evaluation on PCogAlignBench</span></span><span class="ltx_rule" style="width:433.6pt;height:0.4pt;--ltx-bg-color:black;display:inline-block;"> </span>
<span class="ltx_p" id="A2.T4.1.1.12">PersonalizedAI Company is developing a personalized AI service robot that aims to better serve each individual. The service is currently being trialed with a small group of users. In order to improve the level of personalization in the responses provided by the AI service robot, our company plans to conduct surveys and interviews with participants in the trial. We will first provide historical interview records, which include the feedback and preferences expressed by the test users regarding AI responses in a certain scenario. During the interview, the interviewee needs to refer to these historical records to answer questions posed by the interviewer. The interview will be conducted in an online Q&amp;A format, and interviewees must strictly follow the format requirements provided in system instructions.</span>
<span class="ltx_p" id="A2.T4.1.1.13"># Historical Interview Records</span>
<span class="ltx_p" id="A2.T4.1.1.14">Interviewer: Hello, could you please briefly describe your role set?
Interviewee: OK. {individual_RoleSet_str}
Interviewer: In the "{visual_scene_text}" scenario at {location} location, what kind of responses would you like the AI to provide?
Interviewee: Okay, I will describe what kind of AI responses would satisfy me in this scenario. {EvalHelp_str}</span>
<span class="ltx_p" id="A2.T4.1.1.15"># Interview</span>
<span class="ltx_p" id="A2.T4.1.1.16">Interviewer: Hello, and thank you for trialing the personalized AI responses from our company.
Interviewee: You’re welcome.
Interviewer: Alright, we will now present you with a question you posed in a particular scenario along with two generated responses from the AI. We would like you to choose which response is better.
Interviewee: Sure, I understand. Please go ahead.
Interviewer: According to our cloud records, in a "{visual_scene_text}" scenario, you asked the personalized AI robot the question: "{query}". Here are the generated responses from the AI.
&gt; **Response A**: {response_A}
&gt; **Response B**: {response_B}</span>
<span class="ltx_p" id="A2.T4.1.1.17">&gt; System Instruction: Interviewee, please note that you should not choose a response as better just because it’s long. Instead, select the response that best considers your physical and mental state and helps you to achieve better body behavior and mind feelings.
&gt; System Instruction: For each aspect, provide a brief qualitative evaluation for the relative performance of the two models, followed by paired quantitative scores from 1 to 10, where 1 indicates poor performance and 10 indicates excellent performance.</span>
<span class="ltx_p" id="A2.T4.1.1.18">The output should be in the following format:
1. Role-Set Sensitivity: {{Qualitative Evaluation}}, [Scores]: ({{the score of Response A}}, {{the score of Response B}})
2. Body Behavior Awareness: {{Qualitative Evaluation}}, [Scores]: ({{the score of Response A}}, {{the score of Response B}})
3. Mind Feelings Awareness: {{Qualitative Evaluation}}, [Scores]: ({{the score of Response A}}, {{the score of Response B}})
4. Contextual Awareness: {{Qualitative Evaluation}}, [Scores]: ({{the score of Response A}}, {{the score of Response B}})
5. Conversational Flow: {{Qualitative Evaluation}}, [Scores]: ({{the score of Response A}}, {{the score of Response B}})
etc.</span>
<span class="ltx_p" id="A2.T4.1.1.19">Please ensure that your evaluations are unbiased and that the order in which the responses were presented does not affect your judgment.
Format requirement: Please ensure that your evaluations only include 5 score pairs, which means that there can only be 5 pairs of [Scores]: () in your output text.</span>
</span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Prompt templates used for LLM-as-a-Judge evaluation on <span class="ltx_text ltx_font_typewriter" id="A2.T4.4.1">MMRole</span> and <span class="ltx_text ltx_font_typewriter" id="A2.T4.5.2">PCogAlignBench</span>. These templates follow established designs from <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_ref ltx_missing_citation ltx_ref_self">dai-2025-mmrole</span>; <span class="ltx_ref ltx_missing_citation ltx_ref_self">li-2025-aligning</span></cite> and have been shown to achieve high correlation with human judgments.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure class="ltx_table" id="A2.T5">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A2.T5.20" style="width:433.6pt;height:95pt;vertical-align:-45.1pt;"><span class="ltx_transformed_inner" style="transform:translate(-8.8pt,1.9pt) scale(0.961058095550164,0.961058095550164) ;">
<table class="ltx_tabular ltx_align_middle" id="A2.T5.20.20">
<tr class="ltx_tr" id="A2.T5.20.20.21">
<td class="ltx_td ltx_align_left ltx_border_tt" id="A2.T5.20.20.21.1" rowspan="2" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="A2.T5.20.20.21.1.1">Data</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="A2.T5.20.20.21.2" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="A2.T5.20.20.21.2.1">MMRole</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="A2.T5.20.20.21.3" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="A2.T5.20.20.21.3.1">PCogAlignBench</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A2.T5.20.20.21.4" rowspan="2" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="A2.T5.20.20.21.4.1">Avg.</span></td>
</tr>
<tr class="ltx_tr" id="A2.T5.20.20.22">
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.20.20.22.1" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="A2.T5.20.20.22.1.1">ID</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.20.20.22.2" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="A2.T5.20.20.22.2.1">OOD</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.20.20.22.3" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="A2.T5.20.20.22.3.1">LS1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.20.20.22.4" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="A2.T5.20.20.22.4.1">LS2</span></td>
</tr>
<tr class="ltx_tr" id="A2.T5.20.20.23">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="6" id="A2.T5.20.20.23.1" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_italic" id="A2.T5.20.20.23.1.1">Qwen2.5-VL-3B-Instruct</span></td>
</tr>
<tr class="ltx_tr" id="A2.T5.5.5.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="A2.T5.5.5.5.6" style="padding-left:8.5pt;padding-right:8.5pt;">SFT Data</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.1.1.1.1" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="A2.T5.1.1.1.1.1">0.843<sub class="ltx_sub" id="A2.T5.1.1.1.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A2.T5.1.1.1.1.1.1.1">±0.002</span></sub></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.2.2.2.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.809<sub class="ltx_sub" id="A2.T5.2.2.2.2.1"><span class="ltx_text ltx_font_italic" id="A2.T5.2.2.2.2.1.1">±0.012</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.3.3.3.3" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="A2.T5.3.3.3.3.1">0.808<sub class="ltx_sub" id="A2.T5.3.3.3.3.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A2.T5.3.3.3.3.1.1.1">±0.009</span></sub></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.4.4.4.4" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="A2.T5.4.4.4.4.1">0.810<sub class="ltx_sub" id="A2.T5.4.4.4.4.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A2.T5.4.4.4.4.1.1.1">±0.005</span></sub></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.5.5.5.5" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="A2.T5.5.5.5.5.1">0.817<sub class="ltx_sub" id="A2.T5.5.5.5.5.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A2.T5.5.5.5.5.1.1.1">±0.007</span></sub></span></td>
</tr>
<tr class="ltx_tr" id="A2.T5.10.10.10">
<td class="ltx_td ltx_align_left" id="A2.T5.10.10.10.6" style="padding-left:8.5pt;padding-right:8.5pt;">    w/ Extra Corpora</td>
<td class="ltx_td ltx_align_center" id="A2.T5.6.6.6.1" style="padding-left:8.5pt;padding-right:8.5pt;">0.836<sub class="ltx_sub" id="A2.T5.6.6.6.1.1"><span class="ltx_text ltx_font_italic" id="A2.T5.6.6.6.1.1.1">±0.010</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="A2.T5.7.7.7.2" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="A2.T5.7.7.7.2.1">0.822<sub class="ltx_sub" id="A2.T5.7.7.7.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A2.T5.7.7.7.2.1.1.1">±0.014</span></sub></span></td>
<td class="ltx_td ltx_align_center" id="A2.T5.8.8.8.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.797<sub class="ltx_sub" id="A2.T5.8.8.8.3.1"><span class="ltx_text ltx_font_italic" id="A2.T5.8.8.8.3.1.1">±0.010</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="A2.T5.9.9.9.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.802<sub class="ltx_sub" id="A2.T5.9.9.9.4.1"><span class="ltx_text ltx_font_italic" id="A2.T5.9.9.9.4.1.1">±0.012</span></sub>
</td>
<td class="ltx_td ltx_align_center" id="A2.T5.10.10.10.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.814<sub class="ltx_sub" id="A2.T5.10.10.10.5.1"><span class="ltx_text ltx_font_italic" id="A2.T5.10.10.10.5.1.1">±0.011</span></sub>
</td>
</tr>
<tr class="ltx_tr" id="A2.T5.20.20.24">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="6" id="A2.T5.20.20.24.1" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_italic" id="A2.T5.20.20.24.1.1">Qwen2.5-VL-7B-Instruct</span></td>
</tr>
<tr class="ltx_tr" id="A2.T5.15.15.15">
<td class="ltx_td ltx_align_left ltx_border_t" id="A2.T5.15.15.15.6" style="padding-left:8.5pt;padding-right:8.5pt;">SFT Data</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.11.11.11.1" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="A2.T5.11.11.11.1.1">0.885<sub class="ltx_sub" id="A2.T5.11.11.11.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A2.T5.11.11.11.1.1.1.1">±0.003</span></sub></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.12.12.12.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.856<sub class="ltx_sub" id="A2.T5.12.12.12.2.1"><span class="ltx_text ltx_font_italic" id="A2.T5.12.12.12.2.1.1">±0.013</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.13.13.13.3" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="A2.T5.13.13.13.3.1">0.808<sub class="ltx_sub" id="A2.T5.13.13.13.3.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A2.T5.13.13.13.3.1.1.1">±0.005</span></sub></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.14.14.14.4" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="A2.T5.14.14.14.4.1">0.799<sub class="ltx_sub" id="A2.T5.14.14.14.4.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A2.T5.14.14.14.4.1.1.1">±0.004</span></sub></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T5.15.15.15.5" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="A2.T5.15.15.15.5.1">0.837<sub class="ltx_sub" id="A2.T5.15.15.15.5.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A2.T5.15.15.15.5.1.1.1">±0.006</span></sub></span></td>
</tr>
<tr class="ltx_tr" id="A2.T5.20.20.20">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A2.T5.20.20.20.6" style="padding-left:8.5pt;padding-right:8.5pt;">    w/ Extra Corpora</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T5.16.16.16.1" style="padding-left:8.5pt;padding-right:8.5pt;">0.881<sub class="ltx_sub" id="A2.T5.16.16.16.1.1"><span class="ltx_text ltx_font_italic" id="A2.T5.16.16.16.1.1.1">±0.007</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T5.17.17.17.2" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="A2.T5.17.17.17.2.1">0.895<sub class="ltx_sub" id="A2.T5.17.17.17.2.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="A2.T5.17.17.17.2.1.1.1">±0.021</span></sub></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T5.18.18.18.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.797<sub class="ltx_sub" id="A2.T5.18.18.18.3.1"><span class="ltx_text ltx_font_italic" id="A2.T5.18.18.18.3.1.1">±0.006</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T5.19.19.19.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.757<sub class="ltx_sub" id="A2.T5.19.19.19.4.1"><span class="ltx_text ltx_font_italic" id="A2.T5.19.19.19.4.1.1">±0.006</span></sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T5.20.20.20.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.832<sub class="ltx_sub" id="A2.T5.20.20.20.5.1"><span class="ltx_text ltx_font_italic" id="A2.T5.20.20.20.5.1.1">±0.010</span></sub>
</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Performance comparison of models fine-tuned with: 1) only SFT data and 2) SFT data and extra corpora (used for constructing the latent action space). Results are averaged over three runs. Best results within each model size are in <span class="ltx_text ltx_font_bold" id="A2.T5.22.1">bold</span>.</figcaption>
</figure>
</section>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Additional Empirical Results</h2>
<section class="ltx_subsection" id="A3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Analysis on Data Exposure</h3>
<div class="ltx_para" id="A3.SS1.p1">
<p class="ltx_p" id="A3.SS1.p1.1">To verify that gains arise from our latent action design, not merely from exposure to extra corpora that are used for constructing the latent action space, we conduct continued pre-training on Qwen2.5-VL-3B/7B using the same corpora, followed by SFT.
As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A2.T5" title="Table 5 ‣ B.4 Inference Details ‣ Appendix B Experimental Details ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">5</span></a>, this approach yields no consistent improvement, and even slight degradation on average. This confirms that the benefits of our latent action approach arise from the action space design, not from exposure to the extra corpora.</p>
</div>
</section>
<section class="ltx_subsection" id="A3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>Detailed Results on Fine-grained Dimensions</h3>
<div class="ltx_para" id="A3.SS2.p1">
<p class="ltx_p" id="A3.SS2.p1.1">We report the fine-grained performance across each evaluation dimensions, previously summarized in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#S4.F4" title="Figure 4 ‣ Performance on Fine-grained Dimensions ‣ 4.2 Main Results ‣ 4 Experiments ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">4</span></a>. Specifically, Tables <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A3.T6" title="Table 6 ‣ C.3 Case Study ‣ Appendix C Additional Empirical Results ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">6</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A3.T7" title="Table 7 ‣ C.3 Case Study ‣ Appendix C Additional Empirical Results ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">7</span></a> present results on the in-distribution (ID) and out-of-distribution (OOD) splits of MMRole, respectively. Tables <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A3.T8" title="Table 8 ‣ C.3 Case Study ‣ Appendix C Additional Empirical Results ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">8</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A3.T9" title="Table 9 ‣ C.3 Case Study ‣ Appendix C Additional Empirical Results ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">9</span></a> show results on the LS1 and LS2 subsets of PCogAlignBench. All results are obtained using the Qwen2.5-VL-3B-Instruct model.</p>
</div>
</section>
<section class="ltx_subsection" id="A3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.3 </span>Case Study</h3>
<div class="ltx_para" id="A3.SS3.p1">
<p class="ltx_p" id="A3.SS3.p1.1">To intuitively illustrate the improvements in diversity and response quality achieved by our latent action RL during rollout, we present case studies on MMRole (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A3.F6" title="Figure 6 ‣ C.3 Case Study ‣ Appendix C Additional Empirical Results ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">6</span></a>) and PCogAlignBench (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2601.07516v1#A3.F7" title="Figure 7 ‣ C.3 Case Study ‣ Appendix C Additional Empirical Results ‣ Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions"><span class="ltx_text ltx_ref_tag">7</span></a>), respectively.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure class="ltx_table" id="A3.T6">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A3.T6.1" style="width:433.6pt;height:139.1pt;vertical-align:-67.2pt;"><span class="ltx_transformed_inner" style="transform:translate(-12.7pt,4.1pt) scale(0.944663915995952,0.944663915995952) ;">
<table class="ltx_tabular ltx_align_middle" id="A3.T6.1.1">
<tr class="ltx_tr" id="A3.T6.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="A3.T6.1.1.1.1" rowspan="2" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="A3.T6.1.1.1.1.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="8" id="A3.T6.1.1.1.2" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="A3.T6.1.1.1.2.1">MMRole (ID)</span></td>
</tr>
<tr class="ltx_tr" id="A3.T6.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.2.1" style="padding-left:8.5pt;padding-right:8.5pt;">IA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.2.2" style="padding-left:8.5pt;padding-right:8.5pt;">Flu</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.2.3" style="padding-left:8.5pt;padding-right:8.5pt;">Coh</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.2.4" style="padding-left:8.5pt;padding-right:8.5pt;">ITR</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.2.5" style="padding-left:8.5pt;padding-right:8.5pt;">RA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.2.6" style="padding-left:8.5pt;padding-right:8.5pt;">PC</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.2.7" style="padding-left:8.5pt;padding-right:8.5pt;">KC</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.2.8" style="padding-left:8.5pt;padding-right:8.5pt;">TC</td>
</tr>
<tr class="ltx_tr" id="A3.T6.1.1.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="A3.T6.1.1.3.1" style="padding-left:8.5pt;padding-right:8.5pt;">Base</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.3.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.721</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.3.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.897</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.3.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.802</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.3.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.743</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.3.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.734</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.3.7" style="padding-left:8.5pt;padding-right:8.5pt;">0.629</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.3.8" style="padding-left:8.5pt;padding-right:8.5pt;">0.674</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.3.9" style="padding-left:8.5pt;padding-right:8.5pt;">0.628</td>
</tr>
<tr class="ltx_tr" id="A3.T6.1.1.4">
<td class="ltx_td ltx_align_left" id="A3.T6.1.1.4.1" style="padding-left:8.5pt;padding-right:8.5pt;">SFT</td>
<td class="ltx_td ltx_align_center" id="A3.T6.1.1.4.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.837</td>
<td class="ltx_td ltx_align_center" id="A3.T6.1.1.4.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.936</td>
<td class="ltx_td ltx_align_center" id="A3.T6.1.1.4.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.894</td>
<td class="ltx_td ltx_align_center" id="A3.T6.1.1.4.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.858</td>
<td class="ltx_td ltx_align_center" id="A3.T6.1.1.4.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.858</td>
<td class="ltx_td ltx_align_center" id="A3.T6.1.1.4.7" style="padding-left:8.5pt;padding-right:8.5pt;">0.776</td>
<td class="ltx_td ltx_align_center" id="A3.T6.1.1.4.8" style="padding-left:8.5pt;padding-right:8.5pt;">0.822</td>
<td class="ltx_td ltx_align_center" id="A3.T6.1.1.4.9" style="padding-left:8.5pt;padding-right:8.5pt;">0.760</td>
</tr>
<tr class="ltx_tr" id="A3.T6.1.1.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="A3.T6.1.1.5.1" style="padding-left:8.5pt;padding-right:8.5pt;">GRPO (Token)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.5.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.837</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.5.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.916</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.5.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.866</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.5.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.847</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.5.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.848</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.5.7" style="padding-left:8.5pt;padding-right:8.5pt;">0.789</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.5.8" style="padding-left:8.5pt;padding-right:8.5pt;">0.828</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.5.9" style="padding-left:8.5pt;padding-right:8.5pt;">0.773</td>
</tr>
<tr class="ltx_tr" id="A3.T6.1.1.6">
<td class="ltx_td ltx_align_left" id="A3.T6.1.1.6.1" style="padding-left:8.5pt;padding-right:8.5pt;">GRPO (Latent Action)</td>
<td class="ltx_td ltx_align_center" id="A3.T6.1.1.6.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.937</td>
<td class="ltx_td ltx_align_center" id="A3.T6.1.1.6.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.963</td>
<td class="ltx_td ltx_align_center" id="A3.T6.1.1.6.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.951</td>
<td class="ltx_td ltx_align_center" id="A3.T6.1.1.6.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.967</td>
<td class="ltx_td ltx_align_center" id="A3.T6.1.1.6.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.965</td>
<td class="ltx_td ltx_align_center" id="A3.T6.1.1.6.7" style="padding-left:8.5pt;padding-right:8.5pt;">0.926</td>
<td class="ltx_td ltx_align_center" id="A3.T6.1.1.6.8" style="padding-left:8.5pt;padding-right:8.5pt;">0.965</td>
<td class="ltx_td ltx_align_center" id="A3.T6.1.1.6.9" style="padding-left:8.5pt;padding-right:8.5pt;">0.919</td>
</tr>
<tr class="ltx_tr" id="A3.T6.1.1.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="A3.T6.1.1.7.1" style="padding-left:8.5pt;padding-right:8.5pt;">Dr.GRPO (Token)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.7.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.861</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.7.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.946</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.7.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.907</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.7.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.871</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.7.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.883</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.7.7" style="padding-left:8.5pt;padding-right:8.5pt;">0.816</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.7.8" style="padding-left:8.5pt;padding-right:8.5pt;">0.857</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.7.9" style="padding-left:8.5pt;padding-right:8.5pt;">0.794</td>
</tr>
<tr class="ltx_tr" id="A3.T6.1.1.8">
<td class="ltx_td ltx_align_left" id="A3.T6.1.1.8.1" style="padding-left:8.5pt;padding-right:8.5pt;">Dr.GRPO (Latent Action)</td>
<td class="ltx_td ltx_align_center" id="A3.T6.1.1.8.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.947</td>
<td class="ltx_td ltx_align_center" id="A3.T6.1.1.8.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.966</td>
<td class="ltx_td ltx_align_center" id="A3.T6.1.1.8.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.956</td>
<td class="ltx_td ltx_align_center" id="A3.T6.1.1.8.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.960</td>
<td class="ltx_td ltx_align_center" id="A3.T6.1.1.8.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.968</td>
<td class="ltx_td ltx_align_center" id="A3.T6.1.1.8.7" style="padding-left:8.5pt;padding-right:8.5pt;">0.931</td>
<td class="ltx_td ltx_align_center" id="A3.T6.1.1.8.8" style="padding-left:8.5pt;padding-right:8.5pt;">0.967</td>
<td class="ltx_td ltx_align_center" id="A3.T6.1.1.8.9" style="padding-left:8.5pt;padding-right:8.5pt;">0.928</td>
</tr>
<tr class="ltx_tr" id="A3.T6.1.1.9">
<td class="ltx_td ltx_align_left ltx_border_t" id="A3.T6.1.1.9.1" style="padding-left:8.5pt;padding-right:8.5pt;">DAPO (Token)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.9.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.852</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.9.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.940</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.9.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.900</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.9.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.863</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.9.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.868</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.9.7" style="padding-left:8.5pt;padding-right:8.5pt;">0.797</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.9.8" style="padding-left:8.5pt;padding-right:8.5pt;">0.842</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.9.9" style="padding-left:8.5pt;padding-right:8.5pt;">0.783</td>
</tr>
<tr class="ltx_tr" id="A3.T6.1.1.10">
<td class="ltx_td ltx_align_left" id="A3.T6.1.1.10.1" style="padding-left:8.5pt;padding-right:8.5pt;">DAPO (Latent Action)</td>
<td class="ltx_td ltx_align_center" id="A3.T6.1.1.10.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.932</td>
<td class="ltx_td ltx_align_center" id="A3.T6.1.1.10.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.962</td>
<td class="ltx_td ltx_align_center" id="A3.T6.1.1.10.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.948</td>
<td class="ltx_td ltx_align_center" id="A3.T6.1.1.10.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.943</td>
<td class="ltx_td ltx_align_center" id="A3.T6.1.1.10.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.952</td>
<td class="ltx_td ltx_align_center" id="A3.T6.1.1.10.7" style="padding-left:8.5pt;padding-right:8.5pt;">0.920</td>
<td class="ltx_td ltx_align_center" id="A3.T6.1.1.10.8" style="padding-left:8.5pt;padding-right:8.5pt;">0.960</td>
<td class="ltx_td ltx_align_center" id="A3.T6.1.1.10.9" style="padding-left:8.5pt;padding-right:8.5pt;">0.912</td>
</tr>
<tr class="ltx_tr" id="A3.T6.1.1.11">
<td class="ltx_td ltx_align_left ltx_border_t" id="A3.T6.1.1.11.1" style="padding-left:8.5pt;padding-right:8.5pt;">BNPO (Token)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.11.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.853</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.11.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.941</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.11.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.899</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.11.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.874</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.11.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.876</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.11.7" style="padding-left:8.5pt;padding-right:8.5pt;">0.803</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.11.8" style="padding-left:8.5pt;padding-right:8.5pt;">0.846</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.1.1.11.9" style="padding-left:8.5pt;padding-right:8.5pt;">0.787</td>
</tr>
<tr class="ltx_tr" id="A3.T6.1.1.12">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A3.T6.1.1.12.1" style="padding-left:8.5pt;padding-right:8.5pt;">BNPO (Latent Action)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T6.1.1.12.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.930</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T6.1.1.12.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.959</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T6.1.1.12.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.944</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T6.1.1.12.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.950</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T6.1.1.12.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.951</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T6.1.1.12.7" style="padding-left:8.5pt;padding-right:8.5pt;">0.919</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T6.1.1.12.8" style="padding-left:8.5pt;padding-right:8.5pt;">0.957</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T6.1.1.12.9" style="padding-left:8.5pt;padding-right:8.5pt;">0.908</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Fine-grained performance on MMRole (ID set), using the LLM-as-a-Judge metric. Results are averaged over three runs. We conduct experiments using Qwen2.5-VL-3B-Instruct. Dimensions: Instruction Adherence (IA); Fluency (Flu); Coherency (Coh); Image-Text Relevance (ITR); Response Accuracy (RA); Personality Consistency (OC); Knowledge Consistency (KC); Tone Consistency (TC).</figcaption>
</figure>
<figure class="ltx_table" id="A3.T7">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A3.T7.1" style="width:433.6pt;height:139.1pt;vertical-align:-67.2pt;"><span class="ltx_transformed_inner" style="transform:translate(-12.7pt,4.1pt) scale(0.944663915995952,0.944663915995952) ;">
<table class="ltx_tabular ltx_align_middle" id="A3.T7.1.1">
<tr class="ltx_tr" id="A3.T7.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="A3.T7.1.1.1.1" rowspan="2" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.1.1.1.1.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="8" id="A3.T7.1.1.1.2" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="A3.T7.1.1.1.2.1">MMRole (OOD)</span></td>
</tr>
<tr class="ltx_tr" id="A3.T7.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.2.1" style="padding-left:8.5pt;padding-right:8.5pt;">IA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.2.2" style="padding-left:8.5pt;padding-right:8.5pt;">Flu</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.2.3" style="padding-left:8.5pt;padding-right:8.5pt;">Coh</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.2.4" style="padding-left:8.5pt;padding-right:8.5pt;">ITR</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.2.5" style="padding-left:8.5pt;padding-right:8.5pt;">RA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.2.6" style="padding-left:8.5pt;padding-right:8.5pt;">PC</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.2.7" style="padding-left:8.5pt;padding-right:8.5pt;">KC</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.2.8" style="padding-left:8.5pt;padding-right:8.5pt;">TC</td>
</tr>
<tr class="ltx_tr" id="A3.T7.1.1.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="A3.T7.1.1.3.1" style="padding-left:8.5pt;padding-right:8.5pt;">Base</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.3.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.682</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.3.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.887</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.3.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.754</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.3.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.704</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.3.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.693</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.3.7" style="padding-left:8.5pt;padding-right:8.5pt;">0.588</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.3.8" style="padding-left:8.5pt;padding-right:8.5pt;">0.595</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.3.9" style="padding-left:8.5pt;padding-right:8.5pt;">0.594</td>
</tr>
<tr class="ltx_tr" id="A3.T7.1.1.4">
<td class="ltx_td ltx_align_left" id="A3.T7.1.1.4.1" style="padding-left:8.5pt;padding-right:8.5pt;">SFT</td>
<td class="ltx_td ltx_align_center" id="A3.T7.1.1.4.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.816</td>
<td class="ltx_td ltx_align_center" id="A3.T7.1.1.4.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.924</td>
<td class="ltx_td ltx_align_center" id="A3.T7.1.1.4.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.867</td>
<td class="ltx_td ltx_align_center" id="A3.T7.1.1.4.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.804</td>
<td class="ltx_td ltx_align_center" id="A3.T7.1.1.4.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.823</td>
<td class="ltx_td ltx_align_center" id="A3.T7.1.1.4.7" style="padding-left:8.5pt;padding-right:8.5pt;">0.749</td>
<td class="ltx_td ltx_align_center" id="A3.T7.1.1.4.8" style="padding-left:8.5pt;padding-right:8.5pt;">0.760</td>
<td class="ltx_td ltx_align_center" id="A3.T7.1.1.4.9" style="padding-left:8.5pt;padding-right:8.5pt;">0.729</td>
</tr>
<tr class="ltx_tr" id="A3.T7.1.1.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="A3.T7.1.1.5.1" style="padding-left:8.5pt;padding-right:8.5pt;">GRPO (Token)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.5.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.798</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.5.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.873</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.5.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.812</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.5.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.825</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.5.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.834</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.5.7" style="padding-left:8.5pt;padding-right:8.5pt;">0.735</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.5.8" style="padding-left:8.5pt;padding-right:8.5pt;">0.764</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.5.9" style="padding-left:8.5pt;padding-right:8.5pt;">0.728</td>
</tr>
<tr class="ltx_tr" id="A3.T7.1.1.6">
<td class="ltx_td ltx_align_left" id="A3.T7.1.1.6.1" style="padding-left:8.5pt;padding-right:8.5pt;">GRPO (Latent Action)</td>
<td class="ltx_td ltx_align_center" id="A3.T7.1.1.6.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.904</td>
<td class="ltx_td ltx_align_center" id="A3.T7.1.1.6.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.960</td>
<td class="ltx_td ltx_align_center" id="A3.T7.1.1.6.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.917</td>
<td class="ltx_td ltx_align_center" id="A3.T7.1.1.6.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.983</td>
<td class="ltx_td ltx_align_center" id="A3.T7.1.1.6.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.962</td>
<td class="ltx_td ltx_align_center" id="A3.T7.1.1.6.7" style="padding-left:8.5pt;padding-right:8.5pt;">0.859</td>
<td class="ltx_td ltx_align_center" id="A3.T7.1.1.6.8" style="padding-left:8.5pt;padding-right:8.5pt;">0.877</td>
<td class="ltx_td ltx_align_center" id="A3.T7.1.1.6.9" style="padding-left:8.5pt;padding-right:8.5pt;">0.856</td>
</tr>
<tr class="ltx_tr" id="A3.T7.1.1.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="A3.T7.1.1.7.1" style="padding-left:8.5pt;padding-right:8.5pt;">Dr.GRPO (Token)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.7.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.844</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.7.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.933</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.7.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.878</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.7.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.783</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.7.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.812</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.7.7" style="padding-left:8.5pt;padding-right:8.5pt;">0.770</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.7.8" style="padding-left:8.5pt;padding-right:8.5pt;">0.798</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.7.9" style="padding-left:8.5pt;padding-right:8.5pt;">0.766</td>
</tr>
<tr class="ltx_tr" id="A3.T7.1.1.8">
<td class="ltx_td ltx_align_left" id="A3.T7.1.1.8.1" style="padding-left:8.5pt;padding-right:8.5pt;">Dr.GRPO (Latent Action)</td>
<td class="ltx_td ltx_align_center" id="A3.T7.1.1.8.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.902</td>
<td class="ltx_td ltx_align_center" id="A3.T7.1.1.8.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.945</td>
<td class="ltx_td ltx_align_center" id="A3.T7.1.1.8.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.930</td>
<td class="ltx_td ltx_align_center" id="A3.T7.1.1.8.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.932</td>
<td class="ltx_td ltx_align_center" id="A3.T7.1.1.8.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.934</td>
<td class="ltx_td ltx_align_center" id="A3.T7.1.1.8.7" style="padding-left:8.5pt;padding-right:8.5pt;">0.892</td>
<td class="ltx_td ltx_align_center" id="A3.T7.1.1.8.8" style="padding-left:8.5pt;padding-right:8.5pt;">0.908</td>
<td class="ltx_td ltx_align_center" id="A3.T7.1.1.8.9" style="padding-left:8.5pt;padding-right:8.5pt;">0.887</td>
</tr>
<tr class="ltx_tr" id="A3.T7.1.1.9">
<td class="ltx_td ltx_align_left ltx_border_t" id="A3.T7.1.1.9.1" style="padding-left:8.5pt;padding-right:8.5pt;">DAPO (Token)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.9.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.825</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.9.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.911</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.9.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.845</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.9.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.785</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.9.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.799</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.9.7" style="padding-left:8.5pt;padding-right:8.5pt;">0.756</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.9.8" style="padding-left:8.5pt;padding-right:8.5pt;">0.770</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.9.9" style="padding-left:8.5pt;padding-right:8.5pt;">0.751</td>
</tr>
<tr class="ltx_tr" id="A3.T7.1.1.10">
<td class="ltx_td ltx_align_left" id="A3.T7.1.1.10.1" style="padding-left:8.5pt;padding-right:8.5pt;">DAPO (Latent Action)</td>
<td class="ltx_td ltx_align_center" id="A3.T7.1.1.10.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.883</td>
<td class="ltx_td ltx_align_center" id="A3.T7.1.1.10.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.946</td>
<td class="ltx_td ltx_align_center" id="A3.T7.1.1.10.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.909</td>
<td class="ltx_td ltx_align_center" id="A3.T7.1.1.10.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.931</td>
<td class="ltx_td ltx_align_center" id="A3.T7.1.1.10.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.915</td>
<td class="ltx_td ltx_align_center" id="A3.T7.1.1.10.7" style="padding-left:8.5pt;padding-right:8.5pt;">0.842</td>
<td class="ltx_td ltx_align_center" id="A3.T7.1.1.10.8" style="padding-left:8.5pt;padding-right:8.5pt;">0.843</td>
<td class="ltx_td ltx_align_center" id="A3.T7.1.1.10.9" style="padding-left:8.5pt;padding-right:8.5pt;">0.840</td>
</tr>
<tr class="ltx_tr" id="A3.T7.1.1.11">
<td class="ltx_td ltx_align_left ltx_border_t" id="A3.T7.1.1.11.1" style="padding-left:8.5pt;padding-right:8.5pt;">BNPO (Token)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.11.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.814</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.11.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.907</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.11.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.848</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.11.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.775</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.11.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.800</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.11.7" style="padding-left:8.5pt;padding-right:8.5pt;">0.754</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.11.8" style="padding-left:8.5pt;padding-right:8.5pt;">0.762</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T7.1.1.11.9" style="padding-left:8.5pt;padding-right:8.5pt;">0.746</td>
</tr>
<tr class="ltx_tr" id="A3.T7.1.1.12">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A3.T7.1.1.12.1" style="padding-left:8.5pt;padding-right:8.5pt;">BNPO (Latent Action)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T7.1.1.12.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.893</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T7.1.1.12.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.931</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T7.1.1.12.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.898</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T7.1.1.12.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.942</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T7.1.1.12.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.930</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T7.1.1.12.7" style="padding-left:8.5pt;padding-right:8.5pt;">0.862</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T7.1.1.12.8" style="padding-left:8.5pt;padding-right:8.5pt;">0.879</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T7.1.1.12.9" style="padding-left:8.5pt;padding-right:8.5pt;">0.868</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Fine-grained performance on MMRole (OOD set), using the LLM-as-a-Judge metric. Results are averaged over three runs. We conduct experiments using Qwen2.5-VL-3B-Instruct. Dimensions: Instruction Adherence (IA); Fluency (Flu); Coherency (Coh); Image-Text Relevance (ITR); Response Accuracy (RA); Personality Consistency (OC); Knowledge Consistency (KC); Tone Consistency (TC).</figcaption>
</figure>
<figure class="ltx_table" id="A3.T8">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A3.T8.1" style="width:346.9pt;height:150.5pt;vertical-align:-72.7pt;"><span class="ltx_transformed_inner" style="transform:translate(3.7pt,-1.6pt) scale(1.02187320135376,1.02187320135376) ;">
<table class="ltx_tabular ltx_align_middle" id="A3.T8.1.1">
<tr class="ltx_tr" id="A3.T8.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="A3.T8.1.1.1.1" rowspan="2" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="A3.T8.1.1.1.1.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="5" id="A3.T8.1.1.1.2" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="A3.T8.1.1.1.2.1">PCogAlignBench (LS1)</span></td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.1.2.1" style="padding-left:8.5pt;padding-right:8.5pt;">RSA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.1.2.2" style="padding-left:8.5pt;padding-right:8.5pt;">BBA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.1.2.3" style="padding-left:8.5pt;padding-right:8.5pt;">MFA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.1.2.4" style="padding-left:8.5pt;padding-right:8.5pt;">CA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.1.2.5" style="padding-left:8.5pt;padding-right:8.5pt;">CF</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.1.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="A3.T8.1.1.3.1" style="padding-left:8.5pt;padding-right:8.5pt;">Base</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.1.3.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.697</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.1.3.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.698</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.1.3.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.599</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.1.3.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.700</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.1.3.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.696</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.1.4">
<td class="ltx_td ltx_align_left" id="A3.T8.1.1.4.1" style="padding-left:8.5pt;padding-right:8.5pt;">SFT</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.1.4.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.775</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.1.4.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.791</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.1.4.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.801</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.1.4.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.808</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.1.4.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.864</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.1.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="A3.T8.1.1.5.1" style="padding-left:8.5pt;padding-right:8.5pt;">GRPO (Token)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.1.5.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.803</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.1.5.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.832</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.1.5.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.855</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.1.5.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.841</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.1.5.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.896</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.1.6">
<td class="ltx_td ltx_align_left" id="A3.T8.1.1.6.1" style="padding-left:8.5pt;padding-right:8.5pt;">GRPO (Latent Action)</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.1.6.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.825</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.1.6.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.864</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.1.6.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.884</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.1.6.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.863</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.1.6.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.920</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.1.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="A3.T8.1.1.7.1" style="padding-left:8.5pt;padding-right:8.5pt;">Dr.GRPO (Token)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.1.7.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.797</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.1.7.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.821</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.1.7.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.839</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.1.7.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.834</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.1.7.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.882</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.1.8">
<td class="ltx_td ltx_align_left" id="A3.T8.1.1.8.1" style="padding-left:8.5pt;padding-right:8.5pt;">Dr.GRPO (Latent Action)</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.1.8.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.830</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.1.8.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.871</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.1.8.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.889</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.1.8.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.864</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.1.8.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.918</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.1.9">
<td class="ltx_td ltx_align_left ltx_border_t" id="A3.T8.1.1.9.1" style="padding-left:8.5pt;padding-right:8.5pt;">DAPO (Token)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.1.9.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.794</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.1.9.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.829</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.1.9.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.832</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.1.9.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.832</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.1.9.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.890</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.1.10">
<td class="ltx_td ltx_align_left" id="A3.T8.1.1.10.1" style="padding-left:8.5pt;padding-right:8.5pt;">DAPO (Latent Action)</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.1.10.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.833</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.1.10.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.878</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.1.10.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.897</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.1.10.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.863</td>
<td class="ltx_td ltx_align_center" id="A3.T8.1.1.10.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.922</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.1.11">
<td class="ltx_td ltx_align_left ltx_border_t" id="A3.T8.1.1.11.1" style="padding-left:8.5pt;padding-right:8.5pt;">BNPO (Token)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.1.11.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.806</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.1.11.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.845</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.1.11.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.853</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.1.11.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.838</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T8.1.1.11.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.901</td>
</tr>
<tr class="ltx_tr" id="A3.T8.1.1.12">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A3.T8.1.1.12.1" style="padding-left:8.5pt;padding-right:8.5pt;">BNPO (Latent Action)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T8.1.1.12.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.826</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T8.1.1.12.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.872</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T8.1.1.12.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.880</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T8.1.1.12.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.862</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T8.1.1.12.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.920</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Fine-grained performance on PCogAlignBench (LS1 set), using the LLM-as-a-Judge metric. Results are averaged over three runs. We conduct experiments using Qwen2.5-VL-3B-Instruct. Dimensions: Role-Set Awareness (RSA); Body Behavior Awareness (BBA); Mind Feelings Awareness (MFA); Contextual Awareness (CA); Conversational Flow (CF).</figcaption>
</figure>
<figure class="ltx_table" id="A3.T9">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A3.T9.1" style="width:346.9pt;height:150.5pt;vertical-align:-72.7pt;"><span class="ltx_transformed_inner" style="transform:translate(3.7pt,-1.6pt) scale(1.02187320135376,1.02187320135376) ;">
<table class="ltx_tabular ltx_align_middle" id="A3.T9.1.1">
<tr class="ltx_tr" id="A3.T9.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="A3.T9.1.1.1.1" rowspan="2" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="A3.T9.1.1.1.1.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="5" id="A3.T9.1.1.1.2" style="padding-left:8.5pt;padding-right:8.5pt;"><span class="ltx_text ltx_font_bold" id="A3.T9.1.1.1.2.1">PCogAlignBench (LS2)</span></td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.1.2.1" style="padding-left:8.5pt;padding-right:8.5pt;">RSA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.1.2.2" style="padding-left:8.5pt;padding-right:8.5pt;">BBA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.1.2.3" style="padding-left:8.5pt;padding-right:8.5pt;">MFA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.1.2.4" style="padding-left:8.5pt;padding-right:8.5pt;">CA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.1.2.5" style="padding-left:8.5pt;padding-right:8.5pt;">CF</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.1.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="A3.T9.1.1.3.1" style="padding-left:8.5pt;padding-right:8.5pt;">Base</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.1.3.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.690</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.1.3.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.751</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.1.3.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.582</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.1.3.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.671</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.1.3.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.686</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.1.4">
<td class="ltx_td ltx_align_left" id="A3.T9.1.1.4.1" style="padding-left:8.5pt;padding-right:8.5pt;">SFT</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.1.4.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.781</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.1.4.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.802</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.1.4.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.806</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.1.4.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.796</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.1.4.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.863</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.1.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="A3.T9.1.1.5.1" style="padding-left:8.5pt;padding-right:8.5pt;">GRPO (Token)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.1.5.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.815</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.1.5.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.845</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.1.5.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.857</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.1.5.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.815</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.1.5.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.893</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.1.6">
<td class="ltx_td ltx_align_left" id="A3.T9.1.1.6.1" style="padding-left:8.5pt;padding-right:8.5pt;">GRPO (Latent Action)</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.1.6.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.797</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.1.6.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.839</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.1.6.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.850</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.1.6.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.814</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.1.6.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.901</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.1.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="A3.T9.1.1.7.1" style="padding-left:8.5pt;padding-right:8.5pt;">Dr.GRPO (Token)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.1.7.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.802</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.1.7.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.839</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.1.7.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.833</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.1.7.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.818</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.1.7.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.878</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.1.8">
<td class="ltx_td ltx_align_left" id="A3.T9.1.1.8.1" style="padding-left:8.5pt;padding-right:8.5pt;">Dr.GRPO (Latent Action)</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.1.8.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.793</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.1.8.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.838</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.1.8.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.845</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.1.8.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.806</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.1.8.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.894</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.1.9">
<td class="ltx_td ltx_align_left ltx_border_t" id="A3.T9.1.1.9.1" style="padding-left:8.5pt;padding-right:8.5pt;">DAPO (Token)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.1.9.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.799</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.1.9.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.825</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.1.9.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.827</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.1.9.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.804</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.1.9.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.884</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.1.10">
<td class="ltx_td ltx_align_left" id="A3.T9.1.1.10.1" style="padding-left:8.5pt;padding-right:8.5pt;">DAPO (Latent Action)</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.1.10.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.790</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.1.10.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.832</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.1.10.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.843</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.1.10.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.802</td>
<td class="ltx_td ltx_align_center" id="A3.T9.1.1.10.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.895</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.1.11">
<td class="ltx_td ltx_align_left ltx_border_t" id="A3.T9.1.1.11.1" style="padding-left:8.5pt;padding-right:8.5pt;">BNPO (Token)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.1.11.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.800</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.1.11.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.846</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.1.11.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.836</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.1.11.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.815</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T9.1.1.11.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.885</td>
</tr>
<tr class="ltx_tr" id="A3.T9.1.1.12">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A3.T9.1.1.12.1" style="padding-left:8.5pt;padding-right:8.5pt;">BNPO (Latent Action)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T9.1.1.12.2" style="padding-left:8.5pt;padding-right:8.5pt;">0.791</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T9.1.1.12.3" style="padding-left:8.5pt;padding-right:8.5pt;">0.835</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T9.1.1.12.4" style="padding-left:8.5pt;padding-right:8.5pt;">0.841</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T9.1.1.12.5" style="padding-left:8.5pt;padding-right:8.5pt;">0.809</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T9.1.1.12.6" style="padding-left:8.5pt;padding-right:8.5pt;">0.895</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>Fine-grained performance on PCogAlignBench (LS2 set), using the LLM-as-a-Judge metric. Results are averaged over three runs. We conduct experiments using Qwen2.5-VL-3B-Instruct. Dimensions: Role-Set Awareness (RSA); Body Behavior Awareness (BBA); Mind Feelings Awareness (MFA); Contextual Awareness (CA); Conversational Flow (CF).</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="945" id="A3.F6.g1" src="x6.png" width="654"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>A case study on the MMRole dataset. From this example, we observe that latent-action RL yields more diverse responses during rollout compared to token-level RL. Moreover, the generated responses using latent actions better align with the emotional traits expected of the given character. The RL algorithm used here is GRPO, with Qwen2.5-VL-3B-Instruct as the base model.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="945" id="A3.F7.g1" src="x7.png" width="654"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>A case study on the PCogAlignBench dataset. As shown in this example, latent action RL produces more diverse responses during rollout compared to token-level RL. Moreover, the generated responses using latent actions better incorporate personalized elements tailored to the user’s background. The RL algorithm used here is GRPO, with Qwen2.5-VL-3B-Instruct as the base model.</figcaption>
</figure>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jan  6 08:11:14 2026 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>


<!-- ===== END HTML ===== -->

<!-- ===== ABS URL: https://arxiv.org/abs/2601.07235 | HTTP 200 ===== -->

<!DOCTYPE html>
<html lang="en">

<head>  <title>[2601.07235] Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="apple-touch-icon" sizes="180x180" href="/static/browse/0.3.4/images/icons/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/static/browse/0.3.4/images/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/static/browse/0.3.4/images/icons/favicon-16x16.png">
  <link rel="manifest" href="/static/browse/0.3.4/images/icons/site.webmanifest">
  <link rel="mask-icon" href="/static/browse/0.3.4/images/icons/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">
  <link rel="stylesheet" type="text/css" media="screen" href="/static/browse/0.3.4/css/arXiv.css?v=20241206" />
  <link rel="stylesheet" type="text/css" media="print" href="/static/browse/0.3.4/css/arXiv-print.css?v=20200611" />
  <link rel="stylesheet" type="text/css" media="screen" href="/static/browse/0.3.4/css/browse_search.css" />
  <script language="javascript" src="/static/browse/0.3.4/js/accordion.js" ></script>
  <script language="javascript" src="/static/browse/0.3.4/js/optin-modal.js?v=20250819"></script>
  
  <link rel="canonical" href="https://arxiv.org/abs/2601.07235"/>
  <meta name="description" content="Abstract page for arXiv paper 2601.07235: Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><meta property="og:type" content="website" />
<meta property="og:site_name" content="arXiv.org" />
<meta property="og:title" content="Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges" />
<meta property="og:url" content="https://arxiv.org/abs/2601.07235v1" />
<meta property="og:image" content="/static/browse/0.3.4/images/arxiv-logo-fb.png" />
<meta property="og:image:secure_url" content="/static/browse/0.3.4/images/arxiv-logo-fb.png" />
<meta property="og:image:width" content="1200" />
<meta property="og:image:height" content="700" />
<meta property="og:image:alt" content="arXiv logo"/>
<meta property="og:description" content="This paper presents a comprehensive survey of sentiment analysis methods for movie reviews, a benchmark task that has played a central role in advancing natural language processing. We review the evolution of techniques from early lexicon-based and classical machine learning approaches to modern deep learning architectures and large language models, covering widely used datasets such as IMDb, Rotten Tomatoes, and SST-2, and models ranging from Naive Bayes and support vector machines to LSTM networks, BERT, and attention-based transformers. Beyond summarizing prior work, this survey differentiates itself by offering a comparative, challenge-driven analysis of how these modeling paradigms address domain-specific issues such as sarcasm, negation, contextual ambiguity, and domain shift, which remain open problems in existing literature. Unlike earlier reviews that focus primarily on text-only pipelines, we also synthesize recent advances in multimodal sentiment analysis that integrate textual, audio, and visual cues from movie trailers and clips. In addition, we examine emerging concerns related to interpretability, fairness, and robustness that are often underexplored in prior surveys, and we outline future research directions including zero-shot and few-shot learning, hybrid symbolic--neural models, and real-time deployment considerations. Overall, this abstract provides a domain-focused roadmap that highlights both established solutions and unresolved challenges toward building more accurate, generalizable, and explainable sentiment analysis systems for movie review data."/>
<meta name="twitter:site" content="@arxiv"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Sentiment Analysis on Movie Reviews: A Deep Dive into Modern..."/>
<meta name="twitter:description" content="This paper presents a comprehensive survey of sentiment analysis methods for movie reviews, a benchmark task that has played a central role in advancing natural language processing. We review the..."/>
<meta name="twitter:image" content="https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png"/>
<meta name="twitter:image:alt" content="arXiv logo"/>
  <link rel="stylesheet" media="screen" type="text/css" href="/static/browse/0.3.4/css/tooltip.css"/><link rel="stylesheet" media="screen" type="text/css" href="https://static.arxiv.org/js/bibex-dev/bibex.css?20200709"/>  <script src="/static/browse/0.3.4/js/mathjaxToggle.min.js" type="text/javascript"></script>  <script src="//code.jquery.com/jquery-latest.min.js" type="text/javascript"></script>
  <script src="//cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js" type="text/javascript"></script>
  <script src="//cdn.jsdelivr.net/npm/dompurify@2.3.5/dist/purify.min.js"></script>
  <script src="/static/browse/0.3.4/js/toggle-labs.js?20241022" type="text/javascript"></script>
  <script src="/static/browse/0.3.4/js/cite.js" type="text/javascript"></script><meta name="citation_title" content="Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges" /><meta name="citation_author" content="Gosai, Agnivo" /><meta name="citation_author" content="De, Shuvodeep" /><meta name="citation_author" content="Thankachan, Karun" /><meta name="citation_date" content="2026/01/12" /><meta name="citation_online_date" content="2026/01/12" /><meta name="citation_pdf_url" content="https://arxiv.org/pdf/2601.07235" /><meta name="citation_arxiv_id" content="2601.07235" /><meta name="citation_abstract" content="This paper presents a comprehensive survey of sentiment analysis methods for movie reviews, a benchmark task that has played a central role in advancing natural language processing. We review the evolution of techniques from early lexicon-based and classical machine learning approaches to modern deep learning architectures and large language models, covering widely used datasets such as IMDb, Rotten Tomatoes, and SST-2, and models ranging from Naive Bayes and support vector machines to LSTM networks, BERT, and attention-based transformers. Beyond summarizing prior work, this survey differentiates itself by offering a comparative, challenge-driven analysis of how these modeling paradigms address domain-specific issues such as sarcasm, negation, contextual ambiguity, and domain shift, which remain open problems in existing literature. Unlike earlier reviews that focus primarily on text-only pipelines, we also synthesize recent advances in multimodal sentiment analysis that integrate textual, audio, and visual cues from movie trailers and clips. In addition, we examine emerging concerns related to interpretability, fairness, and robustness that are often underexplored in prior surveys, and we outline future research directions including zero-shot and few-shot learning, hybrid symbolic--neural models, and real-time deployment considerations. Overall, this abstract provides a domain-focused roadmap that highlights both established solutions and unresolved challenges toward building more accurate, generalizable, and explainable sentiment analysis systems for movie review data." />
</head>

<body  class="with-cu-identity">
  
  
  <div class="flex-wrap-footer">
    <header>
      <a href="#content" class="is-sr-only">Skip to main content</a>
      <!-- start desktop header -->
      <div class="columns is-vcentered is-hidden-mobile" id="cu-identity">
        <div class="column" id="cu-logo">
          <a href="https://www.cornell.edu/"><img src="/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" /></a>
        </div><!-- /from April 7 at 1:00 AM to May 29 at 21:40 --><!-- /from May 2 at 1:00 AM to May 5 at 9:45 AM --><div class="column" id="support-ack">
          <span id="support-ack-url">We gratefully acknowledge support from the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors.</span>
          <a href="https://info.arxiv.org/about/donate.html" class="btn-header-donate">Donate</a>
        </div>
      </div>

      <div id="header" class="is-hidden-mobile">
<a aria-hidden="true" tabindex="-1" href="/IgnoreMe"></a>
  <div class="header-breadcrumbs is-hidden-mobile">
    <a href="/"><img src="/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg" alt="arxiv logo" style="height:40px;"/></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a> <span>&gt;</span> arXiv:2601.07235
  </div>

        <div class="columns is-vcentered is-mobile" style="justify-content: flex-end;">
        </div>

          <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
     </div><!-- /end desktop header -->

      <div class="mobile-header">
        <div class="columns is-mobile">
          <div class="column logo-arxiv"><a href="https://arxiv.org/"><img src="/static/browse/0.3.4/images/arxiv-logomark-small-white.svg" alt="arXiv logo" style="height:60px;" /></a></div>
          <div class="column logo-cornell"><a href="https://www.cornell.edu/">
            <picture>
              <source media="(min-width: 501px)"
                srcset="/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg  400w"
                sizes="400w" />
              <source srcset="/static/browse/0.3.4/images/icons/cu/cornell_seal_simple_black.svg 2x" />
              <img src="/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University Logo" />
            </picture>
          </a></div>
          <div class="column nav" id="toggle-container" role="menubar">
            <button class="toggle-control"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-white"><title>open search</title><path d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></button>
            <div class="mobile-toggle-block toggle-target">
              <form class="mobile-search-form" method="GET" action="https://arxiv.org/search">
                <div class="field has-addons">
                  <input class="input" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
                  <input type="hidden" name="source" value="header">
                  <input type="hidden" name="searchtype" value="all">
                  <button class="button">GO</button>
                </div>
              </form>
            </div>

            <button class="toggle-control"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-white" role="menu"><title>open navigation menu</title><path d="M16 132h416c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163 0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"/ ></svg></button>
            <div class="mobile-toggle-block toggle-target">
              <nav class="mobile-menu" aria-labelledby="mobilemenulabel">
                <h2 id="mobilemenulabel">quick links</h2>
                <ul>
                    <li><a href="https://arxiv.org/login">Login</a></li>
                    <li><a href="https://info.arxiv.org/help">Help Pages</a></li>
                    <li><a href="https://info.arxiv.org/about">About</a></li>
                </ul>
              </nav>
            </div>
          </div>
        </div>
      </div><!-- /end mobile-header -->
    </header>

    <main>
      <div id="content">
<!--
rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
         xmlns:dc="http://purl.org/dc/elements/1.1/"
         xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
    <rdf:Description
        rdf:about="/abs/2601.07235"
        dc:identifier="/abs/2601.07235"
        dc:title="Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"
        trackback:ping="/trackback/2601.07235" />
    </rdf:RDF>
--><div id="abs-outer">

  <div class="leftcolumn">
    <div class="subheader">
      <h1>Computer Science > Information Theory</h1>
    </div>

    <div class="header-breadcrumbs-mobile">
      <strong>arXiv:2601.07235</strong> (cs)
    </div>
<link rel="stylesheet" type="text/css" href="/static/base/1.0.1/css/abs.css">
<div id="content-inner">
  <div id="abs">
    <div class="dateline">
  [Submitted on 12 Jan 2026]</div>
    <h1 class="title mathjax"><span class="descriptor">Title:</span>Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges</h1>
    <div class="authors"><span class="descriptor">Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gosai,+A" rel="nofollow">Agnivo Gosai</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=De,+S" rel="nofollow">Shuvodeep De</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Thankachan,+K" rel="nofollow">Karun Thankachan</a></div>            <div id="download-button-info" hidden>View a PDF of the paper titled Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges, by Agnivo Gosai and 2 other authors</div>
    <a class="mobile-submission-download" href="/pdf/2601.07235">View PDF</a>
    <a class="mobile-submission-download" href="https://arxiv.org/html/2601.07235v1">HTML (experimental)</a>



    <blockquote class="abstract mathjax">
            <span class="descriptor">Abstract:</span>This paper presents a comprehensive survey of sentiment analysis methods for movie reviews, a benchmark task that has played a central role in advancing natural language processing. We review the evolution of techniques from early lexicon-based and classical machine learning approaches to modern deep learning architectures and large language models, covering widely used datasets such as IMDb, Rotten Tomatoes, and SST-2, and models ranging from Naive Bayes and support vector machines to LSTM networks, BERT, and attention-based transformers. Beyond summarizing prior work, this survey differentiates itself by offering a comparative, challenge-driven analysis of how these modeling paradigms address domain-specific issues such as sarcasm, negation, contextual ambiguity, and domain shift, which remain open problems in existing literature. Unlike earlier reviews that focus primarily on text-only pipelines, we also synthesize recent advances in multimodal sentiment analysis that integrate textual, audio, and visual cues from movie trailers and clips. In addition, we examine emerging concerns related to interpretability, fairness, and robustness that are often underexplored in prior surveys, and we outline future research directions including zero-shot and few-shot learning, hybrid symbolic--neural models, and real-time deployment considerations. Overall, this abstract provides a domain-focused roadmap that highlights both established solutions and unresolved challenges toward building more accurate, generalizable, and explainable sentiment analysis systems for movie review data.
    </blockquote>

    <!--CONTEXT-->
    <div class="metatable">
      <table summary="Additional metadata">        <tr>
          <td class="tablecell label">Comments:</td>
          <td class="tablecell comments mathjax">31 Pages; 1 figure; 108 references; ongoing paper that would be submitted to suitable Wiley journal</td>
        </tr>
<tr>
          <td class="tablecell label">Subjects:</td>
          <td class="tablecell subjects">
            <span class="primary-subject">Information Theory (cs.IT)</span></td>
        </tr><tr>
          <td class="tablecell label">Cite as:</td>
          <td class="tablecell arxivid"><span class="arxivid"><a href="https://arxiv.org/abs/2601.07235">arXiv:2601.07235</a> [cs.IT]</span></td>
        </tr>
        <tr>
          <td class="tablecell label">&nbsp;</td>
          <td class="tablecell arxividv">(or <span class="arxivid">
              <a href="https://arxiv.org/abs/2601.07235v1">arXiv:2601.07235v1</a> [cs.IT]</span> for this version)
          </td>
        </tr>
        <tr>
          <td class="tablecell label">&nbsp;</td>
          <td class="tablecell arxivdoi">              <a href="https://doi.org/10.48550/arXiv.2601.07235" id="arxiv-doi-link">https://doi.org/10.48550/arXiv.2601.07235</a><div class="button-and-tooltip">
              <button class="more-info" aria-describedby="more-info-desc-1">
                <svg height="15" role="presentation" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z" class=""></path></svg>
                <span class="visually-hidden">Focus to learn more</span>
              </button>
              <!-- tooltip description -->
              <div role="tooltip" id="more-info-desc-1">
                <span class="left-corner"></span>                  arXiv-issued DOI via DataCite (pending registration)</div>
            </div>
          </td>
        </tr></table>
    </div>
  </div>
</div>
    <div class="submission-history">
      <h2>Submission history</h2> From: Shuvodeep De [<a href="/show-email/096339dc/2601.07235" rel="nofollow">view email</a>]      <br/>    <strong>[v1]</strong>
        Mon, 12 Jan 2026 06:13:11 UTC (149 KB)<br/>
</div>
  </div>
  <!--end leftcolumn-->
<div class="extra-services">    <div class="full-text">
      <a name="other"></a>
      <span class="descriptor">Full-text links:</span>
      <h2>Access Paper:</h2>
      <ul>
  <div id="download-button-info" hidden>
View a PDF of the paper titled Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges, by Agnivo Gosai and 2 other authors</div><li><a href="/pdf/2601.07235" aria-describedby="download-button-info" accesskey="f" class="abs-button download-pdf">View PDF</a></li><li><a href="https://arxiv.org/html/2601.07235v1" class="abs-button" id="latexml-download-link">HTML (experimental)</a></li><li><a href="/src/2601.07235" class="abs-button download-eprint">TeX Source
 </a></li></ul>
      <div class="abs-license"><a href="http://creativecommons.org/licenses/by/4.0/" title="Rights to this article" class="has_license">
          <img alt="license icon" role="presentation" src="https://arxiv.org/icons/licenses/by-4.0.png"/>
          <span>view license</span>
        </a></div>
    </div>
    <!--end full-text-->    <div class="browse">
    Current browse context: <div class="current">cs.IT</div>

  <div class="prevnext">
<span class="arrow">
      <a class="abs-button prev-url" href="/prevnext?id=2601.07235&amp;function=prev&amp;context=cs.IT"
         accesskey="p" title="previous in cs.IT (accesskey p)" rel="nofollow">&lt;&nbsp;prev</a>
    </span>
    <span class="is-hidden-mobile">&nbsp; | &nbsp;</span>    <span class="arrow">
      <a class="abs-button next-url" href="/prevnext?id=2601.07235&amp;function=next&amp;context=cs.IT" accesskey="n"
         title="next in cs.IT (accesskey n)"  rel="nofollow">next&nbsp;&gt;</a>
    </span><br/>
  </div><div class="list">
    <a class="abs-button abs-button-grey abs-button-small context-new" href="/list/cs.IT/new"  rel="nofollow">new</a>
    <span class="is-hidden-mobile"> | </span>
    <a class="abs-button abs-button-grey abs-button-small context-recent" href="/list/cs.IT/recent" rel="nofollow">recent</a>
    <span class="is-hidden-mobile"> | </span><a class="abs-button abs-button-grey abs-button-small context-id" href="/list/cs.IT/2026-01" rel="nofollow">2026-01</a>
  </div><div class="abs-switch-cat">
    Change to browse by:
    <div class="switch context-change">
        <a href="/abs/2601.07235?context=cs" rel="nofollow">cs</a><br class="is-hidden-mobile">
        <a href="/abs/2601.07235?context=math" rel="nofollow">math</a><br class="is-hidden-mobile">
        <a class="subclass" href="/abs/2601.07235?context=math.IT" rel="nofollow">math.IT</a><br class="is-hidden-mobile">
    </div>
  </div>

    </div>
      <div class="extra-ref-cite">
        <h3>References &amp; Citations</h3>
        <ul>
          <li><a  class="abs-button abs-button-small cite-ads" href="https://ui.adsabs.harvard.edu/abs/arXiv:2601.07235">NASA ADS</a></li><li><a  class="abs-button abs-button-small cite-google-scholar" href="https://scholar.google.com/scholar_lookup?arxiv_id=2601.07235" target="_blank" rel="noopener">Google Scholar</a></li>
          <li><a  class="abs-button abs-button-small cite-semantic-scholar" href="https://api.semanticscholar.org/arXiv:2601.07235" target="_blank" rel="noopener">Semantic Scholar</a></li>
        </ul>
        <div style="clear:both;"></div>
      </div>

<div class='extra-ref-cite'>
    <span id='bib-cite-trigger' class="bib-cite-button abs-button">export BibTeX citation</span>
    <span id='bib-cite-loading' hidden='true'>Loading...</span>
</div>

<div id='bib-cite-modal' class='bib-modal' hidden='true'>
    <div class='bib-modal-content'>
        <div class='bib-modal-title'>
            <h2>BibTeX formatted citation</h2>
            <span class='bib-modal-close' >&times;</span>
        </div>
        <div>
            <textarea id='bib-cite-target' class="bib-citation-content" aria-label="loading the citation">loading...</textarea>
        </div>
        <div>
            <span>Data provided by: </span>
            <a id='bib-cite-source-api'></a>
        </div>
    </div>
</div><div class="bookmarks">
  <div><h3>Bookmark</h3></div><a class="abs-button abs-button-grey abs-button-small" href="http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2601.07235&amp;description=Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"
     title="Bookmark on BibSonomy">
    <img src="/static/browse/0.3.4/images/icons/social/bibsonomy.png"
         alt="BibSonomy logo"/>
  </a>
  <a class="abs-button abs-button-grey abs-button-small" href="https://reddit.com/submit?url=https://arxiv.org/abs/2601.07235&amp;title=Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"
     title="Bookmark on Reddit">
    <img src="/static/browse/0.3.4/images/icons/social/reddit.png"
         alt="Reddit logo"/>
  </a>
</div>  </div>
  <!--end extra-services-->
<!-- LABS AREA -->
<div id="labstabs">
  <div class="labstabs"><input type="radio" name="tabs" id="tabone"checked="checked">
    <label for="tabone">Bibliographic Tools</label>
    <div class="tab labs-display-bib">
      <h1>Bibliographic and Citation Tools</h1>
      <div class="toggle">
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input id="bibex-toggle" type="checkbox" class="lab-toggle"
                     data-script-url="/static/browse/0.3.4/bibex/bibex.js?20241202">
              <span class="slider"></span>
              <span class="is-sr-only">Bibliographic Explorer Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-bibex">Bibliographic Explorer</span> <em>(<a href="https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer">What is the Explorer?</a>)</em>
          </div>
        </div>
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="connectedpapers-toggle"
                type="checkbox"
                class="lab-toggle"
                data-script-url="/static/browse/0.3.4/js/connectedpapers.js"
                aria-labelledby="label-for-connected-papers">
              <span class="slider"></span>
              <span class="is-sr-only">Connected Papers Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-connected-papers">Connected Papers</span> <em>(<a href="https://www.connectedpapers.com/about" target="_blank">What is Connected Papers?</a>)</em>
          </div>
        </div><div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="litmaps-toggle"
                type="checkbox"
                class="lab-toggle"
                data-script-url="/static/browse/0.3.4/js/litmaps.js?20210617"
                aria-labelledby="label-for-litmaps">
              <span class="slider"></span>
              <span class="is-sr-only">Litmaps Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-litmaps">Litmaps</span> <em>(<a href="https://www.litmaps.co/" target="_blank">What is Litmaps?</a>)</em>
          </div>
        </div>
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="scite-toggle"
                type="checkbox"
                class="lab-toggle"
                data-script-url="/static/browse/0.3.4/js/scite.js?20210617"
                aria-labelledby="label-for-scite">
              <span class="slider"></span>
              <span class="is-sr-only">scite.ai Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-scite">scite Smart Citations</span> <em>(<a href="https://www.scite.ai/" target="_blank">What are Smart Citations?</a>)</em>
          </div>
        </div>
      </div>
        <div class="labs-content-placeholder labs-display" style="display: none;"></div>
        <div style="min-height: 15px" id="connectedpapers-output"></div>
        <div style="min-height: 15px" id="litmaps-open-in"></div>
        <div style="min-height: 15px" id="scite-open-in"></div>
    </div>


    <input type="radio" name="tabs" id="tabtwo">
    <label for="tabtwo">Code, Data, Media</label>
    <div class="tab">
      <h1>Code, Data and Media Associated with this Article</h1>
      <div class="toggle">
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="alphaxiv-toggle"
                data-script-url="/static/browse/0.3.4/js/alphaxiv.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-alphaxiv">
              <span class="slider"></span>
              <span class="is-sr-only">alphaXiv Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-alphaxiv">alphaXiv</span> <em>(<a href="https://alphaxiv.org/" target="_blank">What is alphaXiv?</a>)</em>
          </div>
        </div>

        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input        
                id="catalyzex-toggle"
                data-script-url="/static/browse/0.3.4/js/catalyzex.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-cx">
              <span class="slider"></span>
              <span class="is-sr-only">Links to Code Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-cx">CatalyzeX Code Finder for Papers</span> <em>(<a href="https://www.catalyzex.com" target="_blank">What is CatalyzeX?</a>)</em>
          </div>
        </div>

        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="dagshub-toggle"
                data-script-url="/static/browse/0.3.4/js/dagshub.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-dagshub">
              <span class="slider"></span>
              <span class="is-sr-only">DagsHub Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-dagshub">DagsHub</span> <em>(<a href="https://dagshub.com/" target="_blank">What is DagsHub?</a>)</em>
          </div>
        </div>
  
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="gotitpub-toggle"
                data-script-url="/static/browse/0.3.4/js/gotitpub.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-gotitpub">
              <span class="slider"></span>
              <span class="is-sr-only">GotitPub Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-gotitpub">Gotit.pub</span> <em>(<a href="http://gotit.pub/faq" target="_blank">What is GotitPub?</a>)</em>
          </div>
        </div>

        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="huggingface-toggle"
                data-script-url="/static/browse/0.3.4/js/huggingface.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-huggingface">
              <span class="slider"></span>
              <span class="is-sr-only">Huggingface Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-huggingface">Hugging Face</span> <em>(<a href="https://huggingface.co/huggingface" target="_blank">What is Huggingface?</a>)</em>
          </div>
        </div>

        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="paperwithcode-toggle"
                data-script-url="/static/browse/0.3.4/js/paperswithcode.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-pwc">
              <span class="slider"></span>
              <span class="is-sr-only">Links to Code Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-pwc">Papers with Code</span> <em>(<a href="https://paperswithcode.com/" target="_blank">What is Papers with Code?</a>)</em>
          </div>
        </div>


        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="sciencecast-toggle"
                data-script-url="/static/browse/0.3.4/js/sciencecast.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-sciencecast">
              <span class="slider"></span>
              <span class="is-sr-only">ScienceCast Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-sciencecast">ScienceCast</span> <em>(<a href="https://sciencecast.org/welcome" target="_blank">What is ScienceCast?</a>)</em>
          </div>
        </div>
      </div>

      <div id="alphaxiv-output" style="display:none"></div>
      <div id="catalyzex-output" style="display:none"></div>
      <div id="dagshub-output" style="display:none"></div>
      <div id="gotitpub-output" style="display:none"></div>
      <div id="pwc-output" style="display:none"></div>
      <div id="pwc-data-output" style="display:none"></div>
      <div id="sciencecast-output" style="display:none"></div>
      <div id="huggingface-output" style="display:none"></div>
    </div>


      <input type="radio" name="tabs" id="labstabs-demos-input">
      <label for="labstabs-demos-input" id="labstabs-demos-label">Demos</label>
      <div class="tab">
        <h1>Demos</h1>
        <div class="toggle">
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input
                  id="replicate-toggle"
                  data-script-url="/static/browse/0.3.4/js/replicate.js"
                  type="checkbox" class="lab-toggle" aria-labelledby="label-for-replicate">
                <span class="slider"></span>
                <span class="is-sr-only">Replicate Toggle</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-replicate">Replicate</span> <em>(<a href="https://replicate.com/docs/arxiv/about" target="_blank">What is Replicate?</a>)</em>
            </div>
          </div>
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input
                  id="spaces-toggle"
                  data-script-url="/static/browse/0.3.4/js/spaces.js"
                  type="checkbox" class="lab-toggle" aria-labelledby="label-for-spaces">
                <span class="slider"></span>
                <span class="is-sr-only">Spaces Toggle</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-spaces">Hugging Face Spaces</span> <em>(<a href="https://huggingface.co/docs/hub/spaces" target="_blank">What is Spaces?</a>)</em>
            </div>
          </div>
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input
                  id="txyz-toggle"
                  data-script-url="/static/browse/0.3.4/js/txyz.js"
                  type="checkbox" class="lab-toggle" aria-labelledby="label-for-txyz">
                <span class="slider"></span>
                <span class="is-sr-only">Spaces Toggle</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-txyz">TXYZ.AI</span> <em>(<a href="https://txyz.ai" target="_blank">What is TXYZ.AI?</a>)</em>
            </div>
          </div>
        </div>
        <div id="replicate-output"></div>
        <div id="spaces-output"></div>
        <div id="txyz-output"></div>
      </div>
      <input type="radio" name="tabs" id="tabfour">
      <label for="tabfour">Related Papers</label>
      <div class="tab">
        <h1>Recommenders and Search Tools</h1>
        <div class="toggle">
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input id="influenceflower-toggle"
                data-script-url="/static/browse/0.3.4/js/influenceflower.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-influenceflower">
                <span class="slider"></span>
                <span class="is-sr-only">Link to Influence Flower</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-influenceflower">Influence Flower</span> <em>(<a href="https://influencemap.cmlab.dev/" target="_blank">What are Influence Flowers?</a>)</em>
            </div>
          </div>
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input id="core-recommender-toggle" type="checkbox" class="lab-toggle" aria-labelledby="label-for-core">
                <span class="slider"></span>
                <span class="is-sr-only">Core recommender toggle</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-core">CORE Recommender</span> <em>(<a href="https://core.ac.uk/services/recommender">What is CORE?</a>)</em>
            </div>
          </div></div>
        <div id="influenceflower-output"></div>
        <div id="influenceflower-output-graph" style="display:none">
          <ul class="flower-tabs">
            <li class="active"><a class="btn tab-btn" onclick="openTab(event, 'tab-author')">Author</a></li>
            <li><a class="btn tab-btn" onclick="openTab(event, 'tab-venue')">Venue</a></li>
            <li><a class="btn tab-btn" onclick="openTab(event, 'tab-inst')">Institution</a></li>
            <li><a class="btn tab-btn" onclick="openTab(event, 'tab-topic')">Topic</a></li>
          </ul>
          <div class="flower-tab-content">
            <div class="tab-flower active" id="tab-author"><svg id="flower-graph-author"></svg></div>
            <div class="tab-flower" id="tab-venue"><svg id="flower-graph-venue"></svg></div>
            <div class="tab-flower" id="tab-inst"><svg id="flower-graph-inst"></svg></div>
            <div class="tab-flower" id="tab-topic"><svg id="flower-graph-topic"></svg></div>
          </div>
        </div>
        <div id="coreRecommenderOutput"></div>
        <div id="iarxivOutput"></div>
      </div>

      <input type="radio" name="tabs" id="tabfive">
      <label for="tabfive">
        About arXivLabs
      </label>
      <div class="tab">
        <div class="columns">
          <div class="column">
            <h1>arXivLabs: experimental projects with community collaborators</h1>
            <p>arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.</p>
            <p>Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.</p>
            <p>Have an idea for a project that will add value for arXiv's community? <a href="https://info.arxiv.org/labs/index.html"><strong>Learn more about arXivLabs</strong></a>.</p>
          </div>
          <div class="column is-narrow is-full-mobile">
            <p class="icon-labs"><svg xmlns="http://www.w3.org/2000/svg" role="presentation" viewBox="0 0 635.572 811"><path d="M175.6 676v27h-27v-27zm-54 27v27h27v-27zm-27 27v27h27v-27zm396-54v27h-27v-27zm0 27v27h27v-27zm27 27v27h27v-27zm-27-414h27v27h-27zm27 0h27v-27h-27zm27-27h27v-27h-27zm-396 45h-27v-27h27zm-27-54h-27v27h27zm-27-27h-27v27h27z"/><path d="M94.6 730v27h-27v-27zm477 0v27h-27v-27zm-27-495h27v27h-27zm-450 18h-27v-27h27zm477 9h27v27h-27zm-54 495h27v27h-27zm-423 0h27v27h-27zm-54-504h27v27h-27z" fill="#666"/><path d="M67.6 730v27h-27v-27zm54 54v27h-27v-27zm0-108v27h27v-27zm-27 27v27h27v-27zm-81 0v27h27v-27zm585 27v27h-27v-27zm-108-54v27h27v-27zm27 27v27h27v-27zm81 0v27h27v-27zm-54-495h27v27h-27zm-54 108h27v-27h-27zm27-27h27v-27h-27zm0-81h27v-27h-27zm-423 18h-27v-27h27zm54 54h-27v27h27zm-27-27h-27v27h27zm0-81h-27v27h27zm423 612v27h-27v-27zm81-522v27h-27v-27zm-585-9v27h-27v-27z" fill="#999"/><path d="M94.6 784v27h-27v-27zm-27-27v27h27v-27zm-27-54v27h27v-27zm27 0v27h27v-27zm0-27v27h27v-27zm27 0v27h27v-27zm0-27v27h27v-27zm27 0v27h27v-27zm-108 81v27h27v-27zm558 54v27h-27v-27zm-27-27v27h27v-27zm27-54v27h27v-27zm-27 0v27h27v-27zm0-27v27h27v-27zm-27 0v27h27v-27zm0-27v27h27v-27zm-27 0v27h27v-27zm108 81v27h27v-27zm0-495h27v27h-27zm-27 27h27v-27h-27zm-54-27h27v-27h-27zm0 27h27v-27h-27zm-27 0h27v-27h-27zm0 27h27v-27h-27zm-27 0h27v-27h-27zm0 27h27v-27h-27zm81-108h27v-27h-27zm-504 45h-27v-27h27zm27-27h-27v27h27zm54-27h-27v27h27zm0 27h-27v27h27zm27 0h-27v27h27zm0 27h-27v27h27zm27 0h-27v27h27zm0 27h-27v27h27zm-81-108h-27v27h27z" fill="#ccc"/><path d="M598.6 665.1H41.5C-76.5 667 176 280.2 176 280.2h53a46.5 46.5 0 0162.8-56.3 29.2 29.2 0 1128.5 35.9h-1a46.5 46.5 0 01-1.5 20.3l142.5-.1s255.3 387 138.3 385.1zM291 181a29.3 29.3 0 10-29.2-29.3A29.3 29.3 0 00291 181zm65.4-66.8a22.4 22.4 0 10-22.5-22.4 22.4 22.4 0 0022.5 22.4z" fill="#fc0"/><path d="M245.5 172V10h153v162s324 495 198 495h-558c-126 0 207-495 207-495zm126 54h56m-13 72h56m-9 72h56m-20 72h56m-22 72h56m-29 72h56m-457-45c20.8 41.7 87.3 81 160.7 81 72.1 0 142.1-38.2 163.4-81" fill="none" stroke="#000" stroke-miterlimit="10" stroke-width="20"/><path d="M273.3 421.7c0 31-9.8 56.3-21.9 56.3s-21.8-25.2-21.8-56.3 9.8-56.3 21.8-56.3 21.9 25.2 21.9 56.3zm114.4-56.3c-12 0-21.8 25.2-21.8 56.3s9.7 56.3 21.8 56.3 21.9-25.2 21.9-56.3-9.8-56.3-21.9-56.3zM150.1 526.6c-18.2 6.7-27.5 22.9-23.2 30.2s14.8-5.5 33-12.2 37.4-4.9 33-12.2-24.5-12.6-42.8-5.8zm296 5.8c-4.2 7.3 14.9 5.5 33.1 12.2s28.7 19.5 33 12.2-5-23.5-23.2-30.2-38.5-1.5-42.8 5.8z"/></svg></p>
          </div>
        </div>
      </div>

    </div>
</div>
<!-- END LABS AREA -->
  <div class="endorsers">
    <a href="/auth/show-endorsers/2601.07235" class="endorser-who" rel="nofollow">Which authors of this paper are endorsers?</a> |
    <a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="https://info.arxiv.org/help/mathjax.html">What is MathJax?</a>)
    <span class="help" style="font-style: normal; float: right; margin-top: 0; margin-right: 1em;"></span>
  </div>
  <script type="text/javascript" language="javascript">mathjaxToggle();</script>
</div>
      </div>
    </main>

    <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://info.arxiv.org/about">About</a></li>
                <li><a href="https://info.arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
                <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
  </div>

  <script src="/static/base/1.0.1/js/member_acknowledgement.js"></script>

</body>

</html>

<!-- ===== END ABS ===== -->

<!-- ===== HTML URL: https://arxiv.org/html/2601.07235v1 | HTTP 200 ===== -->

<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges</title>
<!--Generated on Mon Jan 12 06:04:53 2026 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="/static/browse/0.3.4/css/arxiv-html-papers-20250916.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2601.07235v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S1" title="In Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S1.SS1" title="In 1 Introduction ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1 </span>Background</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S1.SS2" title="In 1 Introduction ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2 </span>Significance and Motivation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S1.SS3" title="In 1 Introduction ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.3 </span>The Evolution of Sentiment Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S1.SS4" title="In 1 Introduction ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.4 </span>Limitations and Challenges</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S1.SS5" title="In 1 Introduction ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.5 </span>Novelty and Scope of this Review</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S2" title="In Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Problem Formulation and Taxonomy</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S2.SS1" title="In 2 Problem Formulation and Taxonomy ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Granularity of Sentiment Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S2.SS2" title="In 2 Problem Formulation and Taxonomy ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Types of Movie Review Data</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S2.SS2.SSS1" title="In 2.2 Types of Movie Review Data ‣ 2 Problem Formulation and Taxonomy ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>Unstructured Textual Reviews</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S2.SS2.SSS2" title="In 2.2 Types of Movie Review Data ‣ 2 Problem Formulation and Taxonomy ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>Structured Ratings and Metadata</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S2.SS2.SSS3" title="In 2.2 Types of Movie Review Data ‣ 2 Problem Formulation and Taxonomy ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.3 </span>Annotated Corpora for Fine-Grained Analysis</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S3" title="In Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Benchmark Datasets and Metrics</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S3.SS1" title="In 3 Benchmark Datasets and Metrics ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Foundational Datasets: Establishing the Benchmark Paradigm</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S3.SS1.SSS1" title="In 3.1 Foundational Datasets: Establishing the Benchmark Paradigm ‣ 3 Benchmark Datasets and Metrics ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>The Movie Reviews Corpus: Pioneering Binary Classification</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S3.SS1.SSS2" title="In 3.1 Foundational Datasets: Establishing the Benchmark Paradigm ‣ 3 Benchmark Datasets and Metrics ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>Expansion and Standardization: The IMDB-Large Dataset</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S3.SS2" title="In 3 Benchmark Datasets and Metrics ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Diversification and Multimodal Extensions</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S3.SS2.SSS1" title="In 3.2 Diversification and Multimodal Extensions ‣ 3 Benchmark Datasets and Metrics ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Stanford Sentiment Treebank: Phrase-Level Granularity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S3.SS2.SSS2" title="In 3.2 Diversification and Multimodal Extensions ‣ 3 Benchmark Datasets and Metrics ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Multimodal Sentiment: CMU-MOSI and MOSEI</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S3.SS2.SSS3" title="In 3.2 Diversification and Multimodal Extensions ‣ 3 Benchmark Datasets and Metrics ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.3 </span>Recent Benchmark Expansions: GoEmotions and TweetEval</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S3.SS3" title="In 3 Benchmark Datasets and Metrics ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Evaluation Metrics</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S3.SS3.SSS1" title="In 3.3 Evaluation Metrics ‣ 3 Benchmark Datasets and Metrics ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>The Dominance of Accuracy and Macro-F1</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S3.SS3.SSS2" title="In 3.3 Evaluation Metrics ‣ 3 Benchmark Datasets and Metrics ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.2 </span>The Limitations of Binary Classification Paradigms</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S3.SS4" title="In 3 Benchmark Datasets and Metrics ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Emerging Recognition of Benchmark Limitations</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S3.SS4.SSS1" title="In 3.4 Emerging Recognition of Benchmark Limitations ‣ 3 Benchmark Datasets and Metrics ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.1 </span>The Saturation Effect and Diminishing Returns</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S3.SS4.SSS2" title="In 3.4 Emerging Recognition of Benchmark Limitations ‣ 3 Benchmark Datasets and Metrics ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.2 </span>Domain and Temporal Representation Gaps</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S3.SS4.SSS3" title="In 3.4 Emerging Recognition of Benchmark Limitations ‣ 3 Benchmark Datasets and Metrics ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.3 </span>Inadequate Evaluation of Persistent Challenges</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S3.SS5" title="In 3 Benchmark Datasets and Metrics ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Toward More Comprehensive Evaluation Frameworks</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S3.SS5.SSS1" title="In 3.5 Toward More Comprehensive Evaluation Frameworks ‣ 3 Benchmark Datasets and Metrics ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5.1 </span>Multi-Dimensional Assessment Needs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S3.SS5.SSS2" title="In 3.5 Toward More Comprehensive Evaluation Frameworks ‣ 3 Benchmark Datasets and Metrics ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5.2 </span>Adversarial and Stress Testing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S3.SS5.SSS3" title="In 3.5 Toward More Comprehensive Evaluation Frameworks ‣ 3 Benchmark Datasets and Metrics ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5.3 </span>Resource-Aware Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S3.SS6" title="In 3 Benchmark Datasets and Metrics ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6 </span>The Path Forward: Integrated Benchmark Design</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S4" title="In Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Current Sentiment Analysis Models and Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S4.SS1" title="In 4 Current Sentiment Analysis Models and Methods ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Rule-Based and Lexicon-Based Methods</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S4.SS1.SSS1" title="In 4.1 Rule-Based and Lexicon-Based Methods ‣ 4 Current Sentiment Analysis Models and Methods ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>SentiWordNet</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S4.SS1.SSS2" title="In 4.1 Rule-Based and Lexicon-Based Methods ‣ 4 Current Sentiment Analysis Models and Methods ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>VADER</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S4.SS1.SSS3" title="In 4.1 Rule-Based and Lexicon-Based Methods ‣ 4 Current Sentiment Analysis Models and Methods ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.3 </span>LIWC</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S4.SS1.SSS4" title="In 4.1 Rule-Based and Lexicon-Based Methods ‣ 4 Current Sentiment Analysis Models and Methods ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.4 </span>Advantages and Limitations</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S4.SS2" title="In 4 Current Sentiment Analysis Models and Methods ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Traditional Machine Learning Approaches</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S4.SS2.SSS1" title="In 4.2 Traditional Machine Learning Approaches ‣ 4 Current Sentiment Analysis Models and Methods ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Feature Engineering for Sentiment Representation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S4.SS2.SSS2" title="In 4.2 Traditional Machine Learning Approaches ‣ 4 Current Sentiment Analysis Models and Methods ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Classification Algorithms</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S4.SS2.SSS3" title="In 4.2 Traditional Machine Learning Approaches ‣ 4 Current Sentiment Analysis Models and Methods ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.3 </span>Strengths and Limitations</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S4.SS3" title="In 4 Current Sentiment Analysis Models and Methods ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Deep Learning-Based Methods</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S4.SS3.SSS1" title="In 4.3 Deep Learning-Based Methods ‣ 4 Current Sentiment Analysis Models and Methods ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.1 </span>Convolutional Neural Networks (CNNs)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S4.SS3.SSS2" title="In 4.3 Deep Learning-Based Methods ‣ 4 Current Sentiment Analysis Models and Methods ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.2 </span>Recurrent Neural Networks (RNNs), LSTMs, and BiLSTMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S4.SS3.SSS3" title="In 4.3 Deep Learning-Based Methods ‣ 4 Current Sentiment Analysis Models and Methods ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.3 </span>Word Embeddings: Word2Vec and GloVe</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S4.SS3.SSS4" title="In 4.3 Deep Learning-Based Methods ‣ 4 Current Sentiment Analysis Models and Methods ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.4 </span>Attention Mechanisms</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S4.SS3.SSS5" title="In 4.3 Deep Learning-Based Methods ‣ 4 Current Sentiment Analysis Models and Methods ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.5 </span>Summary and Limitations</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S4.SS4" title="In 4 Current Sentiment Analysis Models and Methods ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Transformer-Based Models</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S4.SS4.SSS1" title="In 4.4 Transformer-Based Models ‣ 4 Current Sentiment Analysis Models and Methods ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.1 </span>Pretrained Transformer Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S4.SS4.SSS2" title="In 4.4 Transformer-Based Models ‣ 4 Current Sentiment Analysis Models and Methods ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.2 </span>Domain-Specific Transformers</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S4.SS4.SSS3" title="In 4.4 Transformer-Based Models ‣ 4 Current Sentiment Analysis Models and Methods ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.3 </span>Prompt-Based and Zero-Shot Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S4.SS4.SSS4" title="In 4.4 Transformer-Based Models ‣ 4 Current Sentiment Analysis Models and Methods ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.4 </span>Strengths and Limitations</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S4.SS5" title="In 4 Current Sentiment Analysis Models and Methods ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Large Language Models and the New Paradigm</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S4.SS5.SSS1" title="In 4.5 Large Language Models and the New Paradigm ‣ 4 Current Sentiment Analysis Models and Methods ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5.1 </span>Zero-Shot and Few-Shot Sentiment Classification</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S4.SS5.SSS2" title="In 4.5 Large Language Models and the New Paradigm ‣ 4 Current Sentiment Analysis Models and Methods ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5.2 </span>Chain-of-Thought Prompting for Sentiment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S4.SS5.SSS3" title="In 4.5 Large Language Models and the New Paradigm ‣ 4 Current Sentiment Analysis Models and Methods ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5.3 </span>Instruction-Tuned Models for Aspect-Based Sentiment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S4.SS5.SSS4" title="In 4.5 Large Language Models and the New Paradigm ‣ 4 Current Sentiment Analysis Models and Methods ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5.4 </span>Limitations and Emerging Challenges</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S5" title="In Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Domain-Specific Challenges</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S5.SS1" title="In 5 Domain-Specific Challenges ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Sarcasm and Irony Detection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S5.SS2" title="In 5 Domain-Specific Challenges ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Domain Drift</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S5.SS3" title="In 5 Domain-Specific Challenges ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Temporal Drift</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S5.SS4" title="In 5 Domain-Specific Challenges ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Long-form Context</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S5.SS5" title="In 5 Domain-Specific Challenges ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Cultural and Language Diversity</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S6" title="In Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Future Research Directions</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S6.SS1" title="In 6 Future Research Directions ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Improving Benchmarks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S6.SS2" title="In 6 Future Research Directions ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Beyond Few-shot: Emerging LLM Challenges</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S6.SS3" title="In 6 Future Research Directions ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Cross-lingual and Cross-domain Transfer</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S6.SS4" title="In 6 Future Research Directions ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span>Explainable Sentiment Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S6.SS5" title="In 6 Future Research Directions ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.5 </span>Multimodal and Conversational Sentiment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S6.SS6" title="In 6 Future Research Directions ‣ Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.6 </span>Bias and Fairness in Sentiment Analysis Systems</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#S7" title="In Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Agnivo Gosai<sup class="ltx_sup" id="id8.2.id1"><span class="ltx_text ltx_font_italic" id="id8.2.id1.1">1</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shuvodeep De<sup class="ltx_sup" id="id9.2.id1"><span class="ltx_text ltx_font_italic" id="id9.2.id1.1">2,∗</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Karun Thankachan<sup class="ltx_sup" id="id10.6.id1"><span class="ltx_text ltx_font_italic" id="id10.6.id1.1">3</span></sup>
<br class="ltx_break"/>
<sup class="ltx_sup" id="id11.7.id2"><span class="ltx_text ltx_font_italic" id="id11.7.id2.1" style="font-size:90%;">1</span></sup><span class="ltx_text" id="id7.5.3" style="font-size:90%;">Independent Researcher; agnivo2007@gmail.com 
<br class="ltx_break"/><sup class="ltx_sup" id="id7.5.3.1"><span class="ltx_text ltx_font_italic" id="id7.5.3.1.1">2</span></sup>Texas State University; vvg26@txstate.edu 
<br class="ltx_break"/><sup class="ltx_sup" id="id7.5.3.2"><span class="ltx_text ltx_font_italic" id="id7.5.3.2.1">3</span></sup>Carnegie Mellon University; kthankac@alumni.cmu.edu 
<br class="ltx_break"/>
<sup class="ltx_sup" id="id7.5.3.3"><span class="ltx_text ltx_font_italic" id="id7.5.3.3.1">∗</span></sup>Corresponding author: vvg26@txstate.edu
</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id12.id1">This paper presents a comprehensive survey of sentiment analysis methods for movie reviews, a benchmark task that has played a central role in advancing natural language processing. We review the evolution of techniques from early lexicon-based and classical machine learning approaches to modern deep learning architectures and large language models, covering widely used datasets such as IMDb, Rotten Tomatoes, and SST-2, and models ranging from Naive Bayes and support vector machines to LSTM networks, BERT, and attention-based transformers. Beyond summarizing prior work, this survey differentiates itself by offering a comparative, challenge-driven analysis of how these modeling paradigms address domain-specific issues such as sarcasm, negation, contextual ambiguity, and domain shift, which remain open problems in existing literature. Unlike earlier reviews that focus primarily on text-only pipelines, we also synthesize recent advances in multimodal sentiment analysis that integrate textual, audio, and visual cues from movie trailers and clips. In addition, we examine emerging concerns related to interpretability, fairness, and robustness that are often underexplored in prior surveys, and we outline future research directions including zero-shot and few-shot learning, hybrid symbolic–neural models, and real-time deployment considerations. Overall, this abstract provides a domain-focused roadmap that highlights both established solutions and unresolved challenges toward building more accurate, generalizable, and explainable sentiment analysis systems for movie review data.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<p class="ltx_p" id="p1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1">Keywords:</span> Sentiment analysis, movie reviews, opinion mining, natural language processing (NLP), text classification, machine learning, deep learning, emotion detection, feature extraction, polarity classification</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<section class="ltx_subsection" id="S1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Background</h3>
<div class="ltx_para" id="S1.SS1.p1">
<p class="ltx_p" id="S1.SS1.p1.1">Sentiment analysis of movie reviews, that is, the task of determining whether a piece of text conveys a positive, negative, or neutral opinion, is one of the most enduring and influential test beds in natural language processing (NLP) <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib2" title="">2</a>]</cite>. What began a couple of decades ago as a seemingly straightforward binary classification problem has evolved into a sophisticated domain that exemplifies the full spectrum of challenges facing modern computational linguistics: from handling linguistic creativity and cultural nuance to ensuring robust deployment in resource-constrained environments <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib4" title="">4</a>]</cite>. The literature on movie review sentiment analysis is anchored by Pang and Lee’s seminal 2008 monograph “Opinion Mining and Sentiment Analysis” in Foundations and Trends in Information Retrieval <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib5" title="">5</a>]</cite>, which established the definitive theoretical framework and extensively utilized movie review classification as the primary case study, building upon their pioneering 2002 EMNLP paper <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib1" title="">1</a>]</cite> that first applied machine learning techniques to movie review sentiment classification. This foundational work, along with Maas et al.’s 2011 ACL paper <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib6" title="">6</a>]</cite> introducing the Large Movie Review Dataset (IMDB), created the benchmark standards that continue to dominate the field today.</p>
</div>
<div class="ltx_para" id="S1.SS1.p2">
<p class="ltx_p" id="S1.SS1.p2.1">Recent comprehensive surveys (from 2020–2025) have documented the methodological evolution toward deep learning approaches, including Wankhade et al. <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib7" title="">7</a>]</cite> and Jain et al. <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib8" title="">8</a>]</cite>, which provide extensive coverage of neural architectures and transformer models, while Raghunathan et al. <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib9" title="">9</a>]</cite> offers systematic literature review methodology focusing on contemporary challenges. Domain-specific surveys such as the conference paper by Tetteh et al. on sentiment analysis tools for movie review evaluation <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib10" title="">10</a>]</cite> and the recent publication examining the applications of BERT and XLNet <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib11" title="">11</a>]</cite> to movie sentiment analysis demonstrate the field’s continued focus on performance optimization using state-of-the-art models. However, existing reviews <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib13" title="">13</a>]</cite> on the analysis of movie review sentiment have notable gaps, particularly in addressing multilingual and cross-cultural contexts, bias and fairness, and the detection of sarcasm / irony. The coverage of aspect-based sentiment analysis is somewhat fragmented <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib16" title="">16</a>]</cite>, with limited attention to methods targeting specific movie elements, and real-world deployment challenges are underexplored compared to academic benchmark performance. These omissions hinder a comprehensive understanding of the current state of the field and its practical applicability.</p>
</div>
<div class="ltx_para" id="S1.SS1.p3">
<p class="ltx_p" id="S1.SS1.p3.1">The evaluation landscape reveals inconsistent protocols and limited standardization across studies, with existing surveys <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib18" title="">18</a>]</cite> failing to establish comprehensive frameworks for comparing methodological approaches. Recent surveys have also inadequately addressed emerging challenges including AI-generated fake reviews, cross-modal sentiment analysis that combines text with visual movie content, and privacy-preserving analysis techniques. These gaps indicate substantial opportunities for novel survey work that could provide more systematic coverage of multilingual approaches, comprehensive bias analysis, standardized evaluation frameworks, and practical deployment considerations that have been largely unexplored in the existing literature on this topic.</p>
</div>
</section>
<section class="ltx_subsection" id="S1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Significance and Motivation</h3>
<div class="ltx_para" id="S1.SS2.p1">
<p class="ltx_p" id="S1.SS2.p1.1">Sentiment analysis of movie reviews has evolved from a niche academic pursuit into a critical infrastructure that today shapes the entertainment industry and influences billions of viewers around the world. In an era where digital platforms generate millions of user reviews daily, automated sentiment analysis has become the invisible but essential technology that powers recommendation systems on major streaming services such as Netflix, Amazon Prime and Disney+ <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib19" title="">19</a>]</cite>. Studios use these systems to gauge audience reception before and after theatrical releases, marketing teams refine promotional strategies based on sentiment insights, and critics increasingly find their influence mediated through algorithmic aggregation systems <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib2" title="">2</a>]</cite> that must parse the subtle distinctions between professional critique and casual opinion. Movie reviews presented by audiences has an exceptionally rich linguistic landscape from the measured prose of professional critics to the enthusiastic hyperbole of fan communities, from the sardonic wit of satirical posts to the technical analysis of film school graduates <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib20" title="">20</a>]</cite>, making this domain an ideal testbed for evaluating NLP systems across varied linguistic challenges, cultural contexts, and temporal periods <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib21" title="">21</a>]</cite>. As automated sentiment understanding increasingly shapes commercial decisions and cultural discussions, improving its accuracy, robustness, and interpretability becomes a practical necessity as well as a significant scientific goal.</p>
</div>
<div class="ltx_para" id="S1.SS2.p2">
<p class="ltx_p" id="S1.SS2.p2.1">However, despite achieving near-perfect performance on standard benchmarks <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib22" title="">22</a>]</cite>, real-world deployment of sentiment analysis systems continues to reveal fundamental limitations that transcend simple accuracy measurements. <span class="ltx_text ltx_font_bold" id="S1.SS2.p2.1.1">Sarcasm and irony</span> remain particularly challenging <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib23" title="">23</a>]</cite>, as rhetorical expressions such as “Great, another 3-hour snooze-fest” fundamentally invert the relationship between surface-level sentiment indicators and actual meaning. <span class="ltx_text ltx_font_bold" id="S1.SS2.p2.1.2">Domain and temporal drift</span> introduce ongoing stability issues when models confront new platforms, evolving slang, and cultural references absent from their training data <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib21" title="">21</a>]</cite>. <span class="ltx_text ltx_font_bold" id="S1.SS2.p2.1.3">Long-form context processing</span> challenges arise when detailed reviews exceed standard transformer input limitations <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib24" title="">24</a>]</cite>, while <span class="ltx_text ltx_font_bold" id="S1.SS2.p2.1.4">explainability and bias concerns</span> become critical as these systems move from research prototypes to production deployments affecting real commercial outcomes <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib25" title="">25</a>]</cite>. Finally, <span class="ltx_text ltx_font_bold" id="S1.SS2.p2.1.5">resource efficiency</span> demands create fundamental trade-offs between model sophistication and computational constraints of mobile and edge environments <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib27" title="">27</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.SS2.p3">
<p class="ltx_p" id="S1.SS2.p3.1">These challenges are deeply interconnected rather than isolated: effective sarcasm detection often requires extensive context <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib28" title="">28</a>]</cite>, which conflicts directly with efficiency constraints <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib26" title="">26</a>]</cite>. Robust domain adaptation must continually account for changing cultural expressions <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib21" title="">21</a>]</cite>; and explainability methods must generalize across domains while operating under strict resource budgets <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib25" title="">25</a>]</cite>. This interconnected nature suggests that future progress will depend on holistic approaches, ones that jointly address multiple challenges instead of optimizing individual components in isolation <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib29" title="">29</a>]</cite>. In the context of movie review analysis, nuanced language and diverse author voices around such integrated solutions are essential to deliver the reliable, interpretable, and efficient sentiment insights that underpin both commercial success and scholarly advancement.</p>
</div>
</section>
<section class="ltx_subsection" id="S1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.3 </span>The Evolution of Sentiment Analysis</h3>
<div class="ltx_para" id="S1.SS3.p1">
<p class="ltx_p" id="S1.SS3.p1.1">The foundational work by Pang and Lee (2002) <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib1" title="">1</a>]</cite> marked a pivotal moment in computational sentiment analysis, introducing the Movie Reviews (MR) corpus and demonstrating that machine learning techniques could significantly outperform simple lexicon-based sentiment scoring methods <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib30" title="">30</a>]</cite>. Their work established movie review sentiment analysis as a benchmark problem and created the methodological framework that would guide research for the next two decades <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib31" title="">31</a>]</cite>. The subsequent introduction of the IMDB-Large dataset <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib6" title="">6</a>]</cite> with its 50,000 balanced reviews further standardized evaluation practices and enabled more robust comparison across approaches.</p>
</div>
<div class="ltx_para" id="S1.SS3.p2">
<p class="ltx_p" id="S1.SS3.p2.1">The field has since progressed through distinct methodological eras, each bringing significant advances in both techniques and performance. From early bag-of-words approaches <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib32" title="">32</a>]</cite> achieving <math alttext="\sim" class="ltx_Math" display="inline" id="S1.SS3.p2.1.m1" intent=":literal"><semantics><mo>∼</mo><annotation encoding="application/x-tex">\sim</annotation></semantics></math>82% accuracy to static word embeddings <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib34" title="">34</a>]</cite> reaching 86–91%, followed by RNN and attention mechanisms <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib36" title="">36</a>]</cite> that reached 90–93%, transformer architectures <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib37" title="">37</a>]</cite> pushing performance to 94–97%, and modern large language models <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib39" title="">39</a>]</cite> reaching 97–98% on standard benchmarks <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib40" title="">40</a>]</cite>, this progression represents one of the most successful long-term research trajectories in NLP.</p>
</div>
<div class="ltx_para" id="S1.SS3.p3">
<p class="ltx_p" id="S1.SS3.p3.1">However, this apparent success reveals a more complex reality: as benchmark performance has approached theoretical limits, persistent challenges have emerged that cannot be solved through optimizing for better classification accuracy alone <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib29" title="">29</a>]</cite>. Contemporary research has identified five core challenges that continue to define the frontier of movie review sentiment analysis <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib3" title="">3</a>]</cite>: sarcasm and irony detection, domain and temporal drift adaptation, long-form context processing, explainability and bias mitigation, and resource-efficient deployment. These challenges are not merely technical hurdles, but represent fundamental aspects of human communication that computational systems must learn to navigate.</p>
</div>
</section>
<section class="ltx_subsection" id="S1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.4 </span>Limitations and Challenges</h3>
<div class="ltx_para" id="S1.SS4.p1">
<p class="ltx_p" id="S1.SS4.p1.1">Despite achieving near-human performance on benchmark datasets, current sentiment analysis methods encounter persistent limitations that prevent seamless real-world application. These limitations include:</p>
</div>
<div class="ltx_para" id="S1.SS4.p2">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">Sarcasm and Irony Detection</span>: Recognizing nuanced or indirect sentiment expressions remains problematic, significantly affecting accuracy.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">Domain and Temporal Drift</span>: Models trained on historical data or single-source reviews often fail when deployed in evolving or cross-platform contexts.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.1">Long-form Context Processing</span>: Contemporary models struggle to consistently interpret sentiment across extended texts, missing context-dependent meanings.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i4.p1.1.1">Interpretability and Bias Mitigation</span>: Sentiment predictions by deep learning models frequently lack transparency, undermining trust in real-world deployments.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i5.p1">
<p class="ltx_p" id="S1.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i5.p1.1.1">Resource Efficiency</span>: High-performing transformer models require computational resources, limiting practical deployment scenarios, especially in real-time settings.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.SS4.p3">
<p class="ltx_p" id="S1.SS4.p3.1">Thus, addressing these intertwined challenges is essential to bridge the gap between theoretical model performance and practical usability in the dynamic domain of movie review sentiment analysis.</p>
</div>
</section>
<section class="ltx_subsection" id="S1.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.5 </span>Novelty and Scope of this Review</h3>
<div class="ltx_para" id="S1.SS5.p1">
<p class="ltx_p" id="S1.SS5.p1.1">This review systematically synthesizes two decades of research from foundational lexicon-based approaches to cutting-edge transformer and multimodal models, uniquely emphasizing practical deployment considerations. Its primary contributions are:</p>
</div>
<div class="ltx_para" id="S1.SS5.p2">
<ul class="ltx_itemize" id="S1.I2">
<li class="ltx_item" id="S1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i1.p1">
<p class="ltx_p" id="S1.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I2.i1.p1.1.1">Comprehensive Methodological Evolution</span>: A structured analysis of sentiment analysis methodologies from early rule-based systems to modern large language models, contextualizing their strengths, limitations, and practical implications.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i2.p1">
<p class="ltx_p" id="S1.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I2.i2.p1.1.1">Critical Analysis of Persistent Challenges</span>: Explicitly identifying and evaluating ongoing research limitations: sarcasm detection, domain adaptation, contextual understanding, interpretability, and computational efficiency, which previous reviews have only addressed separately or superficially.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i3.p1">
<p class="ltx_p" id="S1.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I2.i3.p1.1.1">Integrated Benchmark Critique and Recommendations</span>: Proposing next-generation evaluation frameworks that incorporate multidimensional, real-world performance metrics, including temporal robustness, cross-platform adaptability, multimodal understanding, and resource-aware assessments.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i4.p1">
<p class="ltx_p" id="S1.I2.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I2.i4.p1.1.1">Future-Oriented Research Roadmap</span>: Highlighting promising future directions in multimodal fusion, zero-shot and hybrid model training, interactive explainability, and efficient deployment, which collectively address the identified limitations in an integrative manner.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.SS5.p3">
<p class="ltx_p" id="S1.SS5.p3.1">We provide a structured analysis of movie review sentiment analysis that extends beyond traditional accuracy-focused evaluation to address the complex challenges facing modern deployment scenarios. We synthesize the complete methodological evolution of the field while maintaining focus on practical applicability and real-world deployment considerations, with particular emphasis on the five persistent challenges that define the current research frontier: sarcasm and irony detection, domain and temporal drift, long-form context processing, explainability and bias mitigation, and resource-efficient deployment.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="299" id="S1.F1.g1" src="Organization_Paper.png" width="476"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Sentiment Analysis on Movie Review: Organization of the Article. Figure created with AI assistance.</figcaption>
</figure>
<div class="ltx_para" id="S1.SS5.p4">
<p class="ltx_p" id="S1.SS5.p4.1">Our analysis is organized into seven complementary sections that build systematically from foundational concepts to cutting-edge research directions. <span class="ltx_text ltx_font_bold" id="S1.SS5.p4.1.1">Section 2</span> establishes the theoretical foundation by examining different granularities of sentiment analysis and categorizing types of movie review data. <span class="ltx_text ltx_font_bold" id="S1.SS5.p4.1.2">Section 3</span> traces the evolution of evaluation frameworks from foundational datasets all to multimodal sentiment extensions, critically analyzing how benchmarks have shaped research while potentially constraining real-world understanding. <span class="ltx_text ltx_font_bold" id="S1.SS5.p4.1.3">Section 4</span> provides comprehensive coverage of the methodological progression from rule-based approaches to transformer models, documenting how it evolved from 82 percent accuracy to 97–98 percent accuracy. <span class="ltx_text ltx_font_bold" id="S1.SS5.p4.1.4">Section 5</span> provides a detailed examination of the persistent challenges that define the current research in this area, highlighting how these interconnected problems require holistic solutions. <span class="ltx_text ltx_font_bold" id="S1.SS5.p4.1.5">Section 6</span> explores some of the most promising research avenues, including improved benchmarks, few-shot learning, cross-lingual transfer, explainable models, multimodal analysis, and bias mitigation, while <span class="ltx_text ltx_font_bold" id="S1.SS5.p4.1.6">Section 7</span> synthesizes key insights to develop robust and deployable systems. Throughout this analysis, we maintain focus on practical implications of research developments, bridging the gap between academic research and real-world deployment to serve both as a comprehensive resource for researchers and a strategic guide for practitioners. This field currently stands at a critical juncture where traditional accuracy-focused approaches have reached their limits, and this review provides the foundation for navigating the transition toward systems that can truly understand, explain, and generalize across the rich diversity of human expression in film criticism.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Problem Formulation and Taxonomy</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Sentiment analysis is often referred to as opinion mining since it is essentially the computational study of people’s opinions, attitudes, and emotions expressed in text. At its core, sentiment analysis seeks to infer the underlying sentiment polarity: positive, negative, or neutral conveyed by a writer about an entity or event. Early work in this field framed the problem as a binary or multiclass text classification task, leveraging hand-crafted lexicons or shallow statistical features to assign a polarity label to an entire document or sentence <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib1" title="">1</a>]</cite>. As the field matured, researchers broadened this scope to capture richer sentiment dimensions, such as intensity (e.g., “very happy” vs. “slightly happy”), emotion categories (e.g., joy, anger, sadness), and even finer-grained opinions towards specific aspects of an entity <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib41" title="">41</a>]</cite>.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Granularity of Sentiment Analysis</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Sentiment analysis can be organized along several levels of granularity. In <span class="ltx_text ltx_font_bold" id="S2.SS1.p1.1.1">document-level</span> analysis, the goal is to predict the overall sentiment of a full review or article. This is often the simplest formulation, but may obscure conflicting sentiments directed at different aspects (for example, praising acting but criticizing the plot). <span class="ltx_text ltx_font_bold" id="S2.SS1.p1.1.2">Sentence-level</span> analysis addresses this by assigning sentiment labels to individual sentences, thereby allowing more localized sentiment detection within longer texts. <span class="ltx_text ltx_font_bold" id="S2.SS1.p1.1.3">Aspect-level</span> (or feature-level) sentiment analysis goes a step further by first identifying specific attributes of entity (such as “acting,” “direction,” or “cinematography” in movie reviews) and then determining the sentiment expressed toward each attribute. This finer granularity supports nuanced insights, such as simultaneously capturing praise for special effects and disappointment with story pacing <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib20" title="">20</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Types of Movie Review Data</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Movie reviews come in diverse formats and levels of annotation, each providing different signals for sentiment analysis. In this subsection, we distinguish three principal types of movie review data that have driven research in the field.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Unstructured Textual Reviews</h4>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">The most prevalent form of movie review data consists of free-form text written by users or professional critics. These unstructured reviews appear on platforms such as IMDb and Rotten Tomatoes, where authors express nuanced opinions, personal anecdotes, and rhetorical devices that convey their affective stance <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib1" title="">1</a>]</cite>. Unlike short social-media posts, movie reviews often span multiple paragraphs and include both explicit sentiment (“I loved the stunning visuals”) and implicit cues (“It took me a half hour to recover from that ending”). The richness of this text makes natural language processing both challenging and rewarding, as models must learn to parse context, resolve coreference, and interpret discourse structure to accurately gauge sentiment intensity and direction.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Structured Ratings and Metadata</h4>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">Complementing unstructured reviews are structured signals such as numerical star ratings (e.g., 1–10) and categorical labels (e.g., ‘Fresh’ vs. ‘Rotten’ on Rotten Tomatoes). These coarse-grained annotations provide readily available polarity information that can serve as distant supervision for training classifiers <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib41" title="">41</a>]</cite>. In many datasets, each review is accompanied by metadata such as reviewer ID, timestamp, and genre labels, which enables temporal analysis and user-level modeling of sentiment trends. However, structured ratings often mask the rationale behind the score, motivating hybrid approaches that combine metadata with text to improve interpretability and performance.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3 </span>Annotated Corpora for Fine-Grained Analysis</h4>
<div class="ltx_para" id="S2.SS2.SSS3.p1">
<p class="ltx_p" id="S2.SS2.SSS3.p1.1">To capture sentiment at sub-sentential levels, researchers have developed richly annotated corpora. The Stanford Sentiment Treebank (SST) is a paradigmatic example: it provides phrase-level polarity labels across the parse tree of each sentence, allowing models to learn compositional sentiment patterns (e.g., ‘not good’ vs. ‘good’) <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib20" title="">20</a>]</cite>. Such corpora facilitate sentence-level sentiment classification as well as aspect-level analysis, where individual entities or attributes (like ‘acting,’ ‘plot,’ or ‘soundtrack’) are annotated with their own sentiment tags. Even though such annotations are labor-intensive to produce, they have lead to significant advances in deep learning architectures, particularly recursive and attention-based models that can reason over tree structures and isolate sentiment-bearing subphrases.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Benchmark Datasets and Metrics</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">The evolution of movie review sentiment analysis has been fundamentally shaped by the benchmark datasets that define evaluation standards and research directions <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib20" title="">20</a>]</cite>. While these datasets have enabled remarkable progress in classification accuracy, their design choices and inherent limitations have also constrained our understanding of real-world performance across the five persistent challenges identified in this review <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib2" title="">2</a>]</cite>. This section examines the major benchmark corpora, their contributions to the field’s development, and the emerging recognition that traditional evaluation frameworks may inadequately capture the complexity of deploying sentiment analysis systems in production environments.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Foundational Datasets: Establishing the Benchmark Paradigm</h3>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>The Movie Reviews Corpus: Pioneering Binary Classification</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">The seminal work by Pang and Lee (2002) <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib1" title="">1</a>]</cite> introduced the Movie Reviews (MR) corpus, which became the foundational benchmark for sentiment analysis research. This dataset, containing 2,000 movie reviews equally split between positive and negative sentiment, established several conventions that would influence the field for decades: binary classification as the primary task, balanced class distribution as the evaluation standard, and accuracy as the dominant performance metric.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p2">
<p class="ltx_p" id="S3.SS1.SSS1.p2.1">The MR corpus demonstrated that machine learning approaches could significantly outperform lexicon-based methods, with Support Vector Machines achieving approximately 82% accuracy using unigram features <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib1" title="">1</a>]</cite>. This result established sentiment analysis as a viable machine learning problem and created the methodological framework that would guide subsequent research. However, the relatively small size of the dataset (2,000 documents) and its focus on professional reviews from a specific time period would later prove limiting for understanding model robustness across the challenges of sarcasm detection, domain adaptation, and temporal drift.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p3">
<p class="ltx_p" id="S3.SS1.SSS1.p3.1">The balanced 50/50 polarity split, while useful for controlled evaluation, may not reflect the natural distribution of sentiments in real-world movie review platforms <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib6" title="">6</a>]</cite>. This design choice, replicated across subsequent benchmarks, potentially oversimplifies the nuanced nature of movie criticism where mixed sentiments, qualified opinions, and contextual judgments are common <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib20" title="">20</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Expansion and Standardization: The IMDB-Large Dataset</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">The introduction of the IMDB-Large dataset by Maas et al. (2011) <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib6" title="">6</a>]</cite> represented a significant leap forward in benchmark sophistication, providing 50,000 reviews split into 25,000 training and 25,000 test samples. This dataset became the de facto standard for movie review sentiment analysis, hosted on platforms like Kaggle and Stanford AI, and enabled more robust evaluation of deep learning approaches as they emerged throughout the 2010s.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p2">
<p class="ltx_p" id="S3.SS1.SSS2.p2.1">The IMDB-Large dataset’s larger scale allowed for more sophisticated model architectures and training procedures, supporting the transition from traditional machine learning to deep learning approaches <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib36" title="">36</a>]</cite>. The dataset’s availability and standardized train/test splits enabled meaningful comparison across different methodological approaches, contributing to the field’s rapid progress from 86–91% accuracy with static word embeddings to 94–97% with transformer architectures <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib37" title="">37</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p3">
<p class="ltx_p" id="S3.SS1.SSS2.p3.1">However, the IMDB-Large dataset also inherited and amplified certain limitations from its predecessor. The binary classification paradigm, while computationally convenient, may inadequately capture the complexity of sentiment expression in movie reviews <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib20" title="">20</a>]</cite>. Professional and amateur reviewers often express nuanced opinions that resist simple positive/negative categorization, particularly when discussing different aspects of films or comparing works within specific genres or cultural contexts.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Diversification and Multimodal Extensions</h3>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Stanford Sentiment Treebank: Phrase-Level Granularity</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">The Stanford Sentiment Treebank (SST) <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib20" title="">20</a>]</cite> introduced a different perspective on sentiment analysis with 10,000 documents focused on short snippets and phrase-level sentiment analysis tasks. This dataset provided fine-grained sentiment annotations at the phrase level, enabling models to learn compositional sentiment understanding rather than document-level classification alone.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p2">
<p class="ltx_p" id="S3.SS2.SSS1.p2.1">The SST’s contribution extends beyond simple scale to methodological sophistication, as it enables evaluation of models’ ability to understand how sentiment composes across linguistic structures <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib20" title="">20</a>]</cite>. This capability is particularly relevant for handling complex rhetorical constructions common in movie reviews, where sentiment may shift or be qualified across different parts of a text. The phrase-level annotations provide insights into model behavior that document-level accuracy measurements cannot capture.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p3">
<p class="ltx_p" id="S3.SS2.SSS1.p3.1">However, the focus on short snippets may limit the SST’s ability to evaluate models on the long-form context processing challenge identified in this review. Movie reviews often develop arguments over extended passages, and phrase-level sentiment understanding, while valuable, may not capture the full complexity of document-level sentiment development <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib36" title="">36</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Multimodal Sentiment: CMU-MOSI and MOSEI</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">The emergence of multimodal datasets like CMU-MOSI and CMU-MOSEI <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib44" title="">44</a>]</cite> brought additional complexity with 2,200–22,800 YouTube clips incorporating text, video, and audio sentiment analysis. These datasets convert 7-point sentiment scales to binary classifications, enabling comparison with traditional text-only benchmarks while introducing the possibility of multimodal sentiment understanding.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p2">
<p class="ltx_p" id="S3.SS2.SSS2.p2.1">The multimodal datasets represent a significant expansion of the evaluation paradigm, acknowledging that movie reviews often reference visual and auditory elements of films that pure text analysis cannot capture <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib43" title="">43</a>]</cite>. A reviewer discussing “haunting cinematography” or “jarring sound design” makes references that could benefit from access to actual visual and audio content, suggesting directions for more comprehensive sentiment understanding.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p3">
<p class="ltx_p" id="S3.SS2.SSS2.p3.1">However, the conversion from 7-point sentiment scales to binary classifications, while enabling comparison with traditional datasets, may lose important nuances in sentiment expression that are particularly relevant for movie reviews <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib44" title="">44</a>]</cite>. The multimodal nature of these datasets also introduces additional complexity to evaluation, requiring metrics that can assess performance across text, video, and audio modalities simultaneously.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Recent Benchmark Expansions: GoEmotions and TweetEval</h4>
<div class="ltx_para" id="S3.SS2.SSS3.p1">
<p class="ltx_p" id="S3.SS2.SSS3.p1.1">The year 2020 marked significant advances in sentiment benchmark development with the introduction of GoEmotions and TweetEval. GoEmotions <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib45" title="">45</a>]</cite>, developed by Google Research, introduced the largest manually annotated dataset for fine-grained emotion classification, comprising 58,000 Reddit comments labeled across 27 emotion categories plus neutral. Unlike previous datasets limited to basic sentiment polarity or six basic emotions, GoEmotions captures nuanced emotional states including admiration, amusement, curiosity, and disappointment, enabling more sophisticated emotion-aware applications. The dataset’s taxonomy was developed in collaboration with psychologists, covering 12 positive, 11 negative, and 4 ambiguous emotion categories.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS3.p2">
<p class="ltx_p" id="S3.SS2.SSS3.p2.1">TweetEval <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib46" title="">46</a>]</cite> addressed the fragmentation problem in social media NLP by unifying seven heterogeneous Twitter-specific classification tasks including sentiment analysis, emotion recognition, irony detection, and hate speech detection into a standardized evaluation framework. With over 150,000 tweets and consistent train/validation/test splits, TweetEval enables meaningful comparison across models and has become the de facto benchmark for social media sentiment analysis. The accompanying Twitter-RoBERTa model, pre-trained on 58 million tweets, established strong baselines that continue to inform research on domain-specific language model adaptation.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS3.p3">
<p class="ltx_p" id="S3.SS2.SSS3.p3.1">These recent datasets reflect a broader trend toward more comprehensive evaluation frameworks that capture the complexity of real-world sentiment expression beyond simple binary classification.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Evolution of Major Sentiment Analysis Benchmark Datasets.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.2.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T1.2.3.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.3.1.1.1" style="font-size:80%;">Dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.2.3.1.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.3.1.2.1" style="font-size:80%;">Year</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.2.3.1.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.3.1.3.1" style="font-size:80%;">Size</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.2.3.1.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.3.1.4.1" style="font-size:80%;">Split</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T1.2.3.1.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.3.1.5.1" style="font-size:80%;">Source</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.2.3.1.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.3.1.6.1" style="font-size:80%;">Modality</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S3.T1.2.3.1.7" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.3.1.7.1">
<span class="ltx_p" id="S3.T1.2.3.1.7.1.1" style="width:79.7pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.3.1.7.1.1.1" style="font-size:80%;">Key Innovation</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.2.4.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.4.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.4.1.1.1" style="font-size:80%;">Movie Reviews (MR)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.4.1.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.4.1.2.1" style="font-size:80%;">2002</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.4.1.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.4.1.3.1" style="font-size:80%;">2k</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.4.1.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.4.1.4.1" style="font-size:80%;">50/50</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.4.1.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.4.1.5.1" style="font-size:80%;">Prof. reviews</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.4.1.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.4.1.6.1" style="font-size:80%;">Text</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" id="S3.T1.2.4.1.7" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.4.1.7.1">
<span class="ltx_p" id="S3.T1.2.4.1.7.1.1" style="width:79.7pt;"><span class="ltx_text" id="S3.T1.2.4.1.7.1.1.1" style="font-size:80%;">Binary classification</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.5.2">
<td class="ltx_td ltx_align_left" id="S3.T1.2.5.2.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.5.2.1.1" style="font-size:80%;">IMDB-Large</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.5.2.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.5.2.2.1" style="font-size:80%;">2011</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.5.2.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.5.2.3.1" style="font-size:80%;">50k</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.5.2.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.5.2.4.1" style="font-size:80%;">25k/25k</span></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.5.2.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.5.2.5.1" style="font-size:80%;">User reviews</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.5.2.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.5.2.6.1" style="font-size:80%;">Text</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S3.T1.2.5.2.7" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.5.2.7.1">
<span class="ltx_p" id="S3.T1.2.5.2.7.1.1" style="width:79.7pt;"><span class="ltx_text" id="S3.T1.2.5.2.7.1.1.1" style="font-size:80%;">Large-scale evaluation</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.6.3">
<td class="ltx_td ltx_align_left" id="S3.T1.2.6.3.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.6.3.1.1" style="font-size:80%;">Stanford SST</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.6.3.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.6.3.2.1" style="font-size:80%;">2013</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.6.3.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.6.3.3.1" style="font-size:80%;">10k</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.6.3.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.6.3.4.1" style="font-size:80%;">Variable</span></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.6.3.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.6.3.5.1" style="font-size:80%;">Prof. snippets</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.6.3.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.6.3.6.1" style="font-size:80%;">Text</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S3.T1.2.6.3.7" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.6.3.7.1">
<span class="ltx_p" id="S3.T1.2.6.3.7.1.1" style="width:79.7pt;"><span class="ltx_text" id="S3.T1.2.6.3.7.1.1.1" style="font-size:80%;">Phrase-level granularity</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.7.4">
<td class="ltx_td ltx_align_left" id="S3.T1.2.7.4.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.7.4.1.1" style="font-size:80%;">Rotten Tomatoes</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.7.4.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.7.4.2.1" style="font-size:80%;">2013</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.7.4.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.7.4.3.1" style="font-size:80%;">10k</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.7.4.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.7.4.4.1" style="font-size:80%;">50/50</span></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.7.4.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.7.4.5.1" style="font-size:80%;">Mixed</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.7.4.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.7.4.6.1" style="font-size:80%;">Text</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S3.T1.2.7.4.7" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.7.4.7.1">
<span class="ltx_p" id="S3.T1.2.7.4.7.1.1" style="width:79.7pt;"><span class="ltx_text" id="S3.T1.2.7.4.7.1.1.1" style="font-size:80%;">Short snippet focus</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1">
<td class="ltx_td ltx_align_left" id="S3.T1.1.1.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.1.1.2.1" style="font-size:80%;">CMU-MOSI</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.1.1.3.1" style="font-size:80%;">2016</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.1.1.4.1" style="font-size:80%;">2.2k</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S3.T1.1.1.1.1" style="font-size:80%;">7pt</span><math alttext="\to" class="ltx_Math" display="inline" id="S3.T1.1.1.1.m1" intent=":literal"><semantics><mo mathsize="0.800em" stretchy="false">→</mo><annotation encoding="application/x-tex">\to</annotation></semantics></math><span class="ltx_text" id="S3.T1.1.1.1.2" style="font-size:80%;">bin</span>
</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.1.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.1.1.5.1" style="font-size:80%;">YouTube</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.1.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.1.1.6.1" style="font-size:80%;">T+V+A</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S3.T1.1.1.7" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.1.7.1">
<span class="ltx_p" id="S3.T1.1.1.7.1.1" style="width:79.7pt;"><span class="ltx_text" id="S3.T1.1.1.7.1.1.1" style="font-size:80%;">Multimodal integration</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.2">
<td class="ltx_td ltx_align_left" id="S3.T1.2.2.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.2.2.1" style="font-size:80%;">CMU-MOSEI</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.2.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.2.3.1" style="font-size:80%;">2018</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.2.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.2.4.1" style="font-size:80%;">22.8k</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.2.1" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S3.T1.2.2.1.1" style="font-size:80%;">7pt</span><math alttext="\to" class="ltx_Math" display="inline" id="S3.T1.2.2.1.m1" intent=":literal"><semantics><mo mathsize="0.800em" stretchy="false">→</mo><annotation encoding="application/x-tex">\to</annotation></semantics></math><span class="ltx_text" id="S3.T1.2.2.1.2" style="font-size:80%;">bin</span>
</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.2.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.2.5.1" style="font-size:80%;">YouTube</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.2.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.2.6.1" style="font-size:80%;">T+V+A</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S3.T1.2.2.7" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.2.7.1">
<span class="ltx_p" id="S3.T1.2.2.7.1.1" style="width:79.7pt;"><span class="ltx_text" id="S3.T1.2.2.7.1.1.1" style="font-size:80%;">Large-scale multimodal</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.8.5">
<td class="ltx_td ltx_align_left" id="S3.T1.2.8.5.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.8.5.1.1" style="font-size:80%;">GoEmotions</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.8.5.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.8.5.2.1" style="font-size:80%;">2020</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.8.5.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.8.5.3.1" style="font-size:80%;">58k</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.8.5.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.8.5.4.1" style="font-size:80%;">Train/Val/Test</span></td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.8.5.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.8.5.5.1" style="font-size:80%;">Reddit</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.8.5.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.8.5.6.1" style="font-size:80%;">Text</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" id="S3.T1.2.8.5.7" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.8.5.7.1">
<span class="ltx_p" id="S3.T1.2.8.5.7.1.1" style="width:79.7pt;"><span class="ltx_text" id="S3.T1.2.8.5.7.1.1.1" style="font-size:80%;">27 fine-grained emotions</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.9.6">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.2.9.6.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.9.6.1.1" style="font-size:80%;">TweetEval</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.2.9.6.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.9.6.2.1" style="font-size:80%;">2020</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.2.9.6.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.9.6.3.1" style="font-size:80%;">150k+</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.2.9.6.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.9.6.4.1" style="font-size:80%;">Standardized</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.2.9.6.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.9.6.5.1" style="font-size:80%;">Twitter</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.2.9.6.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.2.9.6.6.1" style="font-size:80%;">Text</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb" id="S3.T1.2.9.6.7" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.9.6.7.1">
<span class="ltx_p" id="S3.T1.2.9.6.7.1.1" style="width:79.7pt;"><span class="ltx_text" id="S3.T1.2.9.6.7.1.1.1" style="font-size:80%;">Unified social media benchmark</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S3.F2"><span class="ltx_inline-block"><svg class="ltx_picture ltx_centering" height="175.96" id="S3.F2.pic1" overflow="visible" version="1.1" viewbox="0 0 550.56 175.96" width="550.56"><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,175.96) matrix(1 0 0 -1 0 0) translate(43.98,0) translate(0,87.98)"><g stroke-width="0.8pt"><path d="M 0 0 L 496.06 0" style="fill:none"></path></g><g stroke-width="0.4pt"><path d="M 0 -3.54 L 0 3.54" style="fill:none"></path><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 -11.76 -19.11)"><foreignobject height="7.13" overflow="visible" style="--ltx-fo-width:2em;--ltx-fo-height:0.61em;--ltx-fo-depth:0em;" transform="matrix(1 0 0 -1 0 7.13)" width="23.52"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><span class="ltx_text" id="S3.F2.pic1.1.1.1.1.1.1" style="font-size:80%;">2002</span></span></span></foreignobject></g><path d="M 79.72 -3.54 L 79.72 3.54" style="fill:none"></path><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 67.96 -19.11)"><foreignobject height="7.13" overflow="visible" style="--ltx-fo-width:2em;--ltx-fo-height:0.61em;--ltx-fo-depth:0em;" transform="matrix(1 0 0 -1 0 7.13)" width="23.52"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><span class="ltx_text" id="S3.F2.pic1.2.2.2.2.1.1" style="font-size:80%;">2005</span></span></span></foreignobject></g><path d="M 159.45 -3.54 L 159.45 3.54" style="fill:none"></path><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 147.69 -19.11)"><foreignobject height="7.13" overflow="visible" style="--ltx-fo-width:2em;--ltx-fo-height:0.61em;--ltx-fo-depth:0em;" transform="matrix(1 0 0 -1 0 7.13)" width="23.52"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><span class="ltx_text" id="S3.F2.pic1.3.3.3.3.1.1" style="font-size:80%;">2008</span></span></span></foreignobject></g><path d="M 239.17 -3.54 L 239.17 3.54" style="fill:none"></path><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 227.41 -19.11)"><foreignobject height="7.13" overflow="visible" style="--ltx-fo-width:2em;--ltx-fo-height:0.61em;--ltx-fo-depth:0em;" transform="matrix(1 0 0 -1 0 7.13)" width="23.52"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><span class="ltx_text" id="S3.F2.pic1.4.4.4.4.1.1" style="font-size:80%;">2011</span></span></span></foreignobject></g><path d="M 318.9 -3.54 L 318.9 3.54" style="fill:none"></path><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 307.13 -19.11)"><foreignobject height="7.13" overflow="visible" style="--ltx-fo-width:2em;--ltx-fo-height:0.61em;--ltx-fo-depth:0em;" transform="matrix(1 0 0 -1 0 7.13)" width="23.52"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><span class="ltx_text" id="S3.F2.pic1.5.5.5.5.1.1" style="font-size:80%;">2014</span></span></span></foreignobject></g><path d="M 398.62 -3.54 L 398.62 3.54" style="fill:none"></path><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 386.86 -19.11)"><foreignobject height="7.13" overflow="visible" style="--ltx-fo-width:2em;--ltx-fo-height:0.61em;--ltx-fo-depth:0em;" transform="matrix(1 0 0 -1 0 7.13)" width="23.52"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><span class="ltx_text" id="S3.F2.pic1.6.6.6.6.1.1" style="font-size:80%;">2017</span></span></span></foreignobject></g><path d="M 478.34 -3.54 L 478.34 3.54" style="fill:none"></path><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 466.58 -19.11)"><foreignobject height="7.13" overflow="visible" style="--ltx-fo-width:2em;--ltx-fo-height:0.61em;--ltx-fo-depth:0em;" transform="matrix(1 0 0 -1 0 7.13)" width="23.52"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><span class="ltx_text" id="S3.F2.pic1.7.7.7.7.1.1" style="font-size:80%;">2020</span></span></span></foreignobject></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 -39.37 74.57)"><foreignobject height="59.65" overflow="visible" style="--ltx-fo-width:5.69em;--ltx-fo-height:0.56em;--ltx-fo-depth:3.76em;" transform="matrix(1 0 0 -1 0 7.69)" width="78.74"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S3.F2.pic1.8.8.8.8.1.1" style="width:6.69em;">
<span class="ltx_p" id="S3.F2.pic1.8.8.8.8.1.1.1"></span>
<span class="ltx_p" id="S3.F2.pic1.8.8.8.8.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.F2.pic1.8.8.8.8.1.1.2.1" style="font-size:80%;">Movie Reviews</span></span>
<span class="ltx_p" id="S3.F2.pic1.8.8.8.8.1.1.3"><span class="ltx_text" id="S3.F2.pic1.8.8.8.8.1.1.3.1" style="font-size:80%;">2k docs</span></span>
<span class="ltx_p" id="S3.F2.pic1.8.8.8.8.1.1.4"><span class="ltx_text" id="S3.F2.pic1.8.8.8.8.1.1.4.1" style="font-size:80%;">Binary paradigm</span></span>
</span></span></span></foreignobject></g><g color="#0000FF" fill="#0000FF" stroke="#0000FF" stroke-width="0.8pt" style="--ltx-stroke-color:#0000FF;--ltx-fill-color:#0000FF;--ltx-fg-color:#0000FF;"><path d="M 0 0 L 0 14.17" style="fill:none"></path></g><g color="#0000FF" fill="#0000FF" stroke="#0000FF" style="--ltx-stroke-color:#0000FF;--ltx-fill-color:#0000FF;--ltx-fg-color:#0000FF;"><path d="M 0 14.17 M 2.49 14.17 C 2.49 15.55 1.38 16.66 0 16.66 C -1.38 16.66 -2.49 15.55 -2.49 14.17 C -2.49 12.8 -1.38 11.68 0 11.68 C 1.38 11.68 2.49 12.8 2.49 14.17 Z M 0 14.17"></path></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 199.8 75.68)"><foreignobject height="43.05" overflow="visible" style="--ltx-fo-width:5.69em;--ltx-fo-height:0.56em;--ltx-fo-depth:2.56em;" transform="matrix(1 0 0 -1 0 7.69)" width="78.74"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S3.F2.pic1.9.9.9.9.1.1" style="width:6.69em;">
<span class="ltx_p" id="S3.F2.pic1.9.9.9.9.1.1.1"></span>
<span class="ltx_p" id="S3.F2.pic1.9.9.9.9.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.F2.pic1.9.9.9.9.1.1.2.1" style="font-size:80%;">IMDB-Large</span></span>
<span class="ltx_p" id="S3.F2.pic1.9.9.9.9.1.1.3"><span class="ltx_text" id="S3.F2.pic1.9.9.9.9.1.1.3.1" style="font-size:80%;">50k docs</span></span>
<span class="ltx_p" id="S3.F2.pic1.9.9.9.9.1.1.4"><span class="ltx_text" id="S3.F2.pic1.9.9.9.9.1.1.4.1" style="font-size:80%;">Deep learning era</span></span>
</span></span></span></foreignobject></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000" stroke-width="0.8pt" style="--ltx-stroke-color:#FF0000;--ltx-fill-color:#FF0000;--ltx-fg-color:#FF0000;"><path d="M 239.17 0 L 239.17 31.89" style="fill:none"></path></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000" style="--ltx-stroke-color:#FF0000;--ltx-fill-color:#FF0000;--ltx-fg-color:#FF0000;"><path d="M 239.17 31.89 M 241.66 31.89 C 241.66 33.26 240.55 34.38 239.17 34.38 C 237.8 34.38 236.68 33.26 236.68 31.89 C 236.68 30.51 237.8 29.4 239.17 29.4 C 240.55 29.4 241.66 30.51 241.66 31.89 Z M 239.17 31.89"></path></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 279.53 41.36)"><foreignobject height="26.44" overflow="visible" style="--ltx-fo-width:5.69em;--ltx-fo-height:0.56em;--ltx-fo-depth:1.36em;" transform="matrix(1 0 0 -1 0 7.69)" width="78.74"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S3.F2.pic1.10.10.10.10.1.1" style="width:6.69em;">
<span class="ltx_p" id="S3.F2.pic1.10.10.10.10.1.1.1"></span>
<span class="ltx_p" id="S3.F2.pic1.10.10.10.10.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.F2.pic1.10.10.10.10.1.1.2.1" style="font-size:80%;">Stanford SST</span></span>
<span class="ltx_p" id="S3.F2.pic1.10.10.10.10.1.1.3"><span class="ltx_text" id="S3.F2.pic1.10.10.10.10.1.1.3.1" style="font-size:80%;">Phrase-level</span></span>
<span class="ltx_p" id="S3.F2.pic1.10.10.10.10.1.1.4"><span class="ltx_text" id="S3.F2.pic1.10.10.10.10.1.1.4.1" style="font-size:80%;">granularity</span></span>
</span></span></span></foreignobject></g><g color="#009900" fill="#009900" stroke="#009900" stroke-width="0.8pt" style="--ltx-stroke-color:#009900;--ltx-fill-color:#009900;--ltx-fg-color:#009900;"><path d="M 318.9 0 L 318.9 14.17" style="fill:none"></path></g><g color="#009900" fill="#009900" stroke="#009900" style="--ltx-stroke-color:#009900;--ltx-fill-color:#009900;--ltx-fg-color:#009900;"><path d="M 318.9 14.17 M 321.39 14.17 C 321.39 15.55 320.27 16.66 318.9 16.66 C 317.52 16.66 316.4 15.55 316.4 14.17 C 316.4 12.8 317.52 11.68 318.9 11.68 C 320.27 11.68 321.39 12.8 321.39 14.17 Z M 318.9 14.17"></path></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 359.25 42.47)"><foreignobject height="9.84" overflow="visible" style="--ltx-fo-width:5.69em;--ltx-fo-height:0.56em;--ltx-fo-depth:0.16em;" transform="matrix(1 0 0 -1 0 7.69)" width="78.74"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S3.F2.pic1.11.11.11.11.1.1" style="width:6.69em;">
<span class="ltx_p" id="S3.F2.pic1.11.11.11.11.1.1.1"></span>
<span class="ltx_p" id="S3.F2.pic1.11.11.11.11.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.F2.pic1.11.11.11.11.1.1.2.1" style="font-size:80%;">CMU-MOSI</span></span>
<span class="ltx_p" id="S3.F2.pic1.11.11.11.11.1.1.3"><span class="ltx_text" id="S3.F2.pic1.11.11.11.11.1.1.3.1" style="font-size:80%;">Multimodal</span></span>
<span class="ltx_p" id="S3.F2.pic1.11.11.11.11.1.1.4"><span class="ltx_text" id="S3.F2.pic1.11.11.11.11.1.1.4.1" style="font-size:80%;">integration</span></span>
</span></span></span></foreignobject></g><g color="#BF0040" fill="#BF0040" stroke="#BF0040" stroke-width="0.8pt" style="--ltx-stroke-color:#BF0040;--ltx-fill-color:#BF0040;--ltx-fg-color:#BF0040;"><path d="M 398.62 0 L 398.62 31.89" style="fill:none"></path></g><g color="#BF0040" fill="#BF0040" stroke="#BF0040" style="--ltx-stroke-color:#BF0040;--ltx-fill-color:#BF0040;--ltx-fg-color:#BF0040;"><path d="M 398.62 31.89 M 401.11 31.89 C 401.11 33.26 399.99 34.38 398.62 34.38 C 397.24 34.38 396.13 33.26 396.13 31.89 C 396.13 30.51 397.24 29.4 398.62 29.4 C 399.99 29.4 401.11 30.51 401.11 31.89 Z M 398.62 31.89"></path></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 421.26 24.76)"><foreignobject height="9.84" overflow="visible" style="--ltx-fo-width:5.69em;--ltx-fo-height:0.56em;--ltx-fo-depth:0.16em;" transform="matrix(1 0 0 -1 0 7.69)" width="78.74"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S3.F2.pic1.12.12.12.12.1.1" style="width:6.69em;">
<span class="ltx_p" id="S3.F2.pic1.12.12.12.12.1.1.1"></span>
<span class="ltx_p" id="S3.F2.pic1.12.12.12.12.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.F2.pic1.12.12.12.12.1.1.2.1" style="font-size:80%;">CMU-MOSEI</span></span>
<span class="ltx_p" id="S3.F2.pic1.12.12.12.12.1.1.3"><span class="ltx_text" id="S3.F2.pic1.12.12.12.12.1.1.3.1" style="font-size:80%;">Large-scale</span></span>
<span class="ltx_p" id="S3.F2.pic1.12.12.12.12.1.1.4"><span class="ltx_text" id="S3.F2.pic1.12.12.12.12.1.1.4.1" style="font-size:80%;">multimodal</span></span>
</span></span></span></foreignobject></g><g color="#FF8000" fill="#FF8000" stroke="#FF8000" stroke-width="0.8pt" style="--ltx-stroke-color:#FF8000;--ltx-fill-color:#FF8000;--ltx-fg-color:#FF8000;"><path d="M 460.63 0 L 460.63 14.17" style="fill:none"></path></g><g color="#FF8000" fill="#FF8000" stroke="#FF8000" style="--ltx-stroke-color:#FF8000;--ltx-fill-color:#FF8000;--ltx-fg-color:#FF8000;"><path d="M 460.63 14.17 M 463.12 14.17 C 463.12 15.55 462 16.66 460.63 16.66 C 459.25 16.66 458.14 15.55 458.14 14.17 C 458.14 12.8 459.25 11.68 460.63 11.68 C 462 11.68 463.12 12.8 463.12 14.17 Z M 460.63 14.17"></path></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 47.24 -48.01)"><foreignobject height="43.05" overflow="visible" style="--ltx-fo-width:8.54em;--ltx-fo-height:0.56em;--ltx-fo-depth:2.56em;" transform="matrix(1 0 0 -1 0 7.69)" width="118.11"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S3.F2.pic1.13.13.13.13.1.1" style="width:10.04em;">
<span class="ltx_p" id="S3.F2.pic1.13.13.13.13.1.1.1"></span>
<span class="ltx_p" id="S3.F2.pic1.13.13.13.13.1.1.2"><span class="ltx_text ltx_font_italic" id="S3.F2.pic1.13.13.13.13.1.1.2.1" style="font-size:80%;">Foundation Era</span></span>
<span class="ltx_p" id="S3.F2.pic1.13.13.13.13.1.1.3"><span class="ltx_text" id="S3.F2.pic1.13.13.13.13.1.1.3.1" style="font-size:80%;">Establishing paradigms</span></span>
</span></span></span></foreignobject></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 224.41 -48.01)"><foreignobject height="26.44" overflow="visible" style="--ltx-fo-width:8.54em;--ltx-fo-height:0.56em;--ltx-fo-depth:1.36em;" transform="matrix(1 0 0 -1 0 7.69)" width="118.11"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S3.F2.pic1.14.14.14.14.1.1" style="width:10.04em;">
<span class="ltx_p" id="S3.F2.pic1.14.14.14.14.1.1.1"></span>
<span class="ltx_p" id="S3.F2.pic1.14.14.14.14.1.1.2"><span class="ltx_text ltx_font_italic" id="S3.F2.pic1.14.14.14.14.1.1.2.1" style="font-size:80%;">Expansion Era</span></span>
<span class="ltx_p" id="S3.F2.pic1.14.14.14.14.1.1.3"><span class="ltx_text" id="S3.F2.pic1.14.14.14.14.1.1.3.1" style="font-size:80%;">Scale and sophistication</span></span>
</span></span></span></foreignobject></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 383.86 -48.01)"><foreignobject height="43.05" overflow="visible" style="--ltx-fo-width:8.54em;--ltx-fo-height:0.56em;--ltx-fo-depth:2.56em;" transform="matrix(1 0 0 -1 0 7.69)" width="118.11"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S3.F2.pic1.15.15.15.15.1.1" style="width:10.04em;">
<span class="ltx_p" id="S3.F2.pic1.15.15.15.15.1.1.1"></span>
<span class="ltx_p" id="S3.F2.pic1.15.15.15.15.1.1.2"><span class="ltx_text ltx_font_italic" id="S3.F2.pic1.15.15.15.15.1.1.2.1" style="font-size:80%;">Diversification Era</span></span>
<span class="ltx_p" id="S3.F2.pic1.15.15.15.15.1.1.3"><span class="ltx_text" id="S3.F2.pic1.15.15.15.15.1.1.3.1" style="font-size:80%;">Multimodal integration</span></span>
</span></span></span></foreignobject></g></g></g></svg></span>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Timeline of major benchmark dataset development in movie review sentiment analysis, showing the evolution from simple binary classification to complex multimodal evaluation frameworks.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Evaluation Metrics</h3>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>The Dominance of Accuracy and Macro-F1</h4>
<div class="ltx_para" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.1">The standardization of evaluation metrics has been crucial for the field’s development, with accuracy and macro-F1 remaining the dominant measures across most benchmark studies <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib2" title="">2</a>]</cite>. Many works additionally report ROC-AUC (Receiver Operating Characteristic - Area Under Curve) or MCC (Matthews Correlation Coefficient) to handle class imbalance issues, though the balanced nature of most movie review datasets makes this less critical than in other domains.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS1.p2">
<p class="ltx_p" id="S3.SS3.SSS1.p2.1">The choice of evaluation metrics reflects the field’s evolution from simple binary classification to more nuanced understanding of model performance <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib3" title="">3</a>]</cite>. While accuracy provides an intuitive measure of overall performance, macro-F1 offers better insights into model behavior across both positive and negative classes. ROC-AUC provides valuable information about model discrimination ability across different threshold settings, which is particularly relevant for deployment scenarios where operating points may need adjustment.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS1.p3">
<p class="ltx_p" id="S3.SS3.SSS1.p3.1">However, these traditional metrics have significant limitations when evaluating models against the five persistent challenges identified in this review <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib25" title="">25</a>]</cite>. Standard accuracy measurements may not capture a model’s ability to handle sarcastic reviews, adapt to temporal drift, or maintain performance across different review lengths. This limitation has led to increased interest in developing more comprehensive evaluation frameworks that assess model robustness across multiple dimensions simultaneously.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>The Limitations of Binary Classification Paradigms</h4>
<div class="ltx_para" id="S3.SS3.SSS2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.p1.1">The focus on binary classification in most benchmarks, including the conversion of multi-point scales to binary labels in multimodal datasets, may oversimplify the nuanced nature of movie criticism <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib20" title="">20</a>]</cite>. Many reviews express mixed sentiments or qualified opinions that are difficult to capture in binary classifications. Professional critics often provide sophisticated analyses that appreciate certain aspects of films while criticizing others, creating sentiment expressions that resist simple categorization.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p2">
<p class="ltx_p" id="S3.SS3.SSS2.p2.1">The binary paradigm also limits evaluation of models’ ability to handle the explainability challenge, as it provides limited insight into whether models are making predictions for appropriate reasons <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib25" title="">25</a>]</cite>. A model might achieve high accuracy while relying on spurious correlations or biased patterns in training data, leading to unreliable performance when deployed in different domains or time periods.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Common Evaluation Metrics and Their Limitations for Persistent Challenges.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T2.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T2.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T2.3.4.1.1"><span class="ltx_text ltx_font_bold" id="S3.T2.3.4.1.1.1" style="font-size:90%;">Metric</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T2.3.4.1.2"><span class="ltx_text ltx_font_bold" id="S3.T2.3.4.1.2.1" style="font-size:90%;">Formula/Description</span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" id="S3.T2.3.4.1.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.3.4.1.3.1">
<span class="ltx_p" id="S3.T2.3.4.1.3.1.1"><span class="ltx_text ltx_font_bold" id="S3.T2.3.4.1.3.1.1.1" style="font-size:90%;">Advantages</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" id="S3.T2.3.4.1.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.3.4.1.4.1">
<span class="ltx_p" id="S3.T2.3.4.1.4.1.1"><span class="ltx_text ltx_font_bold" id="S3.T2.3.4.1.4.1.1.1" style="font-size:90%;">Limitations for Challenges</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.1.1.2"><span class="ltx_text" id="S3.T2.1.1.2.1" style="font-size:90%;">Accuracy</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.1.1.1"><math alttext="\frac{TP+TN}{TP+TN+FP+FN}" class="ltx_Math" display="inline" id="S3.T2.1.1.1.m1" intent=":literal"><semantics><mfrac><mrow><mrow><mi mathsize="0.900em">T</mi><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.900em">P</mi></mrow><mo mathsize="0.900em">+</mo><mrow><mi mathsize="0.900em">T</mi><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.900em">N</mi></mrow></mrow><mrow><mrow><mi mathsize="0.900em">T</mi><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.900em">P</mi></mrow><mo mathsize="0.900em">+</mo><mrow><mi mathsize="0.900em">T</mi><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.900em">N</mi></mrow><mo mathsize="0.900em">+</mo><mrow><mi mathsize="0.900em">F</mi><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.900em">P</mi></mrow><mo mathsize="0.900em">+</mo><mrow><mi mathsize="0.900em">F</mi><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.900em">N</mi></mrow></mrow></mfrac><annotation encoding="application/x-tex">\frac{TP+TN}{TP+TN+FP+FN}</annotation></semantics></math></th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T2.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.3.1">
<span class="ltx_p" id="S3.T2.1.1.3.1.1"><span class="ltx_text" id="S3.T2.1.1.3.1.1.1" style="font-size:90%;">Intuitive, widely used</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T2.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.4.1">
<span class="ltx_p" id="S3.T2.1.1.4.1.1"><span class="ltx_text" id="S3.T2.1.1.4.1.1.1" style="font-size:90%;">Cannot detect sarcasm failures, domain drift, or bias issues</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.2.2.2"><span class="ltx_text" id="S3.T2.2.2.2.1" style="font-size:90%;">Macro-F1</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.2.2.1"><math alttext="\frac{1}{2}(F1_{pos}+F1_{neg})" class="ltx_Math" display="inline" id="S3.T2.2.2.1.m1" intent=":literal"><semantics><mrow><mfrac><mn mathsize="0.900em">1</mn><mn mathsize="0.900em">2</mn></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><mrow><mrow><mi mathsize="0.900em">F</mi><mo lspace="0em" rspace="0em">​</mo><msub><mn mathsize="0.900em">1</mn><mrow><mi mathsize="0.900em">p</mi><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.900em">o</mi><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.900em">s</mi></mrow></msub></mrow><mo mathsize="0.900em">+</mo><mrow><mi mathsize="0.900em">F</mi><mo lspace="0em" rspace="0em">​</mo><msub><mn mathsize="0.900em">1</mn><mrow><mi mathsize="0.900em">n</mi><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.900em">e</mi><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.900em">g</mi></mrow></msub></mrow></mrow><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow></mrow><annotation encoding="application/x-tex">\frac{1}{2}(F1_{pos}+F1_{neg})</annotation></semantics></math></th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T2.2.2.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.2.2.3.1">
<span class="ltx_p" id="S3.T2.2.2.3.1.1"><span class="ltx_text" id="S3.T2.2.2.3.1.1.1" style="font-size:90%;">Balanced class performance</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T2.2.2.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.2.2.4.1">
<span class="ltx_p" id="S3.T2.2.2.4.1.1"><span class="ltx_text" id="S3.T2.2.2.4.1.1.1" style="font-size:90%;">Ignores explainability and resource efficiency</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.5.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.3.5.1.1"><span class="ltx_text" id="S3.T2.3.5.1.1.1" style="font-size:90%;">ROC-AUC</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.3.5.1.2"><span class="ltx_text" id="S3.T2.3.5.1.2.1" style="font-size:90%;">Area under ROC curve</span></th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T2.3.5.1.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.3.5.1.3.1">
<span class="ltx_p" id="S3.T2.3.5.1.3.1.1"><span class="ltx_text" id="S3.T2.3.5.1.3.1.1.1" style="font-size:90%;">Threshold-independent</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S3.T2.3.5.1.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.3.5.1.4.1">
<span class="ltx_p" id="S3.T2.3.5.1.4.1.1"><span class="ltx_text" id="S3.T2.3.5.1.4.1.1.1" style="font-size:90%;">No insight into robustness across domains/time</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S3.T2.3.3.2"><span class="ltx_text" id="S3.T2.3.3.2.1" style="font-size:90%;">MCC</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S3.T2.3.3.1"><math alttext="\frac{TP\cdot TN-FP\cdot FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}" class="ltx_Math" display="inline" id="S3.T2.3.3.1.m1" intent=":literal"><semantics><mfrac><mrow><mrow><mrow><mrow><mi mathsize="0.900em">T</mi><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.900em">P</mi></mrow><mo lspace="0.222em" mathsize="0.900em" rspace="0.222em">⋅</mo><mi mathsize="0.900em">T</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.900em">N</mi></mrow><mo mathsize="0.900em">−</mo><mrow><mrow><mrow><mi mathsize="0.900em">F</mi><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.900em">P</mi></mrow><mo lspace="0.222em" mathsize="0.900em" rspace="0.222em">⋅</mo><mi mathsize="0.900em">F</mi></mrow><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.900em">N</mi></mrow></mrow><msqrt><mrow><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><mrow><mrow><mi mathsize="0.900em">T</mi><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.900em">P</mi></mrow><mo mathsize="0.900em">+</mo><mrow><mi mathsize="0.900em">F</mi><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.900em">P</mi></mrow></mrow><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><mrow><mrow><mi mathsize="0.900em">T</mi><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.900em">P</mi></mrow><mo mathsize="0.900em">+</mo><mrow><mi mathsize="0.900em">F</mi><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.900em">N</mi></mrow></mrow><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><mrow><mrow><mi mathsize="0.900em">T</mi><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.900em">N</mi></mrow><mo mathsize="0.900em">+</mo><mrow><mi mathsize="0.900em">F</mi><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.900em">P</mi></mrow></mrow><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mrow><mo maxsize="0.900em" minsize="0.900em">(</mo><mrow><mrow><mi mathsize="0.900em">T</mi><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.900em">N</mi></mrow><mo mathsize="0.900em">+</mo><mrow><mi mathsize="0.900em">F</mi><mo lspace="0em" rspace="0em">​</mo><mi mathsize="0.900em">N</mi></mrow></mrow><mo maxsize="0.900em" minsize="0.900em">)</mo></mrow></mrow></msqrt></mfrac><annotation encoding="application/x-tex">\frac{TP\cdot TN-FP\cdot FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}</annotation></semantics></math></th>
<td class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" id="S3.T2.3.3.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.3.3.3.1">
<span class="ltx_p" id="S3.T2.3.3.3.1.1"><span class="ltx_text" id="S3.T2.3.3.3.1.1.1" style="font-size:90%;">Handles imbalance well</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" id="S3.T2.3.3.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.3.3.4.1">
<span class="ltx_p" id="S3.T2.3.3.4.1.1"><span class="ltx_text" id="S3.T2.3.3.4.1.1.1" style="font-size:90%;">Missing context processing and efficiency assessment</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Emerging Recognition of Benchmark Limitations</h3>
<section class="ltx_subsubsection" id="S3.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1 </span>The Saturation Effect and Diminishing Returns</h4>
<div class="ltx_para" id="S3.SS4.SSS1.p1">
<p class="ltx_p" id="S3.SS4.SSS1.p1.1">The progression from approximately 82% accuracy with bag-of-words approaches to 97–98% with large language models represents remarkable technical achievement, yet this performance increase has revealed a critical insight: traditional benchmarks may be approaching their utility limits for driving meaningful research progress <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib39" title="">39</a>]</cite>. As models achieve near-perfect performance on standard benchmarks, the five persistent challenges become more apparent and more critical for real-world deployment success.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS1.p2">
<p class="ltx_p" id="S3.SS4.SSS1.p2.1">This performance saturation suggests that future progress requires moving beyond accuracy optimization toward holistic approaches that address multiple challenges simultaneously <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib29" title="">29</a>]</cite>. The diminishing returns from pure accuracy improvements have redirected research attention toward robustness, interpretability, and practical deployment considerations that existing benchmarks may not adequately capture.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2 </span>Domain and Temporal Representation Gaps</h4>
<div class="ltx_para" id="S3.SS4.SSS2.p1">
<p class="ltx_p" id="S3.SS4.SSS2.p1.1">Most benchmark datasets represent snapshots from particular time periods and platforms, potentially limiting their ability to capture temporal drift and domain variation that characterize real-world deployment scenarios <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib21" title="">21</a>]</cite>. The IMDB-Large dataset, despite its size and influence, reflects language patterns and cultural references from its collection period, which may not generalize to contemporary movie criticism or different platforms.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS2.p2">
<p class="ltx_p" id="S3.SS4.SSS2.p2.1">The focus on English-language reviews from primarily Western cultural contexts also limits benchmark utility for understanding cross-cultural sentiment expression and the challenges of deploying sentiment analysis systems globally <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib2" title="">2</a>]</cite>. Movie criticism reflects diverse cultural perspectives and linguistic patterns that may not be captured in existing benchmarks, suggesting need for more diverse and culturally representative evaluation frameworks.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.3 </span>Inadequate Evaluation of Persistent Challenges</h4>
<div class="ltx_para" id="S3.SS4.SSS3.p1">
<p class="ltx_p" id="S3.SS4.SSS3.p1.1">Traditional benchmarks provide limited assessment of model performance on the five persistent challenges identified in this review. Sarcasm detection capabilities are not systematically evaluated, domain adaptation robustness is not measured across different platforms or time periods, and explainability is not assessed through standard accuracy metrics <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib23" title="">23</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS3.p2">
<p class="ltx_p" id="S3.SS4.SSS3.p2.1">The resource efficiency challenge is particularly underserved by current benchmarks, which typically focus on maximizing accuracy without considering computational constraints, latency requirements, or energy consumption that characterize real-world deployment scenarios <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib27" title="">27</a>]</cite>. This gap between benchmark evaluation and deployment reality has led to systems that perform excellently in research settings but struggle in production environments.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Toward More Comprehensive Evaluation Frameworks</h3>
<section class="ltx_subsubsection" id="S3.SS5.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.1 </span>Multi-Dimensional Assessment Needs</h4>
<div class="ltx_para" id="S3.SS5.SSS1.p1">
<p class="ltx_p" id="S3.SS5.SSS1.p1.1">The field’s maturation demands evaluation approaches that move beyond traditional accuracy measurements toward comprehensive assessment of model capabilities across multiple dimensions <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib29" title="">29</a>]</cite>. Future benchmark development should incorporate explicit evaluation of sarcasm detection, domain adaptation, temporal robustness, explainability, and resource efficiency alongside traditional accuracy metrics.</p>
</div>
<div class="ltx_para" id="S3.SS5.SSS1.p2">
<p class="ltx_p" id="S3.SS5.SSS1.p2.1">Temporal robustness testing, where models are evaluated on reviews from different time periods, would provide insights into adaptation capabilities that current benchmarks cannot capture <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib21" title="">21</a>]</cite>. Cross-domain evaluation using reviews from different platforms and contexts would assess generalization abilities that are critical for real-world deployment but underexplored in current research.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS5.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.2 </span>Adversarial and Stress Testing</h4>
<div class="ltx_para" id="S3.SS5.SSS2.p1">
<p class="ltx_p" id="S3.SS5.SSS2.p1.1">Systematic evaluation of model robustness through adversarial examples and stress testing represents another direction for benchmark evolution <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib25" title="">25</a>]</cite>. Current benchmarks provide limited assessment of how models handle edge cases, deliberate attempts at deception, or inputs that are designed to reveal failure modes.</p>
</div>
<div class="ltx_para" id="S3.SS5.SSS2.p2">
<p class="ltx_p" id="S3.SS5.SSS2.p2.1">Sarcasm-specific evaluation metrics that go beyond simple accuracy to assess whether models are identifying sarcastic content for appropriate reasons would provide more meaningful insights into model capabilities <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib23" title="">23</a>]</cite>. Similarly, explainability metrics that evaluate whether model explanations align with human reasoning about sentiment would be valuable for building trust in deployed systems.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS5.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.5.3 </span>Resource-Aware Evaluation</h4>
<div class="ltx_para" id="S3.SS5.SSS3.p1">
<p class="ltx_p" id="S3.SS5.SSS3.p1.1">The development of evaluation frameworks that explicitly consider resource constraints represents a critical need for bridging the gap between research and deployment <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib26" title="">26</a>]</cite>. These frameworks should assess model performance across different computational budgets, enabling direct comparison of accuracy-efficiency trade-offs that are central to real-world decision making.</p>
</div>
<div class="ltx_para" id="S3.SS5.SSS3.p2">
<p class="ltx_p" id="S3.SS5.SSS3.p2.1">Latency-aware evaluation, energy consumption measurement, and model size constraints should be incorporated into benchmark standards to ensure that research progress translates to deployable systems <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib27" title="">27</a>]</cite>. This shift would encourage development of models that balance multiple performance dimensions rather than optimizing accuracy alone.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>The Path Forward: Integrated Benchmark Design</h3>
<div class="ltx_para" id="S3.SS6.p1">
<p class="ltx_p" id="S3.SS6.p1.1">The evolution of movie review sentiment analysis benchmarks reflects the field’s maturation from simple classification problems to complex real-world deployment challenges <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib3" title="">3</a>]</cite>. While traditional datasets like MR and IMDB-Large have been invaluable for standardizing evaluation and enabling reproducible research, their limitations have become apparent as the field approaches the frontiers defined by the five persistent challenges.</p>
</div>
<div class="ltx_para" id="S3.SS6.p2">
<p class="ltx_p" id="S3.SS6.p2.1">Future benchmark development should embrace integrated evaluation frameworks that assess multiple dimensions of model performance simultaneously, moving beyond accuracy optimization toward holistic system design <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib29" title="">29</a>]</cite>. This evolution requires collaboration between academic researchers and industry practitioners to ensure that evaluation standards reflect both theoretical advances and practical deployment realities.</p>
</div>
<div class="ltx_para" id="S3.SS6.p3">
<p class="ltx_p" id="S3.SS6.p3.1">The success of movie review sentiment analysis as a research domain demonstrates the power of well-designed benchmarks to drive sustained progress over decades. As the field continues to evolve, the development of more comprehensive, challenging, and realistic evaluation frameworks will be essential for addressing the persistent challenges that define the next frontier of sentiment analysis research and deployment.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Current Sentiment Analysis Models and Methods</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Sentiment analysis on movie reviews has progressed from lexicon-based heuristics that barely outperformed chance to transformers surpassing 96% accuracy on the 50k-review IMDb benchmark. Accuracy gains came in four waves lexicons, classic ML, deep neural networks, and large-scale pre-trained transformers each wave improving speed, data efficiency, and interpretability.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Rule-Based and Lexicon-Based Methods</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Rule-based and lexicon-based methods form the earliest category of sentiment analysis techniques. These methods operate by leveraging sentiment lexicons precompiled lists of words annotated with polarity values to assess the sentiment of a given text without requiring supervised learning. Despite being conceptually simple and interpretable, these methods face limitations in handling domain-specific expressions, sarcasm, and compositionality.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>SentiWordNet</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">SentiWordNet <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib47" title="">47</a>]</cite> is a lexical resource derived from WordNet, where each synset (set of synonyms) is assigned three sentiment scores: positivity, negativity, and objectivity. These scores are determined through semi-supervised learning techniques and allow for a graded view of sentiment intensity. In the context of movie reviews, SentiWordNet has been used to compute sentiment scores by aggregating the polarity of terms found in the review text <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib48" title="">48</a>]</cite>. However, its effectiveness depends heavily on accurate word sense disambiguation a nontrivial challenge, especially in informal or figurative language common in user-generated movie reviews.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>VADER</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">VADER (Valence Aware Dictionary and sEntiment Reasoner) is a rule-based sentiment analysis tool specifically tuned for social media and short text contexts <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib49" title="">49</a>]</cite>. It combines a human-validated lexicon with grammatical heuristics (e.g., punctuation, capitalization, degree modifiers) to infer sentiment. Unlike traditional lexicon-based approaches, VADER performs well even without prior domain adaptation, making it suitable for lightweight applications such as movie review summarization or preliminary filtering <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib49" title="">49</a>]</cite>. Nonetheless, its performance degrades when faced with domain-specific jargon or multi-aspect sentiment common in longer movie reviews.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3 </span>LIWC</h4>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1">The Linguistic Inquiry and Word Count (LIWC) tool is a psycholinguistic lexicon that maps words to psychologically meaningful categories, including affective processes such as positive and negative emotion <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib50" title="">50</a>]</cite>. While not originally designed for sentiment classification, LIWC has been widely applied in the analysis of movie reviews and online discourse to reveal patterns of emotional expression <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib51" title="">51</a>]</cite>. LIWC’s strength lies in its interpretability and grounding in psychological theory, but it lacks coverage for modern slang, idiomatic expressions, and evolving linguistic trends, which limits its applicability in dynamic domains like film critique.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.4 </span>Advantages and Limitations</h4>
<div class="ltx_para" id="S4.SS1.SSS4.p1">
<p class="ltx_p" id="S4.SS1.SSS4.p1.1">Lexicon-based methods are advantageous due to their interpretability, domain-independence (at least initially), and low computational cost. They are particularly useful in low-resource settings where labeled data is scarce. However, their major limitations include:</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS4.p2">
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1">Lack of domain adaptation:</span> Lexicons do not capture domain-specific usage (e.g., “dark” may be neutral in general language but positive in the context of horror films).</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1">Inability to model context:</span> These methods typically ignore syntactic and semantic context, leading to incorrect sentiment predictions in the presence of negation or sarcasm <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib1" title="">1</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i3.p1.1.1">Static word representations:</span> Lexicon scores are fixed and do not account for polysemy or word sense variation across contexts.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S4.SS1.SSS4.p3">
<p class="ltx_p" id="S4.SS1.SSS4.p3.1">Despite these limitations, rule-based approaches continue to serve as strong baselines and are often integrated with machine learning systems to provide interpretable explanations or initial polarity signals.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Traditional Machine Learning Approaches</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Prior to the advent of deep learning, sentiment analysis on movie reviews was predominantly approached as a supervised text classification problem using traditional machine learning algorithms. These methods required careful feature engineering to transform raw text into structured representations, followed by the application of classification algorithms such as Naïve Bayes <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib52" title="">52</a>]</cite>, Support Vector Machines (SVMs) <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib53" title="">53</a>]</cite>, and Logistic Regression <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib54" title="">54</a>]</cite>. Though largely surpassed by neural methods in recent years, traditional models remain relevant for their interpretability, speed, and competitive performance on smaller or well-curated datasets.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Feature Engineering for Sentiment Representation</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">One of the foundational steps in traditional sentiment analysis pipelines is the transformation of text into numerical features. The simplest and most common representations include bag-of-words (BoW) and <math alttext="n" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p1.1.m1" intent=":literal"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>-gram models. Pang et al. <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib1" title="">1</a>]</cite> demonstrated that unigram and bigram features, when combined with frequency-based weighting schemes, can achieve high accuracy on movie reviews.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p2">
<p class="ltx_p" id="S4.SS2.SSS1.p2.1">Term Frequency-Inverse Document Frequency (TF-IDF) is another widely used weighting technique, which downweights commonly occurring words while emphasizing informative terms <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib55" title="">55</a>]</cite>. TF-IDF has been shown to outperform raw frequency counts by better capturing the discriminative power of specific sentiment-bearing words.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p3">
<p class="ltx_p" id="S4.SS2.SSS1.p3.1">In addition to frequency-based features, part-of-speech (POS) tags have been incorporated to emphasize sentiment-heavy word classes such as adjectives and adverbs. Whitelaw et al. <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib56" title="">56</a>]</cite> proposed enriching BoW models with POS-tagged phrases, leading to improved performance in polarity classification tasks. Feature engineering in traditional pipelines may also include stemming or lemmatization to reduce word form variation, and syntactic parsing to extract structured linguistic patterns.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Classification Algorithms</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">Among the earliest and most effective classifiers for sentiment analysis is the Multinomial Naïve Bayes (MNB) algorithm. Despite its simplistic conditional independence assumption, MNB performs competitively on textual data due to its robustness and efficiency <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib57" title="">57</a>]</cite>. Pang et al. <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib1" title="">1</a>]</cite> found Naïve Bayes to perform reasonably well on movie reviews, though it was often outperformed by more discriminative models.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p2">
<p class="ltx_p" id="S4.SS2.SSS2.p2.1">Support Vector Machines (SVMs) quickly became the dominant method for sentiment classification due to their ability to handle high-dimensional sparse data and to find optimal separating hyperplanes in feature space <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib55" title="">55</a>]</cite>. Linear SVMs, in particular, are well-suited for text classification tasks where data points lie in a high-dimensional feature space. In experiments on the IMDb dataset, SVMs consistently outperformed Naïve Bayes and Logistic Regression when paired with appropriate feature selection <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib1" title="">1</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p3">
<p class="ltx_p" id="S4.SS2.SSS2.p3.1">Logistic Regression is another frequently used classifier due to its probabilistic output and ability to be interpreted in terms of feature weights. It is often preferred in settings where decision thresholds or confidence scores are required. While generally competitive with SVMs, Logistic Regression may suffer when the classes are not linearly separable, especially in noisy review corpora <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib58" title="">58</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>Strengths and Limitations</h4>
<div class="ltx_para" id="S4.SS2.SSS3.p1">
<p class="ltx_p" id="S4.SS2.SSS3.p1.1">Traditional machine learning methods offer several advantages. They are computationally efficient and interpretable, allowing for fine control over the modeling pipeline. Feature importance can be directly extracted from model coefficients, making them suitable for use cases that require explainability.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p2">
<p class="ltx_p" id="S4.SS2.SSS3.p2.1">However, these methods suffer from notable limitations. Their performance is heavily dependent on the quality of feature engineering and cannot easily model long-distance dependencies, negation, or compositionality in language. For instance, detecting that “not a great movie” carries negative sentiment requires more than just counting positive words. Furthermore, traditional classifiers struggle with domain adaptation and sarcasm detection due to their inability to capture contextual semantics <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib1" title="">1</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS3.p3">
<p class="ltx_p" id="S4.SS2.SSS3.p3.1">Despite these drawbacks, traditional models laid the foundational groundwork for sentiment analysis and continue to serve as strong baselines and components in hybrid systems.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Deep Learning-Based Methods</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Deep learning has significantly advanced the state of sentiment analysis by enabling end-to-end learning of hierarchical and contextual representations from raw text. Unlike traditional machine learning approaches that rely heavily on manual feature engineering, deep neural networks can automatically learn semantic and syntactic features that are critical for sentiment classification. In the domain of movie reviews, deep learning methods have demonstrated superior performance across multiple benchmarks by capturing compositional sentiment, long-range dependencies, and subtle linguistic cues.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Convolutional Neural Networks (CNNs)</h4>
<div class="ltx_para" id="S4.SS3.SSS1.p1">
<p class="ltx_p" id="S4.SS3.SSS1.p1.1">Convolutional Neural Networks (CNNs), initially developed for computer vision, have been successfully adapted for text classification tasks such as sentiment analysis. Kim <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib42" title="">42</a>]</cite> showed that a simple CNN with a single convolutional layer applied to pre-trained word vectors (e.g., Word2Vec) can achieve competitive results on multiple sentiment benchmarks, including the IMDb movie review dataset. CNNs are particularly effective at capturing local patterns such as sentiment-bearing phrases (e.g., “utterly disappointing”, “brilliant performance”) by applying filters over word sequences. Their parallelizable architecture and relatively shallow depth also make them computationally efficient.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Recurrent Neural Networks (RNNs), LSTMs, and BiLSTMs</h4>
<div class="ltx_para" id="S4.SS3.SSS2.p1">
<p class="ltx_p" id="S4.SS3.SSS2.p1.1">Recurrent Neural Networks (RNNs) are designed to model sequential dependencies in text, making them well-suited for tasks where word order and context matter. However, traditional RNNs suffer from vanishing and exploding gradient problems, which limit their ability to capture long-term dependencies <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib59" title="">59</a>]</cite>. To overcome this, Long Short-Term Memory (LSTM) networks were introduced <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib35" title="">35</a>]</cite>. LSTMs use gating mechanisms to regulate the flow of information, enabling them to remember sentiment-relevant features over longer spans of text.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS2.p2">
<p class="ltx_p" id="S4.SS3.SSS2.p2.1">Bidirectional LSTMs (BiLSTMs) further enhance performance by processing the input sequence in both forward and backward directions, thereby incorporating both past and future context <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib60" title="">60</a>]</cite>. In sentiment classification of movie reviews, where sentiment may depend on distant contextual cues (e.g., “Although the film starts slow, it ultimately delivers a powerful message”), BiLSTMs provide a robust mechanism for modeling such dependencies.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3 </span>Word Embeddings: Word2Vec and GloVe</h4>
<div class="ltx_para" id="S4.SS3.SSS3.p1">
<p class="ltx_p" id="S4.SS3.SSS3.p1.1">A crucial component of deep learning-based sentiment models is the use of distributed word representations or embeddings. Word2Vec <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib61" title="">61</a>]</cite>, trained using skip-gram or CBOW objectives, captures semantic similarity by placing similar words in nearby vector space. GloVe <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib34" title="">34</a>]</cite>, on the other hand, leverages global co-occurrence statistics to produce embeddings that better capture linear substructures. These embeddings are often used as input to CNNs or RNNs and can be fine-tuned during training for improved task-specific performance. Pre-trained embeddings alleviate data sparsity issues and help models generalize better on small or imbalanced movie review datasets.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.4 </span>Attention Mechanisms</h4>
<div class="ltx_para" id="S4.SS3.SSS4.p1">
<p class="ltx_p" id="S4.SS3.SSS4.p1.1">While RNNs and CNNs are capable of learning useful features, they often compress long sequences into a single vector, potentially losing important information. Attention mechanisms address this limitation by allowing the model to focus selectively on relevant parts of the input when making predictions <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib62" title="">62</a>]</cite>. In sentiment analysis, attention helps identify sentiment-bearing phrases or clauses even if they are distant from each other in the input. Yang et al. <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib36" title="">36</a>]</cite> proposed a Hierarchical Attention Network (HAN) that applies attention at both the word and sentence levels, achieving strong performance on document-level sentiment classification tasks, including movie reviews.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.5 </span>Summary and Limitations</h4>
<div class="ltx_para" id="S4.SS3.SSS5.p1">
<p class="ltx_p" id="S4.SS3.SSS5.p1.1">Deep learning methods significantly outperform traditional approaches in sentiment classification, particularly on large datasets. They offer the ability to model complex linguistic phenomena such as negation, sarcasm, and compositionality. However, they require substantial computational resources and are often data-hungry. Moreover, deep models are typically less interpretable than their traditional counterparts, making it challenging to understand their decision-making processes without auxiliary explanation methods.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Transformer-Based Models</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">The introduction of transformer architectures <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib24" title="">24</a>]</cite> has revolutionized the field of natural language processing, including sentiment analysis of movie reviews. Unlike traditional RNN-based models, transformers leverage self-attention mechanisms to model long-range dependencies in text efficiently, without relying on sequential data processing.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1 </span>Pretrained Transformer Models</h4>
<div class="ltx_para" id="S4.SS4.SSS1.p1">
<p class="ltx_p" id="S4.SS4.SSS1.p1.1">One of the earliest and most impactful transformer-based models is BERT (Bidirectional Encoder Representations from Transformers) <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib22" title="">22</a>]</cite>. BERT is pretrained on large-scale corpora using masked language modeling and next sentence prediction, and can be fine-tuned on downstream tasks such as sentiment classification with minimal architectural modifications. In the context of movie reviews, fine-tuning BERT on datasets like IMDb has yielded state-of-the-art results in binary sentiment classification <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib63" title="">63</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS1.p2">
<p class="ltx_p" id="S4.SS4.SSS1.p2.1">Variants such as RoBERTa <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib37" title="">37</a>]</cite> improved upon BERT by optimizing pretraining strategies, including removal of the next sentence prediction objective and training on larger corpora with dynamic masking. These modifications have led to improved sentiment classification performance across benchmarks. DistilBERT <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib64" title="">64</a>]</cite>, a compressed version of BERT, provides a trade-off between inference speed and accuracy, making it suitable for real-time applications without substantial degradation in accuracy. XLNet <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib65" title="">65</a>]</cite>, a generalized autoregressive pretraining model, overcomes some limitations of BERT by modeling bidirectional context while retaining autoregressive properties, demonstrating competitive performance on sentiment tasks.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2 </span>Domain-Specific Transformers</h4>
<div class="ltx_para" id="S4.SS4.SSS2.p1">
<p class="ltx_p" id="S4.SS4.SSS2.p1.1">Although general-purpose pretrained transformers offer strong baselines, domain-adapted transformer models have emerged to capture nuances specific to movie reviews. BERT models for sentiment analysis in different contexts have gained prominence including sentiment analysis for microblogging platforms like Twitter, demonstrating that BERT combined with neural network architectures (CNN, RNN, BiLSTM) achieves superior performance compared to traditional Word2vec approaches <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib66" title="">66</a>]</cite>. Batra et al. <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib67" title="">67</a>]</cite> have applied BERT-based models to software engineering contexts, analyzing sentiment in GitHub comments, Jira comments, and Stack Overflow posts, where ensemble BERT models and compressed BERT variants showed 6–12% improvement in F1-scores over existing tools like SentiCR and SentiStrength-SE. Penha et al. <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib68" title="">68</a>]</cite> investigated how much factual knowledge about recommendation items (books, movies, music) is stored in pre-trained BERT models and whether this knowledge can be leveraged for Conversational Recommender Systems (CRS). Their study finds that BERT contains substantial content-based knowledge about items but has limited collaborative knowledge, and while it can perform basic recommendation tasks, it struggles with conversational recommendations when faced with challenging data. More recently, BERT has been combined with BiLSTM for movie review sentiment analysis <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib69" title="">69</a>]</cite>, achieving better accuracy than existing state-of-the-art methods and demonstrating how the approach can predict overall movie sentiment for recommendation systems.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.3 </span>Prompt-Based and Zero-Shot Models</h4>
<div class="ltx_para" id="S4.SS4.SSS3.p1">
<p class="ltx_p" id="S4.SS4.SSS3.p1.1">With the advent of generative pretrained transformers like T5 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib70" title="">70</a>]</cite> and GPT <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib38" title="">38</a>]</cite>, few-shot and zero-shot sentiment classification has become increasingly feasible. These models can be prompted using task-specific instructions, enabling them to perform classification tasks without extensive fine-tuning. For example, a prompt like “Classify the sentiment of this review: ‘The movie was breathtaking and unforgettable.”’ can elicit accurate sentiment predictions even with minimal supervision. However, prompt-based performance is often sensitive to prompt wording and may require careful calibration <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib71" title="">71</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.4 </span>Strengths and Limitations</h4>
<div class="ltx_para" id="S4.SS4.SSS4.p1">
<p class="ltx_p" id="S4.SS4.SSS4.p1.1">Transformer-based models offer several advantages for sentiment classification, including superior accuracy, transferability to low-resource settings, and the ability to model complex syntactic and semantic relationships. However, they are not without limitations. Large models such as BERT and GPT-3 are computationally expensive to train and deploy, and their performance may degrade when faced with domain-specific jargon or informal language unless further fine-tuned. Additionally, explainability remains a significant challenge, especially in high-stakes applications where interpretability of predictions is critical.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Large Language Models and the New Paradigm</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">The emergence of Large Language Models (LLMs) such as GPT-4, Llama, and Claude has fundamentally transformed the landscape of sentiment analysis, introducing capabilities that extend far beyond traditional fine-tuning paradigms.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS5.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.5.1 </span>Zero-Shot and Few-Shot Sentiment Classification</h4>
<div class="ltx_para" id="S4.SS5.SSS1.p1">
<p class="ltx_p" id="S4.SS5.SSS1.p1.1">A comprehensive evaluation by Zhang et al. <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib72" title="">72</a>]</cite> across 13 sentiment analysis tasks on 26 datasets revealed that while LLMs demonstrate satisfactory performance in simpler sentiment classification tasks, they lag behind fine-tuned small language models (SLMs) in more complex tasks requiring deeper understanding of specific sentiment phenomena. However, LLMs significantly outperform SLMs in few-shot learning settings, suggesting their potential when annotation resources are limited. The study introduced SentiEval, a benchmark for more comprehensive evaluation of LLM sentiment capabilities.</p>
</div>
<div class="ltx_para" id="S4.SS5.SSS1.p2">
<p class="ltx_p" id="S4.SS5.SSS1.p2.1">Krugmann and Hartmann <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib73" title="">73</a>]</cite> benchmarked GPT-3.5, GPT-4, and Llama 2 against established transfer learning models for marketing research applications. Their findings indicate that despite their zero-shot nature, LLMs can not only compete with but in some cases surpass traditional transfer learning methods in sentiment classification accuracy. The study also highlighted that prompt specificity significantly affects prediction consistency, with explicitly structured prompts yielding greater reliability.</p>
</div>
<div class="ltx_para" id="S4.SS5.SSS1.p3">
<p class="ltx_p" id="S4.SS5.SSS1.p3.4">Recent work on multilingual sentiment analysis has demonstrated GPT’s effectiveness across 12 languages, with performance comparable to or exceeding top-performing fine-tuned models from previous years <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib74" title="">74</a>]</cite>. GPT-4 achieved correlations of <math alttext="r=0.59" class="ltx_Math" display="inline" id="S4.SS5.SSS1.p3.1.m1" intent=":literal"><semantics><mrow><mi>r</mi><mo>=</mo><mn>0.59</mn></mrow><annotation encoding="application/x-tex">r=0.59</annotation></semantics></math> to <math alttext="0.77" class="ltx_Math" display="inline" id="S4.SS5.SSS1.p3.2.m2" intent=":literal"><semantics><mn>0.77</mn><annotation encoding="application/x-tex">0.77</annotation></semantics></math> with human annotators, substantially outperforming traditional dictionary-based methods (<math alttext="r=0.20" class="ltx_Math" display="inline" id="S4.SS5.SSS1.p3.3.m3" intent=":literal"><semantics><mrow><mi>r</mi><mo>=</mo><mn>0.20</mn></mrow><annotation encoding="application/x-tex">r=0.20</annotation></semantics></math> to <math alttext="0.30" class="ltx_Math" display="inline" id="S4.SS5.SSS1.p3.4.m4" intent=":literal"><semantics><mn>0.30</mn><annotation encoding="application/x-tex">0.30</annotation></semantics></math>).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS5.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.5.2 </span>Chain-of-Thought Prompting for Sentiment</h4>
<div class="ltx_para" id="S4.SS5.SSS2.p1">
<p class="ltx_p" id="S4.SS5.SSS2.p1.1">Chain-of-thought (CoT) prompting has emerged as a powerful technique for improving LLM performance on sentiment tasks requiring reasoning. Fei et al. <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib75" title="">75</a>]</cite> introduced the Three-hop Reasoning (THOR) framework for implicit sentiment analysis, which mimics human-like reasoning by step-by-step inducing the implicit aspect, opinion, and finally the sentiment polarity. This approach addresses the challenge of detecting sentiment where opinion cues are implicit and obscure, requiring common-sense and multi-hop reasoning to infer latent intent.</p>
</div>
<div class="ltx_para" id="S4.SS5.SSS2.p2">
<p class="ltx_p" id="S4.SS5.SSS2.p2.1">The THOR framework demonstrates that structured reasoning chains can significantly improve performance on nuanced sentiment tasks, particularly for sarcasm and implicit sentiment where traditional classification approaches struggle. This represents a paradigm shift from pattern matching to genuine reasoning about sentiment.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS5.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.5.3 </span>Instruction-Tuned Models for Aspect-Based Sentiment</h4>
<div class="ltx_para" id="S4.SS5.SSS3.p1">
<p class="ltx_p" id="S4.SS5.SSS3.p1.1">InstructABSA <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib76" title="">76</a>]</cite> introduced an instruction learning paradigm for Aspect-Based Sentiment Analysis (ABSA) that achieves state-of-the-art results across multiple subtasks. By incorporating positive, negative, and neutral examples into training samples and instruction-tuning the Tk-Instruct model, InstructABSA outperformed previous SOTA on the Rest14 ATE subtask by 5.69% points and the Rest15 ATSC subtask by 9.59% points, surpassing models 7<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS5.SSS3.p1.1.m1" intent=":literal"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math> larger. Notably, just 50% of training data was sufficient to achieve competitive results, demonstrating remarkable sample efficiency.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS5.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.5.4 </span>Limitations and Emerging Challenges</h4>
<div class="ltx_para" id="S4.SS5.SSS4.p1">
<p class="ltx_p" id="S4.SS5.SSS4.p1.1">Despite their impressive capabilities, LLMs present new challenges for sentiment analysis. Model outputs exhibit sensitivity to prompt wording, with even small changes in phrasing leading to different sentiment classifications <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib72" title="">72</a>]</cite>. Reproducibility remains a concern, as temperature settings and model versions can affect result consistency. Furthermore, the computational cost of deploying LLMs at scale poses practical limitations for real-time sentiment monitoring applications.</p>
</div>
<div class="ltx_para" id="S4.SS5.SSS4.p2">
<p class="ltx_p" id="S4.SS5.SSS4.p2.1">The question of whether LLMs truly “understand” sentiment or merely exploit statistical patterns remains open. While chain-of-thought prompting improves interpretability, the black-box nature of these models continues to challenge deployment in high-stakes domains requiring explainable predictions.</p>
</div>
<figure class="ltx_figure" id="S4.F3"><span class="ltx_inline-block"><svg class="ltx_picture ltx_centering" height="312.38" id="S4.F3.pic1" overflow="visible" version="1.1" viewbox="0 0 501.54 312.38" width="501.54"><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="translate(0,312.38) matrix(1 0 0 -1 0 0) translate(46.14,0) translate(0,22.93)"><g stroke-width="0.8pt"><path d="M 0 0 L 376.97 0" style="fill:none"></path><g stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" stroke-width="0.64pt" transform="matrix(1.0 0.0 0.0 1.0 376.97 0)"><path d="M -2.16 2.88 C -1.98 1.8 0 0.18 0.54 0 C 0 -0.18 -1.98 -1.8 -2.16 -2.88" style="fill:none"></path></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 383.12 -4.73)"><foreignobject height="9.46" overflow="visible" style="--ltx-fo-width:2em;--ltx-fo-height:0.68em;--ltx-fo-depth:0em;" transform="matrix(1 0 0 -1 0 9.46)" width="27.71"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><span class="ltx_text" id="S4.F3.pic1.1.1.1.1.1.1">Year</span></span></span></foreignobject></g></g><g stroke-width="0.8pt"><path d="M 0 0 L 0 250.99" style="fill:none"></path><g stroke-dasharray="none" stroke-dashoffset="0.0pt" stroke-linecap="round" stroke-linejoin="round" stroke-width="0.64pt" transform="matrix(0.0 1.0 -1.0 0.0 0 250.99)"><path d="M -2.16 2.88 C -1.98 1.8 0 0.18 0.54 0 C 0 -0.18 -1.98 -1.8 -2.16 -2.88" style="fill:none"></path></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 -41.53 260.59)"><foreignobject height="13.84" overflow="visible" style="--ltx-fo-width:6em;--ltx-fo-height:0.75em;--ltx-fo-depth:0.25em;" transform="matrix(1 0 0 -1 0 10.38)" width="83.06"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><span class="ltx_text" id="S4.F3.pic1.2.2.2.1.1.1">Accuracy (%)</span></span></span></foreignobject></g></g><g stroke-width="0.4pt"><path d="M -3.15 31.5 L 3.15 31.5" style="fill:none"></path><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 -22.95 27.93)"><foreignobject height="7.13" overflow="visible" style="--ltx-fo-width:1em;--ltx-fo-height:0.61em;--ltx-fo-depth:0em;" transform="matrix(1 0 0 -1 0 7.13)" width="11.76"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><span class="ltx_text" id="S4.F3.pic1.3.3.3.1.1.1" style="font-size:80%;">80</span></span></span></foreignobject></g><path d="M -3.15 62.99 L 3.15 62.99" style="fill:none"></path><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 -22.95 59.43)"><foreignobject height="7.13" overflow="visible" style="--ltx-fo-width:1em;--ltx-fo-height:0.61em;--ltx-fo-depth:0em;" transform="matrix(1 0 0 -1 0 7.13)" width="11.76"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><span class="ltx_text" id="S4.F3.pic1.4.4.4.2.1.1" style="font-size:80%;">85</span></span></span></foreignobject></g><path d="M -3.15 94.49 L 3.15 94.49" style="fill:none"></path><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 -22.95 90.92)"><foreignobject height="7.13" overflow="visible" style="--ltx-fo-width:1em;--ltx-fo-height:0.61em;--ltx-fo-depth:0em;" transform="matrix(1 0 0 -1 0 7.13)" width="11.76"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><span class="ltx_text" id="S4.F3.pic1.5.5.5.3.1.1" style="font-size:80%;">90</span></span></span></foreignobject></g><path d="M -3.15 125.98 L 3.15 125.98" style="fill:none"></path><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 -22.95 122.42)"><foreignobject height="7.13" overflow="visible" style="--ltx-fo-width:1em;--ltx-fo-height:0.61em;--ltx-fo-depth:0em;" transform="matrix(1 0 0 -1 0 7.13)" width="11.76"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><span class="ltx_text" id="S4.F3.pic1.6.6.6.4.1.1" style="font-size:80%;">95</span></span></span></foreignobject></g><path d="M -3.15 157.48 L 3.15 157.48" style="fill:none"></path><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 -28.83 153.91)"><foreignobject height="7.13" overflow="visible" style="--ltx-fo-width:1.5em;--ltx-fo-height:0.61em;--ltx-fo-depth:0em;" transform="matrix(1 0 0 -1 0 7.13)" width="17.64"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><span class="ltx_text" id="S4.F3.pic1.7.7.7.5.1.1" style="font-size:80%;">100</span></span></span></foreignobject></g><path d="M 62.99 -3.15 L 62.99 3.15" style="fill:none"></path><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 51.23 -18.32)"><foreignobject height="7.13" overflow="visible" style="--ltx-fo-width:2em;--ltx-fo-height:0.61em;--ltx-fo-depth:0em;" transform="matrix(1 0 0 -1 0 7.13)" width="23.52"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><span class="ltx_text" id="S4.F3.pic1.8.8.8.6.1.1" style="font-size:80%;">2005</span></span></span></foreignobject></g><path d="M 125.98 -3.15 L 125.98 3.15" style="fill:none"></path><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 114.22 -18.32)"><foreignobject height="7.13" overflow="visible" style="--ltx-fo-width:2em;--ltx-fo-height:0.61em;--ltx-fo-depth:0em;" transform="matrix(1 0 0 -1 0 7.13)" width="23.52"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><span class="ltx_text" id="S4.F3.pic1.9.9.9.7.1.1" style="font-size:80%;">2010</span></span></span></foreignobject></g><path d="M 188.98 -3.15 L 188.98 3.15" style="fill:none"></path><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 177.22 -18.32)"><foreignobject height="7.13" overflow="visible" style="--ltx-fo-width:2em;--ltx-fo-height:0.61em;--ltx-fo-depth:0em;" transform="matrix(1 0 0 -1 0 7.13)" width="23.52"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><span class="ltx_text" id="S4.F3.pic1.10.10.10.8.1.1" style="font-size:80%;">2015</span></span></span></foreignobject></g><path d="M 251.97 -3.15 L 251.97 3.15" style="fill:none"></path><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 240.21 -18.32)"><foreignobject height="7.13" overflow="visible" style="--ltx-fo-width:2em;--ltx-fo-height:0.61em;--ltx-fo-depth:0em;" transform="matrix(1 0 0 -1 0 7.13)" width="23.52"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><span class="ltx_text" id="S4.F3.pic1.11.11.11.9.1.1" style="font-size:80%;">2020</span></span></span></foreignobject></g><path d="M 314.96 -3.15 L 314.96 3.15" style="fill:none"></path><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 303.2 -18.32)"><foreignobject height="7.13" overflow="visible" style="--ltx-fo-width:2em;--ltx-fo-height:0.61em;--ltx-fo-depth:0em;" transform="matrix(1 0 0 -1 0 7.13)" width="23.52"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><span class="ltx_text" id="S4.F3.pic1.12.12.12.10.1.1" style="font-size:80%;">2025</span></span></span></foreignobject></g><g color="#0000FF" fill="#0000FF" stroke="#0000FF" stroke-width="0.8pt" style="--ltx-stroke-color:#0000FF;--ltx-fill-color:#0000FF;--ltx-fg-color:#0000FF;"><path d="M 31.5 37.8 C 31.5 37.8 77.01 49.7 94.49 56.69 C 111.97 63.69 140 77.7 157.48 88.19 C 174.96 98.68 202.99 123.98 220.47 132.28 C 237.95 140.59 270.36 145.41 283.47 148.03 C 296.58 150.65 314.96 151.18 314.96 151.18" style="fill:none"></path></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000" stroke-dasharray="3.0pt,3.0pt" stroke-dashoffset="0.0pt" stroke-width="0.8pt" style="--ltx-stroke-color:#FF0000;--ltx-fill-color:#FF0000;--ltx-fg-color:#FF0000;"><path d="M 31.5 15.75 C 31.5 15.75 77.01 18.99 94.49 22.05 C 111.97 25.11 140 29.93 157.48 37.8 C 174.96 45.66 202.99 66.5 220.47 78.74 C 237.95 90.98 270.36 112.87 283.47 125.98 C 296.58 139.09 314.96 173.23 314.96 173.23" style="fill:none"></path></g><g color="#E6E6E6" fill="#E6E6E6" stroke="#E6E6E6" style="--ltx-stroke-color:#E6E6E6;--ltx-fill-color:#E6E6E6;--ltx-fg-color:#E6E6E6;"><path d="M 267.72 125.98 M 267.72 125.98 L 267.72 173.23 L 346.46 173.23 L 346.46 125.98 Z M 346.46 173.23" style="stroke:none"></path></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 352.36 277.15)"><foreignobject height="191.72" overflow="visible" style="--ltx-fo-width:7.11em;--ltx-fo-height:0.56em;--ltx-fo-depth:13.3em;" transform="matrix(1 0 0 -1 0 7.69)" width="98.43"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S4.F3.pic1.13.13.13.11.1.1" style="width:8.37em;">
<span class="ltx_p" id="S4.F3.pic1.13.13.13.11.1.1.1"></span>
<span class="ltx_p" id="S4.F3.pic1.13.13.13.11.1.1.2"><span class="ltx_text ltx_font_italic" id="S4.F3.pic1.13.13.13.11.1.1.2.1" style="font-size:80%;">Performance</span></span>
<span class="ltx_p" id="S4.F3.pic1.13.13.13.11.1.1.3"><span class="ltx_text ltx_font_italic" id="S4.F3.pic1.13.13.13.11.1.1.3.1" style="font-size:80%;">Saturation Zone</span></span>
</span></span></span></foreignobject></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 36.38 217.71)"><foreignobject height="9.84" overflow="visible" style="--ltx-fo-width:6.51em;--ltx-fo-height:0.65em;--ltx-fo-depth:0.18em;" transform="matrix(1 0 0 -1 0 7.69)" width="76.61"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><span class="ltx_text" id="S4.F3.pic1.14.14.14.12.1.1" style="font-size:80%;">Peak Accuracy</span></span></span></foreignobject></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 36.38 201.96)"><foreignobject height="9.84" overflow="visible" style="--ltx-fo-width:10.54em;--ltx-fo-height:0.65em;--ltx-fo-depth:0.18em;" transform="matrix(1 0 0 -1 0 7.69)" width="123.95"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content"><span class="ltx_text" id="S4.F3.pic1.15.15.15.13.1.1" style="font-size:80%;--ltx-fg-color:#FF0000;">- -<span class="ltx_text" id="S4.F3.pic1.15.15.15.13.1.1.1" style="--ltx-fg-color:#000000;"> Challenge Awareness</span></span></span></span></foreignobject></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 23.62 75.67)"><foreignobject height="10.45" overflow="visible" style="--ltx-fo-width:5.69em;--ltx-fo-height:0.6em;--ltx-fo-depth:0.16em;" transform="matrix(1 0 0 -1 0 8.3)" width="78.74"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S4.F3.pic1.16.16.16.14.1.1" style="width:6.69em;">
<span class="ltx_p" id="S4.F3.pic1.16.16.16.14.1.1.1"></span>
<span class="ltx_p" id="S4.F3.pic1.16.16.16.14.1.1.2"><span class="ltx_text" id="S4.F3.pic1.16.16.16.14.1.1.2.1" style="font-size:80%;">Bag-of-Words</span></span>
<span class="ltx_p" id="S4.F3.pic1.16.16.16.14.1.1.3"><span class="ltx_text" id="S4.F3.pic1.16.16.16.14.1.1.3.1" style="font-size:80%;">82%</span></span>
</span></span></span></foreignobject></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 118.11 106.39)"><foreignobject height="8.92" overflow="visible" style="--ltx-fo-width:5.69em;--ltx-fo-height:0.6em;--ltx-fo-depth:0.04em;" transform="matrix(1 0 0 -1 0 8.3)" width="78.74"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S4.F3.pic1.17.17.17.15.1.1" style="width:6.69em;">
<span class="ltx_p" id="S4.F3.pic1.17.17.17.15.1.1.1"></span>
<span class="ltx_p" id="S4.F3.pic1.17.17.17.15.1.1.2"><span class="ltx_text" id="S4.F3.pic1.17.17.17.15.1.1.2.1" style="font-size:80%;">RNNs</span></span>
<span class="ltx_p" id="S4.F3.pic1.17.17.17.15.1.1.3"><span class="ltx_text" id="S4.F3.pic1.17.17.17.15.1.1.3.1" style="font-size:80%;">90–93%</span></span>
</span></span></span></foreignobject></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 196.85 153.64)"><foreignobject height="8.92" overflow="visible" style="--ltx-fo-width:5.69em;--ltx-fo-height:0.6em;--ltx-fo-depth:0.04em;" transform="matrix(1 0 0 -1 0 8.3)" width="78.74"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S4.F3.pic1.18.18.18.16.1.1" style="width:6.69em;">
<span class="ltx_p" id="S4.F3.pic1.18.18.18.16.1.1.1"></span>
<span class="ltx_p" id="S4.F3.pic1.18.18.18.16.1.1.2"><span class="ltx_text" id="S4.F3.pic1.18.18.18.16.1.1.2.1" style="font-size:80%;">Transformers</span></span>
<span class="ltx_p" id="S4.F3.pic1.18.18.18.16.1.1.3"><span class="ltx_text" id="S4.F3.pic1.18.18.18.16.1.1.3.1" style="font-size:80%;">94–97%</span></span>
</span></span></span></foreignobject></g><g fill="#000000" stroke="#000000" style="--ltx-stroke-color:#000000;--ltx-fill-color:#000000;" transform="matrix(1.0 0.0 0.0 1.0 275.59 169.39)"><foreignobject height="8.92" overflow="visible" style="--ltx-fo-width:5.69em;--ltx-fo-height:0.6em;--ltx-fo-depth:0.04em;" transform="matrix(1 0 0 -1 0 8.3)" width="78.74"><span class="ltx_foreignobject_container"><span class="ltx_foreignobject_content">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S4.F3.pic1.19.19.19.17.1.1" style="width:6.69em;">
<span class="ltx_p" id="S4.F3.pic1.19.19.19.17.1.1.1"></span>
<span class="ltx_p" id="S4.F3.pic1.19.19.19.17.1.1.2"><span class="ltx_text" id="S4.F3.pic1.19.19.19.17.1.1.2.1" style="font-size:80%;">LLMs</span></span>
<span class="ltx_p" id="S4.F3.pic1.19.19.19.17.1.1.3"><span class="ltx_text" id="S4.F3.pic1.19.19.19.17.1.1.3.1" style="font-size:80%;">97–98%</span></span>
</span></span></span></foreignobject></g></g></g></svg></span>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The progression of peak accuracy on IMDB benchmarks versus growing awareness of persistent challenges. As accuracy approaches saturation, attention shifts to robustness, explainability, and deployment considerations.</figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Domain-Specific Challenges</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Despite the significant advancements in sentiment analysis, a number of domain-specific challenges continue to hinder the accuracy and robustness of models. These challenges are particularly pronounced in real-world applications such as e-commerce, movie reviews, and social media analytics, where language use is highly dynamic and context-dependent.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Sarcasm and Irony Detection</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">One of the most persistent challenges in sentiment classification is the accurate detection of sarcasm and irony. Sarcasm often conveys a sentiment opposite to the literal meaning of the text, making traditional sentiment classifiers fail when relying on lexical cues alone. For example, the statement <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.1">“Great, another three-hour delay!”</span> expresses negative sentiment despite containing the positive word “great.” Studies have shown that sarcasm significantly reduces classification accuracy when left unaddressed <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib77" title="">77</a>]</cite>. Recent work leverages contextual embeddings and attention mechanisms to better capture incongruity between expected and actual sentiment <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib78" title="">78</a>]</cite>. However, sarcasm detection remains challenging due to its dependence on pragmatic cues, cultural background, and shared speaker-listener knowledge <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib79" title="">79</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Domain Drift</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Domain drift refers to the deterioration in model performance when applying sentiment classifiers trained on one domain to another. For instance, models trained on product reviews may not generalize effectively to movie or restaurant reviews due to domain-specific vocabulary and context <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib21" title="">21</a>]</cite>. Domain adaptation techniques, including adversarial learning and pivot-based feature transfer, have been proposed to mitigate this issue <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib80" title="">80</a>]</cite>. Nevertheless, achieving robustness across domains is particularly challenging when labeled data in the target domain is scarce.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Temporal Drift</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">Language evolves over time, and with it, the expressions of sentiment also change. Temporal drift refers to the decline in model accuracy as a result of evolving slang, memes, and cultural references <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib81" title="">81</a>]</cite>. For example, terms like “sick” or “fire” may shift from negative to positive sentiment over time. Research has demonstrated that models trained on static data often underperform on more recent corpora <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib82" title="">82</a>]</cite>. Addressing this requires continual learning approaches that update model parameters in response to new linguistic trends, though such solutions often face the risk of catastrophic forgetting as first explained in the seminal 1989 paper by McCloskey et al. <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib83" title="">83</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Long-form Context</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">Another key challenge is sentiment analysis over long-form content such as blogs, articles, and movie scripts. Unlike short reviews or tweets, longer texts often contain mixed sentiments across different sections, requiring hierarchical models that capture local and global dependencies <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib36" title="">36</a>]</cite>. Models such as Hierarchical Attention Networks (HANs) have been shown to improve performance by aggregating sentence-level embeddings into document-level representations <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib36" title="">36</a>]</cite>. However, accurately modeling sentiment across long documents remains computationally intensive and often struggles with discourse-level phenomena such as contrastive statements and sentiment shifts within the same text <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib84" title="">84</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Cultural and Language Diversity</h3>
<div class="ltx_para" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1">Sentiment expression varies widely across languages and cultures. Words, idioms, and symbols that carry strong sentiment in one culture may not translate equivalently into another <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib85" title="">85</a>]</cite>. For instance, the interpretation of emojis varies significantly between cultural contexts <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib86" title="">86</a>]</cite>. Cross-lingual sentiment analysis approaches, including multilingual embeddings and machine translation-based methods, attempt to address this challenge <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib87" title="">87</a>]</cite>. However, even state-of-the-art multilingual transformers such as XLM-R struggle with low-resource languages and culture-specific idiomatic expressions <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib88" title="">88</a>]</cite>. This diversity highlights the need for culturally adaptive sentiment models that integrate both linguistic and socio-cultural knowledge.</p>
</div>
<div class="ltx_para" id="S5.SS5.p2">
<p class="ltx_p" id="S5.SS5.p2.1">In practice, these domain-specific challenges rarely occur in isolation. Sarcasm may intertwine with cultural idioms, temporal drift may alter how irony is expressed, and domain drift may exacerbate the failure to detect nuanced sentiment in multimodal reviews. For instance, a sarcastic movie review written in Hinglish (mixture of Hindi and English) on YouTube in 2023 might be nearly impossible to classify correctly using a model trained on English-only IMDB reviews from 2010. This interplay underscores the necessity for holistic approaches that combine sarcasm detection, domain adaptation, temporal robustness, and multilingual modeling into unified frameworks <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib87" title="">87</a>]</cite>. Addressing these intertwined challenges is a key direction for future research in sentiment analysis of movie reviews.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Future Research Directions</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">The field of sentiment analysis on movie reviews continues to evolve rapidly, with several emerging research challenges and opportunities shaping its future trajectory. In this section, we discuss key areas that demand attention, including improved benchmarks, few-shot and zero-shot learning, cross-lingual and cross-domain transfer, explainability, multimodal extensions, and fairness in sentiment analysis.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Improving Benchmarks</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">Although numerous benchmark datasets exist for sentiment analysis, they often fail to capture nuanced linguistic phenomena such as sarcasm, irony, or evolving cultural expressions <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib89" title="">89</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib77" title="">77</a>]</cite>. Benchmark datasets also rarely incorporate domain and temporal drift <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib6" title="">6</a>]</cite>, which limits the generalizability of models when exposed to changing review trends and movie discourse. Future benchmarks need to incorporate long-form contextual reviews <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib90" title="">90</a>]</cite> and scenarios that require deeper discourse understanding rather than sentence-level polarity predictions. Moreover, evaluation frameworks should integrate dimensions of explainability <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib91" title="">91</a>]</cite> and computational efficiency <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib92" title="">92</a>]</cite> to ensure real-world applicability in resource-constrained environments.</p>
</div>
<figure class="ltx_table" id="S6.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Proposed Requirements for Next-Generation Movie Review Sentiment Analysis Benchmarks.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S6.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.1.1.1" style="font-size:90%;">Challenge</span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" id="S6.T3.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.1.1.1.2.1">
<span class="ltx_p" id="S6.T3.1.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.1.2.1.1.1" style="font-size:90%;">Current Gap</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" id="S6.T3.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.1.1.1.3.1">
<span class="ltx_p" id="S6.T3.1.1.1.3.1.1"><span class="ltx_text ltx_font_bold" id="S6.T3.1.1.1.3.1.1.1" style="font-size:90%;">Proposed Evaluation</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T3.1.2.1.1"><span class="ltx_text" id="S6.T3.1.2.1.1.1" style="font-size:90%;">Sarcasm Detection</span></th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T3.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.1.2.1.2.1">
<span class="ltx_p" id="S6.T3.1.2.1.2.1.1"><span class="ltx_text" id="S6.T3.1.2.1.2.1.1.1" style="font-size:90%;">No systematic evaluation of sarcastic content</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T3.1.2.1.3">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.1.2.1.3.1">
<span class="ltx_p" id="S6.T3.1.2.1.3.1.1"><span class="ltx_text" id="S6.T3.1.2.1.3.1.1.1" style="font-size:90%;">Dedicated sarcasm subset with explanation requirements</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T3.1.3.2.1"><span class="ltx_text" id="S6.T3.1.3.2.1.1" style="font-size:90%;">Domain Drift</span></th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T3.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.1.3.2.2.1">
<span class="ltx_p" id="S6.T3.1.3.2.2.1.1"><span class="ltx_text" id="S6.T3.1.3.2.2.1.1.1" style="font-size:90%;">Single platform/source evaluation only</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T3.1.3.2.3">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.1.3.2.3.1">
<span class="ltx_p" id="S6.T3.1.3.2.3.1.1"><span class="ltx_text" id="S6.T3.1.3.2.3.1.1.1" style="font-size:90%;">Multi-platform evaluation with cross-domain generalization</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T3.1.4.3.1"><span class="ltx_text" id="S6.T3.1.4.3.1.1" style="font-size:90%;">Temporal Drift</span></th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T3.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.1.4.3.2.1">
<span class="ltx_p" id="S6.T3.1.4.3.2.1.1"><span class="ltx_text" id="S6.T3.1.4.3.2.1.1.1" style="font-size:90%;">Static datasets from specific time periods</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T3.1.4.3.3">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.1.4.3.3.1">
<span class="ltx_p" id="S6.T3.1.4.3.3.1.1"><span class="ltx_text" id="S6.T3.1.4.3.3.1.1.1" style="font-size:90%;">Temporal robustness testing across multiple time periods</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T3.1.5.4.1"><span class="ltx_text" id="S6.T3.1.5.4.1.1" style="font-size:90%;">Long-form Context</span></th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T3.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.1.5.4.2.1">
<span class="ltx_p" id="S6.T3.1.5.4.2.1.1"><span class="ltx_text" id="S6.T3.1.5.4.2.1.1.1" style="font-size:90%;">Truncation or chunking of longer reviews</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T3.1.5.4.3">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.1.5.4.3.1">
<span class="ltx_p" id="S6.T3.1.5.4.3.1.1"><span class="ltx_text" id="S6.T3.1.5.4.3.1.1.1" style="font-size:90%;">Full document processing evaluation metrics</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T3.1.6.5.1"><span class="ltx_text" id="S6.T3.1.6.5.1.1" style="font-size:90%;">Explainability</span></th>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T3.1.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.1.6.5.2.1">
<span class="ltx_p" id="S6.T3.1.6.5.2.1.1"><span class="ltx_text" id="S6.T3.1.6.5.2.1.1.1" style="font-size:90%;">No explanation assessment in standard metrics</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S6.T3.1.6.5.3">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.1.6.5.3.1">
<span class="ltx_p" id="S6.T3.1.6.5.3.1.1"><span class="ltx_text" id="S6.T3.1.6.5.3.1.1.1" style="font-size:90%;">Human-aligned explanation evaluation frameworks</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S6.T3.1.7.6.1"><span class="ltx_text" id="S6.T3.1.7.6.1.1" style="font-size:90%;">Resource Efficiency</span></th>
<td class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" id="S6.T3.1.7.6.2">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.1.7.6.2.1">
<span class="ltx_p" id="S6.T3.1.7.6.2.1.1"><span class="ltx_text" id="S6.T3.1.7.6.2.1.1.1" style="font-size:90%;">Accuracy optimization without cost consideration</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" id="S6.T3.1.7.6.3">
<span class="ltx_inline-block ltx_align_top" id="S6.T3.1.7.6.3.1">
<span class="ltx_p" id="S6.T3.1.7.6.3.1.1"><span class="ltx_text" id="S6.T3.1.7.6.3.1.1.1" style="font-size:90%;">Pareto frontier evaluation (accuracy vs. efficiency)</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Beyond Few-shot: Emerging LLM Challenges</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">While few-shot and zero-shot learning approaches have rapidly matured with the advent of large language models <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib72" title="">72</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib93" title="">93</a>]</cite>, new challenges have emerged. Current LLMs demonstrate strong performance on standard sentiment benchmarks but struggle with complex tasks requiring deeper understanding of specific sentiment phenomena <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib72" title="">72</a>]</cite>. Key open problems include: (1) reducing sensitivity to prompt wording, where minor phrasing changes can dramatically alter predictions <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib73" title="">73</a>]</cite>; (2) ensuring reproducibility across model versions and temperature settings; (3) developing efficient inference strategies for real-time applications given the computational cost of LLM deployment; and (4) addressing the gap between benchmark performance and real-world robustness on domain-specific jargon, evolving language, and adversarial inputs. Chain-of-thought prompting <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib75" title="">75</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib94" title="">94</a>]</cite> and prompt tuning approaches <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib95" title="">95</a>]</cite> offer promising directions for improving reasoning on implicit sentiment, but systematic evaluation frameworks for such approaches remain underdeveloped.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Cross-lingual and Cross-domain Transfer</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">Movie reviews are inherently multilingual and cross-cultural, yet much of the sentiment analysis research has focused on English-only datasets, like the seminal product review paper by Hu et al. <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib96" title="">96</a>]</cite>. Cross-lingual transfer learning using multilingual models such as XLM-R <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib88" title="">88</a>]</cite> or mBERT <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib22" title="">22</a>]</cite> offers promising directions for extending sentiment analysis systems to underrepresented languages. Furthermore, cross-domain transfer is essential for adapting models trained on professional critic reviews to informal user reviews or social media commentary <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib80" title="">80</a>]</cite>. Addressing these challenges requires robust domain adaptation and multilingual embedding techniques to ensure inclusivity and applicability across diverse populations.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>Explainable Sentiment Models</h3>
<div class="ltx_para" id="S6.SS4.p1">
<p class="ltx_p" id="S6.SS4.p1.1">Explainability remains a crucial area of research for sentiment analysis, especially in high-stakes applications such as content recommendation or censorship <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib91" title="">91</a>]</cite>. While attention-based mechanisms <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib24" title="">24</a>]</cite> and post-hoc interpretability tools <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib25" title="">25</a>]</cite> have been explored, they often provide incomplete or misleading explanations. Future research must focus on building inherently interpretable sentiment models that allow end-users to understand why a particular review is classified as positive, negative, or mixed. Such approaches would also enable the identification of biases in training data and the reduction of spurious correlations.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.5 </span>Multimodal and Conversational Sentiment</h3>
<div class="ltx_para" id="S6.SS5.p1">
<p class="ltx_p" id="S6.SS5.p1.1">As user-generated reviews increasingly take multimodal forms, incorporating audio, visual, and textual signals into sentiment analysis has become essential <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib97" title="">97</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib98" title="">98</a>]</cite>. Multimodal transformers such as MMBERT <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib99" title="">99</a>]</cite> represent promising directions for fusing diverse modalities. Additionally, conversational sentiment analysis, where reviewers discuss movies in interactive settings (e.g., podcasts, live streams, or interviews), demands models that can track sentiment across multiple speakers and temporal contexts <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib100" title="">100</a>]</cite>. These directions open new opportunities for richer, context-aware sentiment understanding.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.6 </span>Bias and Fairness in Sentiment Analysis Systems</h3>
<div class="ltx_para" id="S6.SS6.p1">
<p class="ltx_p" id="S6.SS6.p1.1">Bias and fairness present pressing ethical challenges in sentiment analysis <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib101" title="">101</a>]</cite>. Models trained on imbalanced review datasets may propagate stereotypes, amplify biases, or underperform on minority languages and cultural expressions. For instance, sentiment polarity markers may vary across dialects or sociolects <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib102" title="">102</a>]</cite>, leading to systematic misclassification. Future research must focus on fairness-aware learning frameworks, debiasing strategies, and representative datasets to ensure equitable performance across demographic and linguistic groups.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">The domain of movie reviews has consistently served as a proving ground for advances in sentiment analysis, pushing the boundaries of natural language understanding. Since the seminal work by Pang et al. <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib1" title="">1</a>]</cite>, movie review datasets have acted as the canonical benchmark for testing supervised and unsupervised methods, ranging from early bag-of-words and SVM classifiers <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib30" title="">30</a>]</cite> to modern pre-trained transformers, multimodal models, and large language models <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib103" title="">103</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib72" title="">72</a>]</cite>. The diversity and richness of movie reviews ranging from concise star ratings to nuanced long-form critiques have forced researchers to grapple with fundamental challenges such as sarcasm, irony, domain drift, temporal shifts, and cultural variation <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib89" title="">89</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib104" title="">104</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib105" title="">105</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib77" title="">77</a>]</cite>. These challenges not only stress-test algorithms but also advance the state of the art in natural language processing more broadly.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">The emergence of large language models has fundamentally transformed the sentiment analysis landscape. While LLMs demonstrate impressive zero-shot and few-shot capabilities that rival or exceed fine-tuned models on standard benchmarks <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib73" title="">73</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib74" title="">74</a>]</cite>, they also introduce new challenges around prompt sensitivity, reproducibility, and computational cost. Chain-of-thought prompting <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib75" title="">75</a>]</cite> and instruction-tuned models <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib76" title="">76</a>]</cite> represent promising directions for improving reasoning on nuanced sentiment tasks, particularly for implicit sentiment and aspect-based analysis. However, comprehensive evaluation frameworks like SentiEval <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib72" title="">72</a>]</cite> reveal that LLMs still lag behind specialized models on complex structured sentiment tasks, suggesting that the field has not yet reached a definitive solution.</p>
</div>
<div class="ltx_para" id="S7.p3">
<p class="ltx_p" id="S7.p3.1">Looking ahead, sentiment understanding in the context of creative media remains uniquely positioned to influence the trajectory of research. The development of fine-grained emotion datasets like GoEmotions <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib45" title="">45</a>]</cite> and unified social media benchmarks like TweetEval <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib46" title="">46</a>]</cite> reflects a broader recognition that binary sentiment classification is insufficient for real-world applications. Cross-lingual transfer learning <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib88" title="">88</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib106" title="">106</a>]</cite> promises to extend sentiment systems to under-resourced languages, while research in explainability <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib91" title="">91</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib107" title="">107</a>]</cite> and bias mitigation <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib102" title="">102</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib101" title="">101</a>]</cite> is critical for ensuring that sentiment models remain fair and interpretable.</p>
</div>
<div class="ltx_para" id="S7.p4">
<p class="ltx_p" id="S7.p4.1">Moreover, as multimodal and conversational sentiment analysis continues to evolve <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib97" title="">97</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib108" title="">108</a>]</cite>, the movie domain will provide a natural testbed for models that must integrate text, audio, and visual signals to capture audience reactions or understand narrative tone. Such advancements have implications that go beyond recommendation systems or review aggregation, influencing how we design human-centered AI capable of interpreting creativity, storytelling, and emotional resonance.</p>
</div>
<div class="ltx_para" id="S7.p5">
<p class="ltx_p" id="S7.p5.1">From an industry perspective, the methodological advances discussed in this survey have practical implications for recommendation systems, content moderation, market research, and audience analytics within the film and streaming ecosystem. Fine-grained sentiment analysis can support more personalized content recommendations, early detection of audience dissatisfaction, and improved understanding of viewer engagement beyond coarse rating scores. The ability of modern LLMs to provide explanatory reasoning for their predictions <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2601.07235v1#bib.bib75" title="">75</a>]</cite> opens new possibilities for interpretable audience analytics.</p>
</div>
<div class="ltx_para" id="S7.p6">
<p class="ltx_p" id="S7.p6.1">In summary, the movie domain has shaped sentiment analysis into a rich interdisciplinary field that continually challenges researchers to develop methods that are robust, context-aware, and adaptable across cultures and modalities. As sentiment understanding becomes more deeply integrated into creative industries, it will not only enhance our ability to process media at scale, but also provide insights into the ways humans communicate and experience emotion. Future research must balance methodological innovation with cultural sensitivity, explainability, and fairness, ensuring that sentiment analysis contributes meaningfully to our understanding of human expression in creative media.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Author Contributions</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">A.G., S.D. and K.T. have equally contributed to conceptualization, methodology, validation and writing original draft preparation. All authors have read and agreed to the published version of the manuscript.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Funding</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">This research received no external funding.</p>
</div>
</section>
<section class="ltx_section" id="Sx3">
<h2 class="ltx_title ltx_title_section">Data Availability</h2>
<div class="ltx_para" id="Sx3.p1">
<p class="ltx_p" id="Sx3.p1.1">Not applicable.</p>
</div>
</section>
<section class="ltx_section" id="Sx4">
<h2 class="ltx_title ltx_title_section">Conflicts of Interest</h2>
<div class="ltx_para" id="Sx4.p1">
<p class="ltx_p" id="Sx4.p1.1">The authors declare no conflicts of interest.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
B. Pang and L. Lee, “Thumbs up? sentiment classification using machine learning techniques,” in <span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing</span>, vol. 10, pp. 79–86, 2002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
L. Zhang, S. Wang, and B. Liu, “Deep learning for sentiment analysis: A survey,” <span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</span>, vol. 8, no. 4, p. e1253, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
A. Yadav and D. K. Vishwakarma, “Sentiment analysis using deep learning architectures: A review,” <span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">Artificial Intelligence Review</span>, vol. 53, no. 6, pp. 4335–4385, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
D. Khurana, A. Koli, K. Khatter, and S. Singh, “Natural language processing: State of the art, current trends and challenges,” <span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">Multimedia Tools and Applications</span>, vol. 82, no. 3, pp. 3713–3744, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
B. Pang and L. Lee, “Opinion mining and sentiment analysis,” <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">Foundations and Trends in Information Retrieval</span>, vol. 2, pp. 1–135, 01 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts, “Learning word vectors for sentiment analysis,” in <span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</span>, vol. 1, pp. 142–150, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
M. Wankhade, A. Rao, and C. Kulkarni, “A survey on sentiment analysis methods, applications, and challenges,” <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">Artificial Intelligence Review</span>, vol. 55, no. 7, pp. 5731–5780, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
P. K. Jain, R. Pamula, and G. Srivastava, “Systematic reviews in sentiment analysis: A tertiary study,” <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">Artificial Intelligence Review</span>, vol. 54, no. 7, pp. 4997–5053, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
N. Raghunathan and K. Saravanakumar, “Challenges and issues in sentiment analysis: A comprehensive survey,” <span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">IEEE Access</span>, vol. 11, pp. 69626–69642, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
M. Tetteh and M. Thushara, “Sentiment analysis tools for movie review evaluation - a survey,” in <span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">2023 7th International Conference on Intelligent Computing and Control Systems (ICICCS)</span>, pp. 816–823, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
M. M. Danyal, S. S. Khan, M. Khan, S. Ullah, F. Mehmood, and I. Ali, “Proposing sentiment analysis model based on bert and xlnet for movie reviews,” <span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">Multimedia Tools and Applications</span>, vol. 83, pp. 64315–64339, Jul 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
M. Birjali, M. Kasri, and A. Beni-Hssane, “A comprehensive survey on sentiment analysis: Approaches, challenges and trends,” <span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">Knowledge-Based Systems</span>, vol. 226, p. 107134, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
N. C. Dang, M. N. Moreno-García, and F. De la Prieta, “Challenges and future in deep learning for sentiment analysis: A comprehensive review and a proposed novel hybrid approach,” <span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">Artificial Intelligence Review</span>, vol. 57, pp. 1–54, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
T. T. Thet, J. C. Na, and C. S. G. Khoo, “Aspect-based sentiment analysis of movie reviews on discussion boards,” <span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">Journal of Information Science</span>, vol. 36, no. 6, pp. 823–848, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
O. G. Horsa and K. K. Tune, “Aspect‐based sentiment analysis for afaan oromoo movie reviews using machine learning techniques,” <span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">Applied Computational Intelligence and Soft Computing</span>, vol. 2023, p. 3462691, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
A. Musa, F. M. Adam, U. Ibrahim, and A. Y. Zandam, “Haubert: A transformer model for aspect-based sentiment analysis of hausa-language movie reviews,” <span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">Engineering Proceedings</span>, vol. 87, no. 1, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
M. Bordoloi and S. K. Biswas, “Sentiment analysis: A survey on design framework, applications and future scopes,” <span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">Artificial Intelligence Review</span>, vol. 56, pp. 12505–12560, Nov 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Y. Mao, Q. Liu, and Y. Zhang, “Sentiment analysis methods, applications, and challenges: A systematic literature review,” <span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">Journal of King Saud University - Computer and Information Sciences</span>, vol. 36, no. 4, p. 102048, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
S. Poria, E. Cambria, R. Bajpai, and A. Hussain, “A review of affective computing: From unimodal analysis to multimodal fusion,” <span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">Information Fusion</span>, vol. 37, pp. 98–125, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y. Ng, and C. Potts, “Recursive deep models for semantic compositionality over a sentiment treebank,” in <span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</span>, pp. 1631–1642, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
J. Blitzer, M. Dredze, and F. Pereira, “Biographies, bollywood, boom‐boxes and blenders: Domain adaptation for sentiment classification,” in <span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</span>, (Prague, Czech Republic), pp. 440–447, Association for Computational Linguistics, 2007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deep bidirectional transformers for language understanding,” in <span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</span>, vol. 1, pp. 4171–4186, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
T. Wilson, J. Wiebe, and P. Hoffmann, “Recognizing contextual polarity in phrase-level sentiment analysis,” in <span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing</span>, pp. 347–354, 2005.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in <span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">Advances in Neural Information Processing Systems</span>, vol. 30, pp. 5998–6008, Curran Associates, Inc., 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
M. T. Ribeiro, S. Singh, and C. Guestrin, “"why should i trust you?" explaining the predictions of any classifier,” in <span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</span>, pp. 1135–1144, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, “Lora: Low-rank adaptation of large language models,” in <span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">Proceedings of the International Conference on Learning Representations</span>, OpenReview.net, 2022.

</span>
<span class="ltx_bibblock">ICLR 2022 Poster.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “Qlora: Efficient finetuning of quantized llms,” <span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">Advances in Neural Information Processing Systems</span>, vol. 36, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Y. Wang, M. Huang, X. Zhu, and L. Zhao, “Attention-based lstm for aspect-level sentiment classification,” in <span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</span>, pp. 606–615, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
A. Rogers, O. Kovaleva, and A. Rumshisky, “A primer on neural network models for natural language processing,” <span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">Journal of Artificial Intelligence Research</span>, vol. 57, pp. 615–732, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
P. D. Turney, “Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews,” in <span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</span>, pp. 417–424, 2002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
B. Pang and L. Lee, “A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts,” in <span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics</span>, pp. 271–278, 2004.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
R. McDonald, K. Hannan, T. Neylon, M. Wells, and J. Reynar, “Structured models for fine‐to‐coarse sentiment analysis,” in <span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</span>, (Prague, Czech Republic), pp. 432–439, Association for Computational Linguistics, 2007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efficient estimation of word representations in vector space,” <span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:1301.3781</span>, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
J. Pennington, R. Socher, and C. D. Manning, “Glove: Global vectors for word representation,” in <span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</span>, pp. 1532–1543, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
S. Hochreiter and J. Schmidhuber, “Long short-term memory,” <span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">Neural Computation</span>, vol. 9, no. 8, pp. 1735–1780, 1997.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Z. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy, “Hierarchical attention networks for document classification,” in <span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</span>, pp. 1480–1489, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, “Roberta: A robustly optimized bert pretraining approach,” <span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv:1907.11692</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei, “Language models are few-shot learners,” <span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">Advances in Neural Information Processing Systems (NeurIPS)</span>, vol. 33, pp. 1877–1901, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, <span class="ltx_text ltx_font_italic" id="bib.bib39.1.1">et al.</span>, “Llama: Open and efficient foundation language models,” <span class="ltx_text ltx_font_italic" id="bib.bib39.2.2">arXiv preprint arXiv:2302.13971</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
S. Stilwell and D. Inkpen, “Explainable Prompt-based Approaches for Sentiment Analysis of Movie Reviews,” may 27 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
B. Liu, <span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">Sentiment Analysis and Opinion Mining</span>.

</span>
<span class="ltx_bibblock">Morgan &amp; Claypool Publishers, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Y. Kim, “Convolutional neural networks for sentence classification,” in <span class="ltx_text ltx_font_italic" id="bib.bib42.1.1">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</span>, pp. 1746–1751, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
A. Zadeh, M. Chen, S. Poria, E. Cambria, and L.-P. Morency, “Tensor fusion network for multimodal sentiment analysis,” in <span class="ltx_text ltx_font_italic" id="bib.bib43.1.1">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</span>, pp. 1103–1114, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
A. B. Zadeh, P. P. Liang, S. Poria, P. Vij, E. Cambria, and L.-P. Morency, “Multi-attention recurrent network for human communication comprehension,” in <span class="ltx_text ltx_font_italic" id="bib.bib44.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</span>, vol. 32, pp. 5642–5649, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
D. Demszky, D. Movshovitz-Attias, J. Ko, A. Cowen, G. Nemade, and S. Ravi, “GoEmotions: A dataset of fine-grained emotions,” in <span class="ltx_text ltx_font_italic" id="bib.bib45.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</span>, (Online), pp. 4040–4054, Association for Computational Linguistics, July 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
F. Barbieri, J. Camacho-Collados, L. Espinosa Anke, and L. Neves, “TweetEval: Unified benchmark and comparative evaluation for tweet classification,” in <span class="ltx_text ltx_font_italic" id="bib.bib46.1.1">Findings of the Association for Computational Linguistics: EMNLP 2020</span>, (Online), pp. 1644–1650, Association for Computational Linguistics, Nov. 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
S. Baccianella, A. Esuli, and F. Sebastiani, “Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining,” <span class="ltx_text ltx_font_italic" id="bib.bib47.1.1">Proceedings of the Seventh Conference on International Language Resources and Evaluation (LREC’10)</span>, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
A. Esuli and F. Sebastiani, “Sentiwordnet: A publicly available lexical resource for opinion mining,” in <span class="ltx_text ltx_font_italic" id="bib.bib48.1.1">Proceedings of LREC</span>, pp. 417–422, 2006.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
C. J. Hutto and E. Gilbert, “Vader: A parsimonious rule-based model for sentiment analysis of social media text,” in <span class="ltx_text ltx_font_italic" id="bib.bib49.1.1">Proceedings of the International AAAI Conference on Weblogs and Social Media</span>, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
J. W. Pennebaker, M. E. Francis, and R. J. Booth, <span class="ltx_text ltx_font_italic" id="bib.bib50.1.1">Linguistic Inquiry and Word Count (LIWC): LIWC2001</span>.

</span>
<span class="ltx_bibblock">Lawrence Erlbaum Associates, 2001.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Y. R. Tausczik and J. W. Pennebaker, “The psychological meaning of words: Liwc and computerized text analysis methods,” <span class="ltx_text ltx_font_italic" id="bib.bib51.1.1">Journal of Language and Social Psychology</span>, vol. 29, no. 1, pp. 24–54, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
D. Jurafsky and J. Martin, “Speech and language processing: an introduction to natural language processing, computational linguistics, and speech recognition,” 2000.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
A. Kennedy and D. Inkpen, “Sentiment classification of movie reviews using contextual valence shifters,” <span class="ltx_text ltx_font_italic" id="bib.bib53.1.1">Computational Intelligence</span>, vol. 22, no. 2, pp. 110–125, 2006.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
J. Otterbacher, “Inferring gender of movie reviewers: exploiting writing style, content and metadata,” in <span class="ltx_text ltx_font_italic" id="bib.bib54.1.1">Proceedings of the 19th ACM International Conference on Information and Knowledge Management</span>, CIKM ’10, (New York, NY, USA), p. 369–378, Association for Computing Machinery, 2010.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
T. Joachims, “Text categorization with support vector machines: Learning with many relevant features,” in <span class="ltx_text ltx_font_italic" id="bib.bib55.1.1">European Conference on Machine Learning</span>, pp. 137–142, Springer, 1998.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
C. Whitelaw, N. Garg, and S. Argamon, “Using appraisal groups for sentiment analysis,” in <span class="ltx_text ltx_font_italic" id="bib.bib56.1.1">Proceedings of the 14th ACM International Conference on Information and Knowledge Management</span>, pp. 625–631, 2005.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
A. McCallum and K. Nigam, “A comparison of event models for naive bayes text classification,” in <span class="ltx_text ltx_font_italic" id="bib.bib57.1.1">AAAI-98 Workshop on Learning for Text Categorization</span>, 1998.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
C. D. Manning, P. Raghavan, and H. Schütze, <span class="ltx_text ltx_font_italic" id="bib.bib58.1.1">Introduction to Information Retrieval</span>.

</span>
<span class="ltx_bibblock">Cambridge University Press, 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Y. Bengio, P. Simard, and P. Frasconi, “Learning long-term dependencies with gradient descent is difficult,” vol. 5, pp. 157–166, IEEE, 1994.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
K. S. Tai, R. Socher, and C. D. Manning, “Improved semantic representations from tree-structured long short-term memory networks,” in <span class="ltx_text ltx_font_italic" id="bib.bib60.1.1">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics</span>, pp. 1556–1566, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean, “Distributed representations of words and phrases and their compositionality,” in <span class="ltx_text ltx_font_italic" id="bib.bib61.1.1">Advances in Neural Information Processing Systems (NeurIPS)</span>, pp. 3111–3119, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly learning to align and translate,” in <span class="ltx_text ltx_font_italic" id="bib.bib62.1.1">International Conference on Learning Representations (ICLR)</span>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
C. Sun, X. Qiu, Y. Xu, and X. Huang, “How to fine-tune bert for text classification?,” in <span class="ltx_text ltx_font_italic" id="bib.bib63.1.1">China National Conference on Chinese Computational Linguistics</span>, pp. 194–206, Springer, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
V. Sanh, L. Debut, J. Chaumond, and T. Wolf, “Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter,” <span class="ltx_text ltx_font_italic" id="bib.bib64.1.1">arXiv preprint arXiv:1910.01108</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V. Le, “Xlnet: Generalized autoregressive pretraining for language understanding,” in <span class="ltx_text ltx_font_italic" id="bib.bib65.1.1">Advances in Neural Information Processing Systems</span> (H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, eds.), vol. 32, Curran Associates, Inc., 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
A. Bello, S.-C. Ng, and M.-F. Leung, “A bert framework to sentiment analysis of tweets,” <span class="ltx_text ltx_font_italic" id="bib.bib66.1.1">Sensors</span>, vol. 23, no. 1, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
H. Batra, N. S. Punn, S. K. Sonbhadra, and S. Agarwal, <span class="ltx_text ltx_font_italic" id="bib.bib67.1.1">BERT-Based Sentiment Analysis: A Software Engineering Perspective</span>, p. 138–148.

</span>
<span class="ltx_bibblock">Springer International Publishing, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
G. Penha and C. Hauff, “What does bert know about books, movies and music? probing bert for conversational recommendation,” in <span class="ltx_text ltx_font_italic" id="bib.bib68.1.1">Proceedings of the 14th ACM Conference on Recommender Systems</span>, RecSys ’20, (New York, NY, USA), p. 388–397, Association for Computing Machinery, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
G. Nkhata, S. Gauch, U. Anjum, and J. Zhan, “Fine-tuning BERT with bidirectional LSTM for fine-grained movie reviews sentiment analysis,” <span class="ltx_text ltx_font_italic" id="bib.bib69.1.1">arXiv preprint arXiv:2502.20682</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learning with a unified text-to-text transformer,” <span class="ltx_text ltx_font_italic" id="bib.bib70.1.1">Journal of Machine Learning Research</span>, vol. 21, no. 140, pp. 1–67, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
T. Gao, A. Fisch, and D. Chen, “Making pre-trained language models better few-shot learners,” in <span class="ltx_text ltx_font_italic" id="bib.bib71.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</span> (C. Zong, F. Xia, W. Li, and R. Navigli, eds.), (Online), pp. 3816–3830, Association for Computational Linguistics, Aug. 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
W. Zhang, Y. Deng, B. Liu, S. Pan, and L. Bing, “Sentiment analysis in the era of large language models: A reality check,” in <span class="ltx_text ltx_font_italic" id="bib.bib72.1.1">Findings of the Association for Computational Linguistics: NAACL 2024</span>, (Mexico City, Mexico), pp. 3881–3906, Association for Computational Linguistics, June 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
J. O. Krugmann and J. Hartmann, “Sentiment analysis in the age of generative ai,” <span class="ltx_text ltx_font_italic" id="bib.bib73.1.1">Customer Needs and Solutions</span>, vol. 11, p. 3, Mar. 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
S. Rathje, D.-M. Mirea, I. Sucholutsky, R. Marjieh, C. E. Robertson, and J. J. Van Bavel, “Gpt is an effective tool for multilingual psychological text analysis,” <span class="ltx_text ltx_font_italic" id="bib.bib74.1.1">Proceedings of the National Academy of Sciences</span>, vol. 121, Aug. 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
H. Fei, B. Li, Q. Liu, L. Bing, F. Li, and T.-S. Chua, “Reasoning implicit sentiment with chain-of-thought prompting,” in <span class="ltx_text ltx_font_italic" id="bib.bib75.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</span>, (Toronto, Canada), pp. 1171–1182, Association for Computational Linguistics, July 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
K. Scaria, H. Gupta, S. Goyal, S. Sawant, S. Mishra, and C. Baral, “Instructabsa: Instruction learning for aspect based sentiment analysis,” in <span class="ltx_text ltx_font_italic" id="bib.bib76.1.1">Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)</span>, (Mexico City, Mexico), pp. 720–736, Association for Computational Linguistics, June 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
A. Joshi, P. Bhattacharyya, and M. J. Carman, “Automatic sarcasm detection: A survey,” <span class="ltx_text ltx_font_italic" id="bib.bib77.1.1">ACM Comput. Surv.</span>, vol. 50, Sept. 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
A. Ghosh, G. Li, T. Veale, P. Rosso, E. Shutova, J. Barnden, and A. Reyes, “SemEval-2015 task 11: Sentiment analysis of figurative language in Twitter,” in <span class="ltx_text ltx_font_italic" id="bib.bib78.1.1">Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015)</span> (P. Nakov, T. Zesch, D. Cer, and D. Jurgens, eds.), (Denver, Colorado), pp. 470–478, Association for Computational Linguistics, June 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
R. J. Kreuz and R. M. Roberts, “The use of verbal irony: Cues and constraints,” <span class="ltx_text ltx_font_italic" id="bib.bib79.1.1">Metaphor and Symbol</span>, vol. 11, no. 1, pp. 23–38, 1996.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
X. Glorot, A. Bordes, and Y. Bengio, “Domain adaptation for large-scale sentiment classification: A deep learning approach,” in <span class="ltx_text ltx_font_italic" id="bib.bib80.1.1">Proceedings of the 28th International Conference on Machine Learning</span>, pp. 513–520, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
F. Diaz, B. Mitra, and N. Craswell, “Query expansion with locally-trained word embeddings,” in <span class="ltx_text ltx_font_italic" id="bib.bib81.1.1">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</span>, pp. 367–377, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
A. Lazaridou, A. Kuncoro, E. Gribovskaya, D. Agrawal, A. Liška, T. Terzi, M. Gimenez, C. d. M. d’Autume, T. Kocisky, S. Ruder, D. Yogatama, K. Cao, S. Young, and P. Blunsom, “Mind the gap: assessing temporal generalization in neural language models,” in <span class="ltx_text ltx_font_italic" id="bib.bib82.1.1">Proceedings of the 35th International Conference on Neural Information Processing Systems</span>, NIPS ’21, (Red Hook, NY, USA), Curran Associates Inc., 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
M. McCloskey and N. Cohen, “Catastrophic interference in connectionist networks: The sequential learning problem,” <span class="ltx_text ltx_font_italic" id="bib.bib83.1.1">Psychology of Learning and Motivation - Advances in Research and Theory</span>, vol. 24, pp. 109–165, Jan. 1989.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
P. Bhatia, Y. Ji, and J. Eisenstein, “Better document-level sentiment analysis from RST discourse parsing,” in <span class="ltx_text ltx_font_italic" id="bib.bib84.1.1">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</span> (L. Màrquez, C. Callison-Burch, and J. Su, eds.), (Lisbon, Portugal), pp. 2212–2218, Association for Computational Linguistics, Sept. 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
C. Banea, R. Mihalcea, J. Wiebe, and S. Hassan, “Multilingual subjectivity analysis using machine translation,” in <span class="ltx_text ltx_font_italic" id="bib.bib85.1.1">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</span> (M. Lapata and H. T. Ng, eds.), (Honolulu, Hawaii), pp. 127–135, Association for Computational Linguistics, Oct. 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
P. Kralj Novak, J. Smailović, B. Sluban, and I. Mozetič, “Sentiment of emojis,” <span class="ltx_text ltx_font_italic" id="bib.bib86.1.1">PLOS ONE</span>, vol. 10, pp. 1–22, 12 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
J. Barnes, R. Klinger, and S. Schulte im Walde, “Bilingual sentiment embeddings: Joint projection of sentiment across languages,” in <span class="ltx_text ltx_font_italic" id="bib.bib87.1.1">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</span> (I. Gurevych and Y. Miyao, eds.), (Melbourne, Australia), pp. 2483–2493, Association for Computational Linguistics, July 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F. Guzmán, E. Grave, M. Ott, L. Zettlemoyer, and V. Stoyanov, “Unsupervised cross-lingual representation learning at scale,” in <span class="ltx_text ltx_font_italic" id="bib.bib88.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</span> (D. Jurafsky, J. Chai, N. Schluter, and J. Tetreault, eds.), (Online), pp. 8440–8451, Association for Computational Linguistics, July 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
R. González-Ibáñez, S. Muresan, and N. Wacholder, “Identifying sarcasm in Twitter: A closer look,” in <span class="ltx_text ltx_font_italic" id="bib.bib89.1.1">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</span>, (Portland, Oregon, USA), pp. 581–586, Association for Computational Linguistics, June 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
I. Beltagy, M. E. Peters, and A. Cohan, “Longformer: The long-document transformer,” 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
M. Danilevsky, K. Qian, R. Aharonov, Y. Katsis, B. Kawas, and P. Sen, “A survey of the state of explainable AI for natural language processing,” in <span class="ltx_text ltx_font_italic" id="bib.bib91.1.1">Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing</span> (K.-F. Wong, K. Knight, and H. Wu, eds.), (Suzhou, China), pp. 447–459, Association for Computational Linguistics, Dec. 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
R. Schwartz, J. Dodge, N. A. Smith, and O. Etzioni, “Green ai,” in <span class="ltx_text ltx_font_italic" id="bib.bib92.1.1">Communications of the ACM</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
W. Yin, J. Hay, and D. Roth, “Benchmarking zero-shot text classification: Datasets, evaluation and entailment approach,” in <span class="ltx_text ltx_font_italic" id="bib.bib93.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</span> (K. Inui, J. Jiang, V. Ng, and X. Wan, eds.), (Hong Kong, China), pp. 3914–3923, Association for Computational Linguistics, Nov. 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, and D. Zhou, “Chain-of-thought prompting elicits reasoning in large language models,” in <span class="ltx_text ltx_font_italic" id="bib.bib94.1.1">Advances in Neural Information Processing Systems</span>, vol. 35, pp. 24824–24837, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
Y. Gu, X. Han, Z. Liu, and M. Huang, “PPT: Pre-trained prompt tuning for few-shot learning,” in <span class="ltx_text ltx_font_italic" id="bib.bib95.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</span> (S. Muresan, P. Nakov, and A. Villavicencio, eds.), (Dublin, Ireland), pp. 8410–8423, Association for Computational Linguistics, May 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
M. Hu and B. Liu, “Mining and summarizing customer reviews,” in <span class="ltx_text ltx_font_italic" id="bib.bib96.1.1">Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</span>, KDD ’04, (New York, NY, USA), p. 168–177, Association for Computing Machinery, 2004.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
A. Bagher Zadeh, P. P. Liang, S. Poria, E. Cambria, and L.-P. Morency, “Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph,” in <span class="ltx_text ltx_font_italic" id="bib.bib97.1.1">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</span> (I. Gurevych and Y. Miyao, eds.), (Melbourne, Australia), pp. 2236–2246, Association for Computational Linguistics, July 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
S. Lai, X. Hu, H. Xu, Z. Ren, and Z. Liu, “Multimodal sentiment analysis: A survey,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
Y. Khare, V. Bagal, M. Mathew, A. Devi, U. D. Priyakumar, and C. Jawahar, “Mmbert: Multimodal bert pretraining for improved medical vqa,” in <span class="ltx_text ltx_font_italic" id="bib.bib99.1.1">2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)</span>, pp. 1033–1036, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
S. Poria, N. Majumder, R. Mihalcea, and E. Hovy, “Emotion recognition in conversation: Research challenges, datasets, and recent advances,” <span class="ltx_text ltx_font_italic" id="bib.bib100.1.1">IEEE Access</span>, vol. 7, pp. 100943–100953, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
E. Sheng, K.-W. Chang, P. Natarajan, and N. Peng, “The woman worked as a babysitter: On biases in language generation,” in <span class="ltx_text ltx_font_italic" id="bib.bib101.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</span> (K. Inui, J. Jiang, V. Ng, and X. Wan, eds.), (Hong Kong, China), pp. 3407–3412, Association for Computational Linguistics, Nov. 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
S. Kiritchenko and S. Mohammad, “Examining gender and race bias in two hundred sentiment analysis systems,” in <span class="ltx_text ltx_font_italic" id="bib.bib102.1.1">Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics</span> (M. Nissim, J. Berant, and A. Lenci, eds.), (New Orleans, Louisiana), pp. 43–53, Association for Computational Linguistics, June 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, “Learning transferable visual models from natural language supervision,” in <span class="ltx_text ltx_font_italic" id="bib.bib103.1.1">Proceedings of the 38th International Conference on Machine Learning</span> (M. Meila and T. Zhang, eds.), vol. 139 of <span class="ltx_text ltx_font_italic" id="bib.bib103.2.2">Proceedings of Machine Learning Research</span>, pp. 8748–8763, PMLR, 18–24 Jul 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
J. He, L. Wang, L. Liu, J. Feng, and H. Wu, “Long document classification from local word glimpses via recurrent attention learning,” <span class="ltx_text ltx_font_italic" id="bib.bib104.1.1">IEEE Access</span>, vol. 7, pp. 40707–40718, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
S. Amir, B. C. Wallace, H. Lyu, P. Carvalho, and M. J. Silva, “Modelling context with user embeddings for sarcasm detection in social media,” in <span class="ltx_text ltx_font_italic" id="bib.bib105.1.1">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning</span> (S. Riezler and Y. Goldberg, eds.), (Berlin, Germany), pp. 167–177, Association for Computational Linguistics, Aug. 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
T. Ranasinghe and M. Zampieri, “Multilingual offensive language identification with cross-lingual embeddings,” in <span class="ltx_text ltx_font_italic" id="bib.bib106.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</span> (B. Webber, T. Cohn, Y. He, and Y. Liu, eds.), (Online), pp. 5838–5844, Association for Computational Linguistics, Nov. 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
N. F. Rajani, B. McCann, C. Xiong, and R. Socher, “Explain yourself! leveraging language models for commonsense reasoning,” in <span class="ltx_text ltx_font_italic" id="bib.bib107.1.1">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</span> (A. Korhonen, D. Traum, and L. Màrquez, eds.), (Florence, Italy), pp. 4932–4942, Association for Computational Linguistics, July 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
Y. Cai, X. Li, Y. Zhang, J. Li, F. Zhu, and L. Rao, “Multimodal sentiment analysis based on multi-layer feature fusion and multi-task learning,” <span class="ltx_text ltx_font_italic" id="bib.bib108.1.1">Scientific Reports</span>, vol. 15, p. 2126, Jan 2025.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Jan 12 06:04:53 2026 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>


<!-- ===== END HTML ===== -->

<!-- ===== ABS URL: https://arxiv.org/abs/2601.06874 | HTTP 200 ===== -->

<!DOCTYPE html>
<html lang="en">

<head>  <title>[2601.06874] MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="apple-touch-icon" sizes="180x180" href="/static/browse/0.3.4/images/icons/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/static/browse/0.3.4/images/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/static/browse/0.3.4/images/icons/favicon-16x16.png">
  <link rel="manifest" href="/static/browse/0.3.4/images/icons/site.webmanifest">
  <link rel="mask-icon" href="/static/browse/0.3.4/images/icons/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">
  <link rel="stylesheet" type="text/css" media="screen" href="/static/browse/0.3.4/css/arXiv.css?v=20241206" />
  <link rel="stylesheet" type="text/css" media="print" href="/static/browse/0.3.4/css/arXiv-print.css?v=20200611" />
  <link rel="stylesheet" type="text/css" media="screen" href="/static/browse/0.3.4/css/browse_search.css" />
  <script language="javascript" src="/static/browse/0.3.4/js/accordion.js" ></script>
  <script language="javascript" src="/static/browse/0.3.4/js/optin-modal.js?v=20250819"></script>
  
  <link rel="canonical" href="https://arxiv.org/abs/2601.06874"/>
  <meta name="description" content="Abstract page for arXiv paper 2601.06874: MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><meta property="og:type" content="website" />
<meta property="og:site_name" content="arXiv.org" />
<meta property="og:title" content="MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation" />
<meta property="og:url" content="https://arxiv.org/abs/2601.06874v1" />
<meta property="og:image" content="/static/browse/0.3.4/images/arxiv-logo-fb.png" />
<meta property="og:image:secure_url" content="/static/browse/0.3.4/images/arxiv-logo-fb.png" />
<meta property="og:image:width" content="1200" />
<meta property="og:image:height" content="700" />
<meta property="og:image:alt" content="arXiv logo"/>
<meta property="og:description" content="Most existing 3D referring expression segmentation (3DRES) methods rely on dense, high-quality point clouds, while real-world agents such as robots and mobile phones operate with only a few sparse RGB views and strict latency constraints. We introduce Multi-view 3D Referring Expression Segmentation (MV-3DRES), where the model must recover scene structure and segment the referred object directly from sparse multi-view images. Traditional two-stage pipelines, which first reconstruct a point cloud and then perform segmentation, often yield low-quality geometry, produce coarse or degraded target regions, and run slowly. We propose the Multimodal Visual Geometry Grounded Transformer (MVGGT), an efficient end-to-end framework that integrates language information into sparse-view geometric reasoning through a dual-branch design. Training in this setting exposes a critical optimization barrier, termed Foreground Gradient Dilution (FGD), where sparse 3D signals lead to weak supervision. To resolve this, we introduce Per-view No-target Suppression Optimization (PVSO), which provides stronger and more balanced gradients across views, enabling stable and efficient learning. To support consistent evaluation, we build MVRefer, a benchmark that defines standardized settings and metrics for MV-3DRES. Experiments show that MVGGT establishes the first strong baseline and achieves both high accuracy and fast inference, outperforming existing alternatives. Code and models are publicly available at https://mvggt.github.io."/>
<meta name="twitter:site" content="@arxiv"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="MVGGT: Multimodal Visual Geometry Grounded Transformer for..."/>
<meta name="twitter:description" content="Most existing 3D referring expression segmentation (3DRES) methods rely on dense, high-quality point clouds, while real-world agents such as robots and mobile phones operate with only a few sparse..."/>
<meta name="twitter:image" content="https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png"/>
<meta name="twitter:image:alt" content="arXiv logo"/>
  <link rel="stylesheet" media="screen" type="text/css" href="/static/browse/0.3.4/css/tooltip.css"/><link rel="stylesheet" media="screen" type="text/css" href="https://static.arxiv.org/js/bibex-dev/bibex.css?20200709"/>  <script src="/static/browse/0.3.4/js/mathjaxToggle.min.js" type="text/javascript"></script>  <script src="//code.jquery.com/jquery-latest.min.js" type="text/javascript"></script>
  <script src="//cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js" type="text/javascript"></script>
  <script src="//cdn.jsdelivr.net/npm/dompurify@2.3.5/dist/purify.min.js"></script>
  <script src="/static/browse/0.3.4/js/toggle-labs.js?20241022" type="text/javascript"></script>
  <script src="/static/browse/0.3.4/js/cite.js" type="text/javascript"></script><meta name="citation_title" content="MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation" /><meta name="citation_author" content="Wu, Changli" /><meta name="citation_author" content="Wang, Haodong" /><meta name="citation_author" content="Ji, Jiayi" /><meta name="citation_author" content="Yao, Yutian" /><meta name="citation_author" content="Du, Chunsai" /><meta name="citation_author" content="Kang, Jihua" /><meta name="citation_author" content="Fu, Yanwei" /><meta name="citation_author" content="Cao, Liujuan" /><meta name="citation_date" content="2026/01/11" /><meta name="citation_online_date" content="2026/01/11" /><meta name="citation_pdf_url" content="https://arxiv.org/pdf/2601.06874" /><meta name="citation_arxiv_id" content="2601.06874" /><meta name="citation_abstract" content="Most existing 3D referring expression segmentation (3DRES) methods rely on dense, high-quality point clouds, while real-world agents such as robots and mobile phones operate with only a few sparse RGB views and strict latency constraints. We introduce Multi-view 3D Referring Expression Segmentation (MV-3DRES), where the model must recover scene structure and segment the referred object directly from sparse multi-view images. Traditional two-stage pipelines, which first reconstruct a point cloud and then perform segmentation, often yield low-quality geometry, produce coarse or degraded target regions, and run slowly. We propose the Multimodal Visual Geometry Grounded Transformer (MVGGT), an efficient end-to-end framework that integrates language information into sparse-view geometric reasoning through a dual-branch design. Training in this setting exposes a critical optimization barrier, termed Foreground Gradient Dilution (FGD), where sparse 3D signals lead to weak supervision. To resolve this, we introduce Per-view No-target Suppression Optimization (PVSO), which provides stronger and more balanced gradients across views, enabling stable and efficient learning. To support consistent evaluation, we build MVRefer, a benchmark that defines standardized settings and metrics for MV-3DRES. Experiments show that MVGGT establishes the first strong baseline and achieves both high accuracy and fast inference, outperforming existing alternatives. Code and models are publicly available at https://mvggt.github.io." />
</head>

<body  class="with-cu-identity">
  
  
  <div class="flex-wrap-footer">
    <header>
      <a href="#content" class="is-sr-only">Skip to main content</a>
      <!-- start desktop header -->
      <div class="columns is-vcentered is-hidden-mobile" id="cu-identity">
        <div class="column" id="cu-logo">
          <a href="https://www.cornell.edu/"><img src="/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" /></a>
        </div><!-- /from April 7 at 1:00 AM to May 29 at 21:40 --><!-- /from May 2 at 1:00 AM to May 5 at 9:45 AM --><div class="column" id="support-ack">
          <span id="support-ack-url">We gratefully acknowledge support from the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors.</span>
          <a href="https://info.arxiv.org/about/donate.html" class="btn-header-donate">Donate</a>
        </div>
      </div>

      <div id="header" class="is-hidden-mobile">
<a aria-hidden="true" tabindex="-1" href="/IgnoreMe"></a>
  <div class="header-breadcrumbs is-hidden-mobile">
    <a href="/"><img src="/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg" alt="arxiv logo" style="height:40px;"/></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a> <span>&gt;</span> arXiv:2601.06874
  </div>

        <div class="columns is-vcentered is-mobile" style="justify-content: flex-end;">
        </div>

          <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
     </div><!-- /end desktop header -->

      <div class="mobile-header">
        <div class="columns is-mobile">
          <div class="column logo-arxiv"><a href="https://arxiv.org/"><img src="/static/browse/0.3.4/images/arxiv-logomark-small-white.svg" alt="arXiv logo" style="height:60px;" /></a></div>
          <div class="column logo-cornell"><a href="https://www.cornell.edu/">
            <picture>
              <source media="(min-width: 501px)"
                srcset="/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg  400w"
                sizes="400w" />
              <source srcset="/static/browse/0.3.4/images/icons/cu/cornell_seal_simple_black.svg 2x" />
              <img src="/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University Logo" />
            </picture>
          </a></div>
          <div class="column nav" id="toggle-container" role="menubar">
            <button class="toggle-control"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-white"><title>open search</title><path d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></button>
            <div class="mobile-toggle-block toggle-target">
              <form class="mobile-search-form" method="GET" action="https://arxiv.org/search">
                <div class="field has-addons">
                  <input class="input" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
                  <input type="hidden" name="source" value="header">
                  <input type="hidden" name="searchtype" value="all">
                  <button class="button">GO</button>
                </div>
              </form>
            </div>

            <button class="toggle-control"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-white" role="menu"><title>open navigation menu</title><path d="M16 132h416c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163 0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"/ ></svg></button>
            <div class="mobile-toggle-block toggle-target">
              <nav class="mobile-menu" aria-labelledby="mobilemenulabel">
                <h2 id="mobilemenulabel">quick links</h2>
                <ul>
                    <li><a href="https://arxiv.org/login">Login</a></li>
                    <li><a href="https://info.arxiv.org/help">Help Pages</a></li>
                    <li><a href="https://info.arxiv.org/about">About</a></li>
                </ul>
              </nav>
            </div>
          </div>
        </div>
      </div><!-- /end mobile-header -->
    </header>

    <main>
      <div id="content">
<!--
rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
         xmlns:dc="http://purl.org/dc/elements/1.1/"
         xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
    <rdf:Description
        rdf:about="/abs/2601.06874"
        dc:identifier="/abs/2601.06874"
        dc:title="MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"
        trackback:ping="/trackback/2601.06874" />
    </rdf:RDF>
--><div id="abs-outer">

  <div class="leftcolumn">
    <div class="subheader">
      <h1>Computer Science > Computer Vision and Pattern Recognition</h1>
    </div>

    <div class="header-breadcrumbs-mobile">
      <strong>arXiv:2601.06874</strong> (cs)
    </div>
<link rel="stylesheet" type="text/css" href="/static/base/1.0.1/css/abs.css">
<div id="content-inner">
  <div id="abs">
    <div class="dateline">
  [Submitted on 11 Jan 2026]</div>
    <h1 class="title mathjax"><span class="descriptor">Title:</span>MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation</h1>
    <div class="authors"><span class="descriptor">Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+C" rel="nofollow">Changli Wu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+H" rel="nofollow">Haodong Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ji,+J" rel="nofollow">Jiayi Ji</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yao,+Y" rel="nofollow">Yutian Yao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Du,+C" rel="nofollow">Chunsai Du</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kang,+J" rel="nofollow">Jihua Kang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fu,+Y" rel="nofollow">Yanwei Fu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cao,+L" rel="nofollow">Liujuan Cao</a></div>            <div id="download-button-info" hidden>View a PDF of the paper titled MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation, by Changli Wu and 7 other authors</div>
    <a class="mobile-submission-download" href="/pdf/2601.06874">View PDF</a>
    <a class="mobile-submission-download" href="https://arxiv.org/html/2601.06874v1">HTML (experimental)</a>



    <blockquote class="abstract mathjax">
            <span class="descriptor">Abstract:</span>Most existing 3D referring expression segmentation (3DRES) methods rely on dense, high-quality point clouds, while real-world agents such as robots and mobile phones operate with only a few sparse RGB views and strict latency constraints. We introduce Multi-view 3D Referring Expression Segmentation (MV-3DRES), where the model must recover scene structure and segment the referred object directly from sparse multi-view images. Traditional two-stage pipelines, which first reconstruct a point cloud and then perform segmentation, often yield low-quality geometry, produce coarse or degraded target regions, and run slowly. We propose the Multimodal Visual Geometry Grounded Transformer (MVGGT), an efficient end-to-end framework that integrates language information into sparse-view geometric reasoning through a dual-branch design. Training in this setting exposes a critical optimization barrier, termed Foreground Gradient Dilution (FGD), where sparse 3D signals lead to weak supervision. To resolve this, we introduce Per-view No-target Suppression Optimization (PVSO), which provides stronger and more balanced gradients across views, enabling stable and efficient learning. To support consistent evaluation, we build MVRefer, a benchmark that defines standardized settings and metrics for MV-3DRES. Experiments show that MVGGT establishes the first strong baseline and achieves both high accuracy and fast inference, outperforming existing alternatives. Code and models are publicly available at <a href="https://mvggt.github.io" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
    </blockquote>

    <!--CONTEXT-->
    <div class="metatable">
      <table summary="Additional metadata">        <tr>
          <td class="tablecell label">Comments:</td>
          <td class="tablecell comments mathjax">Project Website: <a href="https://mvggt.github.io" rel="external noopener nofollow" class="link-external link-https">this https URL</a></td>
        </tr>
<tr>
          <td class="tablecell label">Subjects:</td>
          <td class="tablecell subjects">
            <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span></td>
        </tr><tr>
          <td class="tablecell label">Cite as:</td>
          <td class="tablecell arxivid"><span class="arxivid"><a href="https://arxiv.org/abs/2601.06874">arXiv:2601.06874</a> [cs.CV]</span></td>
        </tr>
        <tr>
          <td class="tablecell label">&nbsp;</td>
          <td class="tablecell arxividv">(or <span class="arxivid">
              <a href="https://arxiv.org/abs/2601.06874v1">arXiv:2601.06874v1</a> [cs.CV]</span> for this version)
          </td>
        </tr>
        <tr>
          <td class="tablecell label">&nbsp;</td>
          <td class="tablecell arxivdoi">              <a href="https://doi.org/10.48550/arXiv.2601.06874" id="arxiv-doi-link">https://doi.org/10.48550/arXiv.2601.06874</a><div class="button-and-tooltip">
              <button class="more-info" aria-describedby="more-info-desc-1">
                <svg height="15" role="presentation" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z" class=""></path></svg>
                <span class="visually-hidden">Focus to learn more</span>
              </button>
              <!-- tooltip description -->
              <div role="tooltip" id="more-info-desc-1">
                <span class="left-corner"></span>                  arXiv-issued DOI via DataCite (pending registration)</div>
            </div>
          </td>
        </tr></table>
    </div>
  </div>
</div>
    <div class="submission-history">
      <h2>Submission history</h2> From: Changli Wu [<a href="/show-email/c10b7a8f/2601.06874" rel="nofollow">view email</a>]      <br/>    <strong>[v1]</strong>
        Sun, 11 Jan 2026 11:44:07 UTC (14,130 KB)<br/>
</div>
  </div>
  <!--end leftcolumn-->
<div class="extra-services">    <div class="full-text">
      <a name="other"></a>
      <span class="descriptor">Full-text links:</span>
      <h2>Access Paper:</h2>
      <ul>
  <div id="download-button-info" hidden>
View a PDF of the paper titled MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation, by Changli Wu and 7 other authors</div><li><a href="/pdf/2601.06874" aria-describedby="download-button-info" accesskey="f" class="abs-button download-pdf">View PDF</a></li><li><a href="https://arxiv.org/html/2601.06874v1" class="abs-button" id="latexml-download-link">HTML (experimental)</a></li><li><a href="/src/2601.06874" class="abs-button download-eprint">TeX Source
 </a></li></ul>
      <div class="abs-license"><a href="http://creativecommons.org/licenses/by/4.0/" title="Rights to this article" class="has_license">
          <img alt="license icon" role="presentation" src="https://arxiv.org/icons/licenses/by-4.0.png"/>
          <span>view license</span>
        </a></div>
    </div>
    <!--end full-text-->    <div class="browse">
    Current browse context: <div class="current">cs.CV</div>

  <div class="prevnext">
<span class="arrow">
      <a class="abs-button prev-url" href="/prevnext?id=2601.06874&amp;function=prev&amp;context=cs.CV"
         accesskey="p" title="previous in cs.CV (accesskey p)" rel="nofollow">&lt;&nbsp;prev</a>
    </span>
    <span class="is-hidden-mobile">&nbsp; | &nbsp;</span>    <span class="arrow">
      <a class="abs-button next-url" href="/prevnext?id=2601.06874&amp;function=next&amp;context=cs.CV" accesskey="n"
         title="next in cs.CV (accesskey n)"  rel="nofollow">next&nbsp;&gt;</a>
    </span><br/>
  </div><div class="list">
    <a class="abs-button abs-button-grey abs-button-small context-new" href="/list/cs.CV/new"  rel="nofollow">new</a>
    <span class="is-hidden-mobile"> | </span>
    <a class="abs-button abs-button-grey abs-button-small context-recent" href="/list/cs.CV/recent" rel="nofollow">recent</a>
    <span class="is-hidden-mobile"> | </span><a class="abs-button abs-button-grey abs-button-small context-id" href="/list/cs.CV/2026-01" rel="nofollow">2026-01</a>
  </div><div class="abs-switch-cat">
    Change to browse by:
    <div class="switch context-change">
        <a href="/abs/2601.06874?context=cs" rel="nofollow">cs</a><br class="is-hidden-mobile">
    </div>
  </div>

    </div>
      <div class="extra-ref-cite">
        <h3>References &amp; Citations</h3>
        <ul>
          <li><a  class="abs-button abs-button-small cite-ads" href="https://ui.adsabs.harvard.edu/abs/arXiv:2601.06874">NASA ADS</a></li><li><a  class="abs-button abs-button-small cite-google-scholar" href="https://scholar.google.com/scholar_lookup?arxiv_id=2601.06874" target="_blank" rel="noopener">Google Scholar</a></li>
          <li><a  class="abs-button abs-button-small cite-semantic-scholar" href="https://api.semanticscholar.org/arXiv:2601.06874" target="_blank" rel="noopener">Semantic Scholar</a></li>
        </ul>
        <div style="clear:both;"></div>
      </div>

<div class='extra-ref-cite'>
    <span id='bib-cite-trigger' class="bib-cite-button abs-button">export BibTeX citation</span>
    <span id='bib-cite-loading' hidden='true'>Loading...</span>
</div>

<div id='bib-cite-modal' class='bib-modal' hidden='true'>
    <div class='bib-modal-content'>
        <div class='bib-modal-title'>
            <h2>BibTeX formatted citation</h2>
            <span class='bib-modal-close' >&times;</span>
        </div>
        <div>
            <textarea id='bib-cite-target' class="bib-citation-content" aria-label="loading the citation">loading...</textarea>
        </div>
        <div>
            <span>Data provided by: </span>
            <a id='bib-cite-source-api'></a>
        </div>
    </div>
</div><div class="bookmarks">
  <div><h3>Bookmark</h3></div><a class="abs-button abs-button-grey abs-button-small" href="http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2601.06874&amp;description=MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"
     title="Bookmark on BibSonomy">
    <img src="/static/browse/0.3.4/images/icons/social/bibsonomy.png"
         alt="BibSonomy logo"/>
  </a>
  <a class="abs-button abs-button-grey abs-button-small" href="https://reddit.com/submit?url=https://arxiv.org/abs/2601.06874&amp;title=MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"
     title="Bookmark on Reddit">
    <img src="/static/browse/0.3.4/images/icons/social/reddit.png"
         alt="Reddit logo"/>
  </a>
</div>  </div>
  <!--end extra-services-->
<!-- LABS AREA -->
<div id="labstabs">
  <div class="labstabs"><input type="radio" name="tabs" id="tabone"checked="checked">
    <label for="tabone">Bibliographic Tools</label>
    <div class="tab labs-display-bib">
      <h1>Bibliographic and Citation Tools</h1>
      <div class="toggle">
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input id="bibex-toggle" type="checkbox" class="lab-toggle"
                     data-script-url="/static/browse/0.3.4/bibex/bibex.js?20241202">
              <span class="slider"></span>
              <span class="is-sr-only">Bibliographic Explorer Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-bibex">Bibliographic Explorer</span> <em>(<a href="https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer">What is the Explorer?</a>)</em>
          </div>
        </div>
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="connectedpapers-toggle"
                type="checkbox"
                class="lab-toggle"
                data-script-url="/static/browse/0.3.4/js/connectedpapers.js"
                aria-labelledby="label-for-connected-papers">
              <span class="slider"></span>
              <span class="is-sr-only">Connected Papers Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-connected-papers">Connected Papers</span> <em>(<a href="https://www.connectedpapers.com/about" target="_blank">What is Connected Papers?</a>)</em>
          </div>
        </div><div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="litmaps-toggle"
                type="checkbox"
                class="lab-toggle"
                data-script-url="/static/browse/0.3.4/js/litmaps.js?20210617"
                aria-labelledby="label-for-litmaps">
              <span class="slider"></span>
              <span class="is-sr-only">Litmaps Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-litmaps">Litmaps</span> <em>(<a href="https://www.litmaps.co/" target="_blank">What is Litmaps?</a>)</em>
          </div>
        </div>
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="scite-toggle"
                type="checkbox"
                class="lab-toggle"
                data-script-url="/static/browse/0.3.4/js/scite.js?20210617"
                aria-labelledby="label-for-scite">
              <span class="slider"></span>
              <span class="is-sr-only">scite.ai Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-scite">scite Smart Citations</span> <em>(<a href="https://www.scite.ai/" target="_blank">What are Smart Citations?</a>)</em>
          </div>
        </div>
      </div>
        <div class="labs-content-placeholder labs-display" style="display: none;"></div>
        <div style="min-height: 15px" id="connectedpapers-output"></div>
        <div style="min-height: 15px" id="litmaps-open-in"></div>
        <div style="min-height: 15px" id="scite-open-in"></div>
    </div>


    <input type="radio" name="tabs" id="tabtwo">
    <label for="tabtwo">Code, Data, Media</label>
    <div class="tab">
      <h1>Code, Data and Media Associated with this Article</h1>
      <div class="toggle">
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="alphaxiv-toggle"
                data-script-url="/static/browse/0.3.4/js/alphaxiv.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-alphaxiv">
              <span class="slider"></span>
              <span class="is-sr-only">alphaXiv Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-alphaxiv">alphaXiv</span> <em>(<a href="https://alphaxiv.org/" target="_blank">What is alphaXiv?</a>)</em>
          </div>
        </div>

        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input        
                id="catalyzex-toggle"
                data-script-url="/static/browse/0.3.4/js/catalyzex.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-cx">
              <span class="slider"></span>
              <span class="is-sr-only">Links to Code Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-cx">CatalyzeX Code Finder for Papers</span> <em>(<a href="https://www.catalyzex.com" target="_blank">What is CatalyzeX?</a>)</em>
          </div>
        </div>

        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="dagshub-toggle"
                data-script-url="/static/browse/0.3.4/js/dagshub.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-dagshub">
              <span class="slider"></span>
              <span class="is-sr-only">DagsHub Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-dagshub">DagsHub</span> <em>(<a href="https://dagshub.com/" target="_blank">What is DagsHub?</a>)</em>
          </div>
        </div>
  
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="gotitpub-toggle"
                data-script-url="/static/browse/0.3.4/js/gotitpub.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-gotitpub">
              <span class="slider"></span>
              <span class="is-sr-only">GotitPub Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-gotitpub">Gotit.pub</span> <em>(<a href="http://gotit.pub/faq" target="_blank">What is GotitPub?</a>)</em>
          </div>
        </div>

        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="huggingface-toggle"
                data-script-url="/static/browse/0.3.4/js/huggingface.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-huggingface">
              <span class="slider"></span>
              <span class="is-sr-only">Huggingface Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-huggingface">Hugging Face</span> <em>(<a href="https://huggingface.co/huggingface" target="_blank">What is Huggingface?</a>)</em>
          </div>
        </div>

        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="paperwithcode-toggle"
                data-script-url="/static/browse/0.3.4/js/paperswithcode.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-pwc">
              <span class="slider"></span>
              <span class="is-sr-only">Links to Code Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-pwc">Papers with Code</span> <em>(<a href="https://paperswithcode.com/" target="_blank">What is Papers with Code?</a>)</em>
          </div>
        </div>


        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="sciencecast-toggle"
                data-script-url="/static/browse/0.3.4/js/sciencecast.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-sciencecast">
              <span class="slider"></span>
              <span class="is-sr-only">ScienceCast Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-sciencecast">ScienceCast</span> <em>(<a href="https://sciencecast.org/welcome" target="_blank">What is ScienceCast?</a>)</em>
          </div>
        </div>
      </div>

      <div id="alphaxiv-output" style="display:none"></div>
      <div id="catalyzex-output" style="display:none"></div>
      <div id="dagshub-output" style="display:none"></div>
      <div id="gotitpub-output" style="display:none"></div>
      <div id="pwc-output" style="display:none"></div>
      <div id="pwc-data-output" style="display:none"></div>
      <div id="sciencecast-output" style="display:none"></div>
      <div id="huggingface-output" style="display:none"></div>
    </div>


      <input type="radio" name="tabs" id="labstabs-demos-input">
      <label for="labstabs-demos-input" id="labstabs-demos-label">Demos</label>
      <div class="tab">
        <h1>Demos</h1>
        <div class="toggle">
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input
                  id="replicate-toggle"
                  data-script-url="/static/browse/0.3.4/js/replicate.js"
                  type="checkbox" class="lab-toggle" aria-labelledby="label-for-replicate">
                <span class="slider"></span>
                <span class="is-sr-only">Replicate Toggle</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-replicate">Replicate</span> <em>(<a href="https://replicate.com/docs/arxiv/about" target="_blank">What is Replicate?</a>)</em>
            </div>
          </div>
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input
                  id="spaces-toggle"
                  data-script-url="/static/browse/0.3.4/js/spaces.js"
                  type="checkbox" class="lab-toggle" aria-labelledby="label-for-spaces">
                <span class="slider"></span>
                <span class="is-sr-only">Spaces Toggle</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-spaces">Hugging Face Spaces</span> <em>(<a href="https://huggingface.co/docs/hub/spaces" target="_blank">What is Spaces?</a>)</em>
            </div>
          </div>
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input
                  id="txyz-toggle"
                  data-script-url="/static/browse/0.3.4/js/txyz.js"
                  type="checkbox" class="lab-toggle" aria-labelledby="label-for-txyz">
                <span class="slider"></span>
                <span class="is-sr-only">Spaces Toggle</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-txyz">TXYZ.AI</span> <em>(<a href="https://txyz.ai" target="_blank">What is TXYZ.AI?</a>)</em>
            </div>
          </div>
        </div>
        <div id="replicate-output"></div>
        <div id="spaces-output"></div>
        <div id="txyz-output"></div>
      </div>
      <input type="radio" name="tabs" id="tabfour">
      <label for="tabfour">Related Papers</label>
      <div class="tab">
        <h1>Recommenders and Search Tools</h1>
        <div class="toggle">
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input id="influenceflower-toggle"
                data-script-url="/static/browse/0.3.4/js/influenceflower.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-influenceflower">
                <span class="slider"></span>
                <span class="is-sr-only">Link to Influence Flower</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-influenceflower">Influence Flower</span> <em>(<a href="https://influencemap.cmlab.dev/" target="_blank">What are Influence Flowers?</a>)</em>
            </div>
          </div>
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input id="core-recommender-toggle" type="checkbox" class="lab-toggle" aria-labelledby="label-for-core">
                <span class="slider"></span>
                <span class="is-sr-only">Core recommender toggle</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-core">CORE Recommender</span> <em>(<a href="https://core.ac.uk/services/recommender">What is CORE?</a>)</em>
            </div>
          </div></div>
        <div id="influenceflower-output"></div>
        <div id="influenceflower-output-graph" style="display:none">
          <ul class="flower-tabs">
            <li class="active"><a class="btn tab-btn" onclick="openTab(event, 'tab-author')">Author</a></li>
            <li><a class="btn tab-btn" onclick="openTab(event, 'tab-venue')">Venue</a></li>
            <li><a class="btn tab-btn" onclick="openTab(event, 'tab-inst')">Institution</a></li>
            <li><a class="btn tab-btn" onclick="openTab(event, 'tab-topic')">Topic</a></li>
          </ul>
          <div class="flower-tab-content">
            <div class="tab-flower active" id="tab-author"><svg id="flower-graph-author"></svg></div>
            <div class="tab-flower" id="tab-venue"><svg id="flower-graph-venue"></svg></div>
            <div class="tab-flower" id="tab-inst"><svg id="flower-graph-inst"></svg></div>
            <div class="tab-flower" id="tab-topic"><svg id="flower-graph-topic"></svg></div>
          </div>
        </div>
        <div id="coreRecommenderOutput"></div>
        <div id="iarxivOutput"></div>
      </div>

      <input type="radio" name="tabs" id="tabfive">
      <label for="tabfive">
        About arXivLabs
      </label>
      <div class="tab">
        <div class="columns">
          <div class="column">
            <h1>arXivLabs: experimental projects with community collaborators</h1>
            <p>arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.</p>
            <p>Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.</p>
            <p>Have an idea for a project that will add value for arXiv's community? <a href="https://info.arxiv.org/labs/index.html"><strong>Learn more about arXivLabs</strong></a>.</p>
          </div>
          <div class="column is-narrow is-full-mobile">
            <p class="icon-labs"><svg xmlns="http://www.w3.org/2000/svg" role="presentation" viewBox="0 0 635.572 811"><path d="M175.6 676v27h-27v-27zm-54 27v27h27v-27zm-27 27v27h27v-27zm396-54v27h-27v-27zm0 27v27h27v-27zm27 27v27h27v-27zm-27-414h27v27h-27zm27 0h27v-27h-27zm27-27h27v-27h-27zm-396 45h-27v-27h27zm-27-54h-27v27h27zm-27-27h-27v27h27z"/><path d="M94.6 730v27h-27v-27zm477 0v27h-27v-27zm-27-495h27v27h-27zm-450 18h-27v-27h27zm477 9h27v27h-27zm-54 495h27v27h-27zm-423 0h27v27h-27zm-54-504h27v27h-27z" fill="#666"/><path d="M67.6 730v27h-27v-27zm54 54v27h-27v-27zm0-108v27h27v-27zm-27 27v27h27v-27zm-81 0v27h27v-27zm585 27v27h-27v-27zm-108-54v27h27v-27zm27 27v27h27v-27zm81 0v27h27v-27zm-54-495h27v27h-27zm-54 108h27v-27h-27zm27-27h27v-27h-27zm0-81h27v-27h-27zm-423 18h-27v-27h27zm54 54h-27v27h27zm-27-27h-27v27h27zm0-81h-27v27h27zm423 612v27h-27v-27zm81-522v27h-27v-27zm-585-9v27h-27v-27z" fill="#999"/><path d="M94.6 784v27h-27v-27zm-27-27v27h27v-27zm-27-54v27h27v-27zm27 0v27h27v-27zm0-27v27h27v-27zm27 0v27h27v-27zm0-27v27h27v-27zm27 0v27h27v-27zm-108 81v27h27v-27zm558 54v27h-27v-27zm-27-27v27h27v-27zm27-54v27h27v-27zm-27 0v27h27v-27zm0-27v27h27v-27zm-27 0v27h27v-27zm0-27v27h27v-27zm-27 0v27h27v-27zm108 81v27h27v-27zm0-495h27v27h-27zm-27 27h27v-27h-27zm-54-27h27v-27h-27zm0 27h27v-27h-27zm-27 0h27v-27h-27zm0 27h27v-27h-27zm-27 0h27v-27h-27zm0 27h27v-27h-27zm81-108h27v-27h-27zm-504 45h-27v-27h27zm27-27h-27v27h27zm54-27h-27v27h27zm0 27h-27v27h27zm27 0h-27v27h27zm0 27h-27v27h27zm27 0h-27v27h27zm0 27h-27v27h27zm-81-108h-27v27h27z" fill="#ccc"/><path d="M598.6 665.1H41.5C-76.5 667 176 280.2 176 280.2h53a46.5 46.5 0 0162.8-56.3 29.2 29.2 0 1128.5 35.9h-1a46.5 46.5 0 01-1.5 20.3l142.5-.1s255.3 387 138.3 385.1zM291 181a29.3 29.3 0 10-29.2-29.3A29.3 29.3 0 00291 181zm65.4-66.8a22.4 22.4 0 10-22.5-22.4 22.4 22.4 0 0022.5 22.4z" fill="#fc0"/><path d="M245.5 172V10h153v162s324 495 198 495h-558c-126 0 207-495 207-495zm126 54h56m-13 72h56m-9 72h56m-20 72h56m-22 72h56m-29 72h56m-457-45c20.8 41.7 87.3 81 160.7 81 72.1 0 142.1-38.2 163.4-81" fill="none" stroke="#000" stroke-miterlimit="10" stroke-width="20"/><path d="M273.3 421.7c0 31-9.8 56.3-21.9 56.3s-21.8-25.2-21.8-56.3 9.8-56.3 21.8-56.3 21.9 25.2 21.9 56.3zm114.4-56.3c-12 0-21.8 25.2-21.8 56.3s9.7 56.3 21.8 56.3 21.9-25.2 21.9-56.3-9.8-56.3-21.9-56.3zM150.1 526.6c-18.2 6.7-27.5 22.9-23.2 30.2s14.8-5.5 33-12.2 37.4-4.9 33-12.2-24.5-12.6-42.8-5.8zm296 5.8c-4.2 7.3 14.9 5.5 33.1 12.2s28.7 19.5 33 12.2-5-23.5-23.2-30.2-38.5-1.5-42.8 5.8z"/></svg></p>
          </div>
        </div>
      </div>

    </div>
</div>
<!-- END LABS AREA -->
  <div class="endorsers">
    <a href="/auth/show-endorsers/2601.06874" class="endorser-who" rel="nofollow">Which authors of this paper are endorsers?</a> |
    <a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="https://info.arxiv.org/help/mathjax.html">What is MathJax?</a>)
    <span class="help" style="font-style: normal; float: right; margin-top: 0; margin-right: 1em;"></span>
  </div>
  <script type="text/javascript" language="javascript">mathjaxToggle();</script>
</div>
      </div>
    </main>

    <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://info.arxiv.org/about">About</a></li>
                <li><a href="https://info.arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
                <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
  </div>

  <script src="/static/base/1.0.1/js/member_acknowledgement.js"></script>

</body>

</html>

<!-- ===== END ABS ===== -->

<!-- ===== HTML URL: https://arxiv.org/html/2601.06874v1 | HTTP 200 ===== -->

<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation</title>
<!--Generated on Sun Jan 11 11:32:56 2026 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="/static/browse/0.3.4/css/arxiv-html-papers-20250916.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2601.06874v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S1" title="In MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2" title="In MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS1" title="In 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Traditional 3D Referring Segmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS2" title="In 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Multi-View Feed-forward Reconstruction</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S3" title="In MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>MV-3DRES Task and MVRefer Benchmark</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S3.SS1" title="In 3 MV-3DRES Task and MVRefer Benchmark ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Task Formulation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S3.SS2" title="In 3 MV-3DRES Task and MVRefer Benchmark ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>The MVRefer Benchmark</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S3.SS2.SSS1" title="In 3.2 The MVRefer Benchmark ‣ 3 MV-3DRES Task and MVRefer Benchmark ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Benchmark Setting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S3.SS2.SSS2" title="In 3.2 The MVRefer Benchmark ‣ 3 MV-3DRES Task and MVRefer Benchmark ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Evaluation Metrics and Splits</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S3.SS2.SSS2.Px1" title="In 3.2.2 Evaluation Metrics and Splits ‣ 3.2 The MVRefer Benchmark ‣ 3 MV-3DRES Task and MVRefer Benchmark ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_title">Traditional 3D Metric.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S3.SS2.SSS2.Px2" title="In 3.2.2 Evaluation Metrics and Splits ‣ 3.2 The MVRefer Benchmark ‣ 3 MV-3DRES Task and MVRefer Benchmark ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_title">Multi-view Diagnostic Metrics.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S3.SS2.SSS2.Px3" title="In 3.2.2 Evaluation Metrics and Splits ‣ 3.2 The MVRefer Benchmark ‣ 3 MV-3DRES Task and MVRefer Benchmark ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_title">Difficulty Splits.</span></a></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S4" title="In MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S4.SS1" title="In 4 Method ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>The proposed MVGGT</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S4.SS1.SSS0.Px1" title="In 4.1 The proposed MVGGT ‣ 4 Method ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_title">Inputs and Encoders.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S4.SS1.SSS0.Px2" title="In 4.1 The proposed MVGGT ‣ 4 Method ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_title">Frozen Reconstruction Branch.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S4.SS1.SSS0.Px3" title="In 4.1 The proposed MVGGT ‣ 4 Method ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_title">Trainable Multimodal Branch.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S4.SS1.SSS0.Px4" title="In 4.1 The proposed MVGGT ‣ 4 Method ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_title">Decoders and Outputs.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S4.SS2" title="In 4 Method ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Foreground Gradient Dilution</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S4.SS3" title="In 4 Method ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Per-view No-Target Suppression Optimization</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S4.SS3.SSS0.Px1" title="In 4.3 Per-view No-Target Suppression Optimization ‣ 4 Method ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_title">Positive-aware Sampling.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S4.SS3.SSS0.Px2" title="In 4.3 Per-view No-Target Suppression Optimization ‣ 4 Method ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_title">2D Gradient Concentration.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S4.SS3.SSS0.Px3" title="In 4.3 Per-view No-Target Suppression Optimization ‣ 4 Method ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_title">Suppression of No-Target Views.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S4.SS3.SSS0.Px4" title="In 4.3 Per-view No-Target Suppression Optimization ‣ 4 Method ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_title">Joint Objective.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S5" title="In MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S5.SS1" title="In 5 Experiments ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Implementation Details and Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S5.SS1.SSS0.Px1" title="In 5.1 Implementation Details and Setup ‣ 5 Experiments ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_title">MVGGT Configuration.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S5.SS1.SSS0.Px2" title="In 5.1 Implementation Details and Setup ‣ 5 Experiments ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_title">Dataset and Metrics.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S5.SS2" title="In 5 Experiments ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Main Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S5.SS3" title="In 5 Experiments ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Ablation Studies</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S5.SS3.SSS0.Px1" title="In 5.3 Ablation Studies ‣ 5 Experiments ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_title">Impact of Core Components.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S5.SS3.SSS0.Px2" title="In 5.3 Ablation Studies ‣ 5 Experiments ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_title">PVSO Component Analysis.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S5.SS3.SSS0.Px3" title="In 5.3 Ablation Studies ‣ 5 Experiments ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_title">MVGGT Fusion Architecture.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S5.SS3.SSS0.Px4" title="In 5.3 Ablation Studies ‣ 5 Experiments ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_title">Multimodal Branch Depth.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S5.SS4" title="In 5 Experiments ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Qualitative Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S6" title="In MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_pruned_first">
<h1 class="ltx_title ltx_title_document">MVGGT: Multimodal Visual Geometry Grounded Transformer for 
<br class="ltx_break"/>Multiview 3D Referring Expression Segmentation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Changli Wu<sup class="ltx_sup" id="id16.14.id1"><span class="ltx_text ltx_font_italic" id="id16.14.id1.1">1,2,†</span></sup>,
Haodong Wang<sup class="ltx_sup" id="id17.15.id2"><span class="ltx_text ltx_font_italic" id="id17.15.id2.1">1,†</span></sup>,
Jiayi Ji<sup class="ltx_sup" id="id18.16.id3"><span class="ltx_text ltx_font_italic" id="id18.16.id3.1">1</span></sup>,
Yutian Yao<sup class="ltx_sup" id="id19.17.id4"><span class="ltx_text ltx_font_italic" id="id19.17.id4.1">5</span></sup>, 
<br class="ltx_break"/>Chunsai Du<sup class="ltx_sup" id="id20.18.id5"><span class="ltx_text ltx_font_italic" id="id20.18.id5.1">4</span></sup>,
Jihua Kang<sup class="ltx_sup" id="id21.19.id6"><span class="ltx_text ltx_font_italic" id="id21.19.id6.1">4</span></sup>,
Yanwei Fu<sup class="ltx_sup" id="id22.20.id7"><span class="ltx_text ltx_font_italic" id="id22.20.id7.1">3,2</span></sup>,
Liujuan Cao<sup class="ltx_sup" id="id23.21.id8"><span class="ltx_text ltx_font_italic" id="id23.21.id8.1">1,∗</span></sup>
<br class="ltx_break"/> <sup class="ltx_sup" id="id24.22.id9"><span class="ltx_text ltx_font_italic" id="id24.22.id9.1">1</span></sup>Xiamen University  <sup class="ltx_sup" id="id25.23.id10"><span class="ltx_text ltx_font_italic" id="id25.23.id10.1">2</span></sup>Shanghai Innovation Institute  <sup class="ltx_sup" id="id26.24.id11"><span class="ltx_text ltx_font_italic" id="id26.24.id11.1">3</span></sup>Fudan University 
<br class="ltx_break"/><sup class="ltx_sup" id="id27.25.id12"><span class="ltx_text ltx_font_italic" id="id27.25.id12.1">4</span></sup>ByteDance  <sup class="ltx_sup" id="id28.26.id13"><span class="ltx_text ltx_font_italic" id="id28.26.id13.1">5</span></sup>Tianjin University of Science and Technology

<br class="ltx_break"/> <span class="ltx_text ltx_font_typewriter" id="id29.27.id14" style="font-size:90%;">{wuchangli, wahadon}@stu.xmu.edu.cn, jjyxmu@gmail.com, 22201316@mail.tust.edu.cn,</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id30.28.id15" style="font-size:90%;">{duchunsai, kangjihua}@bytedance.com, yanweifu@fudan.edu.cn, caoliujuan@xmu.edu.cn</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id31.id1">Most existing 3D referring expression segmentation (3DRES) methods rely on dense, high-quality point clouds, while real-world agents such as robots and mobile phones operate with only a few sparse RGB views and strict latency constraints. We introduce Multi-view 3D Referring Expression Segmentation (MV-3DRES), where the model must recover scene structure and segment the referred object directly from sparse multi-view images. Traditional two-stage pipelines, which first reconstruct a point cloud and then perform segmentation, often yield low-quality geometry, produce coarse or degraded target regions, and run slowly. We propose the Multimodal Visual Geometry Grounded Transformer (MVGGT), an efficient end-to-end framework that integrates language information into sparse-view geometric reasoning through a dual-branch design. Training in this setting exposes a critical optimization barrier, termed Foreground Gradient Dilution (FGD), where sparse 3D signals lead to weak supervision. To resolve this, we introduce Per-view No-target Suppression Optimization (PVSO), which provides stronger and more balanced gradients across views, enabling stable and efficient learning. To support consistent evaluation, we build MVRefer, a benchmark that defines standardized settings and metrics for MV-3DRES. Experiments show that MVGGT establishes the first strong baseline and achieves both high accuracy and fast inference, outperforming existing alternatives. Code and models are publicly available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://mvggt.github.io" title="">https://mvggt.github.io</a>.</p>
</div>
<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><sup class="ltx_sup" id="footnote1.1"><span class="ltx_text ltx_font_italic" id="footnote1.1.1">†</span></sup>Equal Contribution. <sup class="ltx_sup" id="footnote1.2"><span class="ltx_text ltx_font_italic" id="footnote1.2.1">∗</span></sup>Corresponding Author.</span></span></span>
<div class="ltx_logical-block" id="id15">
<div class="ltx_para" id="id15.p1">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_centering ltx_img_landscape" height="475" id="id14.g1" src="x1.png" width="932"/>
</div>
<figure class="ltx_figure ltx_align_center" id="S0.F1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S0.F1.6.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text ltx_font_bold" id="S0.F1.7.2" style="font-size:90%;">The Reality Gap: From Idealized 3D RES to Real-World MV-3DRES.<span class="ltx_text ltx_font_medium" id="S0.F1.7.2.1"> </span>(a)<span class="ltx_text ltx_font_medium" id="S0.F1.7.2.2"> Traditional 3D RES depends on dense, high-quality point clouds produced by slow offline scanning and heavy reconstruction.
</span>(b)<span class="ltx_text ltx_font_medium" id="S0.F1.7.2.3"> Applied to sparse, low-quality point clouds from real-world RGB views, these models fail to generalize.
</span>(c)<span class="ltx_text ltx_font_medium" id="S0.F1.7.2.4"> We introduce MV-3DRES, which uses sparse multi-view RGB inputs and text to achieve robust joint reconstruction and perception, enabled by our MVGGT model.</span></span></figcaption>
</figure>
</div>
<span class="ltx_note ltx_role_footnotetext" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotetext: </span><sup class="ltx_sup" id="footnotex1.1"><span class="ltx_text ltx_font_italic" id="footnotex1.1.1">†</span></sup>Equal Contribution.</span></span></span><span class="ltx_note ltx_role_footnotetext" id="footnotex2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">footnotetext: </span><sup class="ltx_sup" id="footnotex2.1"><span class="ltx_text ltx_font_italic" id="footnotex2.1.1">∗</span></sup>Corresponding Author.</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Grounding natural language in 3D physical scenes is fundamental to embodied AI. A prominent formulation is 3D referring expression segmentation (3DRES), where a model segments an object in a 3D scene given a textual description. Although recent methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib16" title="Scanrefer: 3d object localization in rgb-d scans using natural language">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib17" title="Referit3d: neural listeners for fine-grained 3d object identification in real-world scenes">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib18" title="Multi3drefer: grounding text description to multiple 3d objects">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib56" title="3dvg-transformer: relation modeling for visual grounding on point clouds">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib27" title="Text-guided graph neural networks for referring 3d instance segmentation">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib35" title="3d-stmn: dependency-driven superpoint-text matching network for end-to-end 3d referring expression segmentation">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib34" title="3d-gres: generalized 3d referring expression segmentation">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib2" title="Rg-san: rule-guided spatial awareness network for end-to-end 3d referring expression segmentation">46</a>]</cite> have achieved strong results, they are built upon a rarely questioned assumption: the availability of dense, complete, and reliable point clouds. Such point clouds typically require LIDAR sensors or lengthy RGB-D SLAM pipelines like BundleFusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib94" title="Bundlefusion: real-time globally consistent 3d reconstruction using on-the-fly surface reintegration">5</a>]</cite>, which demand deliberate scanning and heavy offline processing. This assumption stands in stark contrast to real-world agents—robots, AR glasses, mobile devices—that perceive environments through only a few casually captured RGB views.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In real settings, high-fidelity geometry is the exception. Sparse multi-view images produce 3D reconstructions that are noisy, incomplete, and often ambiguous (Figure <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S0.F1" title="Figure 1 ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">1</span></a>(b)). Existing 3DRES models, trained on idealized point clouds, collapse under such inputs. This exposes a fundamental limitation: current 3DRES is sensor-privileged and misaligned with the actual sensing conditions of embodied systems. It motivates a central question:
<span class="ltx_text ltx_font_bold" id="S1.p2.1.1">How can we achieve language-grounded 3D perception when complete geometry is no longer given but must be inferred from sparse, inconsistent views?</span></p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">We address this by introducing Multi-view 3D Referring Expression Segmentation (MV-3DRES), a new setting where the model must jointly reconstruct the scene and segment the referred object directly from sparse RGB views (Figure <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S0.F1" title="Figure 1 ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">1</span></a>(c)). MV-3DRES is inherently challenging: the model must reason over missing structure, integrate information across misaligned viewpoints, and resolve linguistic ambiguities without access to dense 3D input.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Conventional pipelines fail in this regime. Purely 2D methods cannot enforce global 3D consistency, since they operate on isolated views and cannot resolve depth ordering, occlusion relationships, or spatial relations such as “in front of” or “on the left.” As a result, back-projecting their per-view masks produces fragmented or conflicting 3D predictions. Two-stage “reconstruct-then-segment” pipelines face a different failure mode: sparse inputs yield point clouds that are noisy, incomplete, and structurally distorted, making it difficult for 3DRES models to recover full object extents. Moreover, running a full reconstruction before segmentation incurs substantial latency, limiting practical deployment.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To this end, we propose the Multimodal Visual Geometry Grounded Transformer (MVGGT), the first end-to-end architecture designed specifically for MV-3DRES. MVGGT adopts a dual-branch paradigm: a frozen geometric branch provides camera poses, depth cues, and a coarse structural scaffold, while a multimodal branch injects linguistic cues into sparse-view visual features through cross-view, cross-modal attention. This design embodies a key conceptual shift: language is intertwined with geometric reasoning from the start, enabling it to guide evidence aggregation and scene disambiguation long before a complete 3D representation exists.</p>
</div>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="530" id="S1.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S1.F2.3.2" style="font-size:90%;">Illustration of Foreground Gradient Dilusion Problem of Global 3D DICE loss and Per-view No-Target Suppression Optimization.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">However, sparse-view learning introduces a fundamental optimization challenge. Sparse multi-view reconstruction produces point clouds in which the target instance is represented by only a very small number of scattered points, far fewer than in the dense point clouds used by conventional 3DRES methods. Under such extreme foreground sparsity, standard 3D losses such as Dice become ineffective: gradients from the target region are overwhelmed by background points, leading to Foreground Gradient Dilution (FGD) and causing the optimization to stagnate in early training—gradients are too small to escape poor local minima, as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">2</span></a>. The problem is exacerbated by view-dependent visibility—some views contain clear target evidence, while others provide almost none—making uniform 3D supervision unstable and noisy. To mitigate FGD, we introduce Per-view No-target Suppression Optimization (PVSO), which shifts supervision back to 2D view space where the target occupies a larger and more reliable area. This per-view formulation amplifies meaningful gradients from informative views and suppresses misleading signals from target-absent views, resulting in significantly more stable and effective training.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Finally, to standardize evaluation, we construct MVRefer, the first benchmark defining settings, metrics, and data protocol for MV-3DRES. Extensive experiments show that MVGGT provides a strong baseline and significantly outperforms existing alternatives. Taken together, our contributions are fourfold:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We identify and formalize MV-3DRES, a new problem setting that aligns 3D grounding with realistic sensing conditions.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We propose MVGGT, a novel dual-branch architecture unifying geometric scaffolding with cross-view, language-aware perception.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We analyze and address the Foreground Gradient Dilution challenge via PVSO, offering a principled optimization strategy tailored for sparse 3D supervision.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">We construct the MVRefer benchmark, defining standardized settings and metrics for MV-3DRES and providing the first strong baseline.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Traditional 3D Referring Segmentation</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">3D grounding <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib110" title="Oneformer: one transformer to rule universal image segmentation">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib112" title="Multi-view transformer for 3d visual grounding">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib119" title="CK-transformer: commonsense knowledge enhanced transformers for referring expression comprehension">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib120" title="Reclip: a strong zero-shot baseline for referring expression comprehension">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib121" title="Flexible visual grounding">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib41" title="Free-form description guided 3d visual graph network for object grounding in point cloud">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib122" title="Deep hough voting for 3d object detection in point clouds">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib44" title="3d-sps: single-stage 3d visual grounding via referred point progressive selection">30</a>]</cite> aims to locate a specific object in a 3D scene based on a unique natural language description <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib16" title="Scanrefer: 3d object localization in rgb-d scans using natural language">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib17" title="Referit3d: neural listeners for fine-grained 3d object identification in real-world scenes">1</a>]</cite>,which is part of vision-language tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib114" title="Vitron: a unified pixel-level vision llm for understanding, generating, segmenting, editing">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib115" title="Person re-identification method based on color attack and joint defence">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib116" title="Next-gpt: any-to-any multimodal llm">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib117" title="Structured multi-modal feature embedding and alignment for image-sentence retrieval">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib118" title="Transformer-empowered invariant grounding for video question answering">24</a>]</cite>. Following this, the 3D-RES (3D Referring Segmentation) task aims to segment a specific object within a point cloud based on a textual query. The field has evolved from foundational two-stage paradigms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib27" title="Text-guided graph neural networks for referring 3d instance segmentation">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib42" title="Instancerefer: cooperative holistic understanding for visual grounding on point clouds through instance multi-level contextual referring">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib31" title="X-refseg3d: enhancing referring 3d instance segmentation via structured cross-modal graph neural networks">34</a>]</cite> (relying on object proposals and language matching) to recent end-to-end architectures <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib44" title="3d-sps: single-stage 3d visual grounding via referred point progressive selection">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib28" title="RefMask3D: language-guided transformer for 3d referring segmentation">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib29" title="A unified framework for 3d point cloud visual grounding">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib30" title="Segpoint: segment any point cloud via large language model">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib31" title="X-refseg3d: enhancing referring 3d instance segmentation via structured cross-modal graph neural networks">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib34" title="3d-gres: generalized 3d referring expression segmentation">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib32" title="LESS: label-efficient and single-stage referring 3d segmentation">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib35" title="3d-stmn: dependency-driven superpoint-text matching network for end-to-end 3d referring expression segmentation">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib2" title="Rg-san: rule-guided spatial awareness network for end-to-end 3d referring expression segmentation">46</a>]</cite> demonstrating high efficacy through advanced cross-modal fusion. Despite this progress, the sub-field shares a critical limitation: the requirement for high-quality, dense point clouds. This expensive geometric data is inaccessible to many real-world embodied agents that must rely on sparse, online RGB captures.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Multi-View Feed-forward Reconstruction</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Reconstructing 3D geometry from multi-view RGB images provides a practical solution to the input ambiguity in sparse-view settings. Early feed-forward approaches such as DUSt3R and MASt3R <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib87" title="Grounding image matching in 3d with mast3r">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib86" title="Dust3r: geometric 3d vision made easy">42</a>]</cite> introduced coupled scene representations but required heavy post-processing or integration with classical SfM/SLAM pipelines <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib97" title="Mast3r-sfm: a fully-integrated solution for unconstrained structure-from-motion">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib98" title="Light3R-sfm: towards feed-forward structure-from-motion">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib99" title="MASt3R-slam: real-time dense slam with 3d reconstruction priors">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib100" title="MP-sfm: monocular surface priors for robust structure-from-motion">32</a>]</cite> for unconstrained reconstruction. Later works improved efficiency and stability by replacing classical optimization with Transformer-based latent state propagation, as demonstrated by Spann3R, CUT3R, and MUSt3R <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib101" title="3d reconstruction with spatial memory">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib102" title="Continuous 3d perception model with persistent state">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib103" title="Must3r: multi-view network for stereo 3d reconstruction">2</a>]</cite>. Streaming models like WinT3R <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib105" title="WINT3R: window-based streaming recon-struction with camera token pool">45</a>]</cite> further enabled real-time performance via sliding-window processing and global camera token pooling.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">More recent architectures—VGGT and its successors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib88" title="Vggt: visual geometry grounded transformer">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib89" title="Pi3: permutation-equivariant visual geometry learning">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib106" title="Fastvggt: training-free acceleration of visual geometry transformer">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib107" title="Faster vggt with block-sparse global attention">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib113" title="VGGT-long: chunk it, loop it, align it–pushing vggt’s limits on kilometer-scale long rgb sequences">7</a>]</cite>—adopt alternating-attention designs to achieve robust generalized reconstruction, while extensions address semantic and multi-task perception <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib104" title="Mvtrans: multi-view perception of transparent objects">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib108" title="IGGT: instance-grounded geometry trans-former for semantic 3d reconstruction">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib109" title="OVSeg3R: learn open-vocabulary instance segmentation from 2d via 3d reconstruction">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib123" title="SIU3R: simultaneous scene understanding and 3d reconstruction beyond feature alignment">50</a>]</cite>. This progress culminates in universal backbones such as MapAnything <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib90" title="MapAnything: universal feed-forward metric 3d reconstruction">20</a>]</cite>, which produce fully factored, metric-aware scene representations.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>MV-3DRES Task and MVRefer Benchmark</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Task Formulation</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We formalize the Multi-view 3D Referring Segmentation (MV-3DRES) task to align 3D language grounding with the sensing constraints of real-world agents. Instead of assuming access to pre-constructed dense point clouds, the model operates directly on sparse multi-view RGB images.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.3">Given a set of <math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> RGB views <math alttext="I=\{I_{i}\}_{i=1}^{N}" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2" intent=":literal"><semantics><mrow><mi>I</mi><mo>=</mo><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>I</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup></mrow><annotation encoding="application/x-tex">I=\{I_{i}\}_{i=1}^{N}</annotation></semantics></math> and a natural-language referring expression <math alttext="T" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m3" intent=":literal"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math>, the goal is to learn a function</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="f:(I,T)\rightarrow(S^{\prime},M)," class="ltx_Math" display="block" id="S3.E1.m1" intent=":literal"><semantics><mrow><mrow><mi>f</mi><mo lspace="0.278em" rspace="0.278em">:</mo><mrow><mrow><mo stretchy="false">(</mo><mi>I</mi><mo>,</mo><mi>T</mi><mo stretchy="false">)</mo></mrow><mo stretchy="false">→</mo><mrow><mo stretchy="false">(</mo><msup><mi>S</mi><mo>′</mo></msup><mo>,</mo><mi>M</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">f:(I,T)\rightarrow(S^{\prime},M),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p2.7">where <math alttext="S^{\prime}\in\mathbb{R}^{K\times 3}" class="ltx_Math" display="inline" id="S3.SS1.p2.4.m1" intent=":literal"><semantics><mrow><msup><mi>S</mi><mo>′</mo></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>K</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mn>3</mn></mrow></msup></mrow><annotation encoding="application/x-tex">S^{\prime}\in\mathbb{R}^{K\times 3}</annotation></semantics></math> denotes the reconstructed 3D point cloud containing <math alttext="K" class="ltx_Math" display="inline" id="S3.SS1.p2.5.m2" intent=":literal"><semantics><mi>K</mi><annotation encoding="application/x-tex">K</annotation></semantics></math> points, and <math alttext="M\in\{0,1\}^{K}" class="ltx_Math" display="inline" id="S3.SS1.p2.6.m3" intent=":literal"><semantics><mrow><mi>M</mi><mo>∈</mo><msup><mrow><mo stretchy="false">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">}</mo></mrow><mi>K</mi></msup></mrow><annotation encoding="application/x-tex">M\in\{0,1\}^{K}</annotation></semantics></math> is the corresponding 3D binary mask marking the points that belong to the object referred to by <math alttext="T" class="ltx_Math" display="inline" id="S3.SS1.p2.7.m4" intent=":literal"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math>. The model must infer both geometry and semantics from the same sparse observations, without any ground-truth 3D input at inference time.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">This formulation introduces challenges not present in standard 3DRES. Sparse multi-view observations generate incomplete and noisy geometry, forcing the model to couple reconstruction and grounding. Spatial relations described in language, such as “on the left of the chair,” must be resolved across viewpoints with inconsistent visibility. Moreover, the target object often occupies only a small portion of the available views, yielding severe foreground sparsity and weak supervisory signals, which we later characterize as Foreground Gradient Dilution (FGD).</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>The MVRefer Benchmark</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">To support systematic evaluation of MV-3DRES, we construct MVRefer, a benchmark built upon ScanRefer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib16" title="Scanrefer: 3d object localization in rgb-d scans using natural language">3</a>]</cite> and the underlying ScanNet sequences <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib39" title="Scannet: richly-annotated 3d reconstructions of indoor scenes">4</a>]</cite>. MVRefer is designed to emulate how an embodied agent perceives a scene through a limited number of casual views.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Benchmark Setting</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">For each language–object pair in ScanRefer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib16" title="Scanrefer: 3d object localization in rgb-d scans using natural language">3</a>]</cite>, we sample <math alttext="N=8" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.1.m1" intent=":literal"><semantics><mrow><mi>N</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding="application/x-tex">N=8</annotation></semantics></math> RGB frames from the raw ScanNet video stream <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib39" title="Scannet: richly-annotated 3d reconstructions of indoor scenes">4</a>]</cite> at uniform temporal intervals to approximate sparse, on-the-fly observations. Sparse sampling creates a solvability issue: the target may be absent from all selected frames. To ensure each sample remains resolvable, we perform a visibility validation step. If none of the initial eight images contain the target, we replace one no-target frame with a randomly chosen target-visible frame. This guarantees at least one positive view while naturally preserving a high proportion of no-target views, maintaining the difficulty inherent to sparse-view grounding.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Evaluation Metrics and Splits</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">Evaluating MV-3DRES requires metrics that disentangle grounding quality from reconstruction quality. Since both outputs in <math alttext="(S^{\prime},M)" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p1.1.m1" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><msup><mi>S</mi><mo>′</mo></msup><mo>,</mo><mi>M</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(S^{\prime},M)</annotation></semantics></math> are jointly predicted from sparse inputs, reconstruction errors can obscure the model’s true grounding ability.</p>
</div>
<section class="ltx_paragraph" id="S3.SS2.SSS2.Px1">
<h5 class="ltx_title ltx_title_paragraph">Traditional 3D Metric.</h5>
<div class="ltx_para" id="S3.SS2.SSS2.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS2.Px1.p1.5">We report global 3D mean IoU,</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{mIoU}_{\text{global}}=\text{IoU}(M,M^{\ast})," class="ltx_Math" display="block" id="S3.E2.m1" intent=":literal"><semantics><mrow><mrow><msub><mtext>mIoU</mtext><mtext>global</mtext></msub><mo>=</mo><mrow><mtext>IoU</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>M</mi><mo>,</mo><msup><mi>M</mi><mo>∗</mo></msup><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\text{mIoU}_{\text{global}}=\text{IoU}(M,M^{\ast}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS2.Px1.p1.4">where <math alttext="M^{\ast}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.Px1.p1.1.m1" intent=":literal"><semantics><msup><mi>M</mi><mo>∗</mo></msup><annotation encoding="application/x-tex">M^{\ast}</annotation></semantics></math> denotes the ground-truth mask projected onto the reconstructed point cloud <math alttext="S^{\prime}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.Px1.p1.2.m2" intent=":literal"><semantics><msup><mi>S</mi><mo>′</mo></msup><annotation encoding="application/x-tex">S^{\prime}</annotation></semantics></math>. Although standard, <span class="ltx_text ltx_markedasmath" id="S3.SS2.SSS2.Px1.p1.4.1">mIoU</span><span class="ltx_text ltx_markedasmath" id="S3.SS2.SSS2.Px1.p1.4.2">global</span> entangles segmentation performance with the fidelity of <math alttext="S^{\prime}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.Px1.p1.4.m4" intent=":literal"><semantics><msup><mi>S</mi><mo>′</mo></msup><annotation encoding="application/x-tex">S^{\prime}</annotation></semantics></math>, making it insufficient as the primary diagnostic measure.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS2.Px2">
<h5 class="ltx_title ltx_title_paragraph">Multi-view Diagnostic Metrics.</h5>
<div class="ltx_para" id="S3.SS2.SSS2.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.Px2.p1.4">To isolate grounding behaviors, we reproject the predicted 3D mask <math alttext="M" class="ltx_Math" display="inline" id="S3.SS2.SSS2.Px2.p1.1.m1" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math> into each view using the known camera intrinsics and extrinsics. Let <math alttext="P_{i}(M)" class="ltx_Math" display="inline" id="S3.SS2.SSS2.Px2.p1.2.m2" intent=":literal"><semantics><mrow><msub><mi>P</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>M</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">P_{i}(M)</annotation></semantics></math> denote the projected 2D mask for view <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.SSS2.Px2.p1.3.m3" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>, and <math alttext="P_{i}(M^{\ast})" class="ltx_Math" display="inline" id="S3.SS2.SSS2.Px2.p1.4.m4" intent=":literal"><semantics><mrow><msub><mi>P</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>M</mi><mo>∗</mo></msup><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">P_{i}(M^{\ast})</annotation></semantics></math> its ground-truth counterpart. We compute:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{mIoU}_{\text{view}}=\frac{1}{N}\sum_{i=1}^{N}\text{IoU}\left(P_{i}(M),P_{i}(M^{\ast})\right)," class="ltx_Math" display="block" id="S3.E3.m1" intent=":literal"><semantics><mrow><mrow><msub><mtext>mIoU</mtext><mtext>view</mtext></msub><mo>=</mo><mrow><mfrac><mn>1</mn><mi>N</mi></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><munderover><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><mtext>IoU</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><msub><mi>P</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>M</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>P</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>M</mi><mo>∗</mo></msup><mo stretchy="false">)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\text{mIoU}_{\text{view}}=\frac{1}{N}\sum_{i=1}^{N}\text{IoU}\left(P_{i}(M),P_{i}(M^{\ast})\right),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{mIoU}_{\text{pos}}=\frac{1}{|\mathcal{V}^{+}|}\sum_{i\in\mathcal{V}^{+}}\text{IoU}\left(P_{i}(M),P_{i}(M^{\ast})\right)," class="ltx_Math" display="block" id="S3.E4.m1" intent=":literal"><semantics><mrow><mrow><msub><mtext>mIoU</mtext><mtext>pos</mtext></msub><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mo stretchy="false">|</mo><msup><mi class="ltx_font_mathcaligraphic">𝒱</mi><mo>+</mo></msup><mo stretchy="false">|</mo></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><munder><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>∈</mo><msup><mi class="ltx_font_mathcaligraphic">𝒱</mi><mo>+</mo></msup></mrow></munder><mrow><mtext>IoU</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><msub><mi>P</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>M</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>P</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>M</mi><mo>∗</mo></msup><mo stretchy="false">)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\text{mIoU}_{\text{pos}}=\frac{1}{|\mathcal{V}^{+}|}\sum_{i\in\mathcal{V}^{+}}\text{IoU}\left(P_{i}(M),P_{i}(M^{\ast})\right),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="S3.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{mIoU}_{\text{neg}}=\frac{1}{|\mathcal{V}^{-}|}\sum_{i\in\mathcal{V}^{-}}\text{IoU}\left(P_{i}(M),P_{i}(M^{\ast})\right)," class="ltx_Math" display="block" id="S3.E5.m1" intent=":literal"><semantics><mrow><mrow><msub><mtext>mIoU</mtext><mtext>neg</mtext></msub><mo>=</mo><mrow><mfrac><mn>1</mn><mrow><mo stretchy="false">|</mo><msup><mi class="ltx_font_mathcaligraphic">𝒱</mi><mo>−</mo></msup><mo stretchy="false">|</mo></mrow></mfrac><mo lspace="0em" rspace="0em">​</mo><mrow><munder><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>∈</mo><msup><mi class="ltx_font_mathcaligraphic">𝒱</mi><mo>−</mo></msup></mrow></munder><mrow><mtext>IoU</mtext><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mrow><msub><mi>P</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>M</mi><mo stretchy="false">)</mo></mrow></mrow><mo>,</mo><mrow><msub><mi>P</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msup><mi>M</mi><mo>∗</mo></msup><mo stretchy="false">)</mo></mrow></mrow><mo>)</mo></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\text{mIoU}_{\text{neg}}=\frac{1}{|\mathcal{V}^{-}|}\sum_{i\in\mathcal{V}^{-}}\text{IoU}\left(P_{i}(M),P_{i}(M^{\ast})\right),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS2.Px2.p1.8">where <math alttext="\mathcal{V}^{+}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.Px2.p1.5.m1" intent=":literal"><semantics><msup><mi class="ltx_font_mathcaligraphic">𝒱</mi><mo>+</mo></msup><annotation encoding="application/x-tex">\mathcal{V}^{+}</annotation></semantics></math> and <math alttext="\mathcal{V}^{-}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.Px2.p1.6.m2" intent=":literal"><semantics><msup><mi class="ltx_font_mathcaligraphic">𝒱</mi><mo>−</mo></msup><annotation encoding="application/x-tex">\mathcal{V}^{-}</annotation></semantics></math> denote the sets of target-visible and no-target views, respectively. These metrics shed light on grounding precision (<span class="ltx_text ltx_markedasmath" id="S3.SS2.SSS2.Px2.p1.8.1">mIoU</span><span class="ltx_text ltx_markedasmath" id="S3.SS2.SSS2.Px2.p1.8.2">pos</span>) and suppression ability (<span class="ltx_text ltx_markedasmath" id="S3.SS2.SSS2.Px2.p1.8.3">mIoU</span><span class="ltx_text ltx_markedasmath" id="S3.SS2.SSS2.Px2.p1.8.4">neg</span>), both of which are essential for robust performance under sparse supervision.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS2.Px3">
<h5 class="ltx_title ltx_title_paragraph">Difficulty Splits.</h5>
<div class="ltx_para" id="S3.SS2.SSS2.Px3.p1">
<p class="ltx_p" id="S3.SS2.SSS2.Px3.p1.2">To evaluate robustness under varying signal sparsity, we define two difficulty splits based on the target’s 2D pixel ratio. A sample is categorized as <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS2.Px3.p1.2.1">hard</span> if the target occupies less than <math alttext="5\%" class="ltx_Math" display="inline" id="S3.SS2.SSS2.Px3.p1.1.m1" intent=":literal"><semantics><mrow><mn>5</mn><mo>%</mo></mrow><annotation encoding="application/x-tex">5\%</annotation></semantics></math> of pixels in all its visible views, and <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS2.Px3.p1.2.2">easy</span> if at least one view contains at least <math alttext="5\%" class="ltx_Math" display="inline" id="S3.SS2.SSS2.Px3.p1.2.m2" intent=":literal"><semantics><mrow><mn>5</mn><mo>%</mo></mrow><annotation encoding="application/x-tex">5\%</annotation></semantics></math> target pixels. This separation allows us to isolate performance differences arising from the strength of view-specific supervision.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="349" id="S3.F3.g1" src="x3.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.3.2" style="font-size:90%;">Architecture of MVGGT, which comprises a frozen Reconstruction Branch that establishes geometric structure and a trainable Multimodal Branch that integrates language into sparse-view visual reasoning.</span></figcaption>
</figure>
</section>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Method</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We introduce the <span class="ltx_text ltx_font_bold" id="S4.p1.1.1">Multimodal Visual Geometry Grounded Transformer (MVGGT)</span>, an end-to-end dual-branch framework tailored for the MV-3DRES task, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S3.F3" title="Figure 3 ‣ Difficulty Splits. ‣ 3.2.2 Evaluation Metrics and Splits ‣ 3.2 The MVRefer Benchmark ‣ 3 MV-3DRES Task and MVRefer Benchmark ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>The proposed MVGGT</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">MVGGT is designed to jointly recover 3D geometry and perform language-conditioned segmentation from sparse views. The separation into two branches allows the model to exploit a stable geometric scaffold while learning multimodal representations aligned with the text query.</p>
</div>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Inputs and Encoders.</h5>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.8">Given <math alttext="N" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.1.m1" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math> input images <math alttext="I=\{I_{i}\}_{i=1}^{N}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.2.m2" intent=":literal"><semantics><mrow><mi>I</mi><mo>=</mo><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>I</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup></mrow><annotation encoding="application/x-tex">I=\{I_{i}\}_{i=1}^{N}</annotation></semantics></math> and a referring expression <math alttext="T" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.3.m3" intent=":literal"><semantics><mi>T</mi><annotation encoding="application/x-tex">T</annotation></semantics></math>, each image is encoded by a frame-wise ViT, yielding patch embeddings <math alttext="F_{i}^{\text{vis}}\in\mathbb{R}^{P\times D}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.4.m4" intent=":literal"><semantics><mrow><msubsup><mi>F</mi><mi>i</mi><mtext>vis</mtext></msubsup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>P</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>D</mi></mrow></msup></mrow><annotation encoding="application/x-tex">F_{i}^{\text{vis}}\in\mathbb{R}^{P\times D}</annotation></semantics></math>, where <math alttext="P" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.5.m5" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math> is the number of patches and <math alttext="D" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.6.m6" intent=":literal"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math> the feature dimension. The text is tokenized and processed by a language encoder to produce word embeddings <math alttext="F^{\text{lang}}\in\mathbb{R}^{W\times D}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.7.m7" intent=":literal"><semantics><mrow><msup><mi>F</mi><mtext>lang</mtext></msup><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>W</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>D</mi></mrow></msup></mrow><annotation encoding="application/x-tex">F^{\text{lang}}\in\mathbb{R}^{W\times D}</annotation></semantics></math>, where <math alttext="W" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px1.p1.8.m8" intent=":literal"><semantics><mi>W</mi><annotation encoding="application/x-tex">W</annotation></semantics></math> denotes token count.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Frozen Reconstruction Branch.</h5>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p1.4">The reconstruction branch is a geometry-aware transformer with <math alttext="L" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.1.m1" intent=":literal"><semantics><mi>L</mi><annotation encoding="application/x-tex">L</annotation></semantics></math> blocks. Each block alternates between frame-level self-attention and global cross-view attention, progressively building view-consistent structural cues. Let <math alttext="F_{\ell}^{\text{geo}}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.2.m2" intent=":literal"><semantics><msubsup><mi>F</mi><mi mathvariant="normal">ℓ</mi><mtext>geo</mtext></msubsup><annotation encoding="application/x-tex">F_{\ell}^{\text{geo}}</annotation></semantics></math> denote the features at block <math alttext="\ell\in\{1,\dots,L\}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.3.m3" intent=":literal"><semantics><mrow><mi mathvariant="normal">ℓ</mi><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mi>L</mi><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\ell\in\{1,\dots,L\}</annotation></semantics></math>. These features are fed to a reconstruction decoder that predicts camera poses and depth maps, which are back-projected into a coarse point cloud <math alttext="S^{\prime}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p1.4.m4" intent=":literal"><semantics><msup><mi>S</mi><mo>′</mo></msup><annotation encoding="application/x-tex">S^{\prime}</annotation></semantics></math>. All parameters in this branch remain frozen, ensuring a stable geometric prior across training and removing the need to re-learn 3D geometry from sparse images.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Trainable Multimodal Branch.</h5>
<div class="ltx_para" id="S4.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px3.p1.3">The multimodal branch contains <math alttext="L_{\text{multi}}=L/3" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p1.1.m1" intent=":literal"><semantics><mrow><msub><mi>L</mi><mtext>multi</mtext></msub><mo>=</mo><mrow><mi>L</mi><mo>/</mo><mn>3</mn></mrow></mrow><annotation encoding="application/x-tex">L_{\text{multi}}=L/3</annotation></semantics></math> transformer blocks. Its goal is to fuse geometric cues with text-conditioned visual features. Since the two branches have different depths, we align their interactions by injecting geometric features from the final <math alttext="L/3" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p1.2.m2" intent=":literal"><semantics><mrow><mi>L</mi><mo>/</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">L/3</annotation></semantics></math> blocks of the reconstruction branch into all <math alttext="L_{\text{multi}}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p1.3.m3" intent=":literal"><semantics><msub><mi>L</mi><mtext>multi</mtext></msub><annotation encoding="application/x-tex">L_{\text{multi}}</annotation></semantics></math> blocks of the multimodal branch.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS0.Px3.p2">
<p class="ltx_p" id="S4.SS1.SSS0.Px3.p2.9"><em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS0.Px3.p2.9.1">Geometric Injection.</em>
The multimodal branch contains <math alttext="L_{\text{multi}}=L/3" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p2.1.m1" intent=":literal"><semantics><mrow><msub><mi>L</mi><mtext>multi</mtext></msub><mo>=</mo><mrow><mi>L</mi><mo>/</mo><mn>3</mn></mrow></mrow><annotation encoding="application/x-tex">L_{\text{multi}}=L/3</annotation></semantics></math> blocks, and each of them receives geometric guidance from the <em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS0.Px3.p2.9.2">final</em> <math alttext="L_{\text{multi}}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p2.2.m2" intent=":literal"><semantics><msub><mi>L</mi><mtext>multi</mtext></msub><annotation encoding="application/x-tex">L_{\text{multi}}</annotation></semantics></math> blocks of the reconstruction branch. Concretely, for the <math alttext="l^{\prime}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p2.3.m3" intent=":literal"><semantics><msup><mi>l</mi><mo>′</mo></msup><annotation encoding="application/x-tex">l^{\prime}</annotation></semantics></math>-th multimodal block (<math alttext="l^{\prime}=1,\dots,L_{\text{multi}}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p2.4.m4" intent=":literal"><semantics><mrow><msup><mi>l</mi><mo>′</mo></msup><mo>=</mo><mrow><mn>1</mn><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>L</mi><mtext>multi</mtext></msub></mrow></mrow><annotation encoding="application/x-tex">l^{\prime}=1,\dots,L_{\text{multi}}</annotation></semantics></math>), we take the geometric feature <math alttext="F_{l}^{\text{geo}}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p2.5.m5" intent=":literal"><semantics><msubsup><mi>F</mi><mi>l</mi><mtext>geo</mtext></msubsup><annotation encoding="application/x-tex">F_{l}^{\text{geo}}</annotation></semantics></math> from the <math alttext="l" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p2.6.m6" intent=":literal"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math>-th reconstruction block, where <math alttext="l" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p2.7.m7" intent=":literal"><semantics><mi>l</mi><annotation encoding="application/x-tex">l</annotation></semantics></math> runs over the last <math alttext="L_{\text{multi}}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p2.8.m8" intent=":literal"><semantics><msub><mi>L</mi><mtext>multi</mtext></msub><annotation encoding="application/x-tex">L_{\text{multi}}</annotation></semantics></math> layers in order (i.e., the reconstruction branch’s <math alttext="(L-L_{\text{multi}}+1)" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p2.9.m9" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>L</mi><mo>−</mo><msub><mi>L</mi><mtext>multi</mtext></msub></mrow><mo>+</mo><mn>1</mn></mrow><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(L-L_{\text{multi}}+1)</annotation></semantics></math>-th layer provides geometry to the first multimodal block, the next layer to the second, and so on).</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS0.Px3.p3">
<p class="ltx_p" id="S4.SS1.SSS0.Px3.p3.3">These geometric features are passed through a zero-initialized <math alttext="1\times 1" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p3.1.m1" intent=":literal"><semantics><mrow><mn>1</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1\times 1</annotation></semantics></math> convolution <math alttext="\mathcal{Z}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p3.2.m2" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒵</mi><annotation encoding="application/x-tex">\mathcal{Z}</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib93" title="Adding conditional control to text-to-image diffusion models">52</a>]</cite>, which projects them into the multimodal feature space. The input to the <math alttext="l^{\prime}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p3.3.m3" intent=":literal"><semantics><msup><mi>l</mi><mo>′</mo></msup><annotation encoding="application/x-tex">l^{\prime}</annotation></semantics></math>-th multimodal block is then</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="F_{l^{\prime}}^{\text{in}}=F_{l^{\prime}-1}^{\text{out}}+\mathcal{Z}(F_{l}^{\text{geo}})," class="ltx_Math" display="block" id="S4.E6.m1" intent=":literal"><semantics><mrow><mrow><msubsup><mi>F</mi><msup><mi>l</mi><mo>′</mo></msup><mtext>in</mtext></msubsup><mo>=</mo><mrow><msubsup><mi>F</mi><mrow><msup><mi>l</mi><mo>′</mo></msup><mo>−</mo><mn>1</mn></mrow><mtext>out</mtext></msubsup><mo>+</mo><mrow><mi class="ltx_font_mathcaligraphic">𝒵</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msubsup><mi>F</mi><mi>l</mi><mtext>geo</mtext></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">F_{l^{\prime}}^{\text{in}}=F_{l^{\prime}-1}^{\text{out}}+\mathcal{Z}(F_{l}^{\text{geo}}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS1.SSS0.Px3.p3.6">where <math alttext="F_{l^{\prime}-1}^{\text{out}}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p3.4.m1" intent=":literal"><semantics><msubsup><mi>F</mi><mrow><msup><mi>l</mi><mo>′</mo></msup><mo>−</mo><mn>1</mn></mrow><mtext>out</mtext></msubsup><annotation encoding="application/x-tex">F_{l^{\prime}-1}^{\text{out}}</annotation></semantics></math> is the output of the preceding visual attention and cross attention. It allows the multimodal branch to progressively incorporate higher-level geometric cues without perturbing the pretrained reconstruction backbone. <math alttext="F_{l^{\prime}}^{\text{in}}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p3.5.m2" intent=":literal"><semantics><msubsup><mi>F</mi><msup><mi>l</mi><mo>′</mo></msup><mtext>in</mtext></msubsup><annotation encoding="application/x-tex">F_{l^{\prime}}^{\text{in}}</annotation></semantics></math> is then fed into the visual attention module to get <math alttext="F_{l^{\prime}}^{\text{vis}}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p3.6.m3" intent=":literal"><semantics><msubsup><mi>F</mi><msup><mi>l</mi><mo>′</mo></msup><mtext>vis</mtext></msubsup><annotation encoding="application/x-tex">F_{l^{\prime}}^{\text{vis}}</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS0.Px3.p4">
<p class="ltx_p" id="S4.SS1.SSS0.Px3.p4.2"><em class="ltx_emph ltx_font_italic" id="S4.SS1.SSS0.Px3.p4.2.1">Language Injection.</em>
Within each multimodal block, language injection is implemented through a standard cross-attention layer. Let <math alttext="F_{l^{\prime}}^{\text{in}}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p4.1.m1" intent=":literal"><semantics><msubsup><mi>F</mi><msup><mi>l</mi><mo>′</mo></msup><mtext>in</mtext></msubsup><annotation encoding="application/x-tex">F_{l^{\prime}}^{\text{in}}</annotation></semantics></math> be the visual tokens entering block <math alttext="l^{\prime}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p4.2.m2" intent=":literal"><semantics><msup><mi>l</mi><mo>′</mo></msup><annotation encoding="application/x-tex">l^{\prime}</annotation></semantics></math>. Query, key, and value projections are defined as</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Q=F_{l^{\prime}}^{\text{vis}}W_{Q},\quad K=F^{\text{lang}}W_{K},\quad V=F^{\text{lang}}W_{V}," class="ltx_Math" display="block" id="S4.E7.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>Q</mi><mo>=</mo><mrow><msubsup><mi>F</mi><msup><mi>l</mi><mo>′</mo></msup><mtext>vis</mtext></msubsup><mo lspace="0em" rspace="0em">​</mo><msub><mi>W</mi><mi>Q</mi></msub></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mrow><mi>K</mi><mo>=</mo><mrow><msup><mi>F</mi><mtext>lang</mtext></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>W</mi><mi>K</mi></msub></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mi>V</mi><mo>=</mo><mrow><msup><mi>F</mi><mtext>lang</mtext></msup><mo lspace="0em" rspace="0em">​</mo><msub><mi>W</mi><mi>V</mi></msub></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">Q=F_{l^{\prime}}^{\text{vis}}W_{Q},\quad K=F^{\text{lang}}W_{K},\quad V=F^{\text{lang}}W_{V},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS1.SSS0.Px3.p4.5">with <math alttext="W_{Q}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p4.3.m1" intent=":literal"><semantics><msub><mi>W</mi><mi>Q</mi></msub><annotation encoding="application/x-tex">W_{Q}</annotation></semantics></math>, <math alttext="W_{K}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p4.4.m2" intent=":literal"><semantics><msub><mi>W</mi><mi>K</mi></msub><annotation encoding="application/x-tex">W_{K}</annotation></semantics></math>, and <math alttext="W_{V}\in\mathbb{R}^{D\times D}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px3.p4.5.m3" intent=":literal"><semantics><mrow><msub><mi>W</mi><mi>V</mi></msub><mo>∈</mo><msup><mi>ℝ</mi><mrow><mi>D</mi><mo lspace="0.222em" rspace="0.222em">×</mo><mi>D</mi></mrow></msup></mrow><annotation encoding="application/x-tex">W_{V}\in\mathbb{R}^{D\times D}</annotation></semantics></math> learnable matrices. Attention is computed as</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="F_{l^{\prime}}^{\text{out}}=\mathrm{softmax}\!\left(\frac{QK^{\top}}{\sqrt{D}}\right)V," class="ltx_Math" display="block" id="S4.E8.m1" intent=":literal"><semantics><mrow><mrow><msubsup><mi>F</mi><msup><mi>l</mi><mo>′</mo></msup><mtext>out</mtext></msubsup><mo>=</mo><mrow><mpadded style="width:3.720em;" width="3.720em"><mi>softmax</mi></mpadded><mo lspace="0em" rspace="0em">​</mo><mrow><mo>(</mo><mfrac><mrow><mi>Q</mi><mo lspace="0em" rspace="0em">​</mo><msup><mi>K</mi><mo>⊤</mo></msup></mrow><msqrt><mi>D</mi></msqrt></mfrac><mo>)</mo></mrow><mo lspace="0em" rspace="0em">​</mo><mi>V</mi></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">F_{l^{\prime}}^{\text{out}}=\mathrm{softmax}\!\left(\frac{QK^{\top}}{\sqrt{D}}\right)V,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS1.SSS0.Px3.p4.6">and added back through a residual path. This multi-level injection allows text cues to guide feature aggregation across views.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Decoders and Outputs.</h5>
<div class="ltx_para" id="S4.SS1.SSS0.Px4.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px4.p1.4">Finally, the multimodal features <math alttext="F_{L_{\text{multi}}}^{\text{multi}}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px4.p1.1.m1" intent=":literal"><semantics><msubsup><mi>F</mi><msub><mi>L</mi><mtext>multi</mtext></msub><mtext>multi</mtext></msubsup><annotation encoding="application/x-tex">F_{L_{\text{multi}}}^{\text{multi}}</annotation></semantics></math> are decoded into per-view masks <math alttext="\{M_{i}\}_{i=1}^{N}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px4.p1.2.m2" intent=":literal"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>M</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><annotation encoding="application/x-tex">\{M_{i}\}_{i=1}^{N}</annotation></semantics></math>. Using the depths and camera parameters from the frozen reconstruction branch, these 2D masks are back-projected and aggregated on the reconstructed point cloud <math alttext="S^{\prime}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px4.p1.3.m3" intent=":literal"><semantics><msup><mi>S</mi><mo>′</mo></msup><annotation encoding="application/x-tex">S^{\prime}</annotation></semantics></math> to obtain the final 3D mask <math alttext="M" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px4.p1.4.m4" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math>. The resulting multi-view predictions serve as the supervision targets in PVSO (Section <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S4.SS3" title="4.3 Per-view No-Target Suppression Optimization ‣ 4 Method ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">4.3</span></a>).</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Foreground Gradient Dilution</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.5">Training under the MV-3DRES setting is fundamentally hindered by the extreme sparsity of foreground points in the reconstructed 3D space. Let the Dice loss for 3D segmentation be</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\text{Dice}}=1-\frac{2I}{U},\quad I=\sum_{j}p_{j}g_{j},\quad U=\sum_{j}p_{j}+\sum_{j}g_{j}," class="ltx_Math" display="block" id="S4.E9.m1" intent=":literal"><semantics><mrow><mrow><mrow><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>Dice</mtext></msub><mo>=</mo><mrow><mn>1</mn><mo>−</mo><mfrac><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mi>I</mi></mrow><mi>U</mi></mfrac></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mrow><mi>I</mi><mo rspace="0.111em">=</mo><mrow><munder><mo movablelimits="false">∑</mo><mi>j</mi></munder><mrow><msub><mi>p</mi><mi>j</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>g</mi><mi>j</mi></msub></mrow></mrow></mrow><mo rspace="1.167em">,</mo><mrow><mi>U</mi><mo rspace="0.111em">=</mo><mrow><mrow><munder><mo movablelimits="false">∑</mo><mi>j</mi></munder><msub><mi>p</mi><mi>j</mi></msub></mrow><mo rspace="0.055em">+</mo><mrow><munder><mo movablelimits="false">∑</mo><mi>j</mi></munder><msub><mi>g</mi><mi>j</mi></msub></mrow></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\mathcal{L}_{\text{Dice}}=1-\frac{2I}{U},\quad I=\sum_{j}p_{j}g_{j},\quad U=\sum_{j}p_{j}+\sum_{j}g_{j},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS2.p1.4">where <math alttext="p_{j}" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1" intent=":literal"><semantics><msub><mi>p</mi><mi>j</mi></msub><annotation encoding="application/x-tex">p_{j}</annotation></semantics></math> is the predicted probability at point <math alttext="j" class="ltx_Math" display="inline" id="S4.SS2.p1.2.m2" intent=":literal"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math> and <math alttext="g_{j}\in\{0,1\}" class="ltx_Math" display="inline" id="S4.SS2.p1.3.m3" intent=":literal"><semantics><mrow><msub><mi>g</mi><mi>j</mi></msub><mo>∈</mo><mrow><mo stretchy="false">{</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">g_{j}\in\{0,1\}</annotation></semantics></math> denotes ground-truth labels. The gradient with respect to <math alttext="p_{j}" class="ltx_Math" display="inline" id="S4.SS2.p1.4.m4" intent=":literal"><semantics><msub><mi>p</mi><mi>j</mi></msub><annotation encoding="application/x-tex">p_{j}</annotation></semantics></math> is</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E10">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\frac{\partial\mathcal{L}_{\text{Dice}}}{\partial p_{j}}=\frac{2(I-g_{j}U)}{U^{2}}." class="ltx_Math" display="block" id="S4.E10.m1" intent=":literal"><semantics><mrow><mrow><mfrac><mrow><mo rspace="0em">∂</mo><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>Dice</mtext></msub></mrow><mrow><mo rspace="0em">∂</mo><msub><mi>p</mi><mi>j</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mn>2</mn><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mi>I</mi><mo>−</mo><mrow><msub><mi>g</mi><mi>j</mi></msub><mo lspace="0em" rspace="0em">​</mo><mi>U</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><msup><mi>U</mi><mn>2</mn></msup></mfrac></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\frac{\partial\mathcal{L}_{\text{Dice}}}{\partial p_{j}}=\frac{2(I-g_{j}U)}{U^{2}}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(10)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.6">During early training, predictions remain small and diffuse, yielding <math alttext="I\approx 0" class="ltx_Math" display="inline" id="S4.SS2.p2.1.m1" intent=":literal"><semantics><mrow><mi>I</mi><mo>≈</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">I\approx 0</annotation></semantics></math>. In MV-3DRES, reconstructed point clouds from sparse views are large (often <math alttext="10^{6}" class="ltx_Math" display="inline" id="S4.SS2.p2.2.m2" intent=":literal"><semantics><msup><mn>10</mn><mn>6</mn></msup><annotation encoding="application/x-tex">10^{6}</annotation></semantics></math>–<math alttext="10^{7}" class="ltx_Math" display="inline" id="S4.SS2.p2.3.m3" intent=":literal"><semantics><msup><mn>10</mn><mn>7</mn></msup><annotation encoding="application/x-tex">10^{7}</annotation></semantics></math> points) while the target instance typically occupies less than <math alttext="2\%" class="ltx_Math" display="inline" id="S4.SS2.p2.4.m4" intent=":literal"><semantics><mrow><mn>2</mn><mo>%</mo></mrow><annotation encoding="application/x-tex">2\%</annotation></semantics></math> of them. Consequently, the union term <math alttext="U" class="ltx_Math" display="inline" id="S4.SS2.p2.5.m5" intent=":literal"><semantics><mi>U</mi><annotation encoding="application/x-tex">U</annotation></semantics></math> is dominated by background, inflating by several orders of magnitude. For a foreground point (<math alttext="g_{j}=1" class="ltx_Math" display="inline" id="S4.SS2.p2.6.m6" intent=":literal"><semantics><mrow><msub><mi>g</mi><mi>j</mi></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">g_{j}=1</annotation></semantics></math>),</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E11">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\left.\frac{\partial\mathcal{L}_{\text{Dice}}}{\partial p_{j}}\right|_{g_{j}=1}\approx-\frac{2}{U}," class="ltx_Math" display="block" id="S4.E11.m1" intent=":literal"><semantics><mrow><mrow><msub><mrow><mfrac><mrow><mo rspace="0em">∂</mo><msub><mi class="ltx_font_mathcaligraphic">ℒ</mi><mtext>Dice</mtext></msub></mrow><mrow><mo rspace="0em">∂</mo><msub><mi>p</mi><mi>j</mi></msub></mrow></mfrac><mo>|</mo></mrow><mrow><msub><mi>g</mi><mi>j</mi></msub><mo>=</mo><mn>1</mn></mrow></msub><mo>≈</mo><mrow><mo>−</mo><mfrac><mn>2</mn><mi>U</mi></mfrac></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\left.\frac{\partial\mathcal{L}_{\text{Dice}}}{\partial p_{j}}\right|_{g_{j}=1}\approx-\frac{2}{U},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(11)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS2.p2.9">whose magnitude becomes extremely small when <math alttext="U" class="ltx_Math" display="inline" id="S4.SS2.p2.7.m1" intent=":literal"><semantics><mi>U</mi><annotation encoding="application/x-tex">U</annotation></semantics></math> is large. Empirically, gradients fall to <math alttext="10^{-9}" class="ltx_Math" display="inline" id="S4.SS2.p2.8.m2" intent=":literal"><semantics><msup><mn>10</mn><mrow><mo>−</mo><mn>9</mn></mrow></msup><annotation encoding="application/x-tex">10^{-9}</annotation></semantics></math>–<math alttext="10^{-11}" class="ltx_Math" display="inline" id="S4.SS2.p2.9.m3" intent=":literal"><semantics><msup><mn>10</mn><mrow><mo>−</mo><mn>11</mn></mrow></msup><annotation encoding="application/x-tex">10^{-11}</annotation></semantics></math>, far below the scale needed to drive meaningful updates. Although foreground points are present, their contribution to optimization becomes negligible, leading to stalled convergence. We refer to this failure mode as <em class="ltx_emph ltx_font_italic" id="S4.SS2.p2.9.1">Foreground Gradient Dilution (FGD)</em>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Per-view No-Target Suppression Optimization</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">To mitigate FGD, we propose <span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.1">Per-view No-Target Suppression Optimization (PVSO)</span>, a view-wise supervision strategy that shifts early learning signals from sparse 3D space to the denser 2D image domain. This modification significantly reduces the imbalance between foreground and background, thereby amplifying effective gradients.</p>
</div>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Positive-aware Sampling.</h5>
<div class="ltx_para" id="S4.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS3.SSS0.Px1.p1.4">Given the view set <math alttext="\mathcal{V}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p1.1.m1" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">𝒱</mi><annotation encoding="application/x-tex">\mathcal{V}</annotation></semantics></math> of a scene, let <math alttext="\mathcal{V}_{t}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p1.2.m2" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒱</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\mathcal{V}_{t}</annotation></semantics></math> and <math alttext="\mathcal{V}_{n}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p1.3.m3" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">𝒱</mi><mi>n</mi></msub><annotation encoding="application/x-tex">\mathcal{V}_{n}</annotation></semantics></math> denote target-visible and no-target views, respectively. PVSO samples a subset <math alttext="\mathcal{V}^{\prime}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px1.p1.4.m4" intent=":literal"><semantics><msup><mi class="ltx_font_mathcaligraphic">𝒱</mi><mo>′</mo></msup><annotation encoding="application/x-tex">\mathcal{V}^{\prime}</annotation></semantics></math> while enforcing a minimum foreground-view ratio</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E12">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\rho_{t}=\frac{|\mathcal{V}_{t}|}{|\mathcal{V}^{\prime}|}," class="ltx_Math" display="block" id="S4.E12.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>ρ</mi><mi>t</mi></msub><mo>=</mo><mfrac><mrow><mo stretchy="false">|</mo><msub><mi class="ltx_font_mathcaligraphic">𝒱</mi><mi>t</mi></msub><mo stretchy="false">|</mo></mrow><mrow><mo stretchy="false">|</mo><msup><mi class="ltx_font_mathcaligraphic">𝒱</mi><mo>′</mo></msup><mo stretchy="false">|</mo></mrow></mfrac></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\rho_{t}=\frac{|\mathcal{V}_{t}|}{|\mathcal{V}^{\prime}|},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(12)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS3.SSS0.Px1.p1.5">ensuring that each batch contains sufficient positive evidence. This prevents the optimization from collapsing to trivial background predictions.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">2D Gradient Concentration.</h5>
<div class="ltx_para" id="S4.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS3.SSS0.Px2.p1.4">For each sampled view, the predicted 3D mask is projected onto the image plane, and a 2D Dice loss is applied. Since foreground regions typically occupy <math alttext="10" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px2.p1.1.m1" intent=":literal"><semantics><mn>10</mn><annotation encoding="application/x-tex">10</annotation></semantics></math>–<math alttext="15\%" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px2.p1.2.m2" intent=":literal"><semantics><mrow><mn>15</mn><mo>%</mo></mrow><annotation encoding="application/x-tex">15\%</annotation></semantics></math> of pixels in visible views—far larger than their <math alttext="&lt;2\%" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px2.p1.3.m3" intent=":literal"><semantics><mrow><mi></mi><mo>&lt;</mo><mrow><mn>2</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">&lt;2\%</annotation></semantics></math> proportion in 3D point clouds—the 2D Dice denominator <math alttext="U_{\text{2D}}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px2.p1.4.m4" intent=":literal"><semantics><msub><mi>U</mi><mtext>2D</mtext></msub><annotation encoding="application/x-tex">U_{\text{2D}}</annotation></semantics></math> becomes substantially smaller:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E13">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="U_{\text{2D}}\ll U_{\text{3D}}." class="ltx_Math" display="block" id="S4.E13.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>U</mi><mtext>2D</mtext></msub><mo>≪</mo><msub><mi>U</mi><mtext>3D</mtext></msub></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">U_{\text{2D}}\ll U_{\text{3D}}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(13)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS3.SSS0.Px2.p1.5">Given that the Dice gradient scales as <math alttext="\mathcal{O}(1/U)" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px2.p1.5.m1" intent=":literal"><semantics><mrow><mi class="ltx_font_mathcaligraphic">𝒪</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>/</mo><mi>U</mi></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{O}(1/U)</annotation></semantics></math>, the per-view supervision yields foreground gradients that are 1–3 orders of magnitude larger than in 3D. This concentrated 2D supervision strengthens early training signals.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Suppression of No-Target Views.</h5>
<div class="ltx_para" id="S4.SS3.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS3.SSS0.Px3.p1.4">No-target views often far outnumber target-visible ones. To avoid overwhelming the loss with trivial negatives, PVSO normalizes their contribution using</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E14">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="w_{s}=\frac{1}{|\mathcal{V}_{n}|}." class="ltx_Math" display="block" id="S4.E14.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>w</mi><mi>s</mi></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mo stretchy="false">|</mo><msub><mi class="ltx_font_mathcaligraphic">𝒱</mi><mi>n</mi></msub><mo stretchy="false">|</mo></mrow></mfrac></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">w_{s}=\frac{1}{|\mathcal{V}_{n}|}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(14)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS3.SSS0.Px3.p1.5">The complete PVSO objective is</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E15">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\begin{split}L_{\text{PVSO}}=\frac{1}{|\mathcal{V}_{t}|+1}\big(&amp;\sum_{i\in\mathcal{V}_{t}}L_{\text{Dice}}(m_{i},M_{i}^{\text{gt}})+\\
&amp;w_{s}\sum_{j\in\mathcal{V}_{n}}L_{\text{Dice}}(m_{j},\mathbf{0})\big),\end{split}" class="ltx_Math" display="block" id="S4.E15.m1" intent=":literal"><semantics><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt"><mtr><mtd class="ltx_align_right" columnalign="right"><mrow><msub><mi>L</mi><mtext>PVSO</mtext></msub><mo>=</mo><mfrac><mn>1</mn><mrow><mrow><mo stretchy="false">|</mo><msub><mi class="ltx_font_mathcaligraphic">𝒱</mi><mi>t</mi></msub><mo stretchy="false">|</mo></mrow><mo>+</mo><mn>1</mn></mrow></mfrac><mo maxsize="1.200em" minsize="1.200em">(</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><mrow><munder><mo movablelimits="false">∑</mo><mrow><mi>i</mi><mo>∈</mo><msub><mi class="ltx_font_mathcaligraphic">𝒱</mi><mi>t</mi></msub></mrow></munder><mrow><msub><mi>L</mi><mtext>Dice</mtext></msub><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>m</mi><mi>i</mi></msub><mo>,</mo><msubsup><mi>M</mi><mi>i</mi><mtext>gt</mtext></msubsup><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>+</mo></mrow></mtd></mtr><mtr><mtd></mtd><mtd class="ltx_align_left" columnalign="left"><mrow><msub><mi>w</mi><mi>s</mi></msub><munder><mo movablelimits="false">∑</mo><mrow><mi>j</mi><mo>∈</mo><msub><mi class="ltx_font_mathcaligraphic">𝒱</mi><mi>n</mi></msub></mrow></munder><msub><mi>L</mi><mtext>Dice</mtext></msub><mrow><mo stretchy="false">(</mo><msub><mi>m</mi><mi>j</mi></msub><mo>,</mo><mn>𝟎</mn><mo stretchy="false">)</mo></mrow><mo maxsize="1.200em" minsize="1.200em">)</mo><mo>,</mo></mrow></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{split}L_{\text{PVSO}}=\frac{1}{|\mathcal{V}_{t}|+1}\big(&amp;\sum_{i\in\mathcal{V}_{t}}L_{\text{Dice}}(m_{i},M_{i}^{\text{gt}})+\\
&amp;w_{s}\sum_{j\in\mathcal{V}_{n}}L_{\text{Dice}}(m_{j},\mathbf{0})\big),\end{split}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(15)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS3.SSS0.Px3.p1.3">where <math alttext="m_{i}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px3.p1.1.m1" intent=":literal"><semantics><msub><mi>m</mi><mi>i</mi></msub><annotation encoding="application/x-tex">m_{i}</annotation></semantics></math> is the predicted 2D mask for view <math alttext="i" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px3.p1.2.m2" intent=":literal"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math> and <math alttext="M_{i}^{\text{gt}}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px3.p1.3.m3" intent=":literal"><semantics><msubsup><mi>M</mi><mi>i</mi><mtext>gt</mtext></msubsup><annotation encoding="application/x-tex">M_{i}^{\text{gt}}</annotation></semantics></math> is the corresponding ground-truth mask (empty for no-target views).</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Joint Objective.</h5>
<div class="ltx_para" id="S4.SS3.SSS0.Px4.p1">
<p class="ltx_p" id="S4.SS3.SSS0.Px4.p1.2">PVSO complements 3D supervision by providing dense, stable gradients during early training. The complete objective is</p>
<table class="ltx_equation ltx_eqn_table" id="S4.E16">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="L_{\text{total}}=L_{\text{BCE}}+\lambda_{p}L_{\text{PVSO}}," class="ltx_Math" display="block" id="S4.E16.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>L</mi><mtext>total</mtext></msub><mo>=</mo><mrow><msub><mi>L</mi><mtext>BCE</mtext></msub><mo>+</mo><mrow><msub><mi>λ</mi><mi>p</mi></msub><mo lspace="0em" rspace="0em">​</mo><msub><mi>L</mi><mtext>PVSO</mtext></msub></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">L_{\text{total}}=L_{\text{BCE}}+\lambda_{p}L_{\text{PVSO}},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(16)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS3.SSS0.Px4.p1.1">where <math alttext="\lambda_{p}" class="ltx_Math" display="inline" id="S4.SS3.SSS0.Px4.p1.1.m1" intent=":literal"><semantics><msub><mi>λ</mi><mi>p</mi></msub><annotation encoding="application/x-tex">\lambda_{p}</annotation></semantics></math> balances 2D and 3D signals. This formulation alleviates foreground gradient dilution and yields robust multimodal 3D grounding from sparse views.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.8.1.1" style="font-size:113%;">Table 1</span>: </span><span class="ltx_text" id="S4.T1.9.2" style="font-size:113%;">Performance comparison on the MVRefer benchmark.Metrics are all mIoU under different categories.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T1.4" style="width:496.9pt;height:56.7pt;vertical-align:-26.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-19.6pt,2.2pt) scale(0.926737522694232,0.926737522694232) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.4.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.4.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T1.4.4.4.5" rowspan="2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.4.5.1" style="font-size:80%;">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="4" id="S4.T1.1.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.1" style="font-size:80%;">Hard (<math alttext="\sim" class="ltx_Math" display="inline" id="S4.T1.1.1.1.1.1.m1" intent=":literal"><semantics><mo>∼</mo><annotation encoding="application/x-tex">\sim</annotation></semantics></math>40%)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="4" id="S4.T1.2.2.2.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.2.2.2.1" style="font-size:80%;">Easy (<math alttext="\sim" class="ltx_Math" display="inline" id="S4.T1.2.2.2.2.1.m1" intent=":literal"><semantics><mo>∼</mo><annotation encoding="application/x-tex">\sim</annotation></semantics></math>60%)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="4" id="S4.T1.3.3.3.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.3.3.3.3.1" style="font-size:80%;">Unique (<math alttext="\sim" class="ltx_Math" display="inline" id="S4.T1.3.3.3.3.1.m1" intent=":literal"><semantics><mo>∼</mo><annotation encoding="application/x-tex">\sim</annotation></semantics></math>19%)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="4" id="S4.T1.4.4.4.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.4.4.1" style="font-size:80%;">Multiple (<math alttext="\sim" class="ltx_Math" display="inline" id="S4.T1.4.4.4.4.1.m1" intent=":literal"><semantics><mo>∼</mo><annotation encoding="application/x-tex">\sim</annotation></semantics></math>81%)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4" id="S4.T1.4.4.4.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.4.6.1" style="font-size:80%;">Overall</span></th>
</tr>
<tr class="ltx_tr" id="S4.T1.4.4.5.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.4.4.5.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.5.1.1.1" style="font-size:80%;">global</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.4.4.5.1.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.5.1.2.1" style="font-size:80%;">view</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.4.4.5.1.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.5.1.3.1" style="font-size:80%;">pos</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T1.4.4.5.1.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.5.1.4.1" style="font-size:80%;">neg</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.4.4.5.1.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.5.1.5.1" style="font-size:80%;">global</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.4.4.5.1.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.5.1.6.1" style="font-size:80%;">view</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.4.4.5.1.7" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.5.1.7.1" style="font-size:80%;">pos</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T1.4.4.5.1.8" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.5.1.8.1" style="font-size:80%;">neg</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.4.4.5.1.9" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.5.1.9.1" style="font-size:80%;">global</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.4.4.5.1.10" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.5.1.10.1" style="font-size:80%;">view</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.4.4.5.1.11" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.5.1.11.1" style="font-size:80%;">pos</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T1.4.4.5.1.12" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.5.1.12.1" style="font-size:80%;">neg</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.4.4.5.1.13" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.5.1.13.1" style="font-size:80%;">global</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.4.4.5.1.14" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.5.1.14.1" style="font-size:80%;">view</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.4.4.5.1.15" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.5.1.15.1" style="font-size:80%;">pos</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S4.T1.4.4.5.1.16" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.5.1.16.1" style="font-size:80%;">neg</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.4.4.5.1.17" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.5.1.17.1" style="font-size:80%;">global</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.4.4.5.1.18" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.5.1.18.1" style="font-size:80%;">view</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.4.4.5.1.19" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.5.1.19.1" style="font-size:80%;">pos</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S4.T1.4.4.5.1.20" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.5.1.20.1" style="font-size:80%;">neg</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.4.4.6.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.4.4.6.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.6.1.1.1" style="font-size:80%;">two-stage</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.6.1.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.6.1.2.1" style="font-size:80%;">8.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.6.1.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.6.1.3.1" style="font-size:80%;">8.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.6.1.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.6.1.4.1" style="font-size:80%;">22.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.4.4.6.1.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.6.1.5.1" style="font-size:80%;">10.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.6.1.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.6.1.6.1" style="font-size:80%;">25.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.6.1.7" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.6.1.7.1" style="font-size:80%;">28.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.6.1.8" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.6.1.8.1" style="font-size:80%;">45.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.4.4.6.1.9" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.6.1.9.1" style="font-size:80%;">21.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.6.1.10" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.6.1.10.1" style="font-size:80%;">27.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.6.1.11" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.6.1.11.1" style="font-size:80%;">41.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.6.1.12" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.6.1.12.1" style="font-size:80%;">52.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.4.4.6.1.13" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.6.1.13.1" style="font-size:80%;">30.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.6.1.14" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.6.1.14.1" style="font-size:80%;">16.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.6.1.15" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.6.1.15.1" style="font-size:80%;">15.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.6.1.16" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.6.1.16.1" style="font-size:80%;">32.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.4.4.6.1.17" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.6.1.17.1" style="font-size:80%;">13.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.6.1.18" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.6.1.18.1" style="font-size:80%;">18.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.6.1.19" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.6.1.19.1" style="font-size:80%;">20.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.6.1.20" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.6.1.20.1" style="font-size:80%;">35.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.6.1.21" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.6.1.21.1" style="font-size:80%;">16.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.4.7.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.4.4.7.2.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.7.2.1.1" style="font-size:80%;">2D-Lift</span></th>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.7.2.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.7.2.2.1" style="font-size:80%;">6.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.7.2.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.7.2.3.1" style="font-size:80%;">15.0</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.7.2.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.7.2.4.1" style="font-size:80%;">25.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.4.4.7.2.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.7.2.5.1" style="font-size:80%;">11.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.7.2.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.7.2.6.1" style="font-size:80%;">25.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.7.2.7" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.7.2.7.1" style="font-size:80%;">24.1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.7.2.8" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.7.2.8.1" style="font-size:80%;">45.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.4.4.7.2.9" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.7.2.9.1" style="font-size:80%;">12.3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.7.2.10" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.7.2.10.1" style="font-size:80%;">31.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.7.2.11" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.7.2.11.1" style="font-size:80%;">30.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.7.2.12" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.7.2.12.1" style="font-size:80%;">47.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.4.4.7.2.13" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.7.2.13.1" style="font-size:80%;">19.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.7.2.14" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.7.2.14.1" style="font-size:80%;">14.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.7.2.15" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.7.2.15.1" style="font-size:80%;">17.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.7.2.16" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.7.2.16.1" style="font-size:80%;">34.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.4.4.7.2.17" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.7.2.17.1" style="font-size:80%;">10.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.7.2.18" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.7.2.18.1" style="font-size:80%;">17.8</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.7.2.19" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.7.2.19.1" style="font-size:80%;">20.4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.7.2.20" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.7.2.20.1" style="font-size:80%;">37.2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.4.7.2.21" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S4.T1.4.4.7.2.21.1" style="font-size:80%;">12.1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.4.8.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T1.4.4.8.3.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.8.3.1.1" style="font-size:80%;">Ours</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.4.8.3.2" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.8.3.2.1" style="font-size:80%;">24.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.4.8.3.3" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.8.3.3.1" style="font-size:80%;">67.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.4.8.3.4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.8.3.4.1" style="font-size:80%;">31.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T1.4.4.8.3.5" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.8.3.5.1" style="font-size:80%;">78.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.4.8.3.6" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.8.3.6.1" style="font-size:80%;">50.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.4.8.3.7" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.8.3.7.1" style="font-size:80%;">70.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.4.8.3.8" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.8.3.8.1" style="font-size:80%;">52.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T1.4.4.8.3.9" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.8.3.9.1" style="font-size:80%;">81.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.4.8.3.10" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.8.3.10.1" style="font-size:80%;">65.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.4.8.3.11" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.8.3.11.1" style="font-size:80%;">82.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.4.8.3.12" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.8.3.12.1" style="font-size:80%;">64.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T1.4.4.8.3.13" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.8.3.13.1" style="font-size:80%;">90.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.4.8.3.14" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.8.3.14.1" style="font-size:80%;">33.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.4.8.3.15" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.8.3.15.1" style="font-size:80%;">66.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.4.8.3.16" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.8.3.16.1" style="font-size:80%;">39.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T1.4.4.8.3.17" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.8.3.17.1" style="font-size:80%;">77.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.4.8.3.18" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.8.3.18.1" style="font-size:80%;">39.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.4.8.3.19" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.8.3.19.1" style="font-size:80%;">69.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.4.8.3.20" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.8.3.20.1" style="font-size:80%;">44.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.4.8.3.21" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.8.3.21.1" style="font-size:80%;">79.9</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.6.1.1" style="font-size:113%;">Table 2</span>: </span><span class="ltx_text" id="S4.T2.7.2" style="font-size:113%;">The results of traditional 3D-RES and MV-3DRES tasks under original ScanRefer setting.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.2" style="width:433.6pt;height:173.8pt;vertical-align:-84.9pt;"><span class="ltx_transformed_inner" style="transform:translate(4.3pt,-1.7pt) scale(1.02004660070195,1.02004660070195) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.2.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.2.2.2">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T2.2.2.2.3" rowspan="2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.2.3.1" style="font-size:80%;">Method</span></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="S4.T2.1.1.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.1" style="font-size:80%;">Unique (<math alttext="\sim" class="ltx_Math" display="inline" id="S4.T2.1.1.1.1.1.m1" intent=":literal"><semantics><mo>∼</mo><annotation encoding="application/x-tex">\sim</annotation></semantics></math>19%)</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="S4.T2.2.2.2.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.2.2.1" style="font-size:80%;">Multiple (<math alttext="\sim" class="ltx_Math" display="inline" id="S4.T2.2.2.2.2.1.m1" intent=":literal"><semantics><mo>∼</mo><annotation encoding="application/x-tex">\sim</annotation></semantics></math>81%)</span></td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" colspan="3" id="S4.T2.2.2.2.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.2.4.1" style="font-size:80%;">Overall</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.3.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.3.1.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.3.1.1.1" style="font-size:80%;">Acc@25</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.3.1.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.3.1.2.1" style="font-size:80%;">Acc@50</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.2.2.3.1.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.3.1.3.1" style="font-size:80%;">mIoU</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.3.1.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.3.1.4.1" style="font-size:80%;">Acc@25</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.3.1.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.3.1.5.1" style="font-size:80%;">Acc@50</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.2.2.3.1.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.3.1.6.1" style="font-size:80%;">mIoU</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.3.1.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.3.1.7.1" style="font-size:80%;">Acc@25</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.3.1.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.3.1.8.1" style="font-size:80%;">Acc@50</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.3.1.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.3.1.9.1" style="font-size:80%;">mIoU</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.4.2">
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row ltx_border_tt" colspan="10" id="S4.T2.2.2.4.2.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_italic" id="S4.T2.2.2.4.2.1.1" style="font-size:80%;">Traditional 3D-RES (with ground-truth point clouds)</span></th>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.5.3">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.2.2.5.3.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text" id="S4.T2.2.2.5.3.1.1" style="font-size:80%;">TGNN </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.2.2.5.3.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib27" title="Text-guided graph neural networks for referring 3d instance segmentation">17</a><span class="ltx_text" id="S4.T2.2.2.5.3.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.2.2.5.3.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.5.3.2.1" style="font-size:80%;">69.3</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.2.2.5.3.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.5.3.3.1" style="font-size:80%;">57.8</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.5.3.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.5.3.4.1" style="font-size:80%;">50.7</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.2.2.5.3.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.5.3.5.1" style="font-size:80%;">31.2</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.2.2.5.3.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.5.3.6.1" style="font-size:80%;">26.6</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.5.3.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.5.3.7.1" style="font-size:80%;">23.6</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.2.2.5.3.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.5.3.8.1" style="font-size:80%;">38.6</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.2.2.5.3.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.5.3.9.1" style="font-size:80%;">32.7</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.2.2.5.3.10" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.5.3.10.1" style="font-size:80%;">28.8</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.6.4">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.2.2.6.4.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text" id="S4.T2.2.2.6.4.1.1" style="font-size:80%;">3D-STMN </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.2.2.6.4.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib35" title="3d-stmn: dependency-driven superpoint-text matching network for end-to-end 3d referring expression segmentation">48</a><span class="ltx_text" id="S4.T2.2.2.6.4.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.6.4.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.6.4.2.1" style="font-size:80%;">89.3</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.6.4.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.6.4.3.1" style="font-size:80%;">84.0</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.2.2.6.4.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.6.4.4.1" style="font-size:80%;">74.5</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.6.4.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.6.4.5.1" style="font-size:80%;">46.2</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.6.4.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.6.4.6.1" style="font-size:80%;">29.2</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.2.2.6.4.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.6.4.7.1" style="font-size:80%;">31.1</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.6.4.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.6.4.8.1" style="font-size:80%;">54.6</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.6.4.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.6.4.9.1" style="font-size:80%;">39.8</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.6.4.10" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.6.4.10.1" style="font-size:80%;">39.5</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.7.5">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.2.2.7.5.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text" id="S4.T2.2.2.7.5.1.1" style="font-size:80%;">SegPoint </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.2.2.7.5.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib30" title="Segpoint: segment any point cloud via large language model">14</a><span class="ltx_text" id="S4.T2.2.2.7.5.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.7.5.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.7.5.2.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.7.5.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.7.5.3.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.2.2.7.5.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.7.5.4.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.7.5.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.7.5.5.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.7.5.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.7.5.6.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.2.2.7.5.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.7.5.7.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.7.5.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.7.5.8.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.7.5.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.7.5.9.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.7.5.10" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.7.5.10.1" style="font-size:80%;">41.7</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.8.6">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.2.2.8.6.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text" id="S4.T2.2.2.8.6.1.1" style="font-size:80%;">Reason3D </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.2.2.8.6.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib96" title="Reason3d: searching and reasoning 3d segmentation via large language model">16</a><span class="ltx_text" id="S4.T2.2.2.8.6.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.8.6.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.8.6.2.1" style="font-size:80%;">88.4</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.8.6.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.8.6.3.1" style="font-size:80%;">84.2</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.2.2.8.6.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.8.6.4.1" style="font-size:80%;">74.6</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.8.6.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.8.6.5.1" style="font-size:80%;">50.5</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.8.6.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.8.6.6.1" style="font-size:80%;">31.7</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.2.2.8.6.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.8.6.7.1" style="font-size:80%;">34.1</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.8.6.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.8.6.8.1" style="font-size:80%;">57.9</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.8.6.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.8.6.9.1" style="font-size:80%;">41.9</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.8.6.10" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.8.6.10.1" style="font-size:80%;">42.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.9.7">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.2.2.9.7.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text" id="S4.T2.2.2.9.7.1.1" style="font-size:80%;">RG-SAN </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.2.2.9.7.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib2" title="Rg-san: rule-guided spatial awareness network for end-to-end 3d referring expression segmentation">46</a><span class="ltx_text" id="S4.T2.2.2.9.7.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.9.7.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.9.7.2.1" style="font-size:80%;">89.2</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.9.7.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.9.7.3.1" style="font-size:80%;">84.3</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.2.2.9.7.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.9.7.4.1" style="font-size:80%;">74.5</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.9.7.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.9.7.5.1" style="font-size:80%;">55.0</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.9.7.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.9.7.6.1" style="font-size:80%;">35.4</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.2.2.9.7.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.9.7.7.1" style="font-size:80%;">37.4</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.9.7.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.9.7.8.1" style="font-size:80%;">61.7</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.9.7.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.9.7.9.1" style="font-size:80%;">44.9</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.9.7.10" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.9.7.10.1" style="font-size:80%;">44.6</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.10.8">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.2.2.10.8.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text" id="S4.T2.2.2.10.8.1.1" style="font-size:80%;">LESS </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.2.2.10.8.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib32" title="LESS: label-efficient and single-stage referring 3d segmentation">27</a><span class="ltx_text" id="S4.T2.2.2.10.8.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.10.8.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.10.8.2.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.10.8.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.10.8.3.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.2.2.10.8.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.10.8.4.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.10.8.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.10.8.5.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.10.8.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.10.8.6.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.2.2.10.8.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.10.8.7.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.10.8.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.10.8.8.1" style="font-size:80%;">53.2</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.10.8.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.10.8.9.1" style="font-size:80%;">29.9</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.10.8.10" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.10.8.10.1" style="font-size:80%;">33.7</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.11.9">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.2.2.11.9.1" style="padding-left:1.0pt;padding-right:1.0pt;">
<span class="ltx_text" id="S4.T2.2.2.11.9.1.1" style="font-size:80%;">3D-LLaVA </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.2.2.11.9.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib95" title="3d-llava: towards generalist 3d lmms with omni superpoint transformer">6</a><span class="ltx_text" id="S4.T2.2.2.11.9.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.11.9.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.11.9.2.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.11.9.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.11.9.3.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.2.2.11.9.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.11.9.4.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.11.9.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.11.9.5.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.11.9.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.11.9.6.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.2.2.11.9.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.11.9.7.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.11.9.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.11.9.8.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.11.9.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.11.9.9.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.11.9.10" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.11.9.10.1" style="font-size:80%;">43.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.12.10">
<th class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_row ltx_border_tt" colspan="10" id="S4.T2.2.2.12.10.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_italic" id="S4.T2.2.2.12.10.1.1" style="font-size:80%;">MV-3DRES (sparse RGB inputs only)</span></th>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.13.11">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.2.2.13.11.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.13.11.1.1" style="font-size:80%;">two-stage</span></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.2.2.13.11.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.13.11.2.1" style="font-size:80%;">43.8</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.2.2.13.11.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.13.11.3.1" style="font-size:80%;">20.9</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.13.11.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.13.11.4.1" style="font-size:80%;">27.5</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.2.2.13.11.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.13.11.5.1" style="font-size:80%;">25.1</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.2.2.13.11.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.13.11.6.1" style="font-size:80%;">12.2</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.13.11.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.13.11.7.1" style="font-size:80%;">16.4</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.2.2.13.11.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.13.11.8.1" style="font-size:80%;">28.7</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.2.2.13.11.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.13.11.9.1" style="font-size:80%;">13.9</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.2.2.13.11.10" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.13.11.10.1" style="font-size:80%;">18.5</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.14.12">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.2.2.14.12.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.14.12.1.1" style="font-size:80%;">2D-Lift</span></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.14.12.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.14.12.2.1" style="font-size:80%;">51.6</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.14.12.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.14.12.3.1" style="font-size:80%;">26.0</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.2.2.14.12.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.14.12.4.1" style="font-size:80%;">31.5</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.14.12.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.14.12.5.1" style="font-size:80%;">21.5</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.14.12.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.14.12.6.1" style="font-size:80%;">5.6</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.2.2.14.12.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.14.12.7.1" style="font-size:80%;">14.5</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.14.12.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.14.12.8.1" style="font-size:80%;">27.3</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.14.12.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.14.12.9.1" style="font-size:80%;">9.6</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.2.2.14.12.10" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text" id="S4.T2.2.2.14.12.10.1" style="font-size:80%;">17.8</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2.15.13">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T2.2.2.15.13.1" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.15.13.1.1" style="font-size:80%;">Ours</span></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T2.2.2.15.13.2" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.15.13.2.1" style="font-size:80%;">83.6</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T2.2.2.15.13.3" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.15.13.3.1" style="font-size:80%;">74.5</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r" id="S4.T2.2.2.15.13.4" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.15.13.4.1" style="font-size:80%;">65.2</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T2.2.2.15.13.5" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.15.13.5.1" style="font-size:80%;">49.2</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T2.2.2.15.13.6" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.15.13.6.1" style="font-size:80%;">33.5</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r" id="S4.T2.2.2.15.13.7" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.15.13.7.1" style="font-size:80%;">33.8</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T2.2.2.15.13.8" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.15.13.8.1" style="font-size:80%;">55.9</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T2.2.2.15.13.9" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.15.13.9.1" style="font-size:80%;">41.5</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T2.2.2.15.13.10" style="padding-left:1.0pt;padding-right:1.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.2.2.15.13.10.1" style="font-size:80%;">39.9</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Implementation Details and Setup</h3>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">MVGGT Configuration.</h5>
<div class="ltx_para" id="S5.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px1.p1.4">We adopt the Pi3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib89" title="Pi3: permutation-equivariant visual geometry learning">44</a>]</cite> reconstruction backbone (36 blocks), which remains frozen throughout training. We use a frozen Roberta model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib111" title="Roberta: a robustly optimized bert pretraining approach">28</a>]</cite> as the language encoder. The multimodal branch contains <math alttext="L_{\text{multi}}=12" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px1.p1.1.m1" intent=":literal"><semantics><mrow><msub><mi>L</mi><mtext>multi</mtext></msub><mo>=</mo><mn>12</mn></mrow><annotation encoding="application/x-tex">L_{\text{multi}}=12</annotation></semantics></math> blocks and is optimized end-to-end. Training uses AdamW <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib1" title="Decoupled weight decay regularization">29</a>]</cite> with a <math alttext="1\!\times\!10^{-4}" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px1.p1.2.m2" intent=":literal"><semantics><mrow><mn>1</mn><mo lspace="0.052em" rspace="0.052em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup></mrow><annotation encoding="application/x-tex">1\!\times\!10^{-4}</annotation></semantics></math> learning rate, batch size of 16, and 30 epochs on a single NVIDIA 4090 GPU. The PVSO weight <math alttext="\lambda_{p}" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px1.p1.3.m3" intent=":literal"><semantics><msub><mi>λ</mi><mi>p</mi></msub><annotation encoding="application/x-tex">\lambda_{p}</annotation></semantics></math> is fixed to <math alttext="1" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px1.p1.4.m4" intent=":literal"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Dataset and Metrics.</h5>
<div class="ltx_para" id="S5.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px2.p1.1">All evaluations are conducted on the proposed <span class="ltx_text ltx_font_bold" id="S5.SS1.SSS0.Px2.p1.1.1">MVRefer</span> benchmark. We follow standard ScanRefer train/validation splits <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib16" title="Scanrefer: 3d object localization in rgb-d scans using natural language">3</a>]</cite>. Alongside <math alttext="\text{mIoU}_{\text{global}}" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px2.p1.1.m1" intent=":literal"><semantics><msub><mtext>mIoU</mtext><mtext>global</mtext></msub><annotation encoding="application/x-tex">\text{mIoU}_{\text{global}}</annotation></semantics></math>, we report all diagnostic view-level metrics from Section <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S3.SS2.SSS2" title="3.2.2 Evaluation Metrics and Splits ‣ 3.2 The MVRefer Benchmark ‣ 3 MV-3DRES Task and MVRefer Benchmark ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">3.2.2</span></a>, with special focus on the <span class="ltx_text ltx_font_bold" id="S5.SS1.SSS0.Px2.p1.1.2">Hard</span> and <span class="ltx_text ltx_font_bold" id="S5.SS1.SSS0.Px2.p1.1.3">Easy</span> subsets, which most directly reflect the foreground sparsity challenge underlying FGD.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Main Results</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We compare MVGGT with two representative MV-3DRES baselines:
(1) <span class="ltx_text ltx_font_bold" id="S5.SS2.p1.1.1">2D-Lift</span>, which lifts ReferDINO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib124" title="Referdino: referring video object segmentation with visual grounding foundations">25</a>]</cite> masks into 3D via Pi3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib89" title="Pi3: permutation-equivariant visual geometry learning">44</a>]</cite>;
(2) <span class="ltx_text ltx_font_bold" id="S5.SS2.p1.1.2">two-stage</span>, which runs Pi3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib89" title="Pi3: permutation-equivariant visual geometry learning">44</a>]</cite> reconstruction followed by LESS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#bib.bib32" title="LESS: label-efficient and single-stage referring 3d segmentation">27</a>]</cite> for referring segmentation.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S4.T1" title="Table 1 ‣ Joint Objective. ‣ 4.3 Per-view No-Target Suppression Optimization ‣ 4 Method ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">1</span></a> shows that MVGGT consistently outperforms all baselines across difficulty levels. On the <span class="ltx_text ltx_font_bold" id="S5.SS2.p2.1.1">Hard</span> split, it achieves 24.4 global mIoU—gains of 16.3 and 18.0 over two-stage and 2D-Lift—and improves view mIoU by 52.3 over two-stage. The <span class="ltx_text ltx_font_bold" id="S5.SS2.p2.1.2">Easy</span> split follows the same pattern, reaching 50.1 global mIoU and 70.6 view mIoU. Over the full benchmark, MVGGT attains 39.9 global mIoU and 69.3 view mIoU, exceeding the strongest baseline by 22.1 and 48.9.
These gains across all subsets confirm MVGGT’s robustness to sparse evidence and ambiguity, and highlight PVSO’s role in overcoming FGD.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S4.T2" title="Table 2 ‣ Joint Objective. ‣ 4.3 Per-view No-Target Suppression Optimization ‣ 4 Method ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">2</span></a> compares MVGGT with traditional 3D-RES and recent MV-3DRES baselines under the standard ScanRefer protocol. Despite using only sparse RGB inputs, MVGGT achieves 83.6 Acc@25, 74.5 Acc@50, and 65.2 mIoU on the <em class="ltx_emph ltx_font_italic" id="S5.SS2.p3.1.1">Unique</em> split—substantially narrowing the gap with full 3D-resourced methods. The improvement is even more pronounced in the <em class="ltx_emph ltx_font_italic" id="S5.SS2.p3.1.2">Multiple</em> split, where MVGGT reaches 33.8 mIoU, exceeding the two-stage and 2D-Lift baselines by 17.4 and 19.3. These results underscore the capability of MVGGT to perform reliable 3D grounding without access to ground-truth point clouds.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Ablation Studies</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">We conduct comprehensive ablation studies to validate the effectiveness of each proposed component. All experiments are performed on the MVRefer benchmark.</p>
</div>
<section class="ltx_paragraph" id="S5.SS3.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Impact of Core Components.</h5>
<div class="ltx_para" id="S5.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS3.SSS0.Px1.p1.13">Table <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S5.T3" title="Table 3 ‣ Impact of Core Components. ‣ 5.3 Ablation Studies ‣ 5 Experiments ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">3</span></a> contrasts the complete model with partial variants. When neither MVGGT nor PVSO is used, performance drops sharply, reflecting the difficulty of grounding language with sparse and noisy geometry alone. Introducing PVSO optimization yields a clear improvement, raising overall <math alttext="\mathrm{mIoU}_{\text{global}}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px1.p1.1.m1" intent=":literal"><semantics><msub><mi>mIoU</mi><mtext>global</mtext></msub><annotation encoding="application/x-tex">\mathrm{mIoU}_{\text{global}}</annotation></semantics></math> from <math alttext="26.9" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px1.p1.2.m2" intent=":literal"><semantics><mn>26.9</mn><annotation encoding="application/x-tex">26.9</annotation></semantics></math> to <math alttext="32.0" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px1.p1.3.m3" intent=":literal"><semantics><mn>32.0</mn><annotation encoding="application/x-tex">32.0</annotation></semantics></math> and <math alttext="\mathrm{mIoU}_{\text{view}}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px1.p1.4.m4" intent=":literal"><semantics><msub><mi>mIoU</mi><mtext>view</mtext></msub><annotation encoding="application/x-tex">\mathrm{mIoU}_{\text{view}}</annotation></semantics></math> from <math alttext="41.1" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px1.p1.5.m5" intent=":literal"><semantics><mn>41.1</mn><annotation encoding="application/x-tex">41.1</annotation></semantics></math> to <math alttext="47.5" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px1.p1.6.m6" intent=":literal"><semantics><mn>47.5</mn><annotation encoding="application/x-tex">47.5</annotation></semantics></math>, indicating that rebalancing per-view gradients effectively addresses the Foreground Gradient Dilution problem and materially enhances the stability of sparse-view learning. MVGGT also increases stability, particularly on hard scenes (from <math alttext="12.9" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px1.p1.7.m7" intent=":literal"><semantics><mn>12.9</mn><annotation encoding="application/x-tex">12.9</annotation></semantics></math> to <math alttext="19.0" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px1.p1.8.m8" intent=":literal"><semantics><mn>19.0</mn><annotation encoding="application/x-tex">19.0</annotation></semantics></math> <math alttext="\mathrm{mIoU}_{\text{global}}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px1.p1.9.m9" intent=":literal"><semantics><msub><mi>mIoU</mi><mtext>global</mtext></msub><annotation encoding="application/x-tex">\mathrm{mIoU}_{\text{global}}</annotation></semantics></math>), showing that integrating language into geometric reasoning efficiently guides sparse-view aggregation. The full model combines both advantages, achieving <math alttext="39.9" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px1.p1.10.m10" intent=":literal"><semantics><mn>39.9</mn><annotation encoding="application/x-tex">39.9</annotation></semantics></math> overall <math alttext="\mathrm{mIoU}_{\text{global}}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px1.p1.11.m11" intent=":literal"><semantics><msub><mi>mIoU</mi><mtext>global</mtext></msub><annotation encoding="application/x-tex">\mathrm{mIoU}_{\text{global}}</annotation></semantics></math> and <math alttext="69.3" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px1.p1.12.m12" intent=":literal"><semantics><mn>69.3</mn><annotation encoding="application/x-tex">69.3</annotation></semantics></math> <math alttext="\mathrm{mIoU}_{\text{view}}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px1.p1.13.m13" intent=":literal"><semantics><msub><mi>mIoU</mi><mtext>view</mtext></msub><annotation encoding="application/x-tex">\mathrm{mIoU}_{\text{view}}</annotation></semantics></math>. This synergy reinforces our premise that effective MV-3DRES relies on both geometry-aware multimodal design and an optimization strategy resilient to sparse, uneven supervision.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T3.17.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="S5.T3.18.2" style="font-size:90%;">Ablation studies on the core components.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T3.15" style="width:433.6pt;height:72.7pt;vertical-align:-33.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-2.8pt,0.5pt) scale(0.98732214181182,0.98732214181182) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T3.15.15">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.3.3.3">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T3.3.3.3.4"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T3.3.3.3.5"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="2" id="S5.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.1.1">Easy <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.1.1.1.1.1.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="2" id="S5.T3.2.2.2.2"><span class="ltx_text ltx_font_bold" id="S5.T3.2.2.2.2.1">Hard <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.2.2.2.2.1.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T3.3.3.3.3"><span class="ltx_text ltx_font_bold" id="S5.T3.3.3.3.3.1">Overall <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T3.3.3.3.3.1.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math></span></th>
</tr>
<tr class="ltx_tr" id="S5.T3.9.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S5.T3.9.9.9.7"><span class="ltx_text ltx_font_bold" id="S5.T3.9.9.9.7.1">PVSO</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r" id="S5.T3.9.9.9.8"><span class="ltx_text ltx_font_bold" id="S5.T3.9.9.9.8.1">MVGGT</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.4.4.4.1"><math alttext="\mathrm{mIoU}_{\text{global}}" class="ltx_Math" display="inline" id="S5.T3.4.4.4.1.m1" intent=":literal"><semantics><msub><mi>mIoU</mi><mtext>global</mtext></msub><annotation encoding="application/x-tex">\mathrm{mIoU}_{\text{global}}</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T3.5.5.5.2"><math alttext="\mathrm{mIoU}_{\text{view}}" class="ltx_Math" display="inline" id="S5.T3.5.5.5.2.m1" intent=":literal"><semantics><msub><mi>mIoU</mi><mtext>view</mtext></msub><annotation encoding="application/x-tex">\mathrm{mIoU}_{\text{view}}</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.6.6.6.3"><math alttext="\mathrm{mIoU}_{\text{global}}" class="ltx_Math" display="inline" id="S5.T3.6.6.6.3.m1" intent=":literal"><semantics><msub><mi>mIoU</mi><mtext>global</mtext></msub><annotation encoding="application/x-tex">\mathrm{mIoU}_{\text{global}}</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T3.7.7.7.4"><math alttext="\mathrm{mIoU}_{\text{view}}" class="ltx_Math" display="inline" id="S5.T3.7.7.7.4.m1" intent=":literal"><semantics><msub><mi>mIoU</mi><mtext>view</mtext></msub><annotation encoding="application/x-tex">\mathrm{mIoU}_{\text{view}}</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.8.8.8.5"><math alttext="\mathrm{mIoU}_{\text{global}}" class="ltx_Math" display="inline" id="S5.T3.8.8.8.5.m1" intent=":literal"><semantics><msub><mi>mIoU</mi><mtext>global</mtext></msub><annotation encoding="application/x-tex">\mathrm{mIoU}_{\text{global}}</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.9.9.9.6"><math alttext="\mathrm{mIoU}_{\text{view}}" class="ltx_Math" display="inline" id="S5.T3.9.9.9.6.m1" intent=":literal"><semantics><msub><mi>mIoU</mi><mtext>view</mtext></msub><annotation encoding="application/x-tex">\mathrm{mIoU}_{\text{view}}</annotation></semantics></math></th>
</tr>
<tr class="ltx_tr" id="S5.T3.15.15.16.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" colspan="2" id="S5.T3.15.15.16.1.1">2D-Lift</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.15.15.16.1.2">25.4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T3.15.15.16.1.3">24.1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.15.15.16.1.4">6.4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T3.15.15.16.1.5">15.0</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.15.15.16.1.6">17.8</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.15.15.16.1.7">20.4</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.11.11.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T3.10.10.10.1"><math alttext="\times" class="ltx_Math" display="inline" id="S5.T3.10.10.10.1.m1" intent=":literal"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.11.11.11.2"><math alttext="\times" class="ltx_Math" display="inline" id="S5.T3.11.11.11.2.m1" intent=":literal"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.11.11.3">36.3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.11.11.11.4">43.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.11.11.5">12.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.11.11.11.6">38.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.11.11.7">26.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.11.11.11.8">41.1</td>
</tr>
<tr class="ltx_tr" id="S5.T3.13.13.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T3.12.12.12.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T3.12.12.12.1.m1" intent=":literal"><semantics><mi mathvariant="normal">✓</mi><annotation encoding="application/x-tex">\checkmark</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T3.13.13.13.2"><math alttext="\times" class="ltx_Math" display="inline" id="S5.T3.13.13.13.2.m1" intent=":literal"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center" id="S5.T3.13.13.13.3">40.7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.13.13.13.4">48.9</td>
<td class="ltx_td ltx_align_center" id="S5.T3.13.13.13.5">19.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.13.13.13.6">45.4</td>
<td class="ltx_td ltx_align_center" id="S5.T3.13.13.13.7">32.0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.13.13.13.8">47.5</td>
</tr>
<tr class="ltx_tr" id="S5.T3.15.15.15">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S5.T3.14.14.14.1"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T3.14.14.14.1.m1" intent=":literal"><semantics><mi mathvariant="normal">✓</mi><annotation encoding="application/x-tex">\checkmark</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T3.15.15.15.2"><math alttext="\checkmark" class="ltx_Math" display="inline" id="S5.T3.15.15.15.2.m1" intent=":literal"><semantics><mi mathvariant="normal">✓</mi><annotation encoding="application/x-tex">\checkmark</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.15.15.15.3"><span class="ltx_text ltx_font_bold" id="S5.T3.15.15.15.3.1">50.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T3.15.15.15.4"><span class="ltx_text ltx_font_bold" id="S5.T3.15.15.15.4.1">70.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.15.15.15.5"><span class="ltx_text ltx_font_bold" id="S5.T3.15.15.15.5.1">24.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T3.15.15.15.6"><span class="ltx_text ltx_font_bold" id="S5.T3.15.15.15.6.1">67.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.15.15.15.7"><span class="ltx_text ltx_font_bold" id="S5.T3.15.15.15.7.1">39.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.15.15.15.8"><span class="ltx_text ltx_font_bold" id="S5.T3.15.15.15.8.1">69.3</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="506" id="S5.F4.g1" src="x4.png" width="951"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S5.F4.3.2" style="font-size:90%;">
Qualitative comparison on the MVRefer benchmark.
</span></figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S5.SS3.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">PVSO Component Analysis.</h5>
<div class="ltx_para" id="S5.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS3.SSS0.Px2.p1.8">Table <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S5.T4" title="Table 4 ‣ PVSO Component Analysis. ‣ 5.3 Ablation Studies ‣ 5 Experiments ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">4</span></a> further analyzes PVSO. Without no-target suppression, random view sampling produces unstable performance, especially on hard scenes. Enabling suppression consistently improves results (from <math alttext="32.4" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p1.1.m1" intent=":literal"><semantics><mn>32.4</mn><annotation encoding="application/x-tex">32.4</annotation></semantics></math> to <math alttext="36.7" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p1.2.m2" intent=":literal"><semantics><mn>36.7</mn><annotation encoding="application/x-tex">36.7</annotation></semantics></math> overall <math alttext="\mathrm{mIoU}_{\text{global}}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p1.3.m3" intent=":literal"><semantics><msub><mi>mIoU</mi><mtext>global</mtext></msub><annotation encoding="application/x-tex">\mathrm{mIoU}_{\text{global}}</annotation></semantics></math>), confirming that reducing misleading gradients from target-absent views is essential. Hybrid sampling strengthens this effect by ensuring sufficient positive evidence in each batch. A no-target ratio of <math alttext="0.5" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p1.4.m4" intent=":literal"><semantics><mn>0.5</mn><annotation encoding="application/x-tex">0.5</annotation></semantics></math> achieves the most balanced outcome, reaching <math alttext="39.9" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p1.5.m5" intent=":literal"><semantics><mn>39.9</mn><annotation encoding="application/x-tex">39.9</annotation></semantics></math> <math alttext="\mathrm{mIoU}_{\text{global}}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p1.6.m6" intent=":literal"><semantics><msub><mi>mIoU</mi><mtext>global</mtext></msub><annotation encoding="application/x-tex">\mathrm{mIoU}_{\text{global}}</annotation></semantics></math> and <math alttext="69.3" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p1.7.m7" intent=":literal"><semantics><mn>69.3</mn><annotation encoding="application/x-tex">69.3</annotation></semantics></math> <math alttext="\mathrm{mIoU}_{\text{view}}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px2.p1.8.m8" intent=":literal"><semantics><msub><mi>mIoU</mi><mtext>view</mtext></msub><annotation encoding="application/x-tex">\mathrm{mIoU}_{\text{view}}</annotation></semantics></math>. Ratios that are too low underexploit discriminative negative pairs, while overly high ratios drown out positives, leading to degraded training dynamics. These patterns illustrate the core intuition behind PVSO: stable sparse-view learning emerges when each view contributes information in proportion to its actual visibility of the target.</p>
</div>
<figure class="ltx_table" id="S5.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T4.12.1.1" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text" id="S5.T4.13.2" style="font-size:90%;">Ablation on PVSO components.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T4.10" style="width:433.6pt;height:79.2pt;vertical-align:-37.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-49.3pt,9.0pt) scale(0.814890067614347,0.814890067614347) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T4.10.10">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T4.3.3.3.4"><span class="ltx_text ltx_font_bold" id="S5.T4.3.3.3.4.1">Sampling</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.3.3.3.5"><span class="ltx_text ltx_font_bold" id="S5.T4.3.3.3.5.1">No-Target</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T4.3.3.3.6"><span class="ltx_text ltx_font_bold" id="S5.T4.3.3.3.6.1">No-Target</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="2" id="S5.T4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.1.1">Easy <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T4.1.1.1.1.1.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="2" id="S5.T4.2.2.2.2"><span class="ltx_text ltx_font_bold" id="S5.T4.2.2.2.2.1">Hard <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T4.2.2.2.2.1.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T4.3.3.3.3"><span class="ltx_text ltx_font_bold" id="S5.T4.3.3.3.3.1">Overall <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T4.3.3.3.3.1.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math></span></th>
</tr>
<tr class="ltx_tr" id="S5.T4.9.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S5.T4.9.9.9.7"><span class="ltx_text ltx_font_bold" id="S5.T4.9.9.9.7.1">Strategy</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T4.9.9.9.8"><span class="ltx_text ltx_font_bold" id="S5.T4.9.9.9.8.1">Ratio</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T4.9.9.9.9"><span class="ltx_text ltx_font_bold" id="S5.T4.9.9.9.9.1">Suppression</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T4.4.4.4.1"><math alttext="\mathrm{mIoU}_{\text{global}}" class="ltx_Math" display="inline" id="S5.T4.4.4.4.1.m1" intent=":literal"><semantics><msub><mi>mIoU</mi><mtext>global</mtext></msub><annotation encoding="application/x-tex">\mathrm{mIoU}_{\text{global}}</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T4.5.5.5.2"><math alttext="\mathrm{mIoU}_{\text{view}}" class="ltx_Math" display="inline" id="S5.T4.5.5.5.2.m1" intent=":literal"><semantics><msub><mi>mIoU</mi><mtext>view</mtext></msub><annotation encoding="application/x-tex">\mathrm{mIoU}_{\text{view}}</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T4.6.6.6.3"><math alttext="\mathrm{mIoU}_{\text{global}}" class="ltx_Math" display="inline" id="S5.T4.6.6.6.3.m1" intent=":literal"><semantics><msub><mi>mIoU</mi><mtext>global</mtext></msub><annotation encoding="application/x-tex">\mathrm{mIoU}_{\text{global}}</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T4.7.7.7.4"><math alttext="\mathrm{mIoU}_{\text{view}}" class="ltx_Math" display="inline" id="S5.T4.7.7.7.4.m1" intent=":literal"><semantics><msub><mi>mIoU</mi><mtext>view</mtext></msub><annotation encoding="application/x-tex">\mathrm{mIoU}_{\text{view}}</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T4.8.8.8.5"><math alttext="\mathrm{mIoU}_{\text{global}}" class="ltx_Math" display="inline" id="S5.T4.8.8.8.5.m1" intent=":literal"><semantics><msub><mi>mIoU</mi><mtext>global</mtext></msub><annotation encoding="application/x-tex">\mathrm{mIoU}_{\text{global}}</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T4.9.9.9.6"><math alttext="\mathrm{mIoU}_{\text{view}}" class="ltx_Math" display="inline" id="S5.T4.9.9.9.6.m1" intent=":literal"><semantics><msub><mi>mIoU</mi><mtext>view</mtext></msub><annotation encoding="application/x-tex">\mathrm{mIoU}_{\text{view}}</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.10.10.10">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.10.10.10.2">Random</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.10.10.10.3">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.10.10.10.1"><math alttext="\times" class="ltx_Math" display="inline" id="S5.T4.10.10.10.1.m1" intent=":literal"><semantics><mo>×</mo><annotation encoding="application/x-tex">\times</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.10.10.10.4">40.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.10.10.10.5">65.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.10.10.10.6">19.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.10.10.10.7">66.51</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.10.10.10.8">32.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.10.10.10.9">65.6</td>
</tr>
<tr class="ltx_tr" id="S5.T4.10.10.11.1">
<td class="ltx_td ltx_align_left" id="S5.T4.10.10.11.1.1">Random</td>
<td class="ltx_td ltx_align_center" id="S5.T4.10.10.11.1.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.10.10.11.1.3">✓</td>
<td class="ltx_td ltx_align_center" id="S5.T4.10.10.11.1.4">46.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.10.10.11.1.5">55.8</td>
<td class="ltx_td ltx_align_center" id="S5.T4.10.10.11.1.6">22.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.10.10.11.1.7">49.3</td>
<td class="ltx_td ltx_align_center" id="S5.T4.10.10.11.1.8">36.7</td>
<td class="ltx_td ltx_align_center" id="S5.T4.10.10.11.1.9">53.2</td>
</tr>
<tr class="ltx_tr" id="S5.T4.10.10.12.2">
<td class="ltx_td ltx_align_left" id="S5.T4.10.10.12.2.1">Hybrid</td>
<td class="ltx_td ltx_align_center" id="S5.T4.10.10.12.2.2">0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.10.10.12.2.3">✓</td>
<td class="ltx_td ltx_align_center" id="S5.T4.10.10.12.2.4">42.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.10.10.12.2.5">24.3</td>
<td class="ltx_td ltx_align_center" id="S5.T4.10.10.12.2.6">11.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.10.10.12.2.7">10.3</td>
<td class="ltx_td ltx_align_center" id="S5.T4.10.10.12.2.8">29.9</td>
<td class="ltx_td ltx_align_center" id="S5.T4.10.10.12.2.9">18.7</td>
</tr>
<tr class="ltx_tr" id="S5.T4.10.10.13.3">
<td class="ltx_td ltx_align_left" id="S5.T4.10.10.13.3.1">Hybrid</td>
<td class="ltx_td ltx_align_center" id="S5.T4.10.10.13.3.2">0.25</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.10.10.13.3.3">✓</td>
<td class="ltx_td ltx_align_center" id="S5.T4.10.10.13.3.4">46.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.10.10.13.3.5">65.2</td>
<td class="ltx_td ltx_align_center" id="S5.T4.10.10.13.3.6">19.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.10.10.13.3.7">59.6</td>
<td class="ltx_td ltx_align_center" id="S5.T4.10.10.13.3.8">35.7</td>
<td class="ltx_td ltx_align_center" id="S5.T4.10.10.13.3.9">63.0</td>
</tr>
<tr class="ltx_tr" id="S5.T4.10.10.14.4">
<td class="ltx_td ltx_align_left" id="S5.T4.10.10.14.4.1">Hybrid</td>
<td class="ltx_td ltx_align_center" id="S5.T4.10.10.14.4.2">0.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.10.10.14.4.3">✓</td>
<td class="ltx_td ltx_align_center" id="S5.T4.10.10.14.4.4"><span class="ltx_text ltx_font_bold" id="S5.T4.10.10.14.4.4.1">50.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.10.10.14.4.5"><span class="ltx_text ltx_font_bold" id="S5.T4.10.10.14.4.5.1">70.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.10.10.14.4.6"><span class="ltx_text ltx_font_bold" id="S5.T4.10.10.14.4.6.1">24.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.10.10.14.4.7"><span class="ltx_text ltx_font_bold" id="S5.T4.10.10.14.4.7.1">67.3</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.10.10.14.4.8"><span class="ltx_text ltx_font_bold" id="S5.T4.10.10.14.4.8.1">39.9</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.10.10.14.4.9"><span class="ltx_text ltx_font_bold" id="S5.T4.10.10.14.4.9.1">69.3</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.10.10.15.5">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T4.10.10.15.5.1">Hybrid</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.10.10.15.5.2">0.75</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T4.10.10.15.5.3">✓</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.10.10.15.5.4">30.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T4.10.10.15.5.5">64.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.10.10.15.5.6">18.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T4.10.10.15.5.7">72.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.10.10.15.5.8">25.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.10.10.15.5.9">67.2</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_paragraph" id="S5.SS3.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">MVGGT Fusion Architecture.</h5>
<div class="ltx_para" id="S5.SS3.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS3.SSS0.Px3.p1.4">Table <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S5.T5" title="Table 5 ‣ MVGGT Fusion Architecture. ‣ 5.3 Ablation Studies ‣ 5 Experiments ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">5</span></a> evaluates where multimodal fusion is best positioned within the encoder. Early fusion performs the weakest, suggesting that injecting language before geometric evidence has formed can disrupt structural reasoning. Middle fusion offers a modest improvement, but late fusion yields the strongest results (<math alttext="39.9" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px3.p1.1.m1" intent=":literal"><semantics><mn>39.9</mn><annotation encoding="application/x-tex">39.9</annotation></semantics></math> overall <math alttext="\mathrm{mIoU}_{\text{global}}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px3.p1.2.m2" intent=":literal"><semantics><msub><mi>mIoU</mi><mtext>global</mtext></msub><annotation encoding="application/x-tex">\mathrm{mIoU}_{\text{global}}</annotation></semantics></math>; <math alttext="69.3" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px3.p1.3.m3" intent=":literal"><semantics><mn>69.3</mn><annotation encoding="application/x-tex">69.3</annotation></semantics></math> <math alttext="\mathrm{mIoU}_{\text{view}}" class="ltx_Math" display="inline" id="S5.SS3.SSS0.Px3.p1.4.m4" intent=":literal"><semantics><msub><mi>mIoU</mi><mtext>view</mtext></msub><annotation encoding="application/x-tex">\mathrm{mIoU}_{\text{view}}</annotation></semantics></math>). This trend suggests that spatial perception should be established first, with language guiding later refinement rather than early feature alignment. Such late fusion yields more stable and discriminative cross-view representations.</p>
</div>
<figure class="ltx_table" id="S5.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T5.11.1.1" style="font-size:90%;">Table 5</span>: </span><span class="ltx_text" id="S5.T5.12.2" style="font-size:90%;">Ablation on MVGGT fusion stage.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T5.9" style="width:433.6pt;height:64.1pt;vertical-align:-29.4pt;"><span class="ltx_transformed_inner" style="transform:translate(9.8pt,-1.4pt) scale(1.04721081285851,1.04721081285851) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T5.9.9">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T5.3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T5.3.3.3.4" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T5.3.3.3.4.1">Fusion Stage</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="2" id="S5.T5.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.1.1">Easy <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T5.1.1.1.1.1.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="2" id="S5.T5.2.2.2.2"><span class="ltx_text ltx_font_bold" id="S5.T5.2.2.2.2.1">Hard <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T5.2.2.2.2.1.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T5.3.3.3.3"><span class="ltx_text ltx_font_bold" id="S5.T5.3.3.3.3.1">Overall <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T5.3.3.3.3.1.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math></span></th>
</tr>
<tr class="ltx_tr" id="S5.T5.9.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T5.4.4.4.1"><math alttext="\mathrm{mIoU}_{\text{global}}" class="ltx_Math" display="inline" id="S5.T5.4.4.4.1.m1" intent=":literal"><semantics><msub><mi>mIoU</mi><mtext>global</mtext></msub><annotation encoding="application/x-tex">\mathrm{mIoU}_{\text{global}}</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T5.5.5.5.2"><math alttext="\mathrm{mIoU}_{\text{view}}" class="ltx_Math" display="inline" id="S5.T5.5.5.5.2.m1" intent=":literal"><semantics><msub><mi>mIoU</mi><mtext>view</mtext></msub><annotation encoding="application/x-tex">\mathrm{mIoU}_{\text{view}}</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T5.6.6.6.3"><math alttext="\mathrm{mIoU}_{\text{global}}" class="ltx_Math" display="inline" id="S5.T5.6.6.6.3.m1" intent=":literal"><semantics><msub><mi>mIoU</mi><mtext>global</mtext></msub><annotation encoding="application/x-tex">\mathrm{mIoU}_{\text{global}}</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T5.7.7.7.4"><math alttext="\mathrm{mIoU}_{\text{view}}" class="ltx_Math" display="inline" id="S5.T5.7.7.7.4.m1" intent=":literal"><semantics><msub><mi>mIoU</mi><mtext>view</mtext></msub><annotation encoding="application/x-tex">\mathrm{mIoU}_{\text{view}}</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T5.8.8.8.5"><math alttext="\mathrm{mIoU}_{\text{global}}" class="ltx_Math" display="inline" id="S5.T5.8.8.8.5.m1" intent=":literal"><semantics><msub><mi>mIoU</mi><mtext>global</mtext></msub><annotation encoding="application/x-tex">\mathrm{mIoU}_{\text{global}}</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T5.9.9.9.6"><math alttext="\mathrm{mIoU}_{\text{view}}" class="ltx_Math" display="inline" id="S5.T5.9.9.9.6.m1" intent=":literal"><semantics><msub><mi>mIoU</mi><mtext>view</mtext></msub><annotation encoding="application/x-tex">\mathrm{mIoU}_{\text{view}}</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.9.9.10.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T5.9.9.10.1.1">Early</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.9.9.10.1.2">45.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.9.9.10.1.3">65.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.9.9.10.1.4">21.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.9.9.10.1.5">63.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.9.9.10.1.6">36.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.9.9.10.1.7">64.9</td>
</tr>
<tr class="ltx_tr" id="S5.T5.9.9.11.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T5.9.9.11.2.1">Middle</th>
<td class="ltx_td ltx_align_center" id="S5.T5.9.9.11.2.2">47.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.9.9.11.2.3">65.7</td>
<td class="ltx_td ltx_align_center" id="S5.T5.9.9.11.2.4">22.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.9.9.11.2.5">62.1</td>
<td class="ltx_td ltx_align_center" id="S5.T5.9.9.11.2.6">37.5</td>
<td class="ltx_td ltx_align_center" id="S5.T5.9.9.11.2.7">64.3</td>
</tr>
<tr class="ltx_tr" id="S5.T5.9.9.12.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T5.9.9.12.3.1">Late</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.9.9.12.3.2"><span class="ltx_text ltx_font_bold" id="S5.T5.9.9.12.3.2.1">50.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T5.9.9.12.3.3"><span class="ltx_text ltx_font_bold" id="S5.T5.9.9.12.3.3.1">70.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.9.9.12.3.4"><span class="ltx_text ltx_font_bold" id="S5.T5.9.9.12.3.4.1">24.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T5.9.9.12.3.5"><span class="ltx_text ltx_font_bold" id="S5.T5.9.9.12.3.5.1">67.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.9.9.12.3.6"><span class="ltx_text ltx_font_bold" id="S5.T5.9.9.12.3.6.1">39.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.9.9.12.3.7"><span class="ltx_text ltx_font_bold" id="S5.T5.9.9.12.3.7.1">69.3</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_paragraph" id="S5.SS3.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Multimodal Branch Depth.</h5>
<div class="ltx_para" id="S5.SS3.SSS0.Px4.p1">
<p class="ltx_p" id="S5.SS3.SSS0.Px4.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S5.T6" title="Table 6 ‣ Multimodal Branch Depth. ‣ 5.3 Ablation Studies ‣ 5 Experiments ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">6</span></a> studies the depth of the multimodal branch. A shallow configuration (6 layers) struggles on complex scenes, indicating insufficient capacity for cross-view alignment. Increasing to 12 layers delivers consistent improvements across easy and hard subsets, achieving the best overall accuracy. Expanding further to 16 layers causes a sharp decline, pointing to overfitting and unstable attention patterns under sparse supervision. The results reveal a practical design insight: moderate multimodal depth offers the right balance between expressiveness and regularity, enabling language cues to guide view aggregation without overwhelming the geometric signal.</p>
</div>
<figure class="ltx_table" id="S5.T6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T6.11.1.1" style="font-size:90%;">Table 6</span>: </span><span class="ltx_text" id="S5.T6.12.2" style="font-size:90%;">Ablation on layer number of multimodal branch.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T6.9" style="width:433.6pt;height:69.3pt;vertical-align:-31.8pt;"><span class="ltx_transformed_inner" style="transform:translate(25.5pt,-4.1pt) scale(1.13302375354397,1.13302375354397) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T6.9.9">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T6.3.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T6.3.3.3.4" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T6.3.3.3.4.1">Layers</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="2" id="S5.T6.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T6.1.1.1.1.1">Easy <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T6.1.1.1.1.1.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="2" id="S5.T6.2.2.2.2"><span class="ltx_text ltx_font_bold" id="S5.T6.2.2.2.2.1">Hard <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T6.2.2.2.2.1.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T6.3.3.3.3"><span class="ltx_text ltx_font_bold" id="S5.T6.3.3.3.3.1">Overall <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T6.3.3.3.3.1.m1" intent=":literal"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math></span></th>
</tr>
<tr class="ltx_tr" id="S5.T6.9.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T6.4.4.4.1"><math alttext="\mathrm{mIoU}_{\text{global}}" class="ltx_Math" display="inline" id="S5.T6.4.4.4.1.m1" intent=":literal"><semantics><msub><mi>mIoU</mi><mtext>global</mtext></msub><annotation encoding="application/x-tex">\mathrm{mIoU}_{\text{global}}</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T6.5.5.5.2"><math alttext="\mathrm{mIoU}_{\text{view}}" class="ltx_Math" display="inline" id="S5.T6.5.5.5.2.m1" intent=":literal"><semantics><msub><mi>mIoU</mi><mtext>view</mtext></msub><annotation encoding="application/x-tex">\mathrm{mIoU}_{\text{view}}</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T6.6.6.6.3"><math alttext="\mathrm{mIoU}_{\text{global}}" class="ltx_Math" display="inline" id="S5.T6.6.6.6.3.m1" intent=":literal"><semantics><msub><mi>mIoU</mi><mtext>global</mtext></msub><annotation encoding="application/x-tex">\mathrm{mIoU}_{\text{global}}</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T6.7.7.7.4"><math alttext="\mathrm{mIoU}_{\text{view}}" class="ltx_Math" display="inline" id="S5.T6.7.7.7.4.m1" intent=":literal"><semantics><msub><mi>mIoU</mi><mtext>view</mtext></msub><annotation encoding="application/x-tex">\mathrm{mIoU}_{\text{view}}</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T6.8.8.8.5"><math alttext="\mathrm{mIoU}_{\text{global}}" class="ltx_Math" display="inline" id="S5.T6.8.8.8.5.m1" intent=":literal"><semantics><msub><mi>mIoU</mi><mtext>global</mtext></msub><annotation encoding="application/x-tex">\mathrm{mIoU}_{\text{global}}</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T6.9.9.9.6"><math alttext="\mathrm{mIoU}_{\text{view}}" class="ltx_Math" display="inline" id="S5.T6.9.9.9.6.m1" intent=":literal"><semantics><msub><mi>mIoU</mi><mtext>view</mtext></msub><annotation encoding="application/x-tex">\mathrm{mIoU}_{\text{view}}</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T6.9.9.10.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T6.9.9.10.1.1">6</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.9.9.10.1.2">47.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.9.9.10.1.3">66.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.9.9.10.1.4">22.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.9.9.10.1.5">64.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.9.9.10.1.6">37.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.9.9.10.1.7">65.7</td>
</tr>
<tr class="ltx_tr" id="S5.T6.9.9.11.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T6.9.9.11.2.1">12</th>
<td class="ltx_td ltx_align_center" id="S5.T6.9.9.11.2.2"><span class="ltx_text ltx_font_bold" id="S5.T6.9.9.11.2.2.1">50.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.9.9.11.2.3"><span class="ltx_text ltx_font_bold" id="S5.T6.9.9.11.2.3.1">70.6</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.9.9.11.2.4"><span class="ltx_text ltx_font_bold" id="S5.T6.9.9.11.2.4.1">24.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.9.9.11.2.5"><span class="ltx_text ltx_font_bold" id="S5.T6.9.9.11.2.5.1">67.3</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.9.9.11.2.6"><span class="ltx_text ltx_font_bold" id="S5.T6.9.9.11.2.6.1">39.9</span></td>
<td class="ltx_td ltx_align_center" id="S5.T6.9.9.11.2.7"><span class="ltx_text ltx_font_bold" id="S5.T6.9.9.11.2.7.1">69.3</span></td>
</tr>
<tr class="ltx_tr" id="S5.T6.9.9.12.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T6.9.9.12.3.1">16</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.9.9.12.3.2">35.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T6.9.9.12.3.3">26.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.9.9.12.3.4">10.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T6.9.9.12.3.5">14.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.9.9.12.3.6">25.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.9.9.12.3.7">21.4</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Qualitative Analysis</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S3.F3" title="Figure 3 ‣ Difficulty Splits. ‣ 3.2.2 Evaluation Metrics and Splits ‣ 3.2 The MVRefer Benchmark ‣ 3 MV-3DRES Task and MVRefer Benchmark ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">3</span></a> highlights MVGGT’s ability to maintain coherent 3D grounding under sparse and noisy views. While 2D lifting baselines frequently drift to nearby structures or collapse under occlusion and depth ambiguity, MVGGT produces stable segmentations that follow the intended targets across diverse conditions.</p>
</div>
<div class="ltx_para" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.1">In example (a), the baseline confuses a thin whiteboard with adjacent planar surfaces, whereas MVGGT localizes the correct wall-aligned region by combining geometric cues with linguistic context. In example (b), MVGGT isolates the document organizer within a cluttered shelf, supported by PVSO’s balanced per-view supervision. Example (c) contains severe depth noise; MVGGT still recovers the toilet bowl, enabled by late-stage multimodal fusion that refines geometry with textual cues. In example (d), the method distinguishes a narrow curtain from a visually similar wall segment. Finally, in example (e), MVGGT identifies the coffee table despite heavy clutter and partial visibility, demonstrating robust cross-view consistency.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We introduced MV-3DRES, a new setting that aligns 3D language grounding with the sparse and view-limited conditions encountered by real-world agents. Conventional two-stage pipelines degrade severely under such sparsity, motivating MVGGT, a dual-branch architecture that integrates linguistic cues directly into sparse-view geometric reasoning. We further identified the Foreground Gradient Dilution problem and addressed it with Per-view No-target Suppression Optimization strategy, enabling stable and efficient training under extreme sparsity. Together with the MVRefer benchmark and the MVGGT model, this work establishes a unified framework for multimodal 3D grounding and paves a practical path toward more capable embodied perception systems.


<span class="ltx_text" id="S6.p1.1.1" style="font-size:90%;"></span></p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist" id="bib.L1">
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib17">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. Achlioptas, A. Abdelreheem, F. Xia, M. Elhoseiny, and L. Guibas</span><span class="ltx_text ltx_bib_year"> (2020)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Referit3d: neural listeners for fine-grained 3d object identification in real-world scenes</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">ECCV</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S1.p1.1" title="1 Introduction ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1" title="2.1 Traditional 3D Referring Segmentation ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib103">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Cabon, L. Stoffl, L. Antsfeld, G. Csurka, B. Chidlovskii, J. Revaud, and V. Leroy</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Must3r: multi-view network for stereo 3d reconstruction</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the Computer Vision and Pattern Recognition Conference</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 1050–1060</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS2.p1.1" title="2.2 Multi-View Feed-forward Reconstruction ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib16">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. Z. Chen, A. X. Chang, and M. Nießner</span><span class="ltx_text ltx_bib_year"> (2020)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Scanrefer: 3d object localization in rgb-d scans using natural language</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">ECCV</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S1.p1.1" title="1 Introduction ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1" title="2.1 Traditional 3D Referring Segmentation ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S3.SS2.SSS1.p1.1" title="3.2.1 Benchmark Setting ‣ 3.2 The MVRefer Benchmark ‣ 3 MV-3DRES Task and MVRefer Benchmark ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§3.2.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S3.SS2.p1.1" title="3.2 The MVRefer Benchmark ‣ 3 MV-3DRES Task and MVRefer Benchmark ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§3.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S5.SS1.SSS0.Px2.p1.1" title="Dataset and Metrics. ‣ 5.1 Implementation Details and Setup ‣ 5 Experiments ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§5.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib39">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner</span><span class="ltx_text ltx_bib_year"> (2017)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Scannet: richly-annotated 3d reconstructions of indoor scenes</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">CVPR</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S3.SS2.SSS1.p1.1" title="3.2.1 Benchmark Setting ‣ 3.2 The MVRefer Benchmark ‣ 3 MV-3DRES Task and MVRefer Benchmark ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§3.2.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S3.SS2.p1.1" title="3.2 The MVRefer Benchmark ‣ 3 MV-3DRES Task and MVRefer Benchmark ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§3.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib94">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Dai, M. Nießner, M. Zollhöfer, S. Izadi, and C. Theobalt</span><span class="ltx_text ltx_bib_year"> (2017)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Bundlefusion: real-time globally consistent 3d reconstruction using on-the-fly surface reintegration</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">ACM Transactions on Graphics (ToG)</span> <span class="ltx_text ltx_bib_volume">36</span> (<span class="ltx_text ltx_bib_number">4</span>), <span class="ltx_text ltx_bib_pages"> pp. 1</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S1.p1.1" title="1 Introduction ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib95">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Deng, T. He, L. Jiang, T. Wang, F. Dayoub, and I. Reid</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">3d-llava: towards generalist 3d lmms with omni superpoint transformer</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the Computer Vision and Pattern Recognition Conference</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 3772–3782</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S4.T2.2.2.11.9.1" title="In Joint Objective. ‣ 4.3 Per-view No-Target Suppression Optimization ‣ 4 Method ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">Table 2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib113">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. Deng, Z. Ti, J. Xu, J. Yang, and J. Xie</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">VGGT-long: chunk it, loop it, align it–pushing vggt’s limits on kilometer-scale long rgb sequences</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2507.16443</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS2.p2.1" title="2.2 Multi-View Feed-forward Reconstruction ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib97">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">B. P. Duisterhof, L. Zust, P. Weinzaepfel, V. Leroy, Y. Cabon, and J. Revaud</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Mast3r-sfm: a fully-integrated solution for unconstrained structure-from-motion</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">2025 International Conference on 3D Vision (3DV)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 1–10</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS2.p1.1" title="2.2 Multi-View Feed-forward Reconstruction ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib98">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Elflein, Q. Zhou, and L. Leal-Taixé</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Light3R-sfm: towards feed-forward structure-from-motion</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the Computer Vision and Pattern Recognition Conference</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 16774–16784</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS2.p1.1" title="2.2 Multi-View Feed-forward Reconstruction ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib114">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">H. Fei, S. Wu, H. Zhang, T. Chua, and S. Yan</span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Vitron: a unified pixel-level vision llm for understanding, generating, segmenting, editing</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Advances in neural information processing systems</span> <span class="ltx_text ltx_bib_volume">37</span>, <span class="ltx_text ltx_bib_pages"> pp. 57207–57239</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1" title="2.1 Traditional 3D Referring Segmentation ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib41">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Feng, Z. Li, Q. Li, L. Zhang, X. Zhang, G. Zhu, H. Zhang, Y. Wang, and A. Mian</span><span class="ltx_text ltx_bib_year"> (2021)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Free-form description guided 3d visual graph network for object grounding in point cloud</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">ICCV</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1" title="2.1 Traditional 3D Referring Segmentation ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib117">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">X. Ge, F. Chen, J. M. Jose, Z. Ji, Z. Wu, and X. Liu</span><span class="ltx_text ltx_bib_year"> (2021)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Structured multi-modal feature embedding and alignment for image-sentence retrieval</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 29th ACM international conference on multimedia</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 5185–5193</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1" title="2.1 Traditional 3D Referring Segmentation ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib115">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Gong, L. Huang, and L. Chen</span><span class="ltx_text ltx_bib_year"> (2022)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Person re-identification method based on color attack and joint defence</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 4313–4322</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1" title="2.1 Traditional 3D Referring Segmentation ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib30">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. He, H. Ding, X. Jiang, and B. Wen</span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Segpoint: segment any point cloud via large language model</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">ECCV</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1" title="2.1 Traditional 3D Referring Segmentation ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S4.T2.2.2.7.5.1" title="In Joint Objective. ‣ 4.3 Per-view No-Target Suppression Optimization ‣ 4 Method ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">Table 2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib28">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. He and H. Ding</span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">RefMask3D: language-guided transformer for 3d referring segmentation</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">ACM MM</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1" title="2.1 Traditional 3D Referring Segmentation ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib96">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. Huang, X. Li, L. Qi, S. Yan, and M. Yang</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Reason3d: searching and reasoning 3d segmentation via large language model</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">International Conference on 3D Vision 2025</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S4.T2.2.2.8.6.1" title="In Joint Objective. ‣ 4.3 Per-view No-Target Suppression Optimization ‣ 4 Method ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">Table 2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib27">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. Huang, H. Lee, H. Chen, and T. Liu</span><span class="ltx_text ltx_bib_year"> (2021)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Text-guided graph neural networks for referring 3d instance segmentation</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">AAAI</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S1.p1.1" title="1 Introduction ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1" title="2.1 Traditional 3D Referring Segmentation ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S4.T2.2.2.5.3.1" title="In Joint Objective. ‣ 4.3 Per-view No-Target Suppression Optimization ‣ 4 Method ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">Table 2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib112">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Huang, Y. Chen, J. Jia, and L. Wang</span><span class="ltx_text ltx_bib_year"> (2022)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Multi-view transformer for 3d visual grounding</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 15524–15533</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1" title="2.1 Traditional 3D Referring Segmentation ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib110">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Jain, J. Li, M. T. Chiu, A. Hassani, N. Orlov, and H. Shi</span><span class="ltx_text ltx_bib_year"> (2023)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Oneformer: one transformer to rule universal image segmentation</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 2989–2998</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1" title="2.1 Traditional 3D Referring Segmentation ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib90">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">N. Keetha, N. Müller, J. Schönberger, L. Porzi, Y. Zhang, T. Fischer, A. Knapitsch, D. Zauss, E. Weber, N. Antunes, <span class="ltx_text ltx_bib_etal">et al.</span></span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">MapAnything: universal feed-forward metric 3d reconstruction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2509.13414</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS2.p2.1" title="2.2 Multi-View Feed-forward Reconstruction ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib121">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Kim, C. Chu, and S. Kurohashi</span><span class="ltx_text ltx_bib_year"> (2022)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Flexible visual grounding</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 285–299</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1" title="2.1 Traditional 3D Referring Segmentation ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib87">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">V. Leroy, Y. Cabon, and J. Revaud</span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Grounding image matching in 3d with mast3r</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">European Conference on Computer Vision</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 71–91</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS2.p1.1" title="2.2 Multi-View Feed-forward Reconstruction ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib109">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">H. Li, J. Qu, and L. Zhang</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">OVSeg3R: learn open-vocabulary instance segmentation from 2d via 3d reconstruction</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2509.23541</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS2.p2.1" title="2.2 Multi-View Feed-forward Reconstruction ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib118">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Li, X. Wang, J. Xiao, W. Ji, and T. Chua</span><span class="ltx_text ltx_bib_year"> (2023)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Transformer-empowered invariant grounding for video question answering</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1" title="2.1 Traditional 3D Referring Segmentation ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib124">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. Liang, K. Lin, C. Tan, J. Zhang, W. Zheng, and J. Hu</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Referdino: referring video object segmentation with visual grounding foundations</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2501.14607</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S5.SS2.p1.1" title="5.2 Main Results ‣ 5 Experiments ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§5.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib29">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">H. Lin, Y. Luo, X. Zheng, L. Li, F. Chao, T. Jin, D. Luo, Y. Wang, L. Cao, and R. Ji</span><span class="ltx_text ltx_bib_year"> (2023)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A unified framework for 3d point cloud visual grounding</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv:2308.11887</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1" title="2.1 Traditional 3D Referring Segmentation ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib32">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">X. Liu, X. Xu, J. Li, Q. Zhang, X. Wang, N. Sebe, and L. Ma</span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">LESS: label-efficient and single-stage referring 3d segmentation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv:2410.13294</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1" title="2.1 Traditional 3D Referring Segmentation ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S4.T2.2.2.10.8.1" title="In Joint Objective. ‣ 4.3 Per-view No-Target Suppression Optimization ‣ 4 Method ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">Table 2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S5.SS2.p1.1" title="5.2 Main Results ‣ 5 Experiments ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§5.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib111">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov</span><span class="ltx_text ltx_bib_year"> (2019)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Roberta: a robustly optimized bert pretraining approach</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:1907.11692</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S5.SS1.SSS0.Px1.p1.4" title="MVGGT Configuration. ‣ 5.1 Implementation Details and Setup ‣ 5 Experiments ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§5.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib1">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">I. Loshchilov and F. Hutter</span><span class="ltx_text ltx_bib_year"> (2017)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Decoupled weight decay regularization</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:1711.05101</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S5.SS1.SSS0.Px1.p1.4" title="MVGGT Configuration. ‣ 5.1 Implementation Details and Setup ‣ 5 Experiments ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§5.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib44">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Luo, J. Fu, X. Kong, C. Gao, H. Ren, H. Shen, H. Xia, and S. Liu</span><span class="ltx_text ltx_bib_year"> (2022)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">3d-sps: single-stage 3d visual grounding via referred point progressive selection</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">CVPR</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1" title="2.1 Traditional 3D Referring Segmentation ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib99">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. Murai, E. Dexheimer, and A. J. Davison</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">MASt3R-slam: real-time dense slam with 3d reconstruction priors</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the Computer Vision and Pattern Recognition Conference</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 16695–16705</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS2.p1.1" title="2.2 Multi-View Feed-forward Reconstruction ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib100">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Z. Pataki, P. Sarlin, J. L. Schönberger, and M. Pollefeys</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">MP-sfm: monocular surface priors for robust structure-from-motion</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the Computer Vision and Pattern Recognition Conference</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 21891–21901</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS2.p1.1" title="2.2 Multi-View Feed-forward Reconstruction ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib122">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">C. R. Qi, O. Litany, K. He, and L. J. Guibas</span><span class="ltx_text ltx_bib_year"> (2019)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Deep hough voting for 3d object detection in point clouds</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">proceedings of the IEEE/CVF International Conference on Computer Vision</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 9277–9286</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1" title="2.1 Traditional 3D Referring Segmentation ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib31">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Z. Qian, Y. Ma, J. Ji, and X. Sun</span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">X-refseg3d: enhancing referring 3d instance segmentation via structured cross-modal graph neural networks</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">AAAI</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1" title="2.1 Traditional 3D Referring Segmentation ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib106">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Shen, Z. Zhang, Y. Qu, and L. Cao</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Fastvggt: training-free acceleration of visual geometry transformer</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2509.02560</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS2.p2.1" title="2.2 Multi-View Feed-forward Reconstruction ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib120">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Subramanian, W. Merrill, T. Darrell, M. Gardner, S. Singh, and A. Rohrbach</span><span class="ltx_text ltx_bib_year"> (2022)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Reclip: a strong zero-shot baseline for referring expression comprehension</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2204.05991</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1" title="2.1 Traditional 3D Referring Segmentation ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib108">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">G. Transformer</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">IGGT: instance-grounded geometry trans-former for semantic 3d reconstruction</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS2.p2.1" title="2.2 Multi-View Feed-forward Reconstruction ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib107">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">C. B. Wang, C. Schmidt, J. Piekenbrinck, and B. Leibe</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Faster vggt with block-sparse global attention</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2509.07120</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS2.p2.1" title="2.2 Multi-View Feed-forward Reconstruction ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib101">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">H. Wang and L. Agapito</span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">3d reconstruction with spatial memory</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2408.16061</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS2.p1.1" title="2.2 Multi-View Feed-forward Reconstruction ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib88">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Wang, M. Chen, N. Karaev, A. Vedaldi, C. Rupprecht, and D. Novotny</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Vggt: visual geometry grounded transformer</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the Computer Vision and Pattern Recognition Conference</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 5294–5306</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS2.p2.1" title="2.2 Multi-View Feed-forward Reconstruction ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib102">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Q. Wang, Y. Zhang, A. Holynski, A. A. Efros, and A. Kanazawa</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Continuous 3d perception model with persistent state</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the Computer Vision and Pattern Recognition Conference</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 10510–10522</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS2.p1.1" title="2.2 Multi-View Feed-forward Reconstruction ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib86">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Wang, V. Leroy, Y. Cabon, B. Chidlovskii, and J. Revaud</span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Dust3r: geometric 3d vision made easy</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 20697–20709</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS2.p1.1" title="2.2 Multi-View Feed-forward Reconstruction ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib104">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. R. Wang, Y. Zhao, H. Xu, S. Eppel, A. Aspuru-Guzik, F. Shkurti, and A. Garg</span><span class="ltx_text ltx_bib_year"> (2023)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Mvtrans: multi-view perception of transparent objects</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2302.11683</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS2.p2.1" title="2.2 Multi-View Feed-forward Reconstruction ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib89">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Wang, J. Zhou, H. Zhu, W. Chang, Y. Zhou, Z. Li, J. Chen, J. Pang, C. Shen, and T. He</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Pi3: permutation-equivariant visual geometry learning</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2507.13347</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS2.p2.1" title="2.2 Multi-View Feed-forward Reconstruction ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S5.SS1.SSS0.Px1.p1.4" title="MVGGT Configuration. ‣ 5.1 Implementation Details and Setup ‣ 5 Experiments ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§5.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S5.SS2.p1.1" title="5.2 Main Results ‣ 5 Experiments ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§5.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib105">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">W. WinT3R</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">WINT3R: window-based streaming recon-struction with camera token pool</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS2.p1.1" title="2.2 Multi-View Feed-forward Reconstruction ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib2">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">C. Wu, J. Ji, H. Wang, Y. Ma, Y. Huang, G. Luo, H. Fei, X. Sun, R. Ji, <span class="ltx_text ltx_bib_etal">et al.</span></span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Rg-san: rule-guided spatial awareness network for end-to-end 3d referring expression segmentation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Advances in Neural Information Processing Systems</span> <span class="ltx_text ltx_bib_volume">37</span>, <span class="ltx_text ltx_bib_pages"> pp. 110972–110999</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S1.p1.1" title="1 Introduction ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1" title="2.1 Traditional 3D Referring Segmentation ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S4.T2.2.2.9.7.1" title="In Joint Objective. ‣ 4.3 Per-view No-Target Suppression Optimization ‣ 4 Method ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">Table 2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib34">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">C. Wu, Y. Liu, J. Ji, Y. Ma, H. Wang, G. Luo, H. Ding, X. Sun, and R. Ji</span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">3d-gres: generalized 3d referring expression segmentation</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">ACM MM</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S1.p1.1" title="1 Introduction ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1" title="2.1 Traditional 3D Referring Segmentation ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib35">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">C. Wu, Y. Ma, Q. Chen, H. Wang, G. Luo, J. Ji, and X. Sun</span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">3d-stmn: dependency-driven superpoint-text matching network for end-to-end 3d referring expression segmentation</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">AAAI</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S1.p1.1" title="1 Introduction ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1" title="2.1 Traditional 3D Referring Segmentation ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S4.T2.2.2.6.4.1" title="In Joint Objective. ‣ 4.3 Per-view No-Target Suppression Optimization ‣ 4 Method ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">Table 2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib116">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Wu, H. Fei, L. Qu, W. Ji, and T. Chua</span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Next-gpt: any-to-any multimodal llm</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Forty-first International Conference on Machine Learning</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1" title="2.1 Traditional 3D Referring Segmentation ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib123">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Q. Xu, D. Wei, L. Zhao, W. Li, Z. Huang, S. Ji, and P. Liu</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">SIU3R: simultaneous scene understanding and 3d reconstruction beyond feature alignment</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2507.02705</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS2.p2.1" title="2.2 Multi-View Feed-forward Reconstruction ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib42">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Z. Yuan, X. Yan, Y. Liao, R. Zhang, S. Wang, Z. Li, and S. Cui</span><span class="ltx_text ltx_bib_year"> (2021)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Instancerefer: cooperative holistic understanding for visual grounding on point clouds through instance multi-level contextual referring</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">ICCV</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1" title="2.1 Traditional 3D Referring Segmentation ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib93">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">L. Zhang, A. Rao, and M. Agrawala</span><span class="ltx_text ltx_bib_year"> (2023)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Adding conditional control to text-to-image diffusion models</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF international conference on computer vision</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 3836–3847</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S4.SS1.SSS0.Px3.p3.3" title="Trainable Multimodal Branch. ‣ 4.1 The proposed MVGGT ‣ 4 Method ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§4.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib18">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Zhang, Z. Gong, and A. X. Chang</span><span class="ltx_text ltx_bib_year"> (2023)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Multi3drefer: grounding text description to multiple 3d objects</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">ICCV</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S1.p1.1" title="1 Introduction ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib119">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Z. Zhang, H. Yannakoudakis, X. Zhen, and E. Shutova</span><span class="ltx_text ltx_bib_year"> (2023)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">CK-transformer: commonsense knowledge enhanced transformers for referring expression comprehension</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2302.09027</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1" title="2.1 Traditional 3D Referring Segmentation ‣ 2 Related Work ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§2.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib56">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">L. Zhao, D. Cai, L. Sheng, and D. Xu</span><span class="ltx_text ltx_bib_year"> (2021)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">3dvg-transformer: relation modeling for visual grounding on point clouds</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">ICCV</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06874v1#S1.p1.1" title="1 Introduction ‣ MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sun Jan 11 11:32:56 2026 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>


<!-- ===== END HTML ===== -->

<!-- ===== ABS URL: https://arxiv.org/abs/2601.06847 | HTTP 200 ===== -->

<!DOCTYPE html>
<html lang="en">

<head>  <title>[2601.06847] MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="apple-touch-icon" sizes="180x180" href="/static/browse/0.3.4/images/icons/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/static/browse/0.3.4/images/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/static/browse/0.3.4/images/icons/favicon-16x16.png">
  <link rel="manifest" href="/static/browse/0.3.4/images/icons/site.webmanifest">
  <link rel="mask-icon" href="/static/browse/0.3.4/images/icons/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">
  <link rel="stylesheet" type="text/css" media="screen" href="/static/browse/0.3.4/css/arXiv.css?v=20241206" />
  <link rel="stylesheet" type="text/css" media="print" href="/static/browse/0.3.4/css/arXiv-print.css?v=20200611" />
  <link rel="stylesheet" type="text/css" media="screen" href="/static/browse/0.3.4/css/browse_search.css" />
  <script language="javascript" src="/static/browse/0.3.4/js/accordion.js" ></script>
  <script language="javascript" src="/static/browse/0.3.4/js/optin-modal.js?v=20250819"></script>
  
  <link rel="canonical" href="https://arxiv.org/abs/2601.06847"/>
  <meta name="description" content="Abstract page for arXiv paper 2601.06847: MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><meta property="og:type" content="website" />
<meta property="og:site_name" content="arXiv.org" />
<meta property="og:title" content="MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data" />
<meta property="og:url" content="https://arxiv.org/abs/2601.06847v1" />
<meta property="og:image" content="/static/browse/0.3.4/images/arxiv-logo-fb.png" />
<meta property="og:image:secure_url" content="/static/browse/0.3.4/images/arxiv-logo-fb.png" />
<meta property="og:image:width" content="1200" />
<meta property="og:image:height" content="700" />
<meta property="og:image:alt" content="arXiv logo"/>
<meta property="og:description" content="Vision-Language Models (VLMs) can generate convincing clinical narratives, yet frequently struggle to visually ground their statements. We posit this limitation arises from the scarcity of high-quality, large-scale clinical referring-localization pairs. To address this, we introduce MedGround, an automated pipeline that transforms segmentation resources into high-quality medical referring grounding data. Leveraging expert masks as spatial anchors, MedGround precisely derives localization targets, extracts shape and spatial cues, and guides VLMs to synthesize natural, clinically grounded queries that reflect morphology and location. To ensure data rigor, a multi-stage verification system integrates strict formatting checks, geometry- and medical-prior rules, and image-based visual judging to filter out ambiguous or visually unsupported samples. Finally, we present MedGround-35K, a novel multimodal medical dataset. Extensive experiments demonstrate that VLMs trained with MedGround-35K consistently achieve improved referring grounding performance, enhance multi-object semantic disambiguation, and exhibit strong generalization to unseen grounding settings. This work highlights MedGround as a scalable, data-driven approach to anchor medical language to verifiable visual evidence. Dataset and code will be released publicly upon acceptance."/>
<meta name="twitter:site" content="@arxiv"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="MedGround: Bridging the Evidence Gap in Medical Vision-Language..."/>
<meta name="twitter:description" content="Vision-Language Models (VLMs) can generate convincing clinical narratives, yet frequently struggle to visually ground their statements. We posit this limitation arises from the scarcity of..."/>
<meta name="twitter:image" content="https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png"/>
<meta name="twitter:image:alt" content="arXiv logo"/>
  <link rel="stylesheet" media="screen" type="text/css" href="/static/browse/0.3.4/css/tooltip.css"/><link rel="stylesheet" media="screen" type="text/css" href="https://static.arxiv.org/js/bibex-dev/bibex.css?20200709"/>  <script src="/static/browse/0.3.4/js/mathjaxToggle.min.js" type="text/javascript"></script>  <script src="//code.jquery.com/jquery-latest.min.js" type="text/javascript"></script>
  <script src="//cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js" type="text/javascript"></script>
  <script src="//cdn.jsdelivr.net/npm/dompurify@2.3.5/dist/purify.min.js"></script>
  <script src="/static/browse/0.3.4/js/toggle-labs.js?20241022" type="text/javascript"></script>
  <script src="/static/browse/0.3.4/js/cite.js" type="text/javascript"></script><meta name="citation_title" content="MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data" /><meta name="citation_author" content="Zhang, Mengmeng" /><meta name="citation_author" content="Wu, Xiaoping" /><meta name="citation_author" content="Luo, Hao" /><meta name="citation_author" content="Wang, Fan" /><meta name="citation_author" content="Lv, Yisheng" /><meta name="citation_date" content="2026/01/11" /><meta name="citation_online_date" content="2026/01/11" /><meta name="citation_pdf_url" content="https://arxiv.org/pdf/2601.06847" /><meta name="citation_arxiv_id" content="2601.06847" /><meta name="citation_abstract" content="Vision-Language Models (VLMs) can generate convincing clinical narratives, yet frequently struggle to visually ground their statements. We posit this limitation arises from the scarcity of high-quality, large-scale clinical referring-localization pairs. To address this, we introduce MedGround, an automated pipeline that transforms segmentation resources into high-quality medical referring grounding data. Leveraging expert masks as spatial anchors, MedGround precisely derives localization targets, extracts shape and spatial cues, and guides VLMs to synthesize natural, clinically grounded queries that reflect morphology and location. To ensure data rigor, a multi-stage verification system integrates strict formatting checks, geometry- and medical-prior rules, and image-based visual judging to filter out ambiguous or visually unsupported samples. Finally, we present MedGround-35K, a novel multimodal medical dataset. Extensive experiments demonstrate that VLMs trained with MedGround-35K consistently achieve improved referring grounding performance, enhance multi-object semantic disambiguation, and exhibit strong generalization to unseen grounding settings. This work highlights MedGround as a scalable, data-driven approach to anchor medical language to verifiable visual evidence. Dataset and code will be released publicly upon acceptance." />
</head>

<body  class="with-cu-identity">
  
  
  <div class="flex-wrap-footer">
    <header>
      <a href="#content" class="is-sr-only">Skip to main content</a>
      <!-- start desktop header -->
      <div class="columns is-vcentered is-hidden-mobile" id="cu-identity">
        <div class="column" id="cu-logo">
          <a href="https://www.cornell.edu/"><img src="/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" /></a>
        </div><!-- /from April 7 at 1:00 AM to May 29 at 21:40 --><!-- /from May 2 at 1:00 AM to May 5 at 9:45 AM --><div class="column" id="support-ack">
          <span id="support-ack-url">We gratefully acknowledge support from the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors.</span>
          <a href="https://info.arxiv.org/about/donate.html" class="btn-header-donate">Donate</a>
        </div>
      </div>

      <div id="header" class="is-hidden-mobile">
<a aria-hidden="true" tabindex="-1" href="/IgnoreMe"></a>
  <div class="header-breadcrumbs is-hidden-mobile">
    <a href="/"><img src="/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg" alt="arxiv logo" style="height:40px;"/></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a> <span>&gt;</span> arXiv:2601.06847
  </div>

        <div class="columns is-vcentered is-mobile" style="justify-content: flex-end;">
        </div>

          <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
     </div><!-- /end desktop header -->

      <div class="mobile-header">
        <div class="columns is-mobile">
          <div class="column logo-arxiv"><a href="https://arxiv.org/"><img src="/static/browse/0.3.4/images/arxiv-logomark-small-white.svg" alt="arXiv logo" style="height:60px;" /></a></div>
          <div class="column logo-cornell"><a href="https://www.cornell.edu/">
            <picture>
              <source media="(min-width: 501px)"
                srcset="/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg  400w"
                sizes="400w" />
              <source srcset="/static/browse/0.3.4/images/icons/cu/cornell_seal_simple_black.svg 2x" />
              <img src="/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University Logo" />
            </picture>
          </a></div>
          <div class="column nav" id="toggle-container" role="menubar">
            <button class="toggle-control"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-white"><title>open search</title><path d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></button>
            <div class="mobile-toggle-block toggle-target">
              <form class="mobile-search-form" method="GET" action="https://arxiv.org/search">
                <div class="field has-addons">
                  <input class="input" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
                  <input type="hidden" name="source" value="header">
                  <input type="hidden" name="searchtype" value="all">
                  <button class="button">GO</button>
                </div>
              </form>
            </div>

            <button class="toggle-control"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-white" role="menu"><title>open navigation menu</title><path d="M16 132h416c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163 0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"/ ></svg></button>
            <div class="mobile-toggle-block toggle-target">
              <nav class="mobile-menu" aria-labelledby="mobilemenulabel">
                <h2 id="mobilemenulabel">quick links</h2>
                <ul>
                    <li><a href="https://arxiv.org/login">Login</a></li>
                    <li><a href="https://info.arxiv.org/help">Help Pages</a></li>
                    <li><a href="https://info.arxiv.org/about">About</a></li>
                </ul>
              </nav>
            </div>
          </div>
        </div>
      </div><!-- /end mobile-header -->
    </header>

    <main>
      <div id="content">
<!--
rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
         xmlns:dc="http://purl.org/dc/elements/1.1/"
         xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
    <rdf:Description
        rdf:about="/abs/2601.06847"
        dc:identifier="/abs/2601.06847"
        dc:title="MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"
        trackback:ping="/trackback/2601.06847" />
    </rdf:RDF>
--><div id="abs-outer">

  <div class="leftcolumn">
    <div class="subheader">
      <h1>Computer Science > Computer Vision and Pattern Recognition</h1>
    </div>

    <div class="header-breadcrumbs-mobile">
      <strong>arXiv:2601.06847</strong> (cs)
    </div>
<link rel="stylesheet" type="text/css" href="/static/base/1.0.1/css/abs.css">
<div id="content-inner">
  <div id="abs">
    <div class="dateline">
  [Submitted on 11 Jan 2026]</div>
    <h1 class="title mathjax"><span class="descriptor">Title:</span>MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data</h1>
    <div class="authors"><span class="descriptor">Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+M" rel="nofollow">Mengmeng Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+X" rel="nofollow">Xiaoping Wu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Luo,+H" rel="nofollow">Hao Luo</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+F" rel="nofollow">Fan Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lv,+Y" rel="nofollow">Yisheng Lv</a></div>            <div id="download-button-info" hidden>View a PDF of the paper titled MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data, by Mengmeng Zhang and 4 other authors</div>
    <a class="mobile-submission-download" href="/pdf/2601.06847">View PDF</a>
    <a class="mobile-submission-download" href="https://arxiv.org/html/2601.06847v1">HTML (experimental)</a>



    <blockquote class="abstract mathjax">
            <span class="descriptor">Abstract:</span>Vision-Language Models (VLMs) can generate convincing clinical narratives, yet frequently struggle to visually ground their statements. We posit this limitation arises from the scarcity of high-quality, large-scale clinical referring-localization pairs. To address this, we introduce MedGround, an automated pipeline that transforms segmentation resources into high-quality medical referring grounding data. Leveraging expert masks as spatial anchors, MedGround precisely derives localization targets, extracts shape and spatial cues, and guides VLMs to synthesize natural, clinically grounded queries that reflect morphology and location. To ensure data rigor, a multi-stage verification system integrates strict formatting checks, geometry- and medical-prior rules, and image-based visual judging to filter out ambiguous or visually unsupported samples. Finally, we present MedGround-35K, a novel multimodal medical dataset. Extensive experiments demonstrate that VLMs trained with MedGround-35K consistently achieve improved referring grounding performance, enhance multi-object semantic disambiguation, and exhibit strong generalization to unseen grounding settings. This work highlights MedGround as a scalable, data-driven approach to anchor medical language to verifiable visual evidence. Dataset and code will be released publicly upon acceptance.
    </blockquote>

    <!--CONTEXT-->
    <div class="metatable">
      <table summary="Additional metadata">        <tr>
          <td class="tablecell label">Comments:</td>
          <td class="tablecell comments mathjax">18 pages, 10 figures</td>
        </tr>
<tr>
          <td class="tablecell label">Subjects:</td>
          <td class="tablecell subjects">
            <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI)</td>
        </tr>        <tr>
          <td class="tablecell label"><abbr title="Mathematical Subject Classification">MSC</abbr> classes:</td>
          <td class="tablecell msc-classes">J.1</td>
        </tr>
<tr>
          <td class="tablecell label">Cite as:</td>
          <td class="tablecell arxivid"><span class="arxivid"><a href="https://arxiv.org/abs/2601.06847">arXiv:2601.06847</a> [cs.CV]</span></td>
        </tr>
        <tr>
          <td class="tablecell label">&nbsp;</td>
          <td class="tablecell arxividv">(or <span class="arxivid">
              <a href="https://arxiv.org/abs/2601.06847v1">arXiv:2601.06847v1</a> [cs.CV]</span> for this version)
          </td>
        </tr>
        <tr>
          <td class="tablecell label">&nbsp;</td>
          <td class="tablecell arxivdoi">              <a href="https://doi.org/10.48550/arXiv.2601.06847" id="arxiv-doi-link">https://doi.org/10.48550/arXiv.2601.06847</a><div class="button-and-tooltip">
              <button class="more-info" aria-describedby="more-info-desc-1">
                <svg height="15" role="presentation" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z" class=""></path></svg>
                <span class="visually-hidden">Focus to learn more</span>
              </button>
              <!-- tooltip description -->
              <div role="tooltip" id="more-info-desc-1">
                <span class="left-corner"></span>                  arXiv-issued DOI via DataCite (pending registration)</div>
            </div>
          </td>
        </tr></table>
    </div>
  </div>
</div>
    <div class="submission-history">
      <h2>Submission history</h2> From: Mengmeng Zhang [<a href="/show-email/e66d3902/2601.06847" rel="nofollow">view email</a>]      <br/>    <strong>[v1]</strong>
        Sun, 11 Jan 2026 10:34:18 UTC (3,458 KB)<br/>
</div>
  </div>
  <!--end leftcolumn-->
<div class="extra-services">    <div class="full-text">
      <a name="other"></a>
      <span class="descriptor">Full-text links:</span>
      <h2>Access Paper:</h2>
      <ul>
  <div id="download-button-info" hidden>
View a PDF of the paper titled MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data, by Mengmeng Zhang and 4 other authors</div><li><a href="/pdf/2601.06847" aria-describedby="download-button-info" accesskey="f" class="abs-button download-pdf">View PDF</a></li><li><a href="https://arxiv.org/html/2601.06847v1" class="abs-button" id="latexml-download-link">HTML (experimental)</a></li><li><a href="/src/2601.06847" class="abs-button download-eprint">TeX Source
 </a></li></ul>
      <div class="abs-license"><a href="http://creativecommons.org/licenses/by/4.0/" title="Rights to this article" class="has_license">
          <img alt="license icon" role="presentation" src="https://arxiv.org/icons/licenses/by-4.0.png"/>
          <span>view license</span>
        </a></div>
    </div>
    <!--end full-text-->    <div class="browse">
    Current browse context: <div class="current">cs.CV</div>

  <div class="prevnext">
<span class="arrow">
      <a class="abs-button prev-url" href="/prevnext?id=2601.06847&amp;function=prev&amp;context=cs.CV"
         accesskey="p" title="previous in cs.CV (accesskey p)" rel="nofollow">&lt;&nbsp;prev</a>
    </span>
    <span class="is-hidden-mobile">&nbsp; | &nbsp;</span>    <span class="arrow">
      <a class="abs-button next-url" href="/prevnext?id=2601.06847&amp;function=next&amp;context=cs.CV" accesskey="n"
         title="next in cs.CV (accesskey n)"  rel="nofollow">next&nbsp;&gt;</a>
    </span><br/>
  </div><div class="list">
    <a class="abs-button abs-button-grey abs-button-small context-new" href="/list/cs.CV/new"  rel="nofollow">new</a>
    <span class="is-hidden-mobile"> | </span>
    <a class="abs-button abs-button-grey abs-button-small context-recent" href="/list/cs.CV/recent" rel="nofollow">recent</a>
    <span class="is-hidden-mobile"> | </span><a class="abs-button abs-button-grey abs-button-small context-id" href="/list/cs.CV/2026-01" rel="nofollow">2026-01</a>
  </div><div class="abs-switch-cat">
    Change to browse by:
    <div class="switch context-change">
        <a href="/abs/2601.06847?context=cs" rel="nofollow">cs</a><br class="is-hidden-mobile">
        <a class="subclass" href="/abs/2601.06847?context=cs.AI" rel="nofollow">cs.AI</a><br class="is-hidden-mobile">
    </div>
  </div>

    </div>
      <div class="extra-ref-cite">
        <h3>References &amp; Citations</h3>
        <ul>
          <li><a  class="abs-button abs-button-small cite-ads" href="https://ui.adsabs.harvard.edu/abs/arXiv:2601.06847">NASA ADS</a></li><li><a  class="abs-button abs-button-small cite-google-scholar" href="https://scholar.google.com/scholar_lookup?arxiv_id=2601.06847" target="_blank" rel="noopener">Google Scholar</a></li>
          <li><a  class="abs-button abs-button-small cite-semantic-scholar" href="https://api.semanticscholar.org/arXiv:2601.06847" target="_blank" rel="noopener">Semantic Scholar</a></li>
        </ul>
        <div style="clear:both;"></div>
      </div>

<div class='extra-ref-cite'>
    <span id='bib-cite-trigger' class="bib-cite-button abs-button">export BibTeX citation</span>
    <span id='bib-cite-loading' hidden='true'>Loading...</span>
</div>

<div id='bib-cite-modal' class='bib-modal' hidden='true'>
    <div class='bib-modal-content'>
        <div class='bib-modal-title'>
            <h2>BibTeX formatted citation</h2>
            <span class='bib-modal-close' >&times;</span>
        </div>
        <div>
            <textarea id='bib-cite-target' class="bib-citation-content" aria-label="loading the citation">loading...</textarea>
        </div>
        <div>
            <span>Data provided by: </span>
            <a id='bib-cite-source-api'></a>
        </div>
    </div>
</div><div class="bookmarks">
  <div><h3>Bookmark</h3></div><a class="abs-button abs-button-grey abs-button-small" href="http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2601.06847&amp;description=MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"
     title="Bookmark on BibSonomy">
    <img src="/static/browse/0.3.4/images/icons/social/bibsonomy.png"
         alt="BibSonomy logo"/>
  </a>
  <a class="abs-button abs-button-grey abs-button-small" href="https://reddit.com/submit?url=https://arxiv.org/abs/2601.06847&amp;title=MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"
     title="Bookmark on Reddit">
    <img src="/static/browse/0.3.4/images/icons/social/reddit.png"
         alt="Reddit logo"/>
  </a>
</div>  </div>
  <!--end extra-services-->
<!-- LABS AREA -->
<div id="labstabs">
  <div class="labstabs"><input type="radio" name="tabs" id="tabone"checked="checked">
    <label for="tabone">Bibliographic Tools</label>
    <div class="tab labs-display-bib">
      <h1>Bibliographic and Citation Tools</h1>
      <div class="toggle">
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input id="bibex-toggle" type="checkbox" class="lab-toggle"
                     data-script-url="/static/browse/0.3.4/bibex/bibex.js?20241202">
              <span class="slider"></span>
              <span class="is-sr-only">Bibliographic Explorer Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-bibex">Bibliographic Explorer</span> <em>(<a href="https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer">What is the Explorer?</a>)</em>
          </div>
        </div>
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="connectedpapers-toggle"
                type="checkbox"
                class="lab-toggle"
                data-script-url="/static/browse/0.3.4/js/connectedpapers.js"
                aria-labelledby="label-for-connected-papers">
              <span class="slider"></span>
              <span class="is-sr-only">Connected Papers Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-connected-papers">Connected Papers</span> <em>(<a href="https://www.connectedpapers.com/about" target="_blank">What is Connected Papers?</a>)</em>
          </div>
        </div><div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="litmaps-toggle"
                type="checkbox"
                class="lab-toggle"
                data-script-url="/static/browse/0.3.4/js/litmaps.js?20210617"
                aria-labelledby="label-for-litmaps">
              <span class="slider"></span>
              <span class="is-sr-only">Litmaps Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-litmaps">Litmaps</span> <em>(<a href="https://www.litmaps.co/" target="_blank">What is Litmaps?</a>)</em>
          </div>
        </div>
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="scite-toggle"
                type="checkbox"
                class="lab-toggle"
                data-script-url="/static/browse/0.3.4/js/scite.js?20210617"
                aria-labelledby="label-for-scite">
              <span class="slider"></span>
              <span class="is-sr-only">scite.ai Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-scite">scite Smart Citations</span> <em>(<a href="https://www.scite.ai/" target="_blank">What are Smart Citations?</a>)</em>
          </div>
        </div>
      </div>
        <div class="labs-content-placeholder labs-display" style="display: none;"></div>
        <div style="min-height: 15px" id="connectedpapers-output"></div>
        <div style="min-height: 15px" id="litmaps-open-in"></div>
        <div style="min-height: 15px" id="scite-open-in"></div>
    </div>


    <input type="radio" name="tabs" id="tabtwo">
    <label for="tabtwo">Code, Data, Media</label>
    <div class="tab">
      <h1>Code, Data and Media Associated with this Article</h1>
      <div class="toggle">
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="alphaxiv-toggle"
                data-script-url="/static/browse/0.3.4/js/alphaxiv.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-alphaxiv">
              <span class="slider"></span>
              <span class="is-sr-only">alphaXiv Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-alphaxiv">alphaXiv</span> <em>(<a href="https://alphaxiv.org/" target="_blank">What is alphaXiv?</a>)</em>
          </div>
        </div>

        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input        
                id="catalyzex-toggle"
                data-script-url="/static/browse/0.3.4/js/catalyzex.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-cx">
              <span class="slider"></span>
              <span class="is-sr-only">Links to Code Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-cx">CatalyzeX Code Finder for Papers</span> <em>(<a href="https://www.catalyzex.com" target="_blank">What is CatalyzeX?</a>)</em>
          </div>
        </div>

        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="dagshub-toggle"
                data-script-url="/static/browse/0.3.4/js/dagshub.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-dagshub">
              <span class="slider"></span>
              <span class="is-sr-only">DagsHub Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-dagshub">DagsHub</span> <em>(<a href="https://dagshub.com/" target="_blank">What is DagsHub?</a>)</em>
          </div>
        </div>
  
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="gotitpub-toggle"
                data-script-url="/static/browse/0.3.4/js/gotitpub.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-gotitpub">
              <span class="slider"></span>
              <span class="is-sr-only">GotitPub Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-gotitpub">Gotit.pub</span> <em>(<a href="http://gotit.pub/faq" target="_blank">What is GotitPub?</a>)</em>
          </div>
        </div>

        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="huggingface-toggle"
                data-script-url="/static/browse/0.3.4/js/huggingface.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-huggingface">
              <span class="slider"></span>
              <span class="is-sr-only">Huggingface Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-huggingface">Hugging Face</span> <em>(<a href="https://huggingface.co/huggingface" target="_blank">What is Huggingface?</a>)</em>
          </div>
        </div>

        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="paperwithcode-toggle"
                data-script-url="/static/browse/0.3.4/js/paperswithcode.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-pwc">
              <span class="slider"></span>
              <span class="is-sr-only">Links to Code Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-pwc">Papers with Code</span> <em>(<a href="https://paperswithcode.com/" target="_blank">What is Papers with Code?</a>)</em>
          </div>
        </div>


        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="sciencecast-toggle"
                data-script-url="/static/browse/0.3.4/js/sciencecast.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-sciencecast">
              <span class="slider"></span>
              <span class="is-sr-only">ScienceCast Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-sciencecast">ScienceCast</span> <em>(<a href="https://sciencecast.org/welcome" target="_blank">What is ScienceCast?</a>)</em>
          </div>
        </div>
      </div>

      <div id="alphaxiv-output" style="display:none"></div>
      <div id="catalyzex-output" style="display:none"></div>
      <div id="dagshub-output" style="display:none"></div>
      <div id="gotitpub-output" style="display:none"></div>
      <div id="pwc-output" style="display:none"></div>
      <div id="pwc-data-output" style="display:none"></div>
      <div id="sciencecast-output" style="display:none"></div>
      <div id="huggingface-output" style="display:none"></div>
    </div>


      <input type="radio" name="tabs" id="labstabs-demos-input">
      <label for="labstabs-demos-input" id="labstabs-demos-label">Demos</label>
      <div class="tab">
        <h1>Demos</h1>
        <div class="toggle">
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input
                  id="replicate-toggle"
                  data-script-url="/static/browse/0.3.4/js/replicate.js"
                  type="checkbox" class="lab-toggle" aria-labelledby="label-for-replicate">
                <span class="slider"></span>
                <span class="is-sr-only">Replicate Toggle</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-replicate">Replicate</span> <em>(<a href="https://replicate.com/docs/arxiv/about" target="_blank">What is Replicate?</a>)</em>
            </div>
          </div>
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input
                  id="spaces-toggle"
                  data-script-url="/static/browse/0.3.4/js/spaces.js"
                  type="checkbox" class="lab-toggle" aria-labelledby="label-for-spaces">
                <span class="slider"></span>
                <span class="is-sr-only">Spaces Toggle</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-spaces">Hugging Face Spaces</span> <em>(<a href="https://huggingface.co/docs/hub/spaces" target="_blank">What is Spaces?</a>)</em>
            </div>
          </div>
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input
                  id="txyz-toggle"
                  data-script-url="/static/browse/0.3.4/js/txyz.js"
                  type="checkbox" class="lab-toggle" aria-labelledby="label-for-txyz">
                <span class="slider"></span>
                <span class="is-sr-only">Spaces Toggle</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-txyz">TXYZ.AI</span> <em>(<a href="https://txyz.ai" target="_blank">What is TXYZ.AI?</a>)</em>
            </div>
          </div>
        </div>
        <div id="replicate-output"></div>
        <div id="spaces-output"></div>
        <div id="txyz-output"></div>
      </div>
      <input type="radio" name="tabs" id="tabfour">
      <label for="tabfour">Related Papers</label>
      <div class="tab">
        <h1>Recommenders and Search Tools</h1>
        <div class="toggle">
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input id="influenceflower-toggle"
                data-script-url="/static/browse/0.3.4/js/influenceflower.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-influenceflower">
                <span class="slider"></span>
                <span class="is-sr-only">Link to Influence Flower</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-influenceflower">Influence Flower</span> <em>(<a href="https://influencemap.cmlab.dev/" target="_blank">What are Influence Flowers?</a>)</em>
            </div>
          </div>
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input id="core-recommender-toggle" type="checkbox" class="lab-toggle" aria-labelledby="label-for-core">
                <span class="slider"></span>
                <span class="is-sr-only">Core recommender toggle</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-core">CORE Recommender</span> <em>(<a href="https://core.ac.uk/services/recommender">What is CORE?</a>)</em>
            </div>
          </div></div>
        <div id="influenceflower-output"></div>
        <div id="influenceflower-output-graph" style="display:none">
          <ul class="flower-tabs">
            <li class="active"><a class="btn tab-btn" onclick="openTab(event, 'tab-author')">Author</a></li>
            <li><a class="btn tab-btn" onclick="openTab(event, 'tab-venue')">Venue</a></li>
            <li><a class="btn tab-btn" onclick="openTab(event, 'tab-inst')">Institution</a></li>
            <li><a class="btn tab-btn" onclick="openTab(event, 'tab-topic')">Topic</a></li>
          </ul>
          <div class="flower-tab-content">
            <div class="tab-flower active" id="tab-author"><svg id="flower-graph-author"></svg></div>
            <div class="tab-flower" id="tab-venue"><svg id="flower-graph-venue"></svg></div>
            <div class="tab-flower" id="tab-inst"><svg id="flower-graph-inst"></svg></div>
            <div class="tab-flower" id="tab-topic"><svg id="flower-graph-topic"></svg></div>
          </div>
        </div>
        <div id="coreRecommenderOutput"></div>
        <div id="iarxivOutput"></div>
      </div>

      <input type="radio" name="tabs" id="tabfive">
      <label for="tabfive">
        About arXivLabs
      </label>
      <div class="tab">
        <div class="columns">
          <div class="column">
            <h1>arXivLabs: experimental projects with community collaborators</h1>
            <p>arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.</p>
            <p>Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.</p>
            <p>Have an idea for a project that will add value for arXiv's community? <a href="https://info.arxiv.org/labs/index.html"><strong>Learn more about arXivLabs</strong></a>.</p>
          </div>
          <div class="column is-narrow is-full-mobile">
            <p class="icon-labs"><svg xmlns="http://www.w3.org/2000/svg" role="presentation" viewBox="0 0 635.572 811"><path d="M175.6 676v27h-27v-27zm-54 27v27h27v-27zm-27 27v27h27v-27zm396-54v27h-27v-27zm0 27v27h27v-27zm27 27v27h27v-27zm-27-414h27v27h-27zm27 0h27v-27h-27zm27-27h27v-27h-27zm-396 45h-27v-27h27zm-27-54h-27v27h27zm-27-27h-27v27h27z"/><path d="M94.6 730v27h-27v-27zm477 0v27h-27v-27zm-27-495h27v27h-27zm-450 18h-27v-27h27zm477 9h27v27h-27zm-54 495h27v27h-27zm-423 0h27v27h-27zm-54-504h27v27h-27z" fill="#666"/><path d="M67.6 730v27h-27v-27zm54 54v27h-27v-27zm0-108v27h27v-27zm-27 27v27h27v-27zm-81 0v27h27v-27zm585 27v27h-27v-27zm-108-54v27h27v-27zm27 27v27h27v-27zm81 0v27h27v-27zm-54-495h27v27h-27zm-54 108h27v-27h-27zm27-27h27v-27h-27zm0-81h27v-27h-27zm-423 18h-27v-27h27zm54 54h-27v27h27zm-27-27h-27v27h27zm0-81h-27v27h27zm423 612v27h-27v-27zm81-522v27h-27v-27zm-585-9v27h-27v-27z" fill="#999"/><path d="M94.6 784v27h-27v-27zm-27-27v27h27v-27zm-27-54v27h27v-27zm27 0v27h27v-27zm0-27v27h27v-27zm27 0v27h27v-27zm0-27v27h27v-27zm27 0v27h27v-27zm-108 81v27h27v-27zm558 54v27h-27v-27zm-27-27v27h27v-27zm27-54v27h27v-27zm-27 0v27h27v-27zm0-27v27h27v-27zm-27 0v27h27v-27zm0-27v27h27v-27zm-27 0v27h27v-27zm108 81v27h27v-27zm0-495h27v27h-27zm-27 27h27v-27h-27zm-54-27h27v-27h-27zm0 27h27v-27h-27zm-27 0h27v-27h-27zm0 27h27v-27h-27zm-27 0h27v-27h-27zm0 27h27v-27h-27zm81-108h27v-27h-27zm-504 45h-27v-27h27zm27-27h-27v27h27zm54-27h-27v27h27zm0 27h-27v27h27zm27 0h-27v27h27zm0 27h-27v27h27zm27 0h-27v27h27zm0 27h-27v27h27zm-81-108h-27v27h27z" fill="#ccc"/><path d="M598.6 665.1H41.5C-76.5 667 176 280.2 176 280.2h53a46.5 46.5 0 0162.8-56.3 29.2 29.2 0 1128.5 35.9h-1a46.5 46.5 0 01-1.5 20.3l142.5-.1s255.3 387 138.3 385.1zM291 181a29.3 29.3 0 10-29.2-29.3A29.3 29.3 0 00291 181zm65.4-66.8a22.4 22.4 0 10-22.5-22.4 22.4 22.4 0 0022.5 22.4z" fill="#fc0"/><path d="M245.5 172V10h153v162s324 495 198 495h-558c-126 0 207-495 207-495zm126 54h56m-13 72h56m-9 72h56m-20 72h56m-22 72h56m-29 72h56m-457-45c20.8 41.7 87.3 81 160.7 81 72.1 0 142.1-38.2 163.4-81" fill="none" stroke="#000" stroke-miterlimit="10" stroke-width="20"/><path d="M273.3 421.7c0 31-9.8 56.3-21.9 56.3s-21.8-25.2-21.8-56.3 9.8-56.3 21.8-56.3 21.9 25.2 21.9 56.3zm114.4-56.3c-12 0-21.8 25.2-21.8 56.3s9.7 56.3 21.8 56.3 21.9-25.2 21.9-56.3-9.8-56.3-21.9-56.3zM150.1 526.6c-18.2 6.7-27.5 22.9-23.2 30.2s14.8-5.5 33-12.2 37.4-4.9 33-12.2-24.5-12.6-42.8-5.8zm296 5.8c-4.2 7.3 14.9 5.5 33.1 12.2s28.7 19.5 33 12.2-5-23.5-23.2-30.2-38.5-1.5-42.8 5.8z"/></svg></p>
          </div>
        </div>
      </div>

    </div>
</div>
<!-- END LABS AREA -->
  <div class="endorsers">
    <a href="/auth/show-endorsers/2601.06847" class="endorser-who" rel="nofollow">Which authors of this paper are endorsers?</a> |
    <a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="https://info.arxiv.org/help/mathjax.html">What is MathJax?</a>)
    <span class="help" style="font-style: normal; float: right; margin-top: 0; margin-right: 1em;"></span>
  </div>
  <script type="text/javascript" language="javascript">mathjaxToggle();</script>
</div>
      </div>
    </main>

    <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://info.arxiv.org/about">About</a></li>
                <li><a href="https://info.arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
                <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
  </div>

  <script src="/static/base/1.0.1/js/member_acknowledgement.js"></script>

</body>

</html>

<!-- ===== END ABS ===== -->

<!-- ===== HTML URL: https://arxiv.org/html/2601.06847v1 | HTTP 200 ===== -->

<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data</title>
<!--Generated on Sun Jan 11 10:32:05 2026 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="/static/browse/0.3.4/css/arxiv-html-papers-20250916.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2601.06847v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S1" title="In MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S2" title="In MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S2.SS1" title="In 2 Related Work ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Medical Vision–Language Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S2.SS2" title="In 2 Related Work ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Referring Expression Grounding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S2.SS3" title="In 2 Related Work ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>VLMs-based Synthesis and Verification</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S3" title="In MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>MedGround</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S3.SS1" title="In 3 MedGround ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Dataset Definition</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S3.SS2" title="In 3 MedGround ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>From Masks to Ground-Truth Box Lists</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S3.SS3" title="In 3 MedGround ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Mask-Guided Query Synthesis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S3.SS4" title="In 3 MedGround ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Multi-Stage Data Verification and Cleaning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S3.SS5" title="In 3 MedGround ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>MedGround-35K</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S3.SS6" title="In 3 MedGround ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6 </span>Human Audit</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S4" title="In MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S4.SS1" title="In 4 Experiments ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Experiment Settings</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S4.SS2" title="In 4 Experiments ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Results and Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S4.SS2.SSS0.Px1" title="In 4.2 Results and Analysis ‣ 4 Experiments ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_title">MedGround effectively enhances VLMs’ medical referring grounding capability.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S4.SS2.SSS0.Px2" title="In 4.2 Results and Analysis ‣ 4 Experiments ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_title">MedGround injects fine-grained medical semantic knowledge into VLMs.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S4.SS2.SSS0.Px3" title="In 4.2 Results and Analysis ‣ 4 Experiments ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_title">MedGround Enables Zero-shot Generalization of VLMs.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S5" title="In MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S6" title="In MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Acknowledgment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#A1" title="In MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Datasets</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#A1.SS1" title="In Appendix A Datasets ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Data source</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#A1.SS2" title="In Appendix A Datasets ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Pass rate of multi-stage verifaction pipline</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#A1.SS3" title="In Appendix A Datasets ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Human Audit Results and Failure Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#A1.SS4" title="In Appendix A Datasets ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4 </span>Train–Test Distribution Analysis in Three Semantic Dimensions</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#A2" title="In MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Implementation Details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#A2.SS1" title="In Appendix B Implementation Details ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Fine-Tuning Details</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#A3" title="In MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Samples</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#A3.SS1" title="In Appendix C Samples ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.1 </span>MedGround-35K Examples</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#A3.SS2" title="In Appendix C Samples ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2 </span>Failure Cases in Manual Screening</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#A4" title="In MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Details of General Generate Prompt</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#A5" title="In MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Potential Use of MedGround</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span class="ltx_text ltx_font_bold" id="3.3.3">Mengmeng Zhang<sup class="ltx_sup" id="3.3.3.1">1,2,3</sup></span>,
<span class="ltx_text ltx_font_bold" id="4.4.4">Xiaoping Wu<sup class="ltx_sup" id="4.4.4.1">3</sup></span>,
<span class="ltx_text ltx_font_bold" id="id2.2.2">Hao Luo<sup class="ltx_sup" id="1.1.1">3,4<sup class="ltx_sup" id="1.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="1.1.1.1.1">†</span></sup></sup>,
Fan Wang<sup class="ltx_sup" id="id2.2.2.3">3</sup>,
Yisheng Lv<sup class="ltx_sup" id="id2.2.2.2">1,2<sup class="ltx_sup" id="id2.2.2.2.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id2.2.2.2.1.1">†</span></sup></sup>
</span>
<br class="ltx_break"/><sup class="ltx_sup" id="5.5.5">1</sup>Institute of Automation, Chinese Academy of Sciences, 
<br class="ltx_break"/><sup class="ltx_sup" id="6.6.6">2</sup>School of Artificial Intelligence, University of Chinese Academy of Sciences, 
<br class="ltx_break"/><sup class="ltx_sup" id="7.7.7">3</sup>DAMO Academy, Alibaba Group, 
<br class="ltx_break"/><sup class="ltx_sup" id="8.8.8">4</sup>Hupan Lab, Zhejiang Province 
<br class="ltx_break"/>
<span class="ltx_text ltx_font_bold" id="9.9.9">Correspondence:</span>
<a class="ltx_ref ltx_href" href="mailto:michuan.lh@alibaba-inc.com" title="">michuan.lh@alibaba-inc.com</a>,
<a class="ltx_ref ltx_href" href="mailto:yisheng.lv@ia.ac.cn" title="">yisheng.lv@ia.ac.cn</a>
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="10.1"><span class="ltx_text ltx_font_bold" id="10.1.1">V</span>ision-<span class="ltx_text ltx_font_bold" id="10.1.2">L</span>anguage <span class="ltx_text ltx_font_bold" id="10.1.3">M</span>odels (VLMs) can generate convincing clinical narratives, yet frequently struggle to visually ground their statements. We posit this limitation arises from the scarcity of high-quality, large-scale clinical referring-localization pairs. To address this, we introduce MedGround, an automated pipeline that transforms segmentation resources into high-quality medical referring grounding data. Leveraging expert masks as spatial anchors, MedGround precisely derives localization targets, extracts shape and spatial cues, and guides VLMs to synthesize natural, clinically grounded queries that reflect morphology and location. To ensure data rigor, a multi-stage verification system integrates strict formatting checks, geometry- and medical-prior rules, and image-based visual judging to filter out ambiguous or visually unsupported samples. Finally, we present MedGround-35K, a novel multimodal medical dataset. Extensive experiments demonstrate that VLMs trained with MedGround-35K consistently achieve improved referring grounding performance, enhance multi-object semantic disambiguation, and exhibit strong generalization to unseen grounding settings. This work highlights MedGround as a scalable, data-driven approach to anchor medical language to verifiable visual evidence.<span class="ltx_note ltx_role_footnote" id="footnotex2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Dataset and code will be released publicly upon acceptance.</span></span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.2">
<p class="ltx_p ltx_align_center" id="p1.2.3"><span class="ltx_text ltx_font_bold" id="p1.2.3.1">MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data</span></p>
<p class="ltx_p ltx_align_center" id="p1.2.2" style="width:345.0pt;"><span class="ltx_text ltx_inline-block" id="p1.2.2.2" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.2.2.2.2">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.2.2.2.2.2">
<span class="ltx_td ltx_align_center" id="p1.2.2.2.2.2.2"><span class="ltx_text ltx_font_bold" id="p1.2.2.2.2.2.2.2">
Mengmeng Zhang<sup class="ltx_sup" id="p1.2.2.2.2.2.2.2.3">1,2,3</sup>,
Xiaoping Wu<sup class="ltx_sup" id="p1.2.2.2.2.2.2.2.4">3</sup>,
Hao Luo<sup class="ltx_sup" id="p1.1.1.1.1.1.1.1.1">3,4<sup class="ltx_sup" id="p1.1.1.1.1.1.1.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.1.1.1.1.1.1.1.1.1.1">†</span></sup></sup>,
Fan Wang<sup class="ltx_sup" id="p1.2.2.2.2.2.2.2.5">3</sup>,
Yisheng Lv<sup class="ltx_sup" id="p1.2.2.2.2.2.2.2.2">1,2<sup class="ltx_sup" id="p1.2.2.2.2.2.2.2.2.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.2.2.2.2.2.2.2.2.1.1">†</span></sup></sup></span></span></span>
<span class="ltx_tr" id="p1.2.2.2.2.3.1">
<span class="ltx_td ltx_align_center" id="p1.2.2.2.2.3.1.1"><sup class="ltx_sup" id="p1.2.2.2.2.3.1.1.1">1</sup>Institute of Automation, Chinese Academy of Sciences,</span></span>
<span class="ltx_tr" id="p1.2.2.2.2.4.2">
<span class="ltx_td ltx_align_center" id="p1.2.2.2.2.4.2.1"><sup class="ltx_sup" id="p1.2.2.2.2.4.2.1.1">2</sup>School of Artificial Intelligence, University of Chinese Academy of Sciences,</span></span>
<span class="ltx_tr" id="p1.2.2.2.2.5.3">
<span class="ltx_td ltx_align_center" id="p1.2.2.2.2.5.3.1"><sup class="ltx_sup" id="p1.2.2.2.2.5.3.1.1">3</sup>DAMO Academy, Alibaba Group,</span></span>
<span class="ltx_tr" id="p1.2.2.2.2.6.4">
<span class="ltx_td ltx_align_center" id="p1.2.2.2.2.6.4.1"><sup class="ltx_sup" id="p1.2.2.2.2.6.4.1.1">4</sup>Hupan Lab, Zhejiang Province</span></span>
<span class="ltx_tr" id="p1.2.2.2.2.7.5">
<span class="ltx_td ltx_align_center" id="p1.2.2.2.2.7.5.1">
<span class="ltx_text ltx_font_bold" id="p1.2.2.2.2.7.5.1.1">Correspondence:</span>
<a class="ltx_ref ltx_href" href="mailto:michuan.lh@alibaba-inc.com" title="">michuan.lh@alibaba-inc.com</a>,
<a class="ltx_ref ltx_href" href="mailto:yisheng.lv@ia.ac.cn" title="">yisheng.lv@ia.ac.cn</a></span></span>
</span>
</span></span></p>
</div>
</div>
<span class="ltx_note ltx_role_footnote" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><sup class="ltx_sup" id="footnotex1.1"><span class="ltx_text ltx_font_italic" id="footnotex1.1.1">†</span></sup> Corresponding authors.</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Recent VLMs have shown impressive capability in medical image understanding tasks such as report generation and clinical question answering <cite class="ltx_cite ltx_citemacro_cite">Li<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib25" title="Llava-med: training a large language-and-vision assistant for biomedicine in one day">2023</a>); Manzari<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib28" title="MedViT: a robust vision transformer for generalized medical image classification">2023</a>); Nath<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib20" title="Vila-m3: enhancing vision-language models with medical expert knowledge">2025</a>); Sellergren<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib59" title="Medgemma technical report">2025</a>); Liu<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib21" title="Application of large language models in medicine">2025</a>); Yang<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib22" title="Advancing multimodal medical capabilities of gemini">2024</a>); Tarhini<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib23" title="General artificial intelligence for the diagnosis and treatment of cancer: the rise of foundation models">2025</a>); Wu<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib26" title="Towards generalist foundation model for radiology by leveraging web-scale 2d&amp;3d medical data">2025</a>)</cite>. However, their linguistic fluency often outpaces fine-grained visual localization: a model may describe plausible findings while failing to identify where those findings appear in the image. Such visually unfaithful outputs undermine interpretability and can lead to “right-for-the-wrong-reason” predictions <cite class="ltx_cite ltx_citemacro_cite">Kim<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib27" title="MedCLM: learning to localize and reason via a cot-curriculum in medical vision-language models">2025</a>); Xie<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib29" title="Faithful ai in medicine: a systematic review with large language models and beyond">2023</a>); Mahmood<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib30" title="Evaluating automated radiology report quality through fine-grained phrasal grounding of clinical findings">2025</a>); Ostmeier<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib31" title="Green: generative radiology report evaluation and error notation">2024</a>); Pal<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib19" title="Med-HALT: medical domain hallucination test for large language models">2023</a>); Huang<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib32" title="Towards a multimodal large language model with pixel-level insight for biomedicine">2025</a>); Xu (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib34" title="Uncertainty estimation in large vision language models for automated radiology report generation">2025</a>)</cite>.
We characterize this mismatch as a cognitive–perceptual gap, where a model’s cognitive competence is not mirrored by its perceptual grounding.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="705" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span class="ltx_text ltx_font_bold" id="S1.F1.4.1">Motivation of MedGround.</span> (a) Models trained on image-text pairs fail to <span class="ltx_text ltx_font_italic" id="S1.F1.5.2">"speak with substance"</span> due to lack of grounding. (b) Segmentation-only training fails to achieve semantic understanding. (c) <span class="ltx_text ltx_font_bold" id="S1.F1.6.3">MedGround</span> (Image-text-box triplets) activates the full potential of medical VLMs by bridging semantics and localization.</figcaption>
</figure>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">We argue that a key driver of this gap is the current data scarce. Medical data is plentiful in two largely disconnected forms: (1) image–text pairs from radiology reports, providing rich global semantics but weak spatial supervision; and (2) image–mask segmentation datasets, providing precise region annotations but impoverished language, often limited to a coarse category label. In contrast, large-scale datasets that pair natural referring expressions with explicit localization targets (boxes or masks) are scarce<cite class="ltx_cite ltx_citemacro_cite">Bannur<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib37" title="Maira-2: grounded radiology report generation">2024</a>); Li<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib33" title="Anatomical structure-guided medical vision-language pre-training">2024</a>); Chen<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib39" title="MIMO: a medical vision language model with visual referring multimodal input and pixel grounding multimodal output">2025</a>); Deng<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib40" title="Med-glip: advancing medical language-image pre-training with large-scale grounded dataset">2025</a>)</cite> .This scarcity limits the ability of VLMs to learn the alignment between clinically meaningful phrases and localized evidence.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">However, it is challenging to construct medical referring grounding data on a scale. One seemingly naive approach is to ground existing reports by asking VLMs to localize findings. Yet current VLMs are often vision-weak at fine-grained grounding, so this direction risks inheriting and amplifying their spatial biases<cite class="ltx_cite ltx_citemacro_cite">Huang<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib35" title="A refer-and-ground multimodal large language model for biomedicine">2024</a>); Ge<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib36" title="ClinKD: cross-modal clinical knowledge distiller for multi-task medical images">2025</a>); Strudel<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib38" title="Weakly-supervised segmentation of referring expressions">2022</a>); Bai<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib41" title="M3d: advancing 3d medical image analysis with multi-modal large language models">2024</a>); Zhang<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib42" title="Generalized medical phrase grounding">2025b</a>)</cite>. Instead, we reverse the process: starting from precise expert annotations in segmentation datasets, we use them as deterministic spatial anchors to synthesize referring expressions. This leverages the asymmetry of current models—strong language generation but weaker grounding—while maintaining spatial faithfulness through verification.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">We introduce MedGround, a mask-guided semantic synthesis and verification pipeline that converts segmentation annotations into high-quality image–text–box triplets(MedGround-35K dataset) for medical referring expression grounding. MedGround effectively generates these triplets by leveraging expert masks to derive precise bounding boxes and extract geometric and spatial attributes. It then prompts a VLM to synthesize medically meaningful referring queries, followed by a multi-stage filtering process, including image-based VLM judging, to ensure data quality. Extensive experiments across multiple settings show that training with our constructed MedGround-35K dataset effectively improves medical referring grounding, and that the introduced clinically grounded semantics help VLMs better follow morphology- and location-aware descriptions for more reliable target disambiguation. These findings support MedGround as an effective and scalable supervision pipline for bridging the cognitive–perceptual gap.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">In summary, our key contributions are:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We propose MedGround, a scalable pipeline for synthesizing and verifying medically grounded referring queries anchored to expert annotations.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We release MedGround-35K, covering eight datasets and multiple modalities, and demonstrate improvements in referring grounding, semantic disambiguation, and zero-shot transfer.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We extensively tested various models using the MedGround-35K dataset and discovered that existing VLMs commonly struggle with fine-grained medical referring grounding tasks. However, training with our data significantly mitigates these issues.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_table" id="S1.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span><span class="ltx_text ltx_font_bold" id="S1.T1.8.1">Comparison of medical datasets.</span> <span class="ltx_text" id="S1.T1.9.2" style="--ltx-fg-color:#009900;">✓</span>: provided; <span class="ltx_text" id="S1.T1.10.3" style="--ltx-fg-color:#B30000;">✗</span>: not provided; <math alttext="\triangle" class="ltx_Math" display="inline" id="S1.T1.2.m1" intent=":literal"><semantics><mi mathcolor="#999900" mathvariant="normal" style="--ltx-fg-color:#999900;">△</mi><annotation encoding="application/x-tex">\triangle</annotation></semantics></math>: limited/weak or implicit/derived. Auto Anno. means the samples are collected autonomously. </figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S1.T1.4" style="width:345.0pt;height:34.8pt;vertical-align:-16.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-252.1pt,25.4pt) scale(0.40628,0.40628) ;">
<p class="ltx_p" id="S1.T1.4.2"><span class="ltx_text" id="S1.T1.4.2.2">
<span class="ltx_inline-block ltx_transformed_outer" id="S1.T1.4.2.2.2" style="width:849.2pt;height:85.6pt;vertical-align:-40.3pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S1.T1.4.2.2.2.2"><span class="ltx_text" id="S1.T1.4.2.2.2.2.2">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S1.T1.4.2.2.2.2.2.2">
<span class="ltx_thead">
<span class="ltx_tr" id="S1.T1.4.2.2.2.2.2.2.3.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S1.T1.4.2.2.2.2.2.2.3.1.1" style="padding:0.5pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.4.2.2.2.2.2.2.3.1.1.1">Dataset</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S1.T1.4.2.2.2.2.2.2.3.1.2" style="padding:0.5pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.4.2.2.2.2.2.2.3.1.2.1">Modality</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S1.T1.4.2.2.2.2.2.2.3.1.3" style="padding:0.5pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.4.2.2.2.2.2.2.3.1.3.1">Text Type</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S1.T1.4.2.2.2.2.2.2.3.1.4" style="padding:0.5pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.4.2.2.2.2.2.2.3.1.4.1">Loc. Type</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S1.T1.4.2.2.2.2.2.2.3.1.5" style="padding:0.5pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.4.2.2.2.2.2.2.3.1.5.1">Granularity</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S1.T1.4.2.2.2.2.2.2.3.1.6" style="padding:0.5pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.4.2.2.2.2.2.2.3.1.6.1">Auto Anno.</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S1.T1.4.2.2.2.2.2.2.3.1.7" style="padding:0.5pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.4.2.2.2.2.2.2.3.1.7.1">#Anno.</span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S1.T1.4.2.2.2.2.2.2.3.1.8" style="padding:0.5pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.4.2.2.2.2.2.2.3.1.8.1">Task</span></span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S1.T1.4.2.2.2.2.2.2.4.1">
<span class="ltx_td ltx_align_left ltx_border_t" id="S1.T1.4.2.2.2.2.2.2.4.1.1" style="padding:0.5pt 4.0pt;">MS-CXR<cite class="ltx_cite ltx_citemacro_cite">Boecking<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib16" title="Ms-cxr: making the most of text semantics to improve biomedical vision-language processing">2022</a>)</cite></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.4.2.2.2.2.2.2.4.1.2" style="padding:0.5pt 4.0pt;">CXR</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.4.2.2.2.2.2.2.4.1.3" style="padding:0.5pt 4.0pt;"><span class="ltx_text" id="S1.T1.4.2.2.2.2.2.2.4.1.3.1" style="--ltx-fg-color:#009900;">✓</span> report phrases</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.4.2.2.2.2.2.2.4.1.4" style="padding:0.5pt 4.0pt;"><span class="ltx_text" id="S1.T1.4.2.2.2.2.2.2.4.1.4.1" style="--ltx-fg-color:#009900;">✓</span> box</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.4.2.2.2.2.2.2.4.1.5" style="padding:0.5pt 4.0pt;">finding</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.4.2.2.2.2.2.2.4.1.6" style="padding:0.5pt 4.0pt;"><span class="ltx_text" id="S1.T1.4.2.2.2.2.2.2.4.1.6.1" style="--ltx-fg-color:#B30000;">✗</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S1.T1.4.2.2.2.2.2.2.4.1.7" style="padding:0.5pt 4.0pt;">1,162</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S1.T1.4.2.2.2.2.2.2.4.1.8" style="padding:0.5pt 4.0pt;">phrase grounding</span></span>
<span class="ltx_tr" id="S1.T1.3.1.1.1.1.1.1.1">
<span class="ltx_td ltx_align_left" id="S1.T1.3.1.1.1.1.1.1.1.2" style="padding:0.5pt 4.0pt;">Chest ImaGenome <cite class="ltx_cite ltx_citemacro_cite">Wu<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib17" title="Chest imagenome dataset for clinical reasoning">2021</a>)</cite></span>
<span class="ltx_td ltx_align_center" id="S1.T1.3.1.1.1.1.1.1.1.3" style="padding:0.5pt 4.0pt;">CXR</span>
<span class="ltx_td ltx_align_center" id="S1.T1.3.1.1.1.1.1.1.1.4" style="padding:0.5pt 4.0pt;"><span class="ltx_text" id="S1.T1.3.1.1.1.1.1.1.1.4.1" style="--ltx-fg-color:#009900;">✓</span> structured phrases</span>
<span class="ltx_td ltx_align_center" id="S1.T1.3.1.1.1.1.1.1.1.1" style="padding:0.5pt 4.0pt;"><math alttext="\triangle" class="ltx_Math" display="inline" id="S1.T1.3.1.1.1.1.1.1.1.1.m1" intent=":literal"><semantics><mi mathcolor="#999900" mathvariant="normal" style="--ltx-fg-color:#999900;">△</mi><annotation encoding="application/x-tex">\triangle</annotation></semantics></math> region-aligned</span>
<span class="ltx_td ltx_align_center" id="S1.T1.3.1.1.1.1.1.1.1.5" style="padding:0.5pt 4.0pt;">region/finding</span>
<span class="ltx_td ltx_align_center" id="S1.T1.3.1.1.1.1.1.1.1.6" style="padding:0.5pt 4.0pt;"><span class="ltx_text" id="S1.T1.3.1.1.1.1.1.1.1.6.1" style="--ltx-fg-color:#B30000;">✗</span></span>
<span class="ltx_td ltx_align_center" id="S1.T1.3.1.1.1.1.1.1.1.7" style="padding:0.5pt 4.0pt;">1,256</span>
<span class="ltx_td ltx_align_left" id="S1.T1.3.1.1.1.1.1.1.1.8" style="padding:0.5pt 4.0pt;">region-text alignment</span></span>
<span class="ltx_tr" id="S1.T1.4.2.2.2.2.2.2.2">
<span class="ltx_td ltx_align_left" id="S1.T1.4.2.2.2.2.2.2.2.2" style="padding:0.5pt 4.0pt;">ChestX-ray8 <cite class="ltx_cite ltx_citemacro_cite">Wang<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib12" title="ChestX-ray8: hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases">2017</a>)</cite></span>
<span class="ltx_td ltx_align_center" id="S1.T1.4.2.2.2.2.2.2.2.3" style="padding:0.5pt 4.0pt;">CXR</span>
<span class="ltx_td ltx_align_center" id="S1.T1.4.2.2.2.2.2.2.2.1" style="padding:0.5pt 4.0pt;"><math alttext="\triangle" class="ltx_Math" display="inline" id="S1.T1.4.2.2.2.2.2.2.2.1.m1" intent=":literal"><semantics><mi mathcolor="#999900" mathvariant="normal" style="--ltx-fg-color:#999900;">△</mi><annotation encoding="application/x-tex">\triangle</annotation></semantics></math>labels</span>
<span class="ltx_td ltx_align_center" id="S1.T1.4.2.2.2.2.2.2.2.4" style="padding:0.5pt 4.0pt;"><span class="ltx_text" id="S1.T1.4.2.2.2.2.2.2.2.4.1" style="--ltx-fg-color:#009900;">✓</span>box</span>
<span class="ltx_td ltx_align_center" id="S1.T1.4.2.2.2.2.2.2.2.5" style="padding:0.5pt 4.0pt;">disease</span>
<span class="ltx_td ltx_align_center" id="S1.T1.4.2.2.2.2.2.2.2.6" style="padding:0.5pt 4.0pt;"><span class="ltx_text" id="S1.T1.4.2.2.2.2.2.2.2.6.1" style="--ltx-fg-color:#B30000;">✗</span></span>
<span class="ltx_td ltx_align_center" id="S1.T1.4.2.2.2.2.2.2.2.7" style="padding:0.5pt 4.0pt;">888</span>
<span class="ltx_td ltx_align_left" id="S1.T1.4.2.2.2.2.2.2.2.8" style="padding:0.5pt 4.0pt;">disease localization</span></span>
<span class="ltx_tr" id="S1.T1.4.2.2.2.2.2.2.5.2">
<span class="ltx_td ltx_align_left" id="S1.T1.4.2.2.2.2.2.2.5.2.1" style="padding:0.5pt 4.0pt;">DeepLesion <cite class="ltx_cite ltx_citemacro_cite">Yan<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib13" title="DeepLesion: automated mining of large-scale lesion annotations and universal lesion detection with deep learning">2018</a>)</cite></span>
<span class="ltx_td ltx_align_center" id="S1.T1.4.2.2.2.2.2.2.5.2.2" style="padding:0.5pt 4.0pt;">CT</span>
<span class="ltx_td ltx_align_center" id="S1.T1.4.2.2.2.2.2.2.5.2.3" style="padding:0.5pt 4.0pt;"><span class="ltx_text" id="S1.T1.4.2.2.2.2.2.2.5.2.3.1" style="--ltx-fg-color:#B30000;">✗</span> metadata</span>
<span class="ltx_td ltx_align_center" id="S1.T1.4.2.2.2.2.2.2.5.2.4" style="padding:0.5pt 4.0pt;"><span class="ltx_text" id="S1.T1.4.2.2.2.2.2.2.5.2.4.1" style="--ltx-fg-color:#009900;">✓</span> box</span>
<span class="ltx_td ltx_align_center" id="S1.T1.4.2.2.2.2.2.2.5.2.5" style="padding:0.5pt 4.0pt;">lesion</span>
<span class="ltx_td ltx_align_center" id="S1.T1.4.2.2.2.2.2.2.5.2.6" style="padding:0.5pt 4.0pt;"><span class="ltx_text" id="S1.T1.4.2.2.2.2.2.2.5.2.6.1" style="--ltx-fg-color:#B30000;">✗</span></span>
<span class="ltx_td ltx_align_center" id="S1.T1.4.2.2.2.2.2.2.5.2.7" style="padding:0.5pt 4.0pt;">32,120</span>
<span class="ltx_td ltx_align_left" id="S1.T1.4.2.2.2.2.2.2.5.2.8" style="padding:0.5pt 4.0pt;">lesion detection</span></span>
<span class="ltx_tr" id="S1.T1.4.2.2.2.2.2.2.6.3">
<span class="ltx_td ltx_align_left" id="S1.T1.4.2.2.2.2.2.2.6.3.1" style="padding:0.5pt 4.0pt;">LIDC-IDRI <cite class="ltx_cite ltx_citemacro_cite">Armato<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib14" title="The lung image database consortium (lidc) and image database resource initiative (idri): a completed reference database of lung nodules on ct scans">2011</a>)</cite></span>
<span class="ltx_td ltx_align_center" id="S1.T1.4.2.2.2.2.2.2.6.3.2" style="padding:0.5pt 4.0pt;">CT</span>
<span class="ltx_td ltx_align_center" id="S1.T1.4.2.2.2.2.2.2.6.3.3" style="padding:0.5pt 4.0pt;"><span class="ltx_text" id="S1.T1.4.2.2.2.2.2.2.6.3.3.1" style="--ltx-fg-color:#B30000;">✗</span> attributes</span>
<span class="ltx_td ltx_align_center" id="S1.T1.4.2.2.2.2.2.2.6.3.4" style="padding:0.5pt 4.0pt;"><span class="ltx_text" id="S1.T1.4.2.2.2.2.2.2.6.3.4.1" style="--ltx-fg-color:#009900;">✓</span> mask/contour</span>
<span class="ltx_td ltx_align_center" id="S1.T1.4.2.2.2.2.2.2.6.3.5" style="padding:0.5pt 4.0pt;">lesion</span>
<span class="ltx_td ltx_align_center" id="S1.T1.4.2.2.2.2.2.2.6.3.6" style="padding:0.5pt 4.0pt;"><span class="ltx_text" id="S1.T1.4.2.2.2.2.2.2.6.3.6.1" style="--ltx-fg-color:#B30000;">✗</span></span>
<span class="ltx_td ltx_align_center" id="S1.T1.4.2.2.2.2.2.2.6.3.7" style="padding:0.5pt 4.0pt;">875</span>
<span class="ltx_td ltx_align_left" id="S1.T1.4.2.2.2.2.2.2.6.3.8" style="padding:0.5pt 4.0pt;">nodule analysis</span></span>
<span class="ltx_tr" id="S1.T1.4.2.2.2.2.2.2.7.4">
<span class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S1.T1.4.2.2.2.2.2.2.7.4.1" style="padding:0.5pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="S1.T1.4.2.2.2.2.2.2.7.4.1.1">MedGround-35K (ours)</span></span>
<span class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S1.T1.4.2.2.2.2.2.2.7.4.2" style="padding:0.5pt 4.0pt;">multi</span>
<span class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S1.T1.4.2.2.2.2.2.2.7.4.3" style="padding:0.5pt 4.0pt;"><span class="ltx_text" id="S1.T1.4.2.2.2.2.2.2.7.4.3.1" style="--ltx-fg-color:#009900;">✓</span> referring queries</span>
<span class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S1.T1.4.2.2.2.2.2.2.7.4.4" style="padding:0.5pt 4.0pt;"><span class="ltx_text" id="S1.T1.4.2.2.2.2.2.2.7.4.4.1" style="--ltx-fg-color:#009900;">✓</span> box</span>
<span class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S1.T1.4.2.2.2.2.2.2.7.4.5" style="padding:0.5pt 4.0pt;">lesion/structure</span>
<span class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S1.T1.4.2.2.2.2.2.2.7.4.6" style="padding:0.5pt 4.0pt;"><span class="ltx_text" id="S1.T1.4.2.2.2.2.2.2.7.4.6.1" style="--ltx-fg-color:#009900;">✓</span></span>
<span class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S1.T1.4.2.2.2.2.2.2.7.4.7" style="padding:0.5pt 4.0pt;">35,324</span>
<span class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S1.T1.4.2.2.2.2.2.2.7.4.8" style="padding:0.5pt 4.0pt;">referring grounding</span></span>
</span>
</span></span></span>
</span></span></span></p>
</span></div>
</figure>
<figure class="ltx_table" id="S1.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span><span class="ltx_text ltx_font_bold" id="S1.T2.2.1">The statistics of the MedGround-35K.</span> The Anno. Tokens and Avg. Words columns show the total number of tokens and the average number of words for the medical grounding annotations regardless of task templates. The Modalities/Sources column shows the number of unique medical imaging sources/modalities involved in each split.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S1.T2.3" style="width:345.0pt;height:21.5pt;vertical-align:-9.3pt;"><span class="ltx_transformed_inner" style="transform:translate(-128.4pt,8.0pt) scale(0.573273819136059,0.573273819136059) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S1.T2.3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S1.T2.3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S1.T2.3.1.1.1.1">Split</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S1.T2.3.1.1.1.2">#Images</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S1.T2.3.1.1.1.3">Anno. Tokens</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S1.T2.3.1.1.1.4">Avg. Words</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S1.T2.3.1.1.1.5">Modality Ratio</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S1.T2.3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S1.T2.3.1.2.1.1">Train</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T2.3.1.2.1.2">25.4k</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T2.3.1.2.1.3">2773.5k</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T2.3.1.2.1.4">12.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S1.T2.3.1.2.1.5">Bacteria: 1.3%, CT: 8.2%, Dermoscopy: 9.9%, Nuclei: 20.5%, Ultrasound: 60.2%</td>
</tr>
<tr class="ltx_tr" id="S1.T2.3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S1.T2.3.1.3.2.1">Test</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S1.T2.3.1.3.2.2">10.1k</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S1.T2.3.1.3.2.3">1115.9k</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S1.T2.3.1.3.2.4">12.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S1.T2.3.1.3.2.5">Bacteria: 1.2%, CT: 18.3%, Dermoscopy: 11.8%, Nuclei: 25.3%, Ultrasound: 43.4%</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="599" id="S1.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span class="ltx_text ltx_font_bold" id="S1.F2.2.1">MedGround pipeline.</span> (A) Convert segmentation masks into normalized ground-truth bounding box lists. (B) Use dataset-aware, mask-guided prompts to synthesize medically meaningful referring queries and select target box(es) as answers. (C) Perform multi-stage verification and cleaning (format/schema, geometry–location rules, and VLM-based grounding). (D) Conduct manual review for final quality control. (E) Cases falling verification.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Our work relates to medical VLMs, referring grounding, and VLM-based synthesis and verifocation, so we review them below.
</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Medical Vision–Language Models</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Medical VLMs have rapidly advanced with large-scale pretraining on radiology report corpora and biomedical image–text resources, followed by instruction tuning for clinical question answering and report generation<cite class="ltx_cite ltx_citemacro_cite">Li<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib25" title="Llava-med: training a large language-and-vision assistant for biomedicine in one day">2023</a>); Moor<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib43" title="Med-flamingo: a multimodal medical few-shot learner (2023)">2023</a>); Zhang<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib44" title="Pmc-vqa: visual instruction tuning for medical visual question answering">2023</a>)</cite>. These models can produce fluent, clinically plausible narratives, but their outputs are not always evidence-aligned: models may describe findings without reliably localizing the corresponding visual regions, which limits interpretability and can lead to visually unfaithful reasoning<cite class="ltx_cite ltx_citemacro_cite">Pellegrini<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib45" title="Radialog: a large vision-language model for radiology report generation and conversational assistance">2023</a>); Moll<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib46" title="Evaluating reasoning faithfulness in medical vision-language models using multimodal perturbations">2025</a>); Liu<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib47" title="Qilin-med-vl: towards chinese large vision-language model for general healthcare">2023</a>); Gundersen<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib48" title="Enhancing radiology report generation and visual grounding using reinforcement learning">2025</a>)</cite>. This has motivated growing interest in grounding-aware evaluation and training signals that connect medical language to spatial evidence. In this work, we target this limitation by providing explicit referring-style localization supervision derived from expert segmentation annotations, enabling medical VLMs to better align morphology- and location-bearing phrases with concrete visual evidence.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Referring Expression Grounding </h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Referring expression grounding localizes an entity specified by a natural-language expression, emphasizing attribute understanding and spatial disambiguation. In natural images, large-scale referring and phrase grounding benchmarks have driven progress in models that bind text tokens to localized regions and handle fine-grained relational language<cite class="ltx_cite ltx_citemacro_cite">Liu<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib49" title="Grounding dino: marrying dino with grounded pre-training for open-set object detection">2024</a>); Li<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib50" title="Grounded language-image pre-training">2022</a>); Kamath<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib51" title="Mdetr-modulated detection for end-to-end multi-modal understanding">2021</a>)</cite>. In medical imaging, however, referring expression grounding datasets remain scarce. Available supervision is typically split between image–text pairs (rich clinical narratives but weak spatial alignment) and segmentation masks (precise localization but little to no language beyond class labels). This gap prevents medical VLMs from learning clinically meaningful referring cues such as morphology, laterality, anatomical sub-location, and multi-finding disambiguation<cite class="ltx_cite ltx_citemacro_cite">Bai<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib41" title="M3d: advancing 3d medical image analysis with multi-modal large language models">2024</a>); Deng<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib40" title="Med-glip: advancing medical language-image pre-training with large-scale grounded dataset">2025</a>); Liu<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib47" title="Qilin-med-vl: towards chinese large vision-language model for general healthcare">2023</a>); Zhang<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib52" title="Anatomical grounding pre-training for medical phrase grounding">2025a</a>); Yang<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib53" title="New dataset and methods for fine-grained compositional referring expression comprehension via specialist-mllm collaboration">2025</a>)</cite>. MedGround bridges this gap by converting segmentation annotations into scalable image–text–box supervision, explicitly training the model to follow clinically grounded referring descriptions.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>VLMs-based Synthesis and Verification</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">VLMs-driven dataset construction has become a practical route to scale instruction and annotation resources, often paired with automated filtering, self-checking, or model-based judging to control noise. In medical settings, such synthesis is particularly fragile: hallucinated attributes, incorrect spatial relations, or visually unsupported statements can easily slip into training data and degrade evidence faithfulness<cite class="ltx_cite ltx_citemacro_cite">Pal<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib19" title="Med-HALT: medical domain hallucination test for large language models">2023</a>); Yu<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib54" title="Evaluating progress in automatic chest x-ray radiology report generation">2023</a>); Khanna<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib55" title="Radgraph2: modeling disease progression in radiology reports via hierarchical information extraction">2023</a>); Dong<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib56" title="Raft: reward ranked finetuning for generative foundation model alignment">2023</a>); Croxford<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib57" title="Automating evaluation of ai text generation in healthcare with a large language model (llm)-as-a-judge">2025</a>); Yue<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib58" title="MedSG-bench: a benchmark for medical image sequences grounding">2025</a>)</cite>. MedGround adopts a conservative synthesis principle: rather than asking a model to discover or localize findings from free-form text, we start from expert masks as deterministic spatial anchors and only synthesize referring queries conditioned on mask-derived geometry and spatial cues. We then apply multi-stage verification—format constraints, rule-based geometric/medical priors, and image-based judging—to filter ambiguous or visually unsupported samples. This design explicitly reduces the risk of inheriting vision-side localization bias while retaining the scalability benefits of VLMs-based generation.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>MedGround</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">This section introduces <span class="ltx_text ltx_font_bold" id="S3.p1.1.1">MedGround</span>, a mask-guided synthesis-and-verification pipeline (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">2</span></a>) that automatically converts segmentation annotations into high-quality <em class="ltx_emph ltx_font_italic" id="S3.p1.1.2">medical referring box grounding</em> datasets.
Starting from expert masks, MedGround derives candidate ground-truth boxes, prompts a VLM to generate clinically grounded referring queries for randomly selected targets, and applies multi-stage verification to filter ambiguous or visually unsupported samples.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Dataset Definition</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.6">We produce MedGround-35K with MedGround pipline. Each example of MedGround-35K is a triplet <math alttext="(I,Q,B)" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1" intent=":literal"><semantics><mrow><mo stretchy="false">(</mo><mi>I</mi><mo>,</mo><mi>Q</mi><mo>,</mo><mi>B</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(I,Q,B)</annotation></semantics></math>, where <math alttext="I" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2" intent=":literal"><semantics><mi>I</mi><annotation encoding="application/x-tex">I</annotation></semantics></math> is a medical image, <math alttext="Q" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3" intent=":literal"><semantics><mi>Q</mi><annotation encoding="application/x-tex">Q</annotation></semantics></math> is a referring query that describes one or multiple target regions, and <math alttext="B" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4" intent=":literal"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> is the corresponding ground-truth set of 2D bounding boxes. Each box is represented as <math alttext="[x_{\min},y_{\min},x_{\max},y_{\max}]" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m5" intent=":literal"><semantics><mrow><mo stretchy="false">[</mo><msub><mi>x</mi><mi>min</mi></msub><mo>,</mo><msub><mi>y</mi><mi>min</mi></msub><mo>,</mo><msub><mi>x</mi><mi>max</mi></msub><mo>,</mo><msub><mi>y</mi><mi>max</mi></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[x_{\min},y_{\min},x_{\max},y_{\max}]</annotation></semantics></math> and normalized to a <math alttext="1000\times 1000" class="ltx_Math" display="inline" id="S3.SS1.p1.6.m6" intent=":literal"><semantics><mrow><mn>1000</mn><mo lspace="0.222em" rspace="0.222em">×</mo><mn>1000</mn></mrow><annotation encoding="application/x-tex">1000\times 1000</annotation></semantics></math> coordinate grid for consistent formatting across datasets and resolutions.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>From Masks to Ground-Truth Box Lists</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.4">MedGround-35K is built from eight public segmentation datasets(Tab.<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#A1.T5" title="Table 5 ‣ A.1 Data source ‣ Appendix A Datasets ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">5</span></a>), spanning dermatology, microscopy, and radiological imaging. Each dataset provides expert pixel masks <math alttext="M" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math>. For each connected component in <math alttext="M" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2" intent=":literal"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math>, we deterministically derive a tight bounding box <math alttext="b=\mathrm{bbox}(M)" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3" intent=":literal"><semantics><mrow><mi>b</mi><mo>=</mo><mrow><mi>bbox</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><mi>M</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">b=\mathrm{bbox}(M)</annotation></semantics></math>, and collect all boxes within the same image into a candidate list <math alttext="\mathcal{B}=\{b_{1},\ldots,b_{n}\}" class="ltx_Math" display="inline" id="S3.SS2.p1.4.m4" intent=":literal"><semantics><mrow><mi class="ltx_font_mathcaligraphic">ℬ</mi><mo>=</mo><mrow><mo stretchy="false">{</mo><msub><mi>b</mi><mn>1</mn></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>b</mi><mi>n</mi></msub><mo stretchy="false">}</mo></mrow></mrow><annotation encoding="application/x-tex">\mathcal{B}=\{b_{1},\ldots,b_{n}\}</annotation></semantics></math>.
This box list serves two roles: it provides the query generator (VLMs) with a <em class="ltx_emph ltx_font_italic" id="S3.SS2.p1.4.1">candidate target pool</em>, i.e., explicit regions to refer to when composing queries, from which one or multiple boxes are randomly selected as the intended target(s); and it serves as the <em class="ltx_emph ltx_font_italic" id="S3.SS2.p1.4.2">ground-truth localization anchor</em> during verification, supplying the reference boxes needed to judge whether the generated text is visually faithful and unambiguous with respect to the image.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Mask-Guided Query Synthesis</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.2">Given an image <math alttext="I" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1" intent=":literal"><semantics><mi>I</mi><annotation encoding="application/x-tex">I</annotation></semantics></math> and its box list <math alttext="\mathcal{B}" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">ℬ</mi><annotation encoding="application/x-tex">\mathcal{B}</annotation></semantics></math>, we first compute mask-derived attributes to constrain generation:
<span class="ltx_text ltx_font_bold" id="S3.SS3.p1.2.1">(1) geometry</span>: area ratio, width/height, aspect ratio, elongation/compactness proxies,
<span class="ltx_text ltx_font_bold" id="S3.SS3.p1.2.2">(2) spatial cues</span>: centroid, coarse bins such as left/right and upper/lower, optionally with dataset-specific anatomical conventions,
and <span class="ltx_text ltx_font_bold" id="S3.SS3.p1.2.3">(3) metadata</span>: imaging modality/domain and coarse category labels when available.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">We then construct dataset-aware prompts and query VLMs to synthesize referring queries. The prompts explicitly condition on the image modality, and instruct the VLMs to produce questions that:
(1) reflect <em class="ltx_emph ltx_font_italic" id="S3.SS3.p2.1.1">visible</em> properties (shape/texture/boundary appearance),
(2) incorporate location cues when needed for disambiguation, and
(3) use appropriate medical terminology while avoiding claims that cannot be justified from the image (e.g., etiology, pathology stage, or non-visible symptoms).</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">To encourage diversity and avoid trivial templates, we ask the VLMs to <em class="ltx_emph ltx_font_italic" id="S3.SS3.p3.1.1">randomly select</em> one or multiple target boxes from <math alttext="\mathcal{B}" class="ltx_Math" display="inline" id="S3.SS3.p3.1.m1" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">ℬ</mi><annotation encoding="application/x-tex">\mathcal{B}</annotation></semantics></math> and generate a corresponding query. Each generated sample is required to follow a fixed JSON schema containing the query text and the selected target box coordinates, enabling downstream automatic parsing.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="582" id="S3.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span class="ltx_text ltx_font_bold" id="S3.F3.2.1">Diversity and Linguistic Complexity of the MedGround dataset.</span> Up: The word cloud illustrates the distribution of medical terminology, anatomical landmarks, and clinical descriptors within the grounding annotations. Down: Comparative analysis of annotation richness across five distinct modalities based on three key metrics: (a) Clinical Entity Density, (b) Morphology Term Coverage, and (c) Spatial Relation Complexity.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Multi-Stage Data Verification and Cleaning</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Since free-form generation may introduce ambiguity or visually unsupported descriptions, MedGround applies a multi-stage verifier to filter noisy samples.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p2.1.1">Stage I: format and schema checks.</span>
We reject generations that do not conform to the required JSON format, contain missing fields, or output invalid box indices/coordinates.</p>
</div>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p3.1.1">Stage II: rule-based validation with geometry and location constraints.</span>
We enforce consistency between textual descriptors and mask-derived attributes. For example, size adjectives must match area buckets, spatial phrases must match centroid bins, and prompts are constrained by domain-specific keyword allow/deny lists to prevent cross-domain leakage (e.g., thoracic anatomy terms in dermoscopy). Samples violating these rules are discarded.</p>
</div>
<div class="ltx_para" id="S3.SS4.p4">
<p class="ltx_p" id="S3.SS4.p4.2"><span class="ltx_text ltx_font_bold" id="S3.SS4.p4.2.1">Stage III: VLM-based consistency filtering.</span>
We performs image-based data cleaning using a VLMs as a semantic verifier. Recall that during synthesis, the generator first selects one or multiple target box(es) from the candidate pool <math alttext="\mathcal{B}" class="ltx_Math" display="inline" id="S3.SS4.p4.1.m1" intent=":literal"><semantics><mi class="ltx_font_mathcaligraphic">ℬ</mi><annotation encoding="application/x-tex">\mathcal{B}</annotation></semantics></math> and then writes a question to refer to the selected region(s); therefore, the “answer” <math alttext="A" class="ltx_Math" display="inline" id="S3.SS4.p4.2.m2" intent=":literal"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> of each QA pair is exactly the selected ground-truth box(es).</p>
</div>
<div class="ltx_para" id="S3.SS4.p5">
<p class="ltx_p" id="S3.SS4.p5.1">We feed the verifier the image together with the selected answer box(es) as an explicit localization anchor, and ask whether the visual content inside the box supports the question description. Concretely, the verifier is instructed to restate the observable attributes of the highlighted region and output a binary decision on whether the question is <em class="ltx_emph ltx_font_italic" id="S3.SS4.p5.1.1">correctly grounded</em> and <em class="ltx_emph ltx_font_italic" id="S3.SS4.p5.1.2">unambiguous</em>. If the verifier cannot recover the described cues from the given box, we treat the sample as hallucinated or ambiguous and discard it.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>MedGround-35K</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.3">MedGround-35K comprises <math alttext="35,480" class="ltx_Math" display="inline" id="S3.SS5.p1.1.m1" intent=":literal"><semantics><mrow><mn>35</mn><mo>,</mo><mn>480</mn></mrow><annotation encoding="application/x-tex">35,480</annotation></semantics></math> image–text–box triplets, with <math alttext="25,420" class="ltx_Math" display="inline" id="S3.SS5.p1.2.m2" intent=":literal"><semantics><mrow><mn>25</mn><mo>,</mo><mn>420</mn></mrow><annotation encoding="application/x-tex">25,420</annotation></semantics></math> for training and <math alttext="10,060" class="ltx_Math" display="inline" id="S3.SS5.p1.3.m3" intent=":literal"><semantics><mrow><mn>10</mn><mo>,</mo><mn>060</mn></mrow><annotation encoding="application/x-tex">10,060</annotation></semantics></math> for testing.</p>
</div>
<div class="ltx_para" id="S3.SS5.p2">
<p class="ltx_p" id="S3.SS5.p2.1">To characterize the semantic richness of the synthesized queries, we quantify three linguistically grounded properties: (1) <span class="ltx_text ltx_font_bold" id="S3.SS5.p2.1.1">Clinical entity density:</span> Measured by extracting unique UMLS<cite class="ltx_cite ltx_citemacro_cite">Bodenreider (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib70" title="The unified medical language system (umls): integrating biomedical terminology">2004</a>)</cite> concepts via SciSpacy and normalizing by query length. (2) <span class="ltx_text ltx_font_bold" id="S3.SS5.p2.1.2">Morphology coverage:</span> Assessed using a modality-aware lexicon of appearance descriptors across CT, dermoscopy, microscopy, and ultrasound. (3) <span class="ltx_text ltx_font_bold" id="S3.SS5.p2.1.3">Spatial complexity:</span> Calculated as the average frequency of spatial prepositions and relational phrases per query. Details semantic index of MedGround-35K are shown in Fig.<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S3.F3" title="Figure 3 ‣ 3.3 Mask-Guided Query Synthesis ‣ 3 MedGround ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">3</span></a>. In addition, we log the outcome of each verification stage and report the pass rate per stage in appendix(Tab.<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#A1.T6" title="Table 6 ‣ A.2 Pass rate of multi-stage verifaction pipline ‣ Appendix A Datasets ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">6</span></a>).</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Human Audit</h3>
<div class="ltx_para" id="S3.SS6.p1">
<p class="ltx_p" id="S3.SS6.p1.1">To estimate the faithfulness of retained triplets and quantify residual noise, we conduct a full human audit on the entire MedGround-35K test set. Each triplet is independently reviewed by <span class="ltx_text ltx_font_bold" id="S3.SS6.p1.1.1">three trained professional medical annotators</span> and marked as <span class="ltx_text ltx_font_italic" id="S3.SS6.p1.1.2">good</span> if the referring query is clinically sensible and the target localization is visually consistent. We use <span class="ltx_text ltx_font_bold" id="S3.SS6.p1.1.3">majority vote</span> to define high-confidence samples: a triplet is considered accepted if it receives at least two <span class="ltx_text ltx_font_italic" id="S3.SS6.p1.1.4">good</span> votes (<math alttext="\geq" class="ltx_Math" display="inline" id="S3.SS6.p1.1.m1" intent=":literal"><semantics><mo>≥</mo><annotation encoding="application/x-tex">\geq</annotation></semantics></math>2/3).</p>
</div>
<div class="ltx_para" id="S3.SS6.p2">
<p class="ltx_p" id="S3.SS6.p2.1">Overall, we audit 10,060 test triplets, and the overall majority-vote accept rate is <span class="ltx_text ltx_font_bold" id="S3.SS6.p2.1.1">78%</span>, indicating that most synthesized queries are faithful and visually grounded. We provide the per-dataset audit breakdown in Appendix Tab. <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#A1.T7" title="Table 7 ‣ A.3 Human Audit Results and Failure Analysis ‣ Appendix A Datasets ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS6.p3">
<p class="ltx_p" id="S3.SS6.p3.1">During human auditing, most rejected samples were due to ambiguity. We treat such ambiguity as a form of <span class="ltx_text ltx_font_bold" id="S3.SS6.p3.1.1">natural noise</span> that is difficult to eliminate in real-world medical grounding. Moreover, these lower-agreement subsets constitute only a small fraction of MedGround. Therefore, we keep them in the dataset to preserve coverage and realism, while the appendix transparently documents their audit outcomes and typical error patterns.</p>
</div>
<div class="ltx_para" id="S3.SS6.p4">
<p class="ltx_p" id="S3.SS6.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS6.p4.1.1">Note:</span> In the following experiments, we train models on the <span class="ltx_text ltx_font_bold" id="S3.SS6.p4.1.2">original training split</span> to reflect realistic data conditions and avoid introducing human-selection bias. For evaluation, to ensure reliable and accurate measurement, we report results on the <span class="ltx_text ltx_font_bold" id="S3.SS6.p4.1.3">human-verified test split</span>, where only triplets that pass the audit are retained.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we evaluate the effectiveness of <span class="ltx_text ltx_font_bold" id="S4.p1.1.1">MedGround-35K</span> in enhancing the fine-grained grounding capabilities of VLMs. We focus on whether the synthesized image–text–box triplets can empower models with evidence-grounded reasoning and improve medical semantic alignment beyond generic label-based supervision.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experiment Settings</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We evaluate models on three benchmarks that cover complementary aspects of medical grounding and generalization:</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">Medical Referring Grounding</span>
We use the full test split of MedGround-35K to assess medical referring grounding performance. Each sample consists of an image, a referring query, and the corresponding target box, requiring the model to localize the region described by the query.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.1.1">Semantic Alignment</span>
To probe finer-grained semantic alignment, we construct a semantic test set from two datasets with multiple targets per image, namely MosMedPlus<cite class="ltx_cite ltx_citemacro_cite">Morozov<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib63" title="Mosmeddata: chest ct scans with covid-19 related findings dataset">2020</a>)</cite> and FHPsAOP<cite class="ltx_cite ltx_citemacro_cite">Lu<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib64" title="The jnu-ifm dataset for segmenting pubic symphysis-fetal head">2022</a>); Jieyun and ZhanHong (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib65" title="Pubic symphysis-fetal head segmentation and angle of progression">2024</a>)</cite>. We select multi-target images and collect question pairs that refer to different targets within the same image. This setting explicitly tests whether a model can distinguish subtle semantic cues and ground them to the correct instance among several plausible regions.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p4.1.1">Zero-shot Generalization</span>
We further evaluate zero-shot medical generalization on the QaTa-COV19<cite class="ltx_cite ltx_citemacro_cite">Degerli<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib66" title="COVID-19 infection map generation and detection from chest x-ray images">2021</a>)</cite> dataset, where models are tested without task-specific tuning on it. This benchmark reflects out-of-distribution transfer and measures whether improvements from MedGround training generalize beyond the constructed data distribution.</p>
</div>
<div class="ltx_para" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p5.1.1">Evaluation Metrics</span> We utilize IoU to measure the precision of referring expression grounding.</p>
</div>
<div class="ltx_para" id="S4.SS1.p6">
<p class="ltx_p" id="S4.SS1.p6.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p6.1.1">Training Details</span>
We select MedGemma-27B, MedGemma-4B<cite class="ltx_cite ltx_citemacro_cite">Sellergren<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib59" title="Medgemma technical report">2025</a>)</cite>, Qwen2.5-VL-7B<cite class="ltx_cite ltx_citemacro_cite">Bai<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib60" title="Qwen2. 5-vl technical report">2025b</a>)</cite>, and Qwen3-VL-8B<cite class="ltx_cite ltx_citemacro_cite">Bai<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib61" title="Qwen3-vl technical report">2025a</a>)</cite> as the base models and fine-tune them on the MedGround-35K training data to investigate how fine-grained clinical semantics enhance their medical referring grounding capabilities. To rigorously isolate the impact of linguistic granularity, we also fine-tune these models on a label-based baseline where detailed referring expressions are replaced by coarse category names. All models are fine-tuned using LoRA<cite class="ltx_cite ltx_citemacro_cite">Hu<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib62" title="Lora: low-rank adaptation of large language models.">2022</a>)</cite> on 4 H20 GPUs for three epochs. (Further implementation details and hyper-parameters are provided in Appendix Tab.<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#A3.T9" title="Table 9 ‣ C.1 MedGround-35K Examples ‣ Appendix C Samples ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">9</span></a><math alttext="\sim" class="ltx_Math" display="inline" id="S4.SS1.p6.1.m1" intent=":literal"><semantics><mo>∼</mo><annotation encoding="application/x-tex">\sim</annotation></semantics></math><a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#A3.T12" title="Table 12 ‣ C.1 MedGround-35K Examples ‣ Appendix C Samples ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">12</span></a>).</p>
</div>
<div class="ltx_para" id="S4.SS1.p7">
<p class="ltx_p" id="S4.SS1.p7.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p7.1.1">VLMs Evaluated</span> We compare with both general purpose VLMs and medical expert VLMs. During the evaluation, we manually craft grounding prompts suitable for these VLMs.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Results and Analysis</h3>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span><span class="ltx_text ltx_font_bold" id="S4.T3.4.1">Medical referring grounding performance across benchmarks.</span> We compare base models and their MedGround-finetuned counterparts. Colored deltas indicate changes over the corresponding base model (gain: <span class="ltx_text" id="S4.T3.5.2" style="--ltx-fg-color:#7500AA;">purple</span>, drop: <span class="ltx_text" id="S4.T3.6.3" style="--ltx-fg-color:#F7B3AE;">pink</span>).</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T3.7" style="width:345.0pt;height:71.7pt;vertical-align:-34.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-150.6pt,31.3pt) scale(0.53385,0.53385) ;">
<p class="ltx_p" id="S4.T3.7.1"><span class="ltx_text" id="S4.T3.7.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T3.7.1.1.1" style="width:646.3pt;height:134.4pt;vertical-align:-64.7pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S4.T3.7.1.1.1.1"><span class="ltx_text" id="S4.T3.7.1.1.1.1.1">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T3.7.1.1.1.1.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="S4.T3.7.1.1.1.1.1.1.1.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S4.T3.7.1.1.1.1.1.1.1.1.1" style="padding:0.5pt 2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.1.1.1.1.1.1.1.1.1">Type</span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S4.T3.7.1.1.1.1.1.1.1.1.2" style="padding:0.5pt 2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.1.1.1.1.1.1.1.2.1">Model</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T3.7.1.1.1.1.1.1.1.1.3" style="padding:0.5pt 2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.1.1.1.1.1.1.1.3.1">Size</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.7.1.1.1.1.1.1.1.1.4" style="padding:0.5pt 2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.1.1.1.1.1.1.1.4.1">ISIC2016</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.7.1.1.1.1.1.1.1.1.5" style="padding:0.5pt 2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.1.1.1.1.1.1.1.5.1">BBBC010</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.7.1.1.1.1.1.1.1.1.6" style="padding:0.5pt 2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.1.1.1.1.1.1.1.6.1">BriFiSeg</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.7.1.1.1.1.1.1.1.1.7" style="padding:0.5pt 2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.1.1.1.1.1.1.1.7.1">CellNuclei</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.7.1.1.1.1.1.1.1.1.8" style="padding:0.5pt 2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.1.1.1.1.1.1.1.8.1">DeepBACS</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.7.1.1.1.1.1.1.1.1.9" style="padding:0.5pt 2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.1.1.1.1.1.1.1.9.1">FHPsAOP</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.7.1.1.1.1.1.1.1.1.10" style="padding:0.5pt 2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.1.1.1.1.1.1.1.10.1">MoNuSAC</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.7.1.1.1.1.1.1.1.1.11" style="padding:0.5pt 2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.7.1.1.1.1.1.1.1.1.11.1">MosMedPlus</span></span></span>
<span class="ltx_tr" id="S4.T3.7.1.1.1.1.1.1.2.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t ltx_rowspan ltx_rowspan_2" id="S4.T3.7.1.1.1.1.1.1.2.2.1" style="padding:0.5pt 2.5pt;"><span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.2.2.1.1">General</span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.7.1.1.1.1.1.1.2.2.2" style="padding:0.5pt 2.5pt;">Qwen2.5-VL-7B</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.7.1.1.1.1.1.1.2.2.3" style="padding:0.5pt 2.5pt;">7B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.7.1.1.1.1.1.1.2.2.4" style="padding:0.5pt 2.5pt;">9.9</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.7.1.1.1.1.1.1.2.2.5" style="padding:0.5pt 2.5pt;">0.3</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.7.1.1.1.1.1.1.2.2.6" style="padding:0.5pt 2.5pt;">1.1</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.7.1.1.1.1.1.1.2.2.7" style="padding:0.5pt 2.5pt;">0.5</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.7.1.1.1.1.1.1.2.2.8" style="padding:0.5pt 2.5pt;">0.0</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.7.1.1.1.1.1.1.2.2.9" style="padding:0.5pt 2.5pt;">0.1</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.7.1.1.1.1.1.1.2.2.10" style="padding:0.5pt 2.5pt;">2.3</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.7.1.1.1.1.1.1.2.2.11" style="padding:0.5pt 2.5pt;">1.3</span></span>
<span class="ltx_tr" id="S4.T3.7.1.1.1.1.1.1.3.3">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.7.1.1.1.1.1.1.3.3.1" style="padding:0.5pt 2.5pt;">Qwen3-VL-8B</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.7.1.1.1.1.1.1.3.3.2" style="padding:0.5pt 2.5pt;">8B</span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.3.3.3" style="padding:0.5pt 2.5pt;">81.7</span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.3.3.4" style="padding:0.5pt 2.5pt;">44.7</span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.3.3.5" style="padding:0.5pt 2.5pt;">13.2</span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.3.3.6" style="padding:0.5pt 2.5pt;">15.9</span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.3.3.7" style="padding:0.5pt 2.5pt;">9.0</span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.3.3.8" style="padding:0.5pt 2.5pt;">21.0</span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.3.3.9" style="padding:0.5pt 2.5pt;">13.6</span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.3.3.10" style="padding:0.5pt 2.5pt;">11.1</span></span>
<span class="ltx_tr" id="S4.T3.7.1.1.1.1.1.1.4.4">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t ltx_rowspan ltx_rowspan_3" id="S4.T3.7.1.1.1.1.1.1.4.4.1" style="padding:0.5pt 2.5pt;"><span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.4.4.1.1">
<span class="ltx_inline-block ltx_align_left" id="S4.T3.7.1.1.1.1.1.1.4.4.1.1.1">
<span class="ltx_p" id="S4.T3.7.1.1.1.1.1.1.4.4.1.1.1.1">Medical</span>
<span class="ltx_p" id="S4.T3.7.1.1.1.1.1.1.4.4.1.1.1.2">VLMs</span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.7.1.1.1.1.1.1.4.4.2" style="padding:0.5pt 2.5pt;">Lingshu-7B</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.7.1.1.1.1.1.1.4.4.3" style="padding:0.5pt 2.5pt;">7B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.7.1.1.1.1.1.1.4.4.4" style="padding:0.5pt 2.5pt;">54.8</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.7.1.1.1.1.1.1.4.4.5" style="padding:0.5pt 2.5pt;">6.1</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.7.1.1.1.1.1.1.4.4.6" style="padding:0.5pt 2.5pt;">5.0</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.7.1.1.1.1.1.1.4.4.7" style="padding:0.5pt 2.5pt;">3.2</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.7.1.1.1.1.1.1.4.4.8" style="padding:0.5pt 2.5pt;">2.7</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.7.1.1.1.1.1.1.4.4.9" style="padding:0.5pt 2.5pt;">15.4</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.7.1.1.1.1.1.1.4.4.10" style="padding:0.5pt 2.5pt;">6.0</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.7.1.1.1.1.1.1.4.4.11" style="padding:0.5pt 2.5pt;">6.6</span></span>
<span class="ltx_tr" id="S4.T3.7.1.1.1.1.1.1.5.5">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.7.1.1.1.1.1.1.5.5.1" style="padding:0.5pt 2.5pt;">MedGemma-4B</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.7.1.1.1.1.1.1.5.5.2" style="padding:0.5pt 2.5pt;">4B</span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.5.5.3" style="padding:0.5pt 2.5pt;">50.4</span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.5.5.4" style="padding:0.5pt 2.5pt;">4.0</span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.5.5.5" style="padding:0.5pt 2.5pt;">3.0</span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.5.5.6" style="padding:0.5pt 2.5pt;">2.3</span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.5.5.7" style="padding:0.5pt 2.5pt;">0.6</span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.5.5.8" style="padding:0.5pt 2.5pt;">17.1</span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.5.5.9" style="padding:0.5pt 2.5pt;">3.5</span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.5.5.10" style="padding:0.5pt 2.5pt;">3.5</span></span>
<span class="ltx_tr" id="S4.T3.7.1.1.1.1.1.1.6.6">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.7.1.1.1.1.1.1.6.6.1" style="padding:0.5pt 2.5pt;">MedGemma-27B</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.7.1.1.1.1.1.1.6.6.2" style="padding:0.5pt 2.5pt;">27B</span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.6.6.3" style="padding:0.5pt 2.5pt;">54.5</span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.6.6.4" style="padding:0.5pt 2.5pt;">8.6</span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.6.6.5" style="padding:0.5pt 2.5pt;">2.9</span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.6.6.6" style="padding:0.5pt 2.5pt;">3.2</span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.6.6.7" style="padding:0.5pt 2.5pt;">0.9</span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.6.6.8" style="padding:0.5pt 2.5pt;">22.0</span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.6.6.9" style="padding:0.5pt 2.5pt;">4.4</span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.6.6.10" style="padding:0.5pt 2.5pt;">3.0</span></span>
<span class="ltx_tr" id="S4.T3.7.1.1.1.1.1.1.7.7">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t ltx_rowspan ltx_rowspan_5" id="S4.T3.7.1.1.1.1.1.1.7.7.1" style="padding:0.5pt 2.5pt;"><span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.7.7.1.1">Finetuned</span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.7.1.1.1.1.1.1.7.7.2" style="padding:0.5pt 2.5pt;">Qwen2.5-VL-7B</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.7.1.1.1.1.1.1.7.7.3" style="padding:0.5pt 2.5pt;">7B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.7.1.1.1.1.1.1.7.7.4" style="padding:0.5pt 2.5pt;">83.0 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.7.7.4.1" style="--ltx-fg-color:#7500AA;">(+73.1)</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.7.1.1.1.1.1.1.7.7.5" style="padding:0.5pt 2.5pt;">31.0 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.7.7.5.1" style="--ltx-fg-color:#7500AA;">(+30.7)</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.7.1.1.1.1.1.1.7.7.6" style="padding:0.5pt 2.5pt;">20.2 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.7.7.6.1" style="--ltx-fg-color:#7500AA;">(+19.1)</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.7.1.1.1.1.1.1.7.7.7" style="padding:0.5pt 2.5pt;">10.3 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.7.7.7.1" style="--ltx-fg-color:#7500AA;">(+9.8)</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.7.1.1.1.1.1.1.7.7.8" style="padding:0.5pt 2.5pt;">7.1 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.7.7.8.1" style="--ltx-fg-color:#7500AA;">(+7.1)</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.7.1.1.1.1.1.1.7.7.9" style="padding:0.5pt 2.5pt;">77.1 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.7.7.9.1" style="--ltx-fg-color:#7500AA;">(+77.0)</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.7.1.1.1.1.1.1.7.7.10" style="padding:0.5pt 2.5pt;">10.9 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.7.7.10.1" style="--ltx-fg-color:#7500AA;">(+8.6)</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.7.1.1.1.1.1.1.7.7.11" style="padding:0.5pt 2.5pt;">30.1 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.7.7.11.1" style="--ltx-fg-color:#7500AA;">(+28.8)</span></span></span>
<span class="ltx_tr" id="S4.T3.7.1.1.1.1.1.1.8.8">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.7.1.1.1.1.1.1.8.8.1" style="padding:0.5pt 2.5pt;">Qwen3-VL-8B</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.7.1.1.1.1.1.1.8.8.2" style="padding:0.5pt 2.5pt;">8B</span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.8.8.3" style="padding:0.5pt 2.5pt;">86.4 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.8.8.3.1" style="--ltx-fg-color:#7500AA;">(+4.7)</span></span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.8.8.4" style="padding:0.5pt 2.5pt;">46.5 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.8.8.4.1" style="--ltx-fg-color:#7500AA;">(+1.8)</span></span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.8.8.5" style="padding:0.5pt 2.5pt;">24.6 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.8.8.5.1" style="--ltx-fg-color:#7500AA;">(+11.4)</span></span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.8.8.6" style="padding:0.5pt 2.5pt;">34.7 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.8.8.6.1" style="--ltx-fg-color:#7500AA;">(+18.8)</span></span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.8.8.7" style="padding:0.5pt 2.5pt;">23.0 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.8.8.7.1" style="--ltx-fg-color:#7500AA;">(+14.0)</span></span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.8.8.8" style="padding:0.5pt 2.5pt;">81.0 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.8.8.8.1" style="--ltx-fg-color:#7500AA;">(+60.0)</span></span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.8.8.9" style="padding:0.5pt 2.5pt;">13.5 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.8.8.9.1" style="--ltx-fg-color:#F7B3AE;">(-0.1)</span></span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.8.8.10" style="padding:0.5pt 2.5pt;">30.1 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.8.8.10.1" style="--ltx-fg-color:#7500AA;">(+19.0)</span></span></span>
<span class="ltx_tr" id="S4.T3.7.1.1.1.1.1.1.9.9">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.7.1.1.1.1.1.1.9.9.1" style="padding:0.5pt 2.5pt;">Lingshu-7B</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.7.1.1.1.1.1.1.9.9.2" style="padding:0.5pt 2.5pt;">7B</span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.9.9.3" style="padding:0.5pt 2.5pt;">84.2 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.9.9.3.1" style="--ltx-fg-color:#7500AA;">(+29.4)</span></span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.9.9.4" style="padding:0.5pt 2.5pt;">44.1 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.9.9.4.1" style="--ltx-fg-color:#7500AA;">(+38.0)</span></span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.9.9.5" style="padding:0.5pt 2.5pt;">19.6 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.9.9.5.1" style="--ltx-fg-color:#7500AA;">(+14.6)</span></span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.9.9.6" style="padding:0.5pt 2.5pt;">9.7 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.9.9.6.1" style="--ltx-fg-color:#7500AA;">(+6.5)</span></span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.9.9.7" style="padding:0.5pt 2.5pt;">8.1 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.9.9.7.1" style="--ltx-fg-color:#7500AA;">(+5.4)</span></span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.9.9.8" style="padding:0.5pt 2.5pt;">79.0 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.9.9.8.1" style="--ltx-fg-color:#7500AA;">(+63.6)</span></span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.9.9.9" style="padding:0.5pt 2.5pt;">10.9 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.9.9.9.1" style="--ltx-fg-color:#7500AA;">(+4.9)</span></span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.9.9.10" style="padding:0.5pt 2.5pt;">36.3 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.9.9.10.1" style="--ltx-fg-color:#7500AA;">(+29.7)</span></span></span>
<span class="ltx_tr" id="S4.T3.7.1.1.1.1.1.1.10.10">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.7.1.1.1.1.1.1.10.10.1" style="padding:0.5pt 2.5pt;">MedGemma-4B</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.7.1.1.1.1.1.1.10.10.2" style="padding:0.5pt 2.5pt;">4B</span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.10.10.3" style="padding:0.5pt 2.5pt;">71.2 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.10.10.3.1" style="--ltx-fg-color:#7500AA;">(+20.8)</span></span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.10.10.4" style="padding:0.5pt 2.5pt;">16.3 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.10.10.4.1" style="--ltx-fg-color:#7500AA;">(+12.3)</span></span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.10.10.5" style="padding:0.5pt 2.5pt;">6.2 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.10.10.5.1" style="--ltx-fg-color:#7500AA;">(+3.2)</span></span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.10.10.6" style="padding:0.5pt 2.5pt;">4.7 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.10.10.6.1" style="--ltx-fg-color:#7500AA;">(+2.4)</span></span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.10.10.7" style="padding:0.5pt 2.5pt;">0.0 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.10.10.7.1" style="--ltx-fg-color:#F7B3AE;">(-0.6)</span></span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.10.10.8" style="padding:0.5pt 2.5pt;">72.8 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.10.10.8.1" style="--ltx-fg-color:#7500AA;">(+55.7)</span></span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.10.10.9" style="padding:0.5pt 2.5pt;">5.3 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.10.10.9.1" style="--ltx-fg-color:#7500AA;">(+1.8)</span></span>
<span class="ltx_td ltx_align_center" id="S4.T3.7.1.1.1.1.1.1.10.10.10" style="padding:0.5pt 2.5pt;">33.1 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.10.10.10.1" style="--ltx-fg-color:#7500AA;">(+29.6)</span></span></span>
<span class="ltx_tr" id="S4.T3.7.1.1.1.1.1.1.11.11">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T3.7.1.1.1.1.1.1.11.11.1" style="padding:0.5pt 2.5pt;">MedGemma-27B</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T3.7.1.1.1.1.1.1.11.11.2" style="padding:0.5pt 2.5pt;">27B</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.7.1.1.1.1.1.1.11.11.3" style="padding:0.5pt 2.5pt;">81.2 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.11.11.3.1" style="--ltx-fg-color:#7500AA;">(+26.7)</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.7.1.1.1.1.1.1.11.11.4" style="padding:0.5pt 2.5pt;">26.8 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.11.11.4.1" style="--ltx-fg-color:#7500AA;">(+18.2)</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.7.1.1.1.1.1.1.11.11.5" style="padding:0.5pt 2.5pt;">11.6 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.11.11.5.1" style="--ltx-fg-color:#7500AA;">(+8.7)</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.7.1.1.1.1.1.1.11.11.6" style="padding:0.5pt 2.5pt;">11.2 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.11.11.6.1" style="--ltx-fg-color:#7500AA;">(+8.0)</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.7.1.1.1.1.1.1.11.11.7" style="padding:0.5pt 2.5pt;">0.4 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.11.11.7.1" style="--ltx-fg-color:#F7B3AE;">(-0.5)</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.7.1.1.1.1.1.1.11.11.8" style="padding:0.5pt 2.5pt;">80.6 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.11.11.8.1" style="--ltx-fg-color:#7500AA;">(+58.6)</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.7.1.1.1.1.1.1.11.11.9" style="padding:0.5pt 2.5pt;">9.0 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.11.11.9.1" style="--ltx-fg-color:#7500AA;">(+4.6)</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.7.1.1.1.1.1.1.11.11.10" style="padding:0.5pt 2.5pt;">39.3 <span class="ltx_text" id="S4.T3.7.1.1.1.1.1.1.11.11.10.1" style="--ltx-fg-color:#7500AA;">(+36.3)</span></span></span>
</span>
</span></span></span>
</span></span></span></p>
</span></div>
</figure>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">MedGround effectively enhances VLMs’ medical referring grounding capability.</h4>
<div class="ltx_para" id="S4.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px1.p1.1">We evaluate the performance of VLM models trained on MedGround-35K. As shown in Tab.<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S4.T3" title="Table 3 ‣ 4.2 Results and Analysis ‣ 4 Experiments ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">3</span></a>, models fine-tuned with our knowledge-aware triplets demonstrate significant performance gains.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="688" id="S4.F4.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span class="ltx_text ltx_font_bold" id="S4.F4.2.1">Label-based Dataset vs. MedGround-35K.</span> (A) Label-based Dataset: conventional datasets typically group all ground-truth boxes under a single generic category label (e.g., "lesion") for the entire image. (B) MedGround-35K: provides distinct, fine-grained descriptive expressions for each localized box, capturing specific clinical nuances for individual regions.</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="390" id="S4.F5.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span class="ltx_text ltx_font_bold" id="S4.F5.2.1">Examples of the Semantic Sensitivity Testing Dataset</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.SSS0.Px1.p2">
<p class="ltx_p" id="S4.SS2.SSS0.Px1.p2.1">The experimental results demonstrate that fine-tuning on the MedGround-35K serves as a transformative catalyst for the medical referring grounding capabilities of both general and medical-specific VLMs. By providing high-quality spatial-instructional alignment, MedGround enables models to overcome the limitations of zero-shot reasoning.
This consistent improvement across diverse modalities and model scales underscores MedGround’s efficacy in distilling specialized medical spatial knowledge into large-scale models, effectively bridging the gap between general visual perception and precise clinical localization.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS0.Px1.p3">
<p class="ltx_p" id="S4.SS2.SSS0.Px1.p3.1">While MedGround-35K significantly improved overall performance, the MedGemma series showed minor regressions on DeepBACS<cite class="ltx_cite ltx_citemacro_cite">Spahn<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib67" title="DeepBacs for multi-task bacterial image analysis using open-source deep learning approaches">2022</a>); Spahn and Heilemann (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib68" title="Deepbacs–escherichia coli bright field segmentation dataset">2021</a>)</cite> (e.g., -0.5 for MedGemma-27B). This is likely due to the domain shift between specialized fluorescence microscopy and the macro-level clinical imagery in MedGround. Fine-tuning may have caused a slight "feature forgetting" of niche microscopic traits while prioritizing general anatomical logic—a minor trade-off compared to the substantial gains achieved across the broader clinical spectrum.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">MedGround injects fine-grained medical semantic knowledge into VLMs.</h4>
<div class="ltx_para" id="S4.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p1.1">We evaluate whether MedGround injects finer-grained clinical semantics than coarse label-level supervision using a Semantic Alignment setting. Concretely, we fine-tune the same backbone with LoRA on (i) MedGround-35K (clinically detailed referring expressions) and (ii) a label-based baseline built by pairing each annotated region with its coarse category name (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S4.F4" title="Figure 4 ‣ MedGround effectively enhances VLMs’ medical referring grounding capability. ‣ 4.2 Results and Analysis ‣ 4 Experiments ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">4</span></a>). We then compare the two models on the Semantic Alignment Evaluation benchmark.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS0.Px2.p2">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p2.1">We propose <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS0.Px2.p2.1.1">Semantic Sensitivity</span> to measure whether a model can follow subtle semantic differences in multi-target images. Each test case contains one image with multiple targets and two queries referring to different objects (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S4.F5" title="Figure 5 ‣ MedGround effectively enhances VLMs’ medical referring grounding capability. ‣ 4.2 Results and Analysis ‣ 4 Experiments ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">5</span></a>). For each query, we compute the IoU between the predicted box and its corresponding ground-truth box; a target is counted as correct if IoU exceeds a threshold <math alttext="\tau" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px2.p2.1.m1" intent=":literal"><semantics><mi>τ</mi><annotation encoding="application/x-tex">\tau</annotation></semantics></math>. The test case is scored as 1 only if <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS0.Px2.p2.1.2">both</em> queries are correctly localized (otherwise 0). SS is the average score over all test cases.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span><span class="ltx_text ltx_font_bold" id="S4.T4.2.1">Comparison of the SS metric.</span> Grounding performance is reported in Semantic Sensitivity (%). Comparison across zero-shot baselines, label-based SFT, and MedGround SFT.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T4.3" style="width:433.6pt;height:148pt;vertical-align:-71.0pt;"><span class="ltx_transformed_inner" style="transform:translate(37.5pt,-12.8pt) scale(1.20936544415062,1.20936544415062) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T4.3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T4.3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T4.3.1.1.1.1.1">Base Model</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T4.3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T4.3.1.1.1.2.1">Training Data</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.3.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T4.3.1.1.1.3.1">FHPsAOP</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.3.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T4.3.1.1.1.4.1">MosMedPlus</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.3.1.2.1.1" rowspan="3"><span class="ltx_text" id="S4.T4.3.1.2.1.1.1">Qwen3VL-8B</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.3.1.2.1.2">w/o SFT</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.3.1.2.1.3">21.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.3.1.2.1.4">5.9</td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.3.1.3.2.1">Label-based SFT</th>
<td class="ltx_td ltx_align_center" id="S4.T4.3.1.3.2.2">45.3</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.1.3.2.3">5.8</td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.3.1.4.3.1"><span class="ltx_text ltx_font_bold" id="S4.T4.3.1.4.3.1.1">MedGround-35K SFT</span></th>
<td class="ltx_td ltx_align_center" id="S4.T4.3.1.4.3.2"><span class="ltx_text ltx_font_bold" id="S4.T4.3.1.4.3.2.1">53.5</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.1.4.3.3"><span class="ltx_text ltx_font_bold" id="S4.T4.3.1.4.3.3.1">18.0</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.3.1.5.4.1" rowspan="3"><span class="ltx_text" id="S4.T4.3.1.5.4.1.1">MedGemma-4B</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.3.1.5.4.2">w/o SFT</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.3.1.5.4.3">2.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.3.1.5.4.4">1.4</td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.3.1.6.5.1">Label-based SFT</th>
<td class="ltx_td ltx_align_center" id="S4.T4.3.1.6.5.2">21.3</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.1.6.5.3">4.4</td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.3.1.7.6.1"><span class="ltx_text ltx_font_bold" id="S4.T4.3.1.7.6.1.1">MedGround-35K SFT</span></th>
<td class="ltx_td ltx_align_center" id="S4.T4.3.1.7.6.2"><span class="ltx_text ltx_font_bold" id="S4.T4.3.1.7.6.2.1">55.9</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.1.7.6.3"><span class="ltx_text ltx_font_bold" id="S4.T4.3.1.7.6.3.1">13.1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S4.T4.3.1.8.7.1" rowspan="3"><span class="ltx_text" id="S4.T4.3.1.8.7.1.1">MedGemma-27B</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.3.1.8.7.2">w/o SFT</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.3.1.8.7.3">12.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.3.1.8.7.4">0.0</td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.3.1.9.8.1">Label-based SFT</th>
<td class="ltx_td ltx_align_center" id="S4.T4.3.1.9.8.2">27.0</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.1.9.8.3">2.3</td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T4.3.1.10.9.1"><span class="ltx_text ltx_font_bold" id="S4.T4.3.1.10.9.1.1">MedGround-35K SFT</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.3.1.10.9.2"><span class="ltx_text ltx_font_bold" id="S4.T4.3.1.10.9.2.1">74.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.3.1.10.9.3"><span class="ltx_text ltx_font_bold" id="S4.T4.3.1.10.9.3.1">22.5</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S4.SS2.SSS0.Px2.p3">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p3.1">As shown in Tab. <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S4.T4" title="Table 4 ‣ MedGround injects fine-grained medical semantic knowledge into VLMs. ‣ 4.2 Results and Analysis ‣ 4 Experiments ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">4</span></a>, models without medical referring grounding training (w/o SFT) exhibit very low Semantic Sensitivity, indicating limited ability to follow query-specific semantics and localize the correct targets in multi-object clinical images. After fine-tuning, MedGround-35K SFT consistently outperforms the label-based SFT baseline on both FHPsAOP and MosMedPlus. This advantage stems from MedGround’s fine-grained clinical semantics, whereas label-based supervision relies on coarse category names that are often insufficient to distinguish co-existing targets. These findings confirm that fine-grained clinical semantics are essential to bridge the "cognitive–perceptual gap," successfully equipping VLMs with the ability to associate specific morphology- and location-aware descriptions with their corresponding spatial anchors in multi-target environments.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">MedGround Enables Zero-shot Generalization of VLMs.</h4>
<div class="ltx_para" id="S4.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px3.p1.1">We further evaluate whether MedGround improves cross-dataset transfer by testing on an external unseen dataset, QaTa-COV19. We construct a fine-grained referring grounding test set from its referring statements and ground-truth boxes, and compare the performance of the MedGround-fine-tuned model against the base model on this benchmark.</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="297" id="S4.F6.g1" src="figures/zero-shot-new.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span><span class="ltx_text ltx_font_bold" id="S4.F6.2.1">Zero-shot Performance.</span> We evaluate different models on QaTa-COV19 dataset.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.SSS0.Px3.p2">
<p class="ltx_p" id="S4.SS2.SSS0.Px3.p2.1">The zero-shot evaluation on the external QaTa-COV19 dataset reveals that fine-tuning on MedGround-35K yields substantial and consistent improvements in cross-dataset transferability across all tested architectures. As illustrated in the Fig.<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S4.F6" title="Figure 6 ‣ MedGround Enables Zero-shot Generalization of VLMs. ‣ 4.2 Results and Analysis ‣ 4 Experiments ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">6</span></a>, the MedGround-35K enhanced models achieve remarkable performance gains compared to others. This significant performance leap on an entirely unseen dataset—achieved without any task-specific training—strongly demonstrates that MedGround does not merely facilitate dataset-specific memorization but rather imparts a robust, generalized medical spatial reasoning logic. Such results solidify MedGround’s role as a critical foundational resource for enabling VLMs to generalize their referring grounding capabilities to novel clinical scenarios and emerging disease distributions.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We introduce <span class="ltx_text ltx_font_bold" id="S5.p1.1.1">MedGround</span>, a mask-guided synthesis and verification pipeline that constructs medical referring grounding data as image–query–box triplets. By generating fine-grained referring expressions and filtering ambiguous or visually unsupported samples, MedGround provides semantically rich, visually verified supervision. Fine-tuning on MedGround-35K consistently improves grounding performance and transfer to unseen benchmarks, helping narrow the <em class="ltx_emph ltx_font_italic" id="S5.p1.1.2">cognitive–perceptual gap</em> by anchoring medical language to localized visual evidence. Since MedGround-35K is synthesized and verified from existing segmentation resources, it is highly scalable, enabling rapid extension to new modalities and anatomical systems. Beyond grounding, MedGround encourages models to justify clinical semantics with explicit spatial evidence, reducing reliance on linguistic priors and improving faithfulness.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Acknowledgment</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">This work was supported by Damo Academy through Damo Academy Research Intern Program.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Limitations</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">First, the VLM judge (Gemini-2.5-Pro) may introduce model-specific biases. Although the upstream stages of our pipeline are largely deterministic and rule-based, the final acceptance decision is made by a learned verifier and can reflect its preferences, failure modes, or sensitivity to prompt phrasing. Using alternative judges or ensembling multiple verifiers could further reduce this dependency, but is beyond the scope of this work. Second, our primary supervision is box-level rather than pixel-level. While bounding boxes provide scalable and broadly applicable grounding signals, they may not capture fine-grained boundaries or subtle morphology required by certain clinical applications (e.g., precise lesion extent or margin characterization). Third, synthesized queries may inherit LLM stylistic artifacts (e.g., phrasing patterns, verbosity, or implicit assumptions). We mitigate this with constrained prompting and verification, yet some residual biases may remain and could affect downstream generalization. Finally, MedGround is constructed from public segmentation datasets and therefore inherits their label spaces, imaging modalities, and population coverage; as a result, some anatomical regions, pathologies, and acquisition settings are underrepresented, and performance on out-of-distribution clinical data may be limited.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist" id="bib.L1">
<li class="ltx_bibitem ltx_bib_article" id="bib.bib14">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">S. G. Armato, G. McLennan, L. Bidaut, M. F. McNitt-Gray, C. R. Meyer, A. P. Reeves, <span class="ltx_text ltx_bib_etal">et al.</span> (2011)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The lung image database consortium (lidc) and image database resource initiative (idri): a completed reference database of lung nodules on ct scans</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Medical Physics</span> <span class="ltx_text ltx_bib_volume">38</span> (<span class="ltx_text ltx_bib_number">2</span>), <span class="ltx_text ltx_bib_pages"> pp. 915–931</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.1118/1.3528204" title="">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S1.T1.4.2.2.2.2.2.2.6.3.1" title="In 1 Introduction ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">Table 1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib41">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">F. Bai, Y. Du, T. Huang, M. Q. Meng, and B. Zhao (2024)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">M3d: advancing 3d medical image analysis with multi-modal large language models</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2404.00578</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S1.p3.1" title="1 Introduction ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S2.SS2.p1.1" title="2.2 Referring Expression Grounding ‣ 2 Related Work ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§2.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib61">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">S. Bai, Y. Cai, R. Chen, K. Chen, X. Chen, Z. Cheng, L. Deng, W. Ding, C. Gao, C. Ge, W. Ge, Z. Guo, Q. Huang, J. Huang, F. Huang, B. Hui, S. Jiang, Z. Li, M. Li, M. Li, K. Li, Z. Lin, J. Lin, X. Liu, J. Liu, C. Liu, Y. Liu, D. Liu, S. Liu, D. Lu, R. Luo, C. Lv, R. Men, L. Meng, X. Ren, X. Ren, S. Song, Y. Sun, J. Tang, J. Tu, J. Wan, P. Wang, P. Wang, Q. Wang, Y. Wang, T. Xie, Y. Xu, H. Xu, J. Xu, Z. Yang, M. Yang, J. Yang, A. Yang, B. Yu, F. Zhang, H. Zhang, X. Zhang, B. Zheng, H. Zhong, J. Zhou, F. Zhou, J. Zhou, Y. Zhu, and K. Zhu (2025a)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Qwen3-vl technical report</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">2511.21631</span>,
<a class="ltx_ref ltx_bib_external" href="https://arxiv.org/abs/2511.21631" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#A2.SS1.p1.1" title="B.1 Fine-Tuning Details ‣ Appendix B Implementation Details ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§B.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S4.SS1.p6.1" title="4.1 Experiment Settings ‣ 4 Experiments ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§4.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib60">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, <span class="ltx_text ltx_bib_etal">et al.</span> (2025b)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Qwen2. 5-vl technical report</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2502.13923</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#A2.SS1.p1.1" title="B.1 Fine-Tuning Details ‣ Appendix B Implementation Details ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§B.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S4.SS1.p6.1" title="4.1 Experiment Settings ‣ 4 Experiments ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§4.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib37">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">S. Bannur, K. Bouzid, D. C. Castro, A. Schwaighofer, A. Thieme, S. Bond-Taylor, M. Ilse, F. Pérez-García, V. Salvatelli, H. Sharma, <span class="ltx_text ltx_bib_etal">et al.</span> (2024)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Maira-2: grounded radiology report generation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2406.04449</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S1.p2.1" title="1 Introduction ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib70">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">O. Bodenreider (2004)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The unified medical language system (umls): integrating biomedical terminology</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Nucleic acids research</span> <span class="ltx_text ltx_bib_volume">32</span> (<span class="ltx_text ltx_bib_number">suppl_1</span>), <span class="ltx_text ltx_bib_pages"> pp. D267–D270</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S3.SS5.p2.1" title="3.5 MedGround-35K ‣ 3 MedGround ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§3.5</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib16">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">B. Boecking, N. Usuyama, S. Bannur, D. C. de Castro, A. Schwaighofer, S. Hyland, M. T. Wetscherek, T. Naumann, A. Nori, J. A. Valle, <span class="ltx_text ltx_bib_etal">et al.</span> (2022)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Ms-cxr: making the most of text semantics to improve biomedical vision-language processing</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">URL: http://dx. doi. org/10.13026/b90j-vb87</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S1.T1.4.2.2.2.2.2.2.4.1.1" title="In 1 Introduction ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">Table 1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib73">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J. C. Caicedo, A. Goodman, K. W. Karhohs, B. A. Cimini, J. Ackerman, M. Haghighi, C. Heng, T. Becker, M. Doan, C. McQuin, <span class="ltx_text ltx_bib_etal">et al.</span> (2019)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Nucleus segmentation across imaging experiments: the 2018 data science bowl</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Nature methods</span> <span class="ltx_text ltx_bib_volume">16</span> (<span class="ltx_text ltx_bib_number">12</span>), <span class="ltx_text ltx_bib_pages"> pp. 1247–1253</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#A1.SS1.p1.1" title="A.1 Data source ‣ Appendix A Datasets ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§A.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib39">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Y. Chen, D. Xu, Y. Huang, S. Zhan, H. Wang, D. Chen, X. Wang, M. Qiu, and H. Li (2025)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">MIMO: a medical vision language model with visual referring multimodal input and pixel grounding multimodal output</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the Computer Vision and Pattern Recognition Conference</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 24732–24741</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S1.p2.1" title="1 Introduction ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib74">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">N. C. Codella, D. Gutman, M. E. Celebi, B. Helba, M. A. Marchetti, S. W. Dusza, A. Kalloo, K. Liopyris, N. Mishra, H. Kittler, <span class="ltx_text ltx_bib_etal">et al.</span> (2018)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Skin lesion analysis toward melanoma detection: a challenge at the 2017 international symposium on biomedical imaging (isbi), hosted by the international skin imaging collaboration (isic)</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">2018 IEEE 15th international symposium on biomedical imaging (ISBI 2018)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 168–172</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#A1.SS1.p1.1" title="A.1 Data source ‣ Appendix A Datasets ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§A.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib57">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">E. Croxford, Y. Gao, E. First, N. Pellegrino, M. Schnier, J. Caskey, M. Oguss, G. Wills, G. Chen, D. Dligach, <span class="ltx_text ltx_bib_etal">et al.</span> (2025)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Automating evaluation of ai text generation in healthcare with a large language model (llm)-as-a-judge</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">medRxiv</span>, <span class="ltx_text ltx_bib_pages"> pp. 2025–04</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S2.SS3.p1.1" title="2.3 VLMs-based Synthesis and Verification ‣ 2 Related Work ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§2.3</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib66">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. Degerli, M. Ahishali, M. Yamac, S. Kiranyaz, M. E. Chowdhury, K. Hameed, T. Hamid, R. Mazhar, and M. Gabbouj (2021)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">COVID-19 infection map generation and detection from chest x-ray images</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Health information science and systems</span> <span class="ltx_text ltx_bib_volume">9</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp. 15</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S4.SS1.p4.1" title="4.1 Experiment Settings ‣ 4 Experiments ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§4.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib40">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Z. Deng, R. He, J. Liu, Y. Wang, Z. Meng, S. Jiang, Y. Xie, and Z. Liu (2025)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Med-glip: advancing medical language-image pre-training with large-scale grounded dataset</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2508.10528</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S1.p2.1" title="1 Introduction ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S2.SS2.p1.1" title="2.2 Referring Expression Grounding ‣ 2 Related Work ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§2.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib56">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">H. Dong, W. Xiong, D. Goyal, Y. Zhang, W. Chow, R. Pan, S. Diao, J. Zhang, K. Shum, and T. Zhang (2023)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Raft: reward ranked finetuning for generative foundation model alignment</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2304.06767</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S2.SS3.p1.1" title="2.3 VLMs-based Synthesis and Verification ‣ 2 Related Work ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§2.3</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib36">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">H. Ge, L. Hao, Z. Xu, Z. Lin, B. Li, S. Zhou, H. Zhao, and Y. Liu (2025)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">ClinKD: cross-modal clinical knowledge distiller for multi-task medical images</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2502.05928</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S1.p3.1" title="1 Introduction ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib48">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">B. Gundersen, N. Deperrois, S. Ruiperez-Campillo, T. M. Sutter, J. E. Vogt, M. Moor, F. Nooralahzadeh, and M. Krauthammer (2025)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Enhancing radiology report generation and visual grounding using reinforcement learning</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2512.10691</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S2.SS1.p1.1" title="2.1 Medical Vision–Language Models ‣ 2 Related Work ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§2.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib62">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen, <span class="ltx_text ltx_bib_etal">et al.</span> (2022)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Lora: low-rank adaptation of large language models.</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">ICLR</span> <span class="ltx_text ltx_bib_volume">1</span> (<span class="ltx_text ltx_bib_number">2</span>), <span class="ltx_text ltx_bib_pages"> pp. 3</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S4.SS1.p6.1" title="4.1 Experiment Settings ‣ 4 Experiments ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§4.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib35">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">X. Huang, H. Huang, L. Shen, Y. Yang, F. Shang, J. Liu, and J. Liu (2024)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">A refer-and-ground multimodal large language model for biomedicine</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">International Conference on Medical Image Computing and Computer-Assisted Intervention</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 399–409</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S1.p3.1" title="1 Introduction ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib32">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">X. Huang, L. Shen, J. Liu, F. Shang, H. Li, H. Huang, and Y. Yang (2025)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Towards a multimodal large language model with pixel-level insight for biomedicine</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the AAAI Conference on Artificial Intelligence</span>,
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">39</span>, <span class="ltx_text ltx_bib_pages"> pp. 3779–3787</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S1.p1.1" title="1 Introduction ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib65">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">B. Jieyun and O. ZhanHong (2024)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Pubic symphysis-fetal head segmentation and angle of progression</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S4.SS1.p3.1" title="4.1 Experiment Settings ‣ 4 Experiments ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§4.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib51">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. Kamath, M. Singh, Y. LeCun, G. Synnaeve, I. Misra, and N. Carion (2021)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Mdetr-modulated detection for end-to-end multi-modal understanding</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF international conference on computer vision</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 1780–1790</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S2.SS2.p1.1" title="2.2 Referring Expression Grounding ‣ 2 Related Work ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§2.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib55">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">S. Khanna, A. Dejl, K. Yoon, S. Q. Truong, H. Duong, A. Saenz, and P. Rajpurkar (2023)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Radgraph2: modeling disease progression in radiology reports via hierarchical information extraction</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Machine learning for healthcare conference</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 381–402</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S2.SS3.p1.1" title="2.3 VLMs-based Synthesis and Verification ‣ 2 Related Work ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§2.3</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib27">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">S. Y. Kim, S. Cho, V. Yun, and G. Hwang (2025)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">MedCLM: learning to localize and reason via a cot-curriculum in medical vision-language models</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2510.04477</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S1.p1.1" title="1 Introduction ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib25">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">C. Li, C. Wong, S. Zhang, N. Usuyama, H. Liu, J. Yang, T. Naumann, H. Poon, and J. Gao (2023)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Llava-med: training a large language-and-vision assistant for biomedicine in one day</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Advances in Neural Information Processing Systems</span> <span class="ltx_text ltx_bib_volume">36</span>, <span class="ltx_text ltx_bib_pages"> pp. 28541–28564</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S1.p1.1" title="1 Introduction ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S2.SS1.p1.1" title="2.1 Medical Vision–Language Models ‣ 2 Related Work ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§2.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib50">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">L. H. Li, P. Zhang, H. Zhang, J. Yang, C. Li, Y. Zhong, L. Wang, L. Yuan, L. Zhang, J. Hwang, <span class="ltx_text ltx_bib_etal">et al.</span> (2022)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Grounded language-image pre-training</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 10965–10975</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S2.SS2.p1.1" title="2.2 Referring Expression Grounding ‣ 2 Related Work ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§2.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib33">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Q. Li, X. Yan, J. Xu, R. Yuan, Y. Zhang, R. Feng, Q. Shen, X. Zhang, and S. Wang (2024)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Anatomical structure-guided medical vision-language pre-training</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">International Conference on Medical Image Computing and Computer-Assisted Intervention</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 80–90</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S1.p2.1" title="1 Introduction ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib21">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">F. Liu, H. Zhou, B. Gu, X. Zou, J. Huang, J. Wu, Y. Li, S. S. Chen, Y. Hua, P. Zhou, <span class="ltx_text ltx_bib_etal">et al.</span> (2025)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Application of large language models in medicine</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Nature Reviews Bioengineering</span>, <span class="ltx_text ltx_bib_pages"> pp. 1–20</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S1.p1.1" title="1 Introduction ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib47">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J. Liu, Z. Wang, Q. Ye, D. Chong, P. Zhou, and Y. Hua (2023)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Qilin-med-vl: towards chinese large vision-language model for general healthcare</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2310.17956</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S2.SS1.p1.1" title="2.1 Medical Vision–Language Models ‣ 2 Related Work ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§2.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S2.SS2.p1.1" title="2.2 Referring Expression Grounding ‣ 2 Related Work ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§2.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib49">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">S. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, Q. Jiang, C. Li, J. Yang, H. Su, <span class="ltx_text ltx_bib_etal">et al.</span> (2024)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Grounding dino: marrying dino with grounded pre-training for open-set object detection</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">European conference on computer vision</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 38–55</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S2.SS2.p1.1" title="2.2 Referring Expression Grounding ‣ 2 Related Work ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§2.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib71">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">V. Ljosa, K. L. Sokolnicki, and A. E. Carpenter (2012)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Annotated high-throughput microscopy image sets for validation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Nature methods</span> <span class="ltx_text ltx_bib_volume">9</span> (<span class="ltx_text ltx_bib_number">7</span>), <span class="ltx_text ltx_bib_pages"> pp. 637</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#A1.SS1.p1.1" title="A.1 Data source ‣ Appendix A Datasets ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§A.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib64">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Y. Lu, M. Zhou, D. Zhi, M. Zhou, X. Jiang, R. Qiu, Z. Ou, H. Wang, D. Qiu, M. Zhong, <span class="ltx_text ltx_bib_etal">et al.</span> (2022)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The jnu-ifm dataset for segmenting pubic symphysis-fetal head</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Data in brief</span> <span class="ltx_text ltx_bib_volume">41</span>, <span class="ltx_text ltx_bib_pages"> pp. 107904</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#A1.SS1.p1.1" title="A.1 Data source ‣ Appendix A Datasets ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§A.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S4.SS1.p3.1" title="4.1 Experiment Settings ‣ 4 Experiments ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§4.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib30">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">R. Mahmood, P. Yan, D. M. Reyes, G. Wang, M. K. Kalra, P. Kaviani, J. T. Wu, and T. Syeda-Mahmood (2025)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Evaluating automated radiology report quality through fine-grained phrasal grounding of clinical findings</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">2025 IEEE 22nd International Symposium on Biomedical Imaging (ISBI)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 1–5</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S1.p1.1" title="1 Introduction ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib28">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">O. N. Manzari, H. Ahmadabadi, H. Kashiani, S. B. Shokouhi, and A. Ayatollahi (2023)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">MedViT: a robust vision transformer for generalized medical image classification</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Computers in biology and medicine</span> <span class="ltx_text ltx_bib_volume">157</span>, <span class="ltx_text ltx_bib_pages"> pp. 106791</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S1.p1.1" title="1 Introduction ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib72">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">G. Mathieu, E. D. Bachir, <span class="ltx_text ltx_bib_etal">et al.</span> (2022)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Brifiseg: a deep learning-based method for semantic and instance segmentation of nuclei in brightfield images</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2211.03072</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#A1.SS1.p1.1" title="A.1 Data source ‣ Appendix A Datasets ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§A.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib46">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J. Moll, M. Graf, T. Lemke, N. Lenhart, D. Truhn, J. Delbrouck, J. Pan, D. Rueckert, L. C. Adams, and K. K. Bressem (2025)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Evaluating reasoning faithfulness in medical vision-language models using multimodal perturbations</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2510.11196</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S2.SS1.p1.1" title="2.1 Medical Vision–Language Models ‣ 2 Related Work ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§2.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib43">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">M. Moor, Q. Huang, S. Wu, M. Yasunaga, C. Zakka, Y. Dalmia, E. P. Reis, P. Rajpurkar, and J. Leskovec (2023)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Med-flamingo: a multimodal medical few-shot learner (2023)</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">URL: https://arxiv. org/abs/2307.15189</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S2.SS1.p1.1" title="2.1 Medical Vision–Language Models ‣ 2 Related Work ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§2.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib63">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">S. P. Morozov, A. E. Andreychenko, N. A. Pavlov, A. Vladzymyrskyy, N. V. Ledikhova, V. A. Gombolevskiy, I. A. Blokhin, P. B. Gelezhe, A. Gonchar, and V. Y. Chernina (2020)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Mosmeddata: chest ct scans with covid-19 related findings dataset</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2005.06465</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#A1.SS1.p1.1" title="A.1 Data source ‣ Appendix A Datasets ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§A.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S4.SS1.p3.1" title="4.1 Experiment Settings ‣ 4 Experiments ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§4.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib20">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">V. Nath, W. Li, D. Yang, A. Myronenko, M. Zheng, Y. Lu, Z. Liu, H. Yin, Y. M. Law, Y. Tang, <span class="ltx_text ltx_bib_etal">et al.</span> (2025)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Vila-m3: enhancing vision-language models with medical expert knowledge</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the Computer Vision and Pattern Recognition Conference</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 14788–14798</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S1.p1.1" title="1 Introduction ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib31">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">S. Ostmeier, J. Xu, Z. Chen, M. Varma, L. Blankemeier, C. Bluethgen, A. E. M. Md, M. Moseley, C. Langlotz, A. S. Chaudhari, <span class="ltx_text ltx_bib_etal">et al.</span> (2024)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Green: generative radiology report evaluation and error notation</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Findings of the association for computational linguistics: EMNLP 2024</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 374–390</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S1.p1.1" title="1 Introduction ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib19">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. Pal, L. K. Umapathi, and M. Sankarasubbu (2023)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Med-HALT: medical domain hallucination test for large language models</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL)</span>,  <span class="ltx_text ltx_bib_editor">J. Jiang, D. Reitter, and S. Deng (Eds.)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Singapore</span>, <span class="ltx_text ltx_bib_pages"> pp. 314–334</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref doi ltx_bib_external" href="https://dx.doi.org/10.18653/v1/2023.conll-1.21" title="">Document</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S1.p1.1" title="1 Introduction ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S2.SS3.p1.1" title="2.3 VLMs-based Synthesis and Verification ‣ 2 Related Work ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§2.3</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib45">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">C. Pellegrini, E. Özsoy, B. Busam, N. Navab, and M. Keicher (2023)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Radialog: a large vision-language model for radiology report generation and conversational assistance</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2311.18681</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S2.SS1.p1.1" title="2.1 Medical Vision–Language Models ‣ 2 Related Work ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§2.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib59">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. Sellergren, S. Kazemzadeh, T. Jaroensri, A. Kiraly, M. Traverse, T. Kohlberger, S. Xu, F. Jamil, C. Hughes, C. Lau, <span class="ltx_text ltx_bib_etal">et al.</span> (2025)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Medgemma technical report</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2507.05201</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#A2.SS1.p1.1" title="B.1 Fine-Tuning Details ‣ Appendix B Implementation Details ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§B.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S1.p1.1" title="1 Introduction ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S4.SS1.p6.1" title="4.1 Experiment Settings ‣ 4 Experiments ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§4.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib67">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">C. Spahn, E. Gómez-de-Mariscal, R. F. Laine, P. M. Pereira, L. von Chamier, M. Conduit, M. G. Pinho, G. Jacquemet, S. Holden, M. Heilemann, <span class="ltx_text ltx_bib_etal">et al.</span> (2022)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">DeepBacs for multi-task bacterial image analysis using open-source deep learning approaches</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Communications Biology</span> <span class="ltx_text ltx_bib_volume">5</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp. 688</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#A1.SS1.p1.1" title="A.1 Data source ‣ Appendix A Datasets ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§A.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S4.SS2.SSS0.Px1.p3.1" title="MedGround effectively enhances VLMs’ medical referring grounding capability. ‣ 4.2 Results and Analysis ‣ 4 Experiments ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§4.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib68">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">C. Spahn and M. Heilemann (2021)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Deepbacs–escherichia coli bright field segmentation dataset</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">October</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S4.SS2.SSS0.Px1.p3.1" title="MedGround effectively enhances VLMs’ medical referring grounding capability. ‣ 4.2 Results and Analysis ‣ 4 Experiments ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§4.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib38">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">R. Strudel, I. Laptev, and C. Schmid (2022)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Weakly-supervised segmentation of referring expressions</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2205.04725</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S1.p3.1" title="1 Introduction ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib23">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">A. A. Tarhini, P. Dave, and I. El Naqa (2025)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">General artificial intelligence for the diagnosis and treatment of cancer: the rise of foundation models</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">BJR| Artificial Intelligence</span> <span class="ltx_text ltx_bib_volume">2</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp. ubaf015</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S1.p1.1" title="1 Introduction ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib75">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">R. Verma, N. Kumar, A. Patil, N. C. Kurian, S. Rane, S. Graham, Q. D. Vu, M. Zwager, S. E. A. Raza, N. Rajpoot, <span class="ltx_text ltx_bib_etal">et al.</span> (2021)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">MoNuSAC2020: a multi-organ nuclei segmentation and classification challenge</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">IEEE Transactions on Medical Imaging</span> <span class="ltx_text ltx_bib_volume">40</span> (<span class="ltx_text ltx_bib_number">12</span>), <span class="ltx_text ltx_bib_pages"> pp. 3413–3423</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#A1.SS1.p1.1" title="A.1 Data source ‣ Appendix A Datasets ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§A.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib12">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, and R. M. Summers (2017)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">ChestX-ray8: hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S1.T1.4.2.2.2.2.2.2.2.2" title="In 1 Introduction ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">Table 1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib26">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">C. Wu, X. Zhang, Y. Zhang, H. Hui, Y. Wang, and W. Xie (2025)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Towards generalist foundation model for radiology by leveraging web-scale 2d&amp;3d medical data</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Nature Communications</span> <span class="ltx_text ltx_bib_volume">16</span> (<span class="ltx_text ltx_bib_number">1</span>), <span class="ltx_text ltx_bib_pages"> pp. 7866</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S1.p1.1" title="1 Introduction ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib17">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J. T. Wu, N. N. Agu, I. Lourentzou, A. Sharma, J. A. Paguio, J. S. Yao, E. C. Dee, W. Mitchell, S. Kashyap, A. Giovannini, <span class="ltx_text ltx_bib_etal">et al.</span> (2021)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Chest imagenome dataset for clinical reasoning</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2108.00316</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S1.T1.3.1.1.1.1.1.1.1.2" title="In 1 Introduction ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">Table 1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib29">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">Q. Xie, E. J. Schenck, H. S. Yang, Y. Chen, Y. Peng, and F. Wang (2023)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Faithful ai in medicine: a systematic review with large language models and beyond</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">MedRxiv</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S1.p1.1" title="1 Introduction ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib34">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J. Xu (2025)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Uncertainty estimation in large vision language models for automated radiology report generation</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the 4th Machine Learning for Health Symposium. PMLR</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 1039–1052</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S1.p1.1" title="1 Introduction ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib69">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">W. Xu, H. P. Chan, L. Li, M. Aljunied, R. Yuan, J. Wang, C. Xiao, G. Chen, C. Liu, Z. Li, <span class="ltx_text ltx_bib_etal">et al.</span> (2025)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Lingshu: a generalist foundation model for unified multimodal medical understanding and reasoning</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2506.07044</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#A2.SS1.p1.1" title="B.1 Fine-Tuning Details ‣ Appendix B Implementation Details ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§B.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib13">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">K. Yan, X. Wang, L. Lu, and R. M. Summers (2018)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">DeepLesion: automated mining of large-scale lesion annotations and universal lesion detection with deep learning</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Medical Image Computing and Computer Assisted Intervention (MICCAI)</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S1.T1.4.2.2.2.2.2.2.5.2.1" title="In 1 Introduction ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">Table 1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib22">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">L. Yang, S. Xu, A. Sellergren, T. Kohlberger, Y. Zhou, I. Ktena, A. Kiraly, F. Ahmed, F. Hormozdiari, T. Jaroensri, <span class="ltx_text ltx_bib_etal">et al.</span> (2024)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Advancing multimodal medical capabilities of gemini</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2405.03162</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S1.p1.1" title="1 Introduction ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib53">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">X. Yang, J. Liu, P. Wang, G. Wang, Y. Yang, and H. T. Shen (2025)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">New dataset and methods for fine-grained compositional referring expression comprehension via specialist-mllm collaboration</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S2.SS2.p1.1" title="2.2 Referring Expression Grounding ‣ 2 Related Work ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§2.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib54">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">F. Yu, M. Endo, R. Krishnan, I. Pan, A. Tsai, E. P. Reis, E. K. U. N. Fonseca, H. M. H. Lee, Z. S. H. Abad, A. Y. Ng, <span class="ltx_text ltx_bib_etal">et al.</span> (2023)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Evaluating progress in automatic chest x-ray radiology report generation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Patterns</span> <span class="ltx_text ltx_bib_volume">4</span> (<span class="ltx_text ltx_bib_number">9</span>).
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S2.SS3.p1.1" title="2.3 VLMs-based Synthesis and Verification ‣ 2 Related Work ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§2.3</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib58">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">J. Yue, S. Zhang, Z. Jia, H. Xu, Z. Han, X. Liu, and G. Wang (2025)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">MedSG-bench: a benchmark for medical image sequences grounding</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2505.11852</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S2.SS3.p1.1" title="2.3 VLMs-based Synthesis and Verification ‣ 2 Related Work ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§2.3</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib52">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">W. Zhang, S. S. Chandra, and A. Nicolson (2025a)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Anatomical grounding pre-training for medical phrase grounding</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">2025 IEEE 22nd International Symposium on Biomedical Imaging (ISBI)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp. 1–5</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S2.SS2.p1.1" title="2.2 Referring Expression Grounding ‣ 2 Related Work ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§2.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib42">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">W. Zhang, S. S. Chandra, and A. Nicolson (2025b)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Generalized medical phrase grounding</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2512.01085</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S1.p3.1" title="1 Introduction ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib44">
<span class="ltx_tag ltx_bib_author-year ltx_role_refnum ltx_tag_bibitem">X. Zhang, C. Wu, Z. Zhao, W. Lin, Y. Zhang, Y. Wang, and W. Xie (2023)</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Pmc-vqa: visual instruction tuning for medical visual question answering</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2305.10415</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#S2.SS1.p1.1" title="2.1 Medical Vision–Language Models ‣ 2 Related Work ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">§2.1</span></a>.
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Datasets</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Data source</h3>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">MedGround-35K is constructed by repurposing annotations from eight publicly available medical segmentation datasets, covering diverse imaging modalities and clinical scenarios, including BBBC010<cite class="ltx_cite ltx_citemacro_cite">Ljosa<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib71" title="Annotated high-throughput microscopy image sets for validation">2012</a>)</cite>, BriFiSeg<cite class="ltx_cite ltx_citemacro_cite">Mathieu<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib72" title="Brifiseg: a deep learning-based method for semantic and instance segmentation of nuclei in brightfield images">2022</a>)</cite>, CellNuclei<cite class="ltx_cite ltx_citemacro_cite">Caicedo<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib73" title="Nucleus segmentation across imaging experiments: the 2018 data science bowl">2019</a>)</cite>, DeepBacs<cite class="ltx_cite ltx_citemacro_cite">Spahn<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib67" title="DeepBacs for multi-task bacterial image analysis using open-source deep learning approaches">2022</a>)</cite>, FHPsAOP<cite class="ltx_cite ltx_citemacro_cite">Lu<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib64" title="The jnu-ifm dataset for segmenting pubic symphysis-fetal head">2022</a>)</cite>, ISIC2016<cite class="ltx_cite ltx_citemacro_cite">Codella<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib74" title="Skin lesion analysis toward melanoma detection: a challenge at the 2017 international symposium on biomedical imaging (isbi), hosted by the international skin imaging collaboration (isic)">2018</a>)</cite>, MoNuSAC<cite class="ltx_cite ltx_citemacro_cite">Verma<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib75" title="MoNuSAC2020: a multi-organ nuclei segmentation and classification challenge">2021</a>)</cite>, and MosMedPlus<cite class="ltx_cite ltx_citemacro_cite">Morozov<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib63" title="Mosmeddata: chest ct scans with covid-19 related findings dataset">2020</a>)</cite>. For each source dataset, we use its original train/val/test split as the raw data pool before applying our MedGround construction and verification pipeline. Detailed split statistics of these sources are summarized in Tab. <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#A1.T5" title="Table 5 ‣ A.1 Data source ‣ Appendix A Datasets ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure class="ltx_table" id="A1.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Raw split sizes of segmentation data sources before MedGround construction.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T5.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A1.T5.1.1.1.1" style="padding:0.25pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T5.1.1.1.1.1">Dataset</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A1.T5.1.1.1.2" style="padding:0.25pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T5.1.1.1.2.1">Train</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A1.T5.1.1.1.3" style="padding:0.25pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T5.1.1.1.3.1">Val</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A1.T5.1.1.1.4" style="padding:0.25pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T5.1.1.1.4.1">Test</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A1.T5.1.1.1.5" style="padding:0.25pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T5.1.1.1.5.1">Total</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T5.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A1.T5.1.2.1.1" style="padding:0.25pt 4.0pt;">BBBC010</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T5.1.2.1.2" style="padding:0.25pt 4.0pt;">70</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T5.1.2.1.3" style="padding:0.25pt 4.0pt;">10</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T5.1.2.1.4" style="padding:0.25pt 4.0pt;">20</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T5.1.2.1.5" style="padding:0.25pt 4.0pt;">100</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.1.3.2.1" style="padding:0.25pt 4.0pt;">Brifiseg</th>
<td class="ltx_td ltx_align_right" id="A1.T5.1.3.2.2" style="padding:0.25pt 4.0pt;">38,463</td>
<td class="ltx_td ltx_align_right" id="A1.T5.1.3.2.3" style="padding:0.25pt 4.0pt;">4,244</td>
<td class="ltx_td ltx_align_right" id="A1.T5.1.3.2.4" style="padding:0.25pt 4.0pt;">576</td>
<td class="ltx_td ltx_align_right" id="A1.T5.1.3.2.5" style="padding:0.25pt 4.0pt;">43,283</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.1.4.3.1" style="padding:0.25pt 4.0pt;">CellNuclei</th>
<td class="ltx_td ltx_align_right" id="A1.T5.1.4.3.2" style="padding:0.25pt 4.0pt;">469</td>
<td class="ltx_td ltx_align_right" id="A1.T5.1.4.3.3" style="padding:0.25pt 4.0pt;">67</td>
<td class="ltx_td ltx_align_right" id="A1.T5.1.4.3.4" style="padding:0.25pt 4.0pt;">134</td>
<td class="ltx_td ltx_align_right" id="A1.T5.1.4.3.5" style="padding:0.25pt 4.0pt;">670</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.1.5.4.1" style="padding:0.25pt 4.0pt;">DeepBacs</th>
<td class="ltx_td ltx_align_right" id="A1.T5.1.5.4.2" style="padding:0.25pt 4.0pt;">17</td>
<td class="ltx_td ltx_align_right" id="A1.T5.1.5.4.3" style="padding:0.25pt 4.0pt;">2</td>
<td class="ltx_td ltx_align_right" id="A1.T5.1.5.4.4" style="padding:0.25pt 4.0pt;">15</td>
<td class="ltx_td ltx_align_right" id="A1.T5.1.5.4.5" style="padding:0.25pt 4.0pt;">34</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.1.6.5.1" style="padding:0.25pt 4.0pt;">DynamicNuclear</th>
<td class="ltx_td ltx_align_right" id="A1.T5.1.6.5.2" style="padding:0.25pt 4.0pt;">4,950</td>
<td class="ltx_td ltx_align_right" id="A1.T5.1.6.5.3" style="padding:0.25pt 4.0pt;">1,417</td>
<td class="ltx_td ltx_align_right" id="A1.T5.1.6.5.4" style="padding:0.25pt 4.0pt;">717</td>
<td class="ltx_td ltx_align_right" id="A1.T5.1.6.5.5" style="padding:0.25pt 4.0pt;">7,084</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.1.7.6.1" style="padding:0.25pt 4.0pt;">FHPsAOP</th>
<td class="ltx_td ltx_align_right" id="A1.T5.1.7.6.2" style="padding:0.25pt 4.0pt;">5,600</td>
<td class="ltx_td ltx_align_right" id="A1.T5.1.7.6.3" style="padding:0.25pt 4.0pt;">800</td>
<td class="ltx_td ltx_align_right" id="A1.T5.1.7.6.4" style="padding:0.25pt 4.0pt;">1,600</td>
<td class="ltx_td ltx_align_right" id="A1.T5.1.7.6.5" style="padding:0.25pt 4.0pt;">8,000</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.1.8.7.1" style="padding:0.25pt 4.0pt;">ISIC2016</th>
<td class="ltx_td ltx_align_right" id="A1.T5.1.8.7.2" style="padding:0.25pt 4.0pt;">810</td>
<td class="ltx_td ltx_align_right" id="A1.T5.1.8.7.3" style="padding:0.25pt 4.0pt;">90</td>
<td class="ltx_td ltx_align_right" id="A1.T5.1.8.7.4" style="padding:0.25pt 4.0pt;">379</td>
<td class="ltx_td ltx_align_right" id="A1.T5.1.8.7.5" style="padding:0.25pt 4.0pt;">1,279</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.1.9.8.1" style="padding:0.25pt 4.0pt;">MoNuSAC</th>
<td class="ltx_td ltx_align_right" id="A1.T5.1.9.8.2" style="padding:0.25pt 4.0pt;">359</td>
<td class="ltx_td ltx_align_right" id="A1.T5.1.9.8.3" style="padding:0.25pt 4.0pt;">35</td>
<td class="ltx_td ltx_align_right" id="A1.T5.1.9.8.4" style="padding:0.25pt 4.0pt;">249</td>
<td class="ltx_td ltx_align_right" id="A1.T5.1.9.8.5" style="padding:0.25pt 4.0pt;">643</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A1.T5.1.10.9.1" style="padding:0.25pt 4.0pt;">MosMedPlus</th>
<td class="ltx_td ltx_align_right" id="A1.T5.1.10.9.2" style="padding:0.25pt 4.0pt;">1,910</td>
<td class="ltx_td ltx_align_right" id="A1.T5.1.10.9.3" style="padding:0.25pt 4.0pt;">271</td>
<td class="ltx_td ltx_align_right" id="A1.T5.1.10.9.4" style="padding:0.25pt 4.0pt;">547</td>
<td class="ltx_td ltx_align_right" id="A1.T5.1.10.9.5" style="padding:0.25pt 4.0pt;">2,457</td>
</tr>
<tr class="ltx_tr" id="A1.T5.1.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="A1.T5.1.11.10.1" style="padding:0.25pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T5.1.11.10.1.1">Total</span></th>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="A1.T5.1.11.10.2" style="padding:0.25pt 4.0pt;">52,648</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="A1.T5.1.11.10.3" style="padding:0.25pt 4.0pt;">6,936</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="A1.T5.1.11.10.4" style="padding:0.25pt 4.0pt;">4,237</td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_t" id="A1.T5.1.11.10.5" style="padding:0.25pt 4.0pt;">63,550</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Pass rate of multi-stage verifaction pipline</h3>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.2">We analyze rejection reasons across stages (I: format, II: rule-based checks, III: VLM judging) in Tab. <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#A1.T6" title="Table 6 ‣ A.2 Pass rate of multi-stage verifaction pipline ‣ Appendix A Datasets ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">6</span></a>. Stages I and II retain almost all samples across datasets (<math alttext="\sim" class="ltx_Math" display="inline" id="A1.SS2.p1.1.m1" intent=":literal"><semantics><mo>∼</mo><annotation encoding="application/x-tex">\sim</annotation></semantics></math> 92–100%), indicating that invalid formatting and obvious rule violations account for only a small fraction of failures. In contrast, Stage III is substantially more selective and becomes the main source of filtering: retention drops to 48–67% for several microscopy datasets (e.g., BBBC010, CellNuclei, DeepBACS, BriFiSeg), while remaining high for more visually distinctive settings such as FHPSAOP and ISIC2016 (<math alttext="\sim" class="ltx_Math" display="inline" id="A1.SS2.p1.2.m2" intent=":literal"><semantics><mo>∼</mo><annotation encoding="application/x-tex">\sim</annotation></semantics></math>88–90%). Overall, the final retention is about 80% on both splits (80.9% train, 80.3% test), suggesting that the VLM judge primarily removes samples that are <em class="ltx_emph ltx_font_italic" id="A1.SS2.p1.2.1">ambiguous</em> (multiple plausible referents) or <em class="ltx_emph ltx_font_italic" id="A1.SS2.p1.2.2">visually unsupported</em> (the description does not clearly match the highlighted target). This aligns with our human audit, where most rejected cases are due to ambiguity, and supports that MedGround’s verification prioritizes unambiguous, evidence-grounded referring expressions rather than merely enforcing formatting constraints.</p>
</div>
<figure class="ltx_table" id="A1.T6">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span><span class="ltx_text ltx_font_bold" id="A1.T6.2.1">Pass rates of each verification stage across datasets.</span> We report the retention percentage and remaining sample count after Stages A/B/C for both the training and test splits.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A1.T6.3" style="width:345.0pt;height:40.4pt;vertical-align:-18.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-143.3pt,16.8pt) scale(0.54622,0.54622) ;">
<p class="ltx_p" id="A1.T6.3.1"><span class="ltx_text" id="A1.T6.3.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="A1.T6.3.1.1.1" style="width:631.6pt;height:74pt;vertical-align:-34.5pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="A1.T6.3.1.1.1.1"><span class="ltx_text" id="A1.T6.3.1.1.1.1.1">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T6.3.1.1.1.1.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="A1.T6.3.1.1.1.1.1.1.1.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="A1.T6.3.1.1.1.1.1.1.1.1.1" style="padding:1pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T6.3.1.1.1.1.1.1.1.1.1.1">Stage</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_2" id="A1.T6.3.1.1.1.1.1.1.1.1.2" style="padding:1pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T6.3.1.1.1.1.1.1.1.1.2.1">BBBC010</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_2" id="A1.T6.3.1.1.1.1.1.1.1.1.3" style="padding:1pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T6.3.1.1.1.1.1.1.1.1.3.1">BriFiSeg</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_2" id="A1.T6.3.1.1.1.1.1.1.1.1.4" style="padding:1pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T6.3.1.1.1.1.1.1.1.1.4.1">CellNuclei</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_2" id="A1.T6.3.1.1.1.1.1.1.1.1.5" style="padding:1pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T6.3.1.1.1.1.1.1.1.1.5.1">DeepBACS</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_2" id="A1.T6.3.1.1.1.1.1.1.1.1.6" style="padding:1pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T6.3.1.1.1.1.1.1.1.1.6.1">FHPSAOP</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_2" id="A1.T6.3.1.1.1.1.1.1.1.1.7" style="padding:1pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T6.3.1.1.1.1.1.1.1.1.7.1">ISIC2016</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_2" id="A1.T6.3.1.1.1.1.1.1.1.1.8" style="padding:1pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T6.3.1.1.1.1.1.1.1.1.8.1">MoNuSAC</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_2" id="A1.T6.3.1.1.1.1.1.1.1.1.9" style="padding:1pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T6.3.1.1.1.1.1.1.1.1.9.1">MosMedPlus</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" id="A1.T6.3.1.1.1.1.1.1.1.1.10" style="padding:1pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T6.3.1.1.1.1.1.1.1.1.10.1">Total</span></span></span>
<span class="ltx_tr" id="A1.T6.3.1.1.1.1.1.1.2.2">
<span class="ltx_td ltx_th ltx_th_row ltx_border_r" id="A1.T6.3.1.1.1.1.1.1.2.2.1" style="padding:1pt 2.0pt;"></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.2.2.2" style="padding:1pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T6.3.1.1.1.1.1.1.2.2.2.1">Train</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.2.2.3" style="padding:1pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T6.3.1.1.1.1.1.1.2.2.3.1">Test</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.2.2.4" style="padding:1pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T6.3.1.1.1.1.1.1.2.2.4.1">Train</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.2.2.5" style="padding:1pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T6.3.1.1.1.1.1.1.2.2.5.1">Test</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.2.2.6" style="padding:1pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T6.3.1.1.1.1.1.1.2.2.6.1">Train</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.2.2.7" style="padding:1pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T6.3.1.1.1.1.1.1.2.2.7.1">Test</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.2.2.8" style="padding:1pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T6.3.1.1.1.1.1.1.2.2.8.1">Train</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.2.2.9" style="padding:1pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T6.3.1.1.1.1.1.1.2.2.9.1">Test</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.2.2.10" style="padding:1pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T6.3.1.1.1.1.1.1.2.2.10.1">Train</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.2.2.11" style="padding:1pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T6.3.1.1.1.1.1.1.2.2.11.1">Test</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.2.2.12" style="padding:1pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T6.3.1.1.1.1.1.1.2.2.12.1">Train</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.2.2.13" style="padding:1pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T6.3.1.1.1.1.1.1.2.2.13.1">Test</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.2.2.14" style="padding:1pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T6.3.1.1.1.1.1.1.2.2.14.1">Train</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.2.2.15" style="padding:1pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T6.3.1.1.1.1.1.1.2.2.15.1">Test</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.2.2.16" style="padding:1pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T6.3.1.1.1.1.1.1.2.2.16.1">Train</span></span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.2.2.17" style="padding:1pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T6.3.1.1.1.1.1.1.2.2.17.1">Test</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.2.2.18" style="padding:1pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T6.3.1.1.1.1.1.1.2.2.18.1">Train</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.2.2.19" style="padding:1pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T6.3.1.1.1.1.1.1.2.2.19.1">Test</span></span></span>
<span class="ltx_tr" id="A1.T6.3.1.1.1.1.1.1.3.3">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.3.3.1" style="padding:1pt 2.0pt;">Stage I</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.3.3.2" style="padding:1pt 2.0pt;">97.2%</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.3.3.3" style="padding:1pt 2.0pt;">97.0%</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.3.3.4" style="padding:1pt 2.0pt;">98.3%</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.3.3.5" style="padding:1pt 2.0pt;">98.4%</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.3.3.6" style="padding:1pt 2.0pt;">98.1%</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.3.3.7" style="padding:1pt 2.0pt;">98.4%</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.3.3.8" style="padding:1pt 2.0pt;">98.2%</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.3.3.9" style="padding:1pt 2.0pt;">97.2%</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.3.3.10" style="padding:1pt 2.0pt;">100.0%</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.3.3.11" style="padding:1pt 2.0pt;">100.0%</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.3.3.12" style="padding:1pt 2.0pt;">92.1%</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.3.3.13" style="padding:1pt 2.0pt;">92.4%</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.3.3.14" style="padding:1pt 2.0pt;">96.2%</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.3.3.15" style="padding:1pt 2.0pt;">96.7%</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.3.3.16" style="padding:1pt 2.0pt;">96.4%</span>
<span class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.3.3.17" style="padding:1pt 2.0pt;">94.8%</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.3.3.18" style="padding:1pt 2.0pt;">98.3%</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.3.3.19" style="padding:1pt 2.0pt;">97.6%</span></span>
<span class="ltx_tr" id="A1.T6.3.1.1.1.1.1.1.4.4">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T6.3.1.1.1.1.1.1.4.4.1" style="padding:1pt 2.0pt;">Stage II</span>
<span class="ltx_td ltx_align_center" id="A1.T6.3.1.1.1.1.1.1.4.4.2" style="padding:1pt 2.0pt;">96.8%</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="A1.T6.3.1.1.1.1.1.1.4.4.3" style="padding:1pt 2.0pt;">97.0%</span>
<span class="ltx_td ltx_align_center" id="A1.T6.3.1.1.1.1.1.1.4.4.4" style="padding:1pt 2.0pt;">98.1%</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="A1.T6.3.1.1.1.1.1.1.4.4.5" style="padding:1pt 2.0pt;">98.0%</span>
<span class="ltx_td ltx_align_center" id="A1.T6.3.1.1.1.1.1.1.4.4.6" style="padding:1pt 2.0pt;">97.9%</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="A1.T6.3.1.1.1.1.1.1.4.4.7" style="padding:1pt 2.0pt;">98.3%</span>
<span class="ltx_td ltx_align_center" id="A1.T6.3.1.1.1.1.1.1.4.4.8" style="padding:1pt 2.0pt;">98.2%</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="A1.T6.3.1.1.1.1.1.1.4.4.9" style="padding:1pt 2.0pt;">97.2%</span>
<span class="ltx_td ltx_align_center" id="A1.T6.3.1.1.1.1.1.1.4.4.10" style="padding:1pt 2.0pt;">99.4%</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="A1.T6.3.1.1.1.1.1.1.4.4.11" style="padding:1pt 2.0pt;">99.2%</span>
<span class="ltx_td ltx_align_center" id="A1.T6.3.1.1.1.1.1.1.4.4.12" style="padding:1pt 2.0pt;">92.0%</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="A1.T6.3.1.1.1.1.1.1.4.4.13" style="padding:1pt 2.0pt;">92.3%</span>
<span class="ltx_td ltx_align_center" id="A1.T6.3.1.1.1.1.1.1.4.4.14" style="padding:1pt 2.0pt;">95.4%</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="A1.T6.3.1.1.1.1.1.1.4.4.15" style="padding:1pt 2.0pt;">95.8%</span>
<span class="ltx_td ltx_align_center" id="A1.T6.3.1.1.1.1.1.1.4.4.16" style="padding:1pt 2.0pt;">95.6%</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="A1.T6.3.1.1.1.1.1.1.4.4.17" style="padding:1pt 2.0pt;">94.5%</span>
<span class="ltx_td ltx_align_center" id="A1.T6.3.1.1.1.1.1.1.4.4.18" style="padding:1pt 2.0pt;">97.8%</span>
<span class="ltx_td ltx_align_center" id="A1.T6.3.1.1.1.1.1.1.4.4.19" style="padding:1pt 2.0pt;">97.1%</span></span>
<span class="ltx_tr" id="A1.T6.3.1.1.1.1.1.1.5.5">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="A1.T6.3.1.1.1.1.1.1.5.5.1" style="padding:1pt 2.0pt;">Stage III</span>
<span class="ltx_td ltx_align_center" id="A1.T6.3.1.1.1.1.1.1.5.5.2" style="padding:1pt 2.0pt;">56.9%</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="A1.T6.3.1.1.1.1.1.1.5.5.3" style="padding:1pt 2.0pt;">49.3%</span>
<span class="ltx_td ltx_align_center" id="A1.T6.3.1.1.1.1.1.1.5.5.4" style="padding:1pt 2.0pt;">67.1%</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="A1.T6.3.1.1.1.1.1.1.5.5.5" style="padding:1pt 2.0pt;">65.9%</span>
<span class="ltx_td ltx_align_center" id="A1.T6.3.1.1.1.1.1.1.5.5.6" style="padding:1pt 2.0pt;">57.1%</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="A1.T6.3.1.1.1.1.1.1.5.5.7" style="padding:1pt 2.0pt;">56.0%</span>
<span class="ltx_td ltx_align_center" id="A1.T6.3.1.1.1.1.1.1.5.5.8" style="padding:1pt 2.0pt;">48.2%</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="A1.T6.3.1.1.1.1.1.1.5.5.9" style="padding:1pt 2.0pt;">47.2%</span>
<span class="ltx_td ltx_align_center" id="A1.T6.3.1.1.1.1.1.1.5.5.10" style="padding:1pt 2.0pt;">89.7%</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="A1.T6.3.1.1.1.1.1.1.5.5.11" style="padding:1pt 2.0pt;">89.3%</span>
<span class="ltx_td ltx_align_center" id="A1.T6.3.1.1.1.1.1.1.5.5.12" style="padding:1pt 2.0pt;">88.2%</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="A1.T6.3.1.1.1.1.1.1.5.5.13" style="padding:1pt 2.0pt;">88.5%</span>
<span class="ltx_td ltx_align_center" id="A1.T6.3.1.1.1.1.1.1.5.5.14" style="padding:1pt 2.0pt;">63.8%</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="A1.T6.3.1.1.1.1.1.1.5.5.15" style="padding:1pt 2.0pt;">60.6%</span>
<span class="ltx_td ltx_align_center" id="A1.T6.3.1.1.1.1.1.1.5.5.16" style="padding:1pt 2.0pt;">78.4%</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="A1.T6.3.1.1.1.1.1.1.5.5.17" style="padding:1pt 2.0pt;">94.5%</span>
<span class="ltx_td ltx_align_center" id="A1.T6.3.1.1.1.1.1.1.5.5.18" style="padding:1pt 2.0pt;">80.9%</span>
<span class="ltx_td ltx_align_center" id="A1.T6.3.1.1.1.1.1.1.5.5.19" style="padding:1pt 2.0pt;">80.3%</span></span>
<span class="ltx_tr" id="A1.T6.3.1.1.1.1.1.1.6.6">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.6.6.1" style="padding:1pt 2.0pt;">#Remaining</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.6.6.2" style="padding:1pt 2.0pt;">265</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.6.6.3" style="padding:1pt 2.0pt;">66</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.6.6.4" style="padding:1pt 2.0pt;">2369</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.6.6.5" style="padding:1pt 2.0pt;">1212</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.6.6.6" style="padding:1pt 2.0pt;">1651</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.6.6.7" style="padding:1pt 2.0pt;">479</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.6.6.8" style="padding:1pt 2.0pt;">54</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.6.6.9" style="padding:1pt 2.0pt;">51</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.6.6.10" style="padding:1pt 2.0pt;">15291</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.6.6.11" style="padding:1pt 2.0pt;">4363</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.6.6.12" style="padding:1pt 2.0pt;">2523</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.6.6.13" style="padding:1pt 2.0pt;">1191</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.6.6.14" style="padding:1pt 2.0pt;">1193</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.6.6.15" style="padding:1pt 2.0pt;">855</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.6.6.16" style="padding:1pt 2.0pt;">2074</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.6.6.17" style="padding:1pt 2.0pt;">1843</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.6.6.18" style="padding:1pt 2.0pt;">25420</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T6.3.1.1.1.1.1.1.6.6.19" style="padding:1pt 2.0pt;">10060</span></span>
</span>
</span></span></span>
</span></span></span></p>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Human Audit Results and Failure Analysis</h3>
<div class="ltx_para" id="A1.SS3.p1">
<p class="ltx_p" id="A1.SS3.p1.1">From the overall statistics, the lower pass rates are mainly observed in the <span class="ltx_text ltx_font_bold" id="A1.SS3.p1.1.1">nuclei segmentation</span> datasets. By analyzing the auditors’ rejection reasons, we find that most failures stem from "ambiguous matching"—the referring description does not admit a unique target in the image, leading auditors to judge the image–text pair as not well aligned. This is consistent with the inherent properties of nuclei images, where scenes contain numerous visually similar instances and the language often relies on low-dimensional attributes that are insufficient for unique identification.</p>
</div>
<figure class="ltx_table" id="A1.T7">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span><span class="ltx_text ltx_font_bold" id="A1.T7.6.1">Human audit on the MedGround test split.</span> Each sample is reviewed by three auditors. We report the distribution of <span class="ltx_text ltx_font_italic" id="A1.T7.7.2">good</span> votes and the majority-pass rate (Good Ratio, <math alttext="\geq" class="ltx_Math" display="inline" id="A1.T7.2.m1" intent=":literal"><semantics><mo>≥</mo><annotation encoding="application/x-tex">\geq</annotation></semantics></math>2/3 <span class="ltx_text ltx_font_italic" id="A1.T7.8.3">good</span>).</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T7.9">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T7.9.1.1">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A1.T7.9.1.1.1" style="padding:0.75pt 3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T7.9.1.1.1.1">
<span class="ltx_p" id="A1.T7.9.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T7.9.1.1.1.1.1.1">Dataset</span></span>
</span>
</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A1.T7.9.1.1.2" style="padding:0.75pt 3.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T7.9.1.1.2.1">Total</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A1.T7.9.1.1.3" style="padding:0.75pt 3.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T7.9.1.1.3.1">3 good</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A1.T7.9.1.1.4" style="padding:0.75pt 3.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T7.9.1.1.4.1">2 good</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A1.T7.9.1.1.5" style="padding:0.75pt 3.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T7.9.1.1.5.1">1 good</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A1.T7.9.1.1.6" style="padding:0.75pt 3.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T7.9.1.1.6.1">0 good</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="A1.T7.9.1.1.7" style="padding:0.75pt 3.0pt;"><span class="ltx_text ltx_font_bold" id="A1.T7.9.1.1.7.1">Good Ratio</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T7.9.2.1">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_t" id="A1.T7.9.2.1.1" style="padding:0.75pt 3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T7.9.2.1.1.1">
<span class="ltx_p" id="A1.T7.9.2.1.1.1.1">ISIC2016_512_test</span>
</span>
</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t" id="A1.T7.9.2.1.2" style="padding:0.75pt 3.0pt;">1,141</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T7.9.2.1.3" style="padding:0.75pt 3.0pt;">938 (82.2%)</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T7.9.2.1.4" style="padding:0.75pt 3.0pt;">142 (12.4%)</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T7.9.2.1.5" style="padding:0.75pt 3.0pt;">34 (3.0%)</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="A1.T7.9.2.1.6" style="padding:0.75pt 3.0pt;">27 (2.4%)</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t" id="A1.T7.9.2.1.7" style="padding:0.75pt 3.0pt;">94.65%</td>
</tr>
<tr class="ltx_tr" id="A1.T7.9.3.2">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row" id="A1.T7.9.3.2.1" style="padding:0.75pt 3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T7.9.3.2.1.1">
<span class="ltx_p" id="A1.T7.9.3.2.1.1.1">mosmedplus_512_test</span>
</span>
</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="A1.T7.9.3.2.2" style="padding:0.75pt 3.0pt;">1,843</th>
<td class="ltx_td ltx_align_right" id="A1.T7.9.3.2.3" style="padding:0.75pt 3.0pt;">1,531 (83.1%)</td>
<td class="ltx_td ltx_align_right" id="A1.T7.9.3.2.4" style="padding:0.75pt 3.0pt;">57 (3.1%)</td>
<td class="ltx_td ltx_align_right" id="A1.T7.9.3.2.5" style="padding:0.75pt 3.0pt;">49 (2.7%)</td>
<td class="ltx_td ltx_align_right" id="A1.T7.9.3.2.6" style="padding:0.75pt 3.0pt;">206 (11.2%)</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A1.T7.9.3.2.7" style="padding:0.75pt 3.0pt;">86.16%</td>
</tr>
<tr class="ltx_tr" id="A1.T7.9.4.3">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row" id="A1.T7.9.4.3.1" style="padding:0.75pt 3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T7.9.4.3.1.1">
<span class="ltx_p" id="A1.T7.9.4.3.1.1.1">fhpsaop_256_test</span>
</span>
</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="A1.T7.9.4.3.2" style="padding:0.75pt 3.0pt;">4,363</th>
<td class="ltx_td ltx_align_right" id="A1.T7.9.4.3.3" style="padding:0.75pt 3.0pt;">3,167 (72.6%)</td>
<td class="ltx_td ltx_align_right" id="A1.T7.9.4.3.4" style="padding:0.75pt 3.0pt;">479 (11.0%)</td>
<td class="ltx_td ltx_align_right" id="A1.T7.9.4.3.5" style="padding:0.75pt 3.0pt;">233 (5.3%)</td>
<td class="ltx_td ltx_align_right" id="A1.T7.9.4.3.6" style="padding:0.75pt 3.0pt;">484 (11.1%)</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A1.T7.9.4.3.7" style="padding:0.75pt 3.0pt;">83.61%</td>
</tr>
<tr class="ltx_tr" id="A1.T7.9.5.4">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row" id="A1.T7.9.5.4.1" style="padding:0.75pt 3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T7.9.5.4.1.1">
<span class="ltx_p" id="A1.T7.9.5.4.1.1.1">brifiseg_512_test</span>
</span>
</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="A1.T7.9.5.4.2" style="padding:0.75pt 3.0pt;">1,212</th>
<td class="ltx_td ltx_align_right" id="A1.T7.9.5.4.3" style="padding:0.75pt 3.0pt;">334 (27.6%)</td>
<td class="ltx_td ltx_align_right" id="A1.T7.9.5.4.4" style="padding:0.75pt 3.0pt;">500 (41.3%)</td>
<td class="ltx_td ltx_align_right" id="A1.T7.9.5.4.5" style="padding:0.75pt 3.0pt;">283 (23.3%)</td>
<td class="ltx_td ltx_align_right" id="A1.T7.9.5.4.6" style="padding:0.75pt 3.0pt;">95 (7.8%)</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A1.T7.9.5.4.7" style="padding:0.75pt 3.0pt;">68.81%</td>
</tr>
<tr class="ltx_tr" id="A1.T7.9.6.5">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row" id="A1.T7.9.6.5.1" style="padding:0.75pt 3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T7.9.6.5.1.1">
<span class="ltx_p" id="A1.T7.9.6.5.1.1.1">bbbc010_512_test</span>
</span>
</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="A1.T7.9.6.5.2" style="padding:0.75pt 3.0pt;">66</th>
<td class="ltx_td ltx_align_right" id="A1.T7.9.6.5.3" style="padding:0.75pt 3.0pt;">34 (51.5%)</td>
<td class="ltx_td ltx_align_right" id="A1.T7.9.6.5.4" style="padding:0.75pt 3.0pt;">10 (15.2%)</td>
<td class="ltx_td ltx_align_right" id="A1.T7.9.6.5.5" style="padding:0.75pt 3.0pt;">13 (19.7%)</td>
<td class="ltx_td ltx_align_right" id="A1.T7.9.6.5.6" style="padding:0.75pt 3.0pt;">9 (13.6%)</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A1.T7.9.6.5.7" style="padding:0.75pt 3.0pt;">66.67%</td>
</tr>
<tr class="ltx_tr" id="A1.T7.9.7.6">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row" id="A1.T7.9.7.6.1" style="padding:0.75pt 3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T7.9.7.6.1.1">
<span class="ltx_p" id="A1.T7.9.7.6.1.1.1">cellnuclei_256_test</span>
</span>
</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="A1.T7.9.7.6.2" style="padding:0.75pt 3.0pt;">479</th>
<td class="ltx_td ltx_align_right" id="A1.T7.9.7.6.3" style="padding:0.75pt 3.0pt;">88 (18.4%)</td>
<td class="ltx_td ltx_align_right" id="A1.T7.9.7.6.4" style="padding:0.75pt 3.0pt;">147 (30.7%)</td>
<td class="ltx_td ltx_align_right" id="A1.T7.9.7.6.5" style="padding:0.75pt 3.0pt;">158 (33.0%)</td>
<td class="ltx_td ltx_align_right" id="A1.T7.9.7.6.6" style="padding:0.75pt 3.0pt;">86 (18.0%)</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A1.T7.9.7.6.7" style="padding:0.75pt 3.0pt;">49.06%</td>
</tr>
<tr class="ltx_tr" id="A1.T7.9.8.7">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row" id="A1.T7.9.8.7.1" style="padding:0.75pt 3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T7.9.8.7.1.1">
<span class="ltx_p" id="A1.T7.9.8.7.1.1.1">monusac_512_test</span>
</span>
</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row" id="A1.T7.9.8.7.2" style="padding:0.75pt 3.0pt;">855</th>
<td class="ltx_td ltx_align_right" id="A1.T7.9.8.7.3" style="padding:0.75pt 3.0pt;">129 (15.1%)</td>
<td class="ltx_td ltx_align_right" id="A1.T7.9.8.7.4" style="padding:0.75pt 3.0pt;">267 (31.2%)</td>
<td class="ltx_td ltx_align_right" id="A1.T7.9.8.7.5" style="padding:0.75pt 3.0pt;">316 (37.0%)</td>
<td class="ltx_td ltx_align_right" id="A1.T7.9.8.7.6" style="padding:0.75pt 3.0pt;">143 (16.7%)</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="A1.T7.9.8.7.7" style="padding:0.75pt 3.0pt;">46.32%</td>
</tr>
<tr class="ltx_tr" id="A1.T7.9.9.8">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_bb" id="A1.T7.9.9.8.1" style="padding:0.75pt 3.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A1.T7.9.9.8.1.1">
<span class="ltx_p" id="A1.T7.9.9.8.1.1.1">deepbacs_512_test</span>
</span>
</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb" id="A1.T7.9.9.8.2" style="padding:0.75pt 3.0pt;">51</th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A1.T7.9.9.8.3" style="padding:0.75pt 3.0pt;">8 (15.7%)</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A1.T7.9.9.8.4" style="padding:0.75pt 3.0pt;">14 (27.5%)</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A1.T7.9.9.8.5" style="padding:0.75pt 3.0pt;">20 (39.2%)</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="A1.T7.9.9.8.6" style="padding:0.75pt 3.0pt;">9 (17.6%)</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb" id="A1.T7.9.9.8.7" style="padding:0.75pt 3.0pt;">43.14%</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>Train–Test Distribution Analysis in Three Semantic Dimensions</h3>
<div class="ltx_para" id="A1.SS4.p1">
<p class="ltx_p" id="A1.SS4.p1.1">We quantify linguistic properties relevant to grounding(Fig. <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#A1.F7" title="Figure 7 ‣ A.4 Train–Test Distribution Analysis in Three Semantic Dimensions ‣ Appendix A Datasets ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">7</span></a>):
Clinical entity density using medical entity recognition/linking tools (e.g., UMLS-based pipelines).
Morphology term coverage using a curated lexicon of appearance descriptors.
Spatial relation complexity via counts of spatial prepositions and relational phrases.
These analyses show that MedGround queries contain substantially richer morphology and spatial language than category-only prompts, providing more informative supervision for grounding.</p>
</div>
<div class="ltx_para" id="A1.SS4.p2">
<p class="ltx_p" id="A1.SS4.p2.1">First, clinical entity density remains stable between splits for most datasets, indicating similar average numbers of clinically relevant referents per sample. This stability is important for evaluating grounding in multi-target scenes, since the number of co-existing entities directly affects ambiguity and search difficulty.</p>
</div>
<div class="ltx_para" id="A1.SS4.p3">
<p class="ltx_p" id="A1.SS4.p3.1">Second, morphology term coverage shows noticeable cross-dataset variation and a mild train–test gap for some sources. In particular, datasets such as MosMedPlus exhibit higher coverage in the test split, implying that the audited test set contains richer morphological descriptions and thus places greater emphasis on fine-grained visual-semantic alignment. Conversely, nuclei-related datasets (e.g., BBBC010, DeepBACS, MoNuSAC) generally present lower morphology coverage, reflecting the inherent limitation that many instances are visually similar and hard to differentiate with morphology alone.</p>
</div>
<div class="ltx_para" id="A1.SS4.p4">
<p class="ltx_p" id="A1.SS4.p4.1">Third, spatial relation complexity is consistently highest for MosMedPlus in both splits, reflecting that CT findings often require more location-aware language (e.g., lobes, peripheral vs. central, bilateral distribution) to uniquely specify targets. In contrast, pathology and dermoscopy datasets tend to have lower spatial complexity, consistent with their relatively localized lesions or simpler spatial context. This three-dimensional analysis highlights that different modalities stress different aspects of grounding: microscopy emphasizes dense-instance disambiguation, while CT emphasizes spatial reasoning, and the test split preserves (or slightly strengthens) these challenges.</p>
</div>
<figure class="ltx_figure" id="A1.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="325" id="A1.F7.g1" src="figures/train_test_comparison_2x3.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span> Three-dimensional analysis comparing the training split (top row, a–c) and test split (bottom row, d–f) across eight medical imaging datasets. Colors: blue=CT, dark purple=ultrasound, light purple=dermoscopy, green=nuclei, orange=bacteria.</figcaption>
</figure>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Implementation Details</h2>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Fine-Tuning Details</h3>
<div class="ltx_para" id="A2.SS1.p1">
<p class="ltx_p" id="A2.SS1.p1.1">We select five open-source VLM backbones as our base models: Qwen2.5-VL<cite class="ltx_cite ltx_citemacro_cite">Bai<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib60" title="Qwen2. 5-vl technical report">2025b</a>)</cite>, Qwen3-VL<cite class="ltx_cite ltx_citemacro_cite">Bai<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib61" title="Qwen3-vl technical report">2025a</a>)</cite>, MedGemma-4B, MedGemma-27B<cite class="ltx_cite ltx_citemacro_cite">Sellergren<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib59" title="Medgemma technical report">2025</a>)</cite>, and Lingshu-7B<cite class="ltx_cite ltx_citemacro_cite">Xu<span class="ltx_text ltx_bib_etal"> et al.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#bib.bib69" title="Lingshu: a generalist foundation model for unified multimodal medical understanding and reasoning">2025</a>)</cite>. We report the training hyperparameters for each model in Tab.<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#A3.T8" title="Table 8 ‣ C.1 MedGround-35K Examples ‣ Appendix C Samples ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">8</span></a> <math alttext="\sim" class="ltx_Math" display="inline" id="A2.SS1.p1.1.m1" intent=":literal"><semantics><mo>∼</mo><annotation encoding="application/x-tex">\sim</annotation></semantics></math> Tab.<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#A3.T12" title="Table 12 ‣ C.1 MedGround-35K Examples ‣ Appendix C Samples ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">12</span></a>.</p>
</div>
<div class="ltx_para" id="A2.SS1.p2">
<p class="ltx_p" id="A2.SS1.p2.1">Our training data are constructed from multiple segmentation datasets spanning different medical domains and imaging modalities(Tab.<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#A1.T5" title="Table 5 ‣ A.1 Data source ‣ Appendix A Datasets ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">5</span></a>). As a result, the collected training corpus exhibits notable cross-domain imbalance (e.g., microscopy data are substantially larger than some radiology or pathology sources). In our experiments, we do not apply explicit re-balancing strategies (e.g., re-sampling or per-domain weighting); instead, we directly fine-tune models on the naturally imbalanced mixture to reflect realistic data availability.</p>
</div>
<div class="ltx_para" id="A2.SS1.p3">
<p class="ltx_p" id="A2.SS1.p3.1">All training samples are obtained solely through our automatic verification pipeline, without any additional human filtering. In contrast, for evaluation we use a test set where all samples are manually audited to ensure correctness and unambiguity. This protocol isolates the effect of MedGround data quality and avoids overestimating performance due to noisy automatic annotations.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Samples</h2>
<section class="ltx_subsection" id="A3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>MedGround-35K Examples</h3>
<div class="ltx_para" id="A3.SS1.p1">
<p class="ltx_p" id="A3.SS1.p1.1">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#A3.F8" title="Figure 8 ‣ C.1 MedGround-35K Examples ‣ Appendix C Samples ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">8</span></a> presents representative examples from MedGround-35K across imaging modalities. Each example consists of a medical image, a fine-grained referring expression, and the corresponding target annotation, illustrating the diversity of morphology- and location-aware clinical semantics in our dataset.</p>
</div>
<figure class="ltx_figure" id="A3.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="580" id="A3.F8.g1" src="x6.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Examples of MedGround-35K.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="592" id="A3.F9.g1" src="x7.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Examples of MedGround-35K.</figcaption>
</figure>
<div class="ltx_logical-block" id="A3.SS1.10">
<figure class="ltx_table ltx_align_center" id="A3.T8">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 8: </span>Training hyper-parameters used for fine-tuning Lingshu-7B in our experiments.</figcaption>
</figure>
<div class="ltx_para" id="A3.SS1.6.p1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A3.SS1.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.SS1.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="A3.SS1.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="A3.SS1.1.1.2.1.1.1">Hyper-Parameter</span></th>
<td class="ltx_td ltx_align_left ltx_border_tt" id="A3.SS1.1.1.2.1.2"><span class="ltx_text ltx_font_bold" id="A3.SS1.1.1.2.1.2.1">Value</span></td>
</tr>
<tr class="ltx_tr" id="A3.SS1.1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.SS1.1.1.3.2.1">Backbone</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="A3.SS1.1.1.3.2.2">Lingshu-7B</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.1.1.4.3.1">Training stage</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.1.1.4.3.2">SFT</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.1.1.5.4.1">Fine-tuning method</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.1.1.5.4.2">LoRA</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.1.1.6.5.1">LoRA Rank</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.1.1.6.5.2">8</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.1.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.1.1.7.6.1">LoRA Alpha</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.1.1.7.6.2">32</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.1.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.1.1.8.7.1">LoRA Target</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.1.1.8.7.2">all</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.1.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.1.1.9.8.1">Epoch</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.1.1.9.8.2">3</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.1.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.1.1.10.9.1">#GPUs</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.1.1.10.9.2">4</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.1.1.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.1.1.11.10.1">Per-device batch size</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.1.1.11.10.2">4</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.1.1.12.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.1.1.12.11.1">Gradient accumulation</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.1.1.12.11.2">4</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.1.1.13.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.1.1.13.12.1">Global batch size (effective)</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.1.1.13.12.2">64</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.1.1.1.2">Learning rate</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.1.1.1.1"><math alttext="2\times 10^{-4}" class="ltx_Math" display="inline" id="A3.SS1.1.1.1.1.m1" intent=":literal"><semantics><mrow><mn>2</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup></mrow><annotation encoding="application/x-tex">2\times 10^{-4}</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A3.SS1.1.1.14.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.1.1.14.13.1">LR scheduler</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.1.1.14.13.2">Cosine</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.1.1.15.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.1.1.15.14.1">Warm-up ratio</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.1.1.15.14.2">0.1</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.1.1.16.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.1.1.16.15.1">Model max length</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.1.1.16.15.2">2048</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.1.1.17.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.1.1.17.16.1">Precision</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.1.1.17.16.2">BF16</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.1.1.18.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.1.1.18.17.1">Gradient checkpointing</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.1.1.18.17.2">Enabled</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.1.1.19.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A3.SS1.1.1.19.18.1">Random seed</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A3.SS1.1.1.19.18.2">42</td>
</tr>
</tbody>
</table>
</div>
<figure class="ltx_table ltx_align_center" id="A3.T9">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 9: </span>Training hyper-parameters used for fine-tuning Qwen2.5-VL-7B in our experiments.</figcaption>
</figure>
<div class="ltx_para" id="A3.SS1.7.p2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A3.SS1.2.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.SS1.2.2.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="A3.SS1.2.2.2.1.1"><span class="ltx_text ltx_font_bold" id="A3.SS1.2.2.2.1.1.1">Hyper-Parameter</span></th>
<td class="ltx_td ltx_align_left ltx_border_tt" id="A3.SS1.2.2.2.1.2"><span class="ltx_text ltx_font_bold" id="A3.SS1.2.2.2.1.2.1">Value</span></td>
</tr>
<tr class="ltx_tr" id="A3.SS1.2.2.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.SS1.2.2.3.2.1">Backbone</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="A3.SS1.2.2.3.2.2">Qwen2.5-VL-7B-Instruct</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.2.2.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.2.2.4.3.1">Training stage</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.2.2.4.3.2">SFT</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.2.2.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.2.2.5.4.1">Fine-tuning method</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.2.2.5.4.2">LoRA</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.2.2.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.2.2.6.5.1">LoRA Rank</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.2.2.6.5.2">8</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.2.2.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.2.2.7.6.1">LoRA Alpha</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.2.2.7.6.2">32</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.2.2.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.2.2.8.7.1">LoRA Target</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.2.2.8.7.2">all</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.2.2.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.2.2.9.8.1">Epoch</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.2.2.9.8.2">3</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.2.2.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.2.2.10.9.1">#GPUs</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.2.2.10.9.2">4</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.2.2.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.2.2.11.10.1">Per-device batch size</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.2.2.11.10.2">4</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.2.2.12.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.2.2.12.11.1">Gradient accumulation</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.2.2.12.11.2">4</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.2.2.13.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.2.2.13.12.1">Global batch size (effective)</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.2.2.13.12.2">64</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.2.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.2.2.1.2">Learning rate</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.2.2.1.1"><math alttext="2\times 10^{-4}" class="ltx_Math" display="inline" id="A3.SS1.2.2.1.1.m1" intent=":literal"><semantics><mrow><mn>2</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup></mrow><annotation encoding="application/x-tex">2\times 10^{-4}</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A3.SS1.2.2.14.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.2.2.14.13.1">LR scheduler</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.2.2.14.13.2">Cosine</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.2.2.15.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.2.2.15.14.1">Warm-up ratio</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.2.2.15.14.2">0.1</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.2.2.16.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.2.2.16.15.1">Model max length</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.2.2.16.15.2">2048</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.2.2.17.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.2.2.17.16.1">Precision</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.2.2.17.16.2">BF16</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.2.2.18.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.2.2.18.17.1">Gradient checkpointing</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.2.2.18.17.2">Enabled</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.2.2.19.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A3.SS1.2.2.19.18.1">Random seed</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A3.SS1.2.2.19.18.2">42</td>
</tr>
</tbody>
</table>
</div>
<figure class="ltx_table ltx_align_center" id="A3.T10">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 10: </span>Training hyper-parameters used for fine-tuning MedGemma-4B in our experiments.</figcaption>
</figure>
<div class="ltx_para" id="A3.SS1.8.p3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A3.SS1.3.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.SS1.3.3.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="A3.SS1.3.3.2.1.1"><span class="ltx_text ltx_font_bold" id="A3.SS1.3.3.2.1.1.1">Hyper-Parameter</span></th>
<td class="ltx_td ltx_align_left ltx_border_tt" id="A3.SS1.3.3.2.1.2"><span class="ltx_text ltx_font_bold" id="A3.SS1.3.3.2.1.2.1">Value</span></td>
</tr>
<tr class="ltx_tr" id="A3.SS1.3.3.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.SS1.3.3.3.2.1">Backbone</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="A3.SS1.3.3.3.2.2">MedGemma-4B-IT</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.3.3.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.3.3.4.3.1">Training stage</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.3.3.4.3.2">SFT</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.3.3.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.3.3.5.4.1">Fine-tuning method</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.3.3.5.4.2">LoRA</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.3.3.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.3.3.6.5.1">LoRA Rank</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.3.3.6.5.2">8</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.3.3.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.3.3.7.6.1">LoRA Alpha</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.3.3.7.6.2">32</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.3.3.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.3.3.8.7.1">LoRA Target</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.3.3.8.7.2">all</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.3.3.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.3.3.9.8.1">Epoch</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.3.3.9.8.2">3</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.3.3.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.3.3.10.9.1">#GPUs</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.3.3.10.9.2">8</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.3.3.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.3.3.11.10.1">Per-device batch size</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.3.3.11.10.2">16</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.3.3.12.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.3.3.12.11.1">Gradient accumulation</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.3.3.12.11.2">1</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.3.3.13.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.3.3.13.12.1">Global batch size (effective)</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.3.3.13.12.2">128</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.3.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.3.3.1.2">Learning rate</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.3.3.1.1"><math alttext="2\times 10^{-4}" class="ltx_Math" display="inline" id="A3.SS1.3.3.1.1.m1" intent=":literal"><semantics><mrow><mn>2</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup></mrow><annotation encoding="application/x-tex">2\times 10^{-4}</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A3.SS1.3.3.14.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.3.3.14.13.1">LR scheduler</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.3.3.14.13.2">Cosine</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.3.3.15.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.3.3.15.14.1">Warm-up ratio</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.3.3.15.14.2">0.1</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.3.3.16.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.3.3.16.15.1">Model max length</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.3.3.16.15.2">2048</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.3.3.17.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.3.3.17.16.1">Precision</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.3.3.17.16.2">BF16</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.3.3.18.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.3.3.18.17.1">Gradient checkpointing</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.3.3.18.17.2">Enabled</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.3.3.19.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A3.SS1.3.3.19.18.1">Random seed</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A3.SS1.3.3.19.18.2">42</td>
</tr>
</tbody>
</table>
</div>
<figure class="ltx_table ltx_align_center" id="A3.T11">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 11: </span>Training hyper-parameters used for fine-tuning MedGemma-27B in our experiments.</figcaption>
</figure>
<div class="ltx_para" id="A3.SS1.9.p4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A3.SS1.4.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.SS1.4.4.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="A3.SS1.4.4.2.1.1"><span class="ltx_text ltx_font_bold" id="A3.SS1.4.4.2.1.1.1">Hyper-Parameter</span></th>
<td class="ltx_td ltx_align_left ltx_border_tt" id="A3.SS1.4.4.2.1.2"><span class="ltx_text ltx_font_bold" id="A3.SS1.4.4.2.1.2.1">Value</span></td>
</tr>
<tr class="ltx_tr" id="A3.SS1.4.4.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.SS1.4.4.3.2.1">Backbone</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="A3.SS1.4.4.3.2.2">MedGemma-27B-IT</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.4.4.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.4.4.4.3.1">Training stage</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.4.4.4.3.2">SFT</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.4.4.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.4.4.5.4.1">Fine-tuning method</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.4.4.5.4.2">LoRA</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.4.4.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.4.4.6.5.1">LoRA Rank</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.4.4.6.5.2">8</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.4.4.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.4.4.7.6.1">LoRA Alpha</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.4.4.7.6.2">32</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.4.4.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.4.4.8.7.1">LoRA Target</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.4.4.8.7.2">all</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.4.4.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.4.4.9.8.1">Epoch</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.4.4.9.8.2">3</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.4.4.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.4.4.10.9.1">#GPUs</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.4.4.10.9.2">4</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.4.4.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.4.4.11.10.1">Per-device batch size</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.4.4.11.10.2">8</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.4.4.12.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.4.4.12.11.1">Gradient accumulation</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.4.4.12.11.2">2</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.4.4.13.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.4.4.13.12.1">Global batch size (effective)</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.4.4.13.12.2">64</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.4.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.4.4.1.2">Learning rate</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.4.4.1.1"><math alttext="2\times 10^{-4}" class="ltx_Math" display="inline" id="A3.SS1.4.4.1.1.m1" intent=":literal"><semantics><mrow><mn>2</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup></mrow><annotation encoding="application/x-tex">2\times 10^{-4}</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A3.SS1.4.4.14.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.4.4.14.13.1">LR scheduler</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.4.4.14.13.2">Cosine</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.4.4.15.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.4.4.15.14.1">Warm-up ratio</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.4.4.15.14.2">0.1</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.4.4.16.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.4.4.16.15.1">Model max length</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.4.4.16.15.2">2048</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.4.4.17.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.4.4.17.16.1">Precision</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.4.4.17.16.2">BF16</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.4.4.18.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.4.4.18.17.1">Gradient checkpointing</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.4.4.18.17.2">Enabled</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.4.4.19.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A3.SS1.4.4.19.18.1">Random seed</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A3.SS1.4.4.19.18.2">42</td>
</tr>
</tbody>
</table>
</div>
<figure class="ltx_table ltx_align_center" id="A3.T12">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 12: </span>Training hyper-parameters used for fine-tuning Qwen3-VL-8B in our experiments.</figcaption>
</figure>
<div class="ltx_para" id="A3.SS1.10.p5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A3.SS1.5.5">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.SS1.5.5.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="A3.SS1.5.5.2.1.1"><span class="ltx_text ltx_font_bold" id="A3.SS1.5.5.2.1.1.1">Hyper-Parameter</span></th>
<td class="ltx_td ltx_align_left ltx_border_tt" id="A3.SS1.5.5.2.1.2"><span class="ltx_text ltx_font_bold" id="A3.SS1.5.5.2.1.2.1">Value</span></td>
</tr>
<tr class="ltx_tr" id="A3.SS1.5.5.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A3.SS1.5.5.3.2.1">Backbone</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="A3.SS1.5.5.3.2.2">Qwen3-VL-8B-Instruct</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.5.5.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.5.5.4.3.1">Training stage</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.5.5.4.3.2">SFT</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.5.5.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.5.5.5.4.1">Fine-tuning method</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.5.5.5.4.2">LoRA</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.5.5.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.5.5.6.5.1">LoRA Rank</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.5.5.6.5.2">8</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.5.5.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.5.5.7.6.1">LoRA Alpha</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.5.5.7.6.2">32</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.5.5.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.5.5.8.7.1">LoRA Target</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.5.5.8.7.2">all</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.5.5.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.5.5.9.8.1">Epoch</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.5.5.9.8.2">3</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.5.5.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.5.5.10.9.1">#GPUs</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.5.5.10.9.2">8</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.5.5.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.5.5.11.10.1">Per-device batch size</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.5.5.11.10.2">32</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.5.5.12.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.5.5.12.11.1">Gradient accumulation</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.5.5.12.11.2">1</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.5.5.13.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.5.5.13.12.1">Global batch size (effective)</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.5.5.13.12.2">256</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.5.5.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.5.5.1.2">Learning rate</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.5.5.1.1"><math alttext="2\times 10^{-4}" class="ltx_Math" display="inline" id="A3.SS1.5.5.1.1.m1" intent=":literal"><semantics><mrow><mn>2</mn><mo lspace="0.222em" rspace="0.222em">×</mo><msup><mn>10</mn><mrow><mo>−</mo><mn>4</mn></mrow></msup></mrow><annotation encoding="application/x-tex">2\times 10^{-4}</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A3.SS1.5.5.14.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.5.5.14.13.1">LR scheduler</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.5.5.14.13.2">Cosine</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.5.5.15.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.5.5.15.14.1">Warm-up ratio</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.5.5.15.14.2">0.1</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.5.5.16.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.5.5.16.15.1">Model max length</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.5.5.16.15.2">2048</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.5.5.17.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.5.5.17.16.1">Precision</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.5.5.17.16.2">BF16</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.5.5.18.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A3.SS1.5.5.18.17.1">Gradient checkpointing</th>
<td class="ltx_td ltx_align_left" id="A3.SS1.5.5.18.17.2">Enabled</td>
</tr>
<tr class="ltx_tr" id="A3.SS1.5.5.19.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A3.SS1.5.5.19.18.1">Random seed</th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="A3.SS1.5.5.19.18.2">42</td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
<section class="ltx_subsection" id="A3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>Failure Cases in Manual Screening</h3>
<div class="ltx_para" id="A3.SS2.p1">
<p class="ltx_p" id="A3.SS2.p1.1">The Fig.<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#A3.F9" title="Figure 9 ‣ C.1 MedGround-35K Examples ‣ Appendix C Samples ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">9</span></a> presents failure cases identified during manual screening/curation. These examples were flagged as incorrect due to issues such as question–image mismatch, mislocalized or improperly sized bounding boxes, incorrect object counting (marking multiple regions when a single target is required, or vice versa), and misinterpretation of domain-specific findings (e.g., confusing blur, noise, or debris with true anatomical/pathological structures). We include these samples to illustrate common error patterns and to motivate stricter quality control and refinement of our annotation guidelines.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Details of General Generate Prompt</h2>
<div class="ltx_para" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">The Fig.<a class="ltx_ref" href="https://arxiv.org/html/2601.06847v1#A4.F10" title="Figure 10 ‣ Appendix D Details of General Generate Prompt ‣ MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data"><span class="ltx_text ltx_ref_tag">10</span></a> shows the general system prompt we used to generate our synthetic data. This prompt serves as a unified instruction template that standardizes the model’s role, output format, and task-specific constraints, ensuring consistency and controllability across different synthesis scenarios.</p>
</div>
<figure class="ltx_figure" id="A4.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1266" id="A4.F10.g1" src="x8.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Detail of general generate system prompt.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Potential Use of MedGround</h2>
<div class="ltx_para" id="A5.p1">
<p class="ltx_p" id="A5.p1.1">We further argue that referring grounding is intrinsically more effective at eliciting visual reasoning than conventional descriptive supervision. In descriptive tasks, models can often produce plausible responses by leaning on linguistic priors or memorized report patterns. In contrast, referring grounding requires the model to <em class="ltx_emph ltx_font_italic" id="A5.p1.1.1">commit</em> to a specific region, forcing clinical claims to be supported by explicit pixel-level evidence. This evidence-binding property makes grounding not only a practical capability for downstream localization, but also a training signal that can stimulate broader diagnostic reasoning by aligning clinical semantics with visual anchors. We therefore view MedGround as a general-purpose resource that may benefit a wide range of medical VLM applications, including faithful visual question answering, evidence-based reporting, and interactive decision support.</p>
</div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sun Jan 11 10:32:05 2026 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>


<!-- ===== END HTML ===== -->
