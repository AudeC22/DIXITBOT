<!-- ===== SEARCH URL: https://arxiv.org/search/cs?query=multimodal%20transformer&searchtype=all&abstracts=show&size=50&start=0 | HTTP 200 ===== -->

<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/1.0.0a5/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;50 of 2,812 results for all: <span class="mathjax">multimodal transformer</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/cs"  aria-role="search">
    
      Searching in archive <strong>cs</strong>. <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=0">Search in all archives.</a>
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="multimodal transformer">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option selected value="all">All fields</option><option value="title">Title</option><option value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=multimodal+transformer&amp;terms-0-field=all&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option selected value="all">All fields</option><option value="title">Title</option><option value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="multimodal transformer">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                     
          
          <li>
            <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=50"
              class="pagination-link "
              aria-label="Page 2"
              aria-current="page">2
            </a>
          </li>
          
          <li>
            <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=100"
              class="pagination-link "
              aria-label="Page 3"
              aria-current="page">3
            </a>
          </li>
          
          <li>
            <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=150"
              class="pagination-link "
              aria-label="Page 4"
              aria-current="page">4
            </a>
          </li>
          
          <li>
            <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=200"
              class="pagination-link "
              aria-label="Page 5"
              aria-current="page">5
            </a>
          </li>
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.08457">arXiv:2601.08457</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.08457">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An Under-Explored Application for Explainable <span class="search-hit mathjax">Multimodal</span> Misogyny Detection in code-mixed Hindi-English
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Yadav%2C+S">Sargam Yadav</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kaushik%2C+A">Abhishek Kaushik</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Daid%2C+K+M">Kevin Mc Daid</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.08457v1-abstract-short" style="display: inline;">
        &hellip;we present a multi-modal and explainable web application for detecting misogyny in text and memes in code-mixed Hindi and English. The system leverages state-of-the-art <span class="search-hit mathjax">transformer</span>-based models that support multilingual and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.08457v1-abstract-full').style.display = 'inline'; document.getElementById('2601.08457v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.08457v1-abstract-full" style="display: none;">
        Digital platforms have an ever-expanding user base, and act as a hub for communication, business, and connectivity. However, this has also allowed for the spread of hate speech and misogyny. Artificial intelligence models have emerged as an effective solution for countering online hate speech but are under explored for low resource and code-mixed languages and suffer from a lack of interpretability. Explainable Artificial Intelligence (XAI) can enhance transparency in the decisions of deep learning models, which is crucial for a sensitive domain such as hate speech detection. In this paper, we present a multi-modal and explainable web application for detecting misogyny in text and memes in code-mixed Hindi and English. The system leverages state-of-the-art <span class="search-hit mathjax">transformer</span>-based models that support multilingual and <span class="search-hit mathjax">multimodal</span> settings. For text-based misogyny identification, the system utilizes XLM-RoBERTa (XLM-R) and multilingual Bidirectional Encoder Representations from <span class="search-hit mathjax">Transformers</span> (mBERT) on a dataset of approximately 4,193 comments. For <span class="search-hit mathjax">multimodal</span> misogyny identification from memes, the system utilizes mBERT + EfficientNet, and mBERT + ResNET trained on a dataset of approximately 4,218 memes. It also provides feature importance scores using explainability techniques including Shapley Additive Values (SHAP) and Local Interpretable Model Agnostic Explanations (LIME). The application aims to serve as a tool for both researchers and content moderators, to promote further research in the field, combat gender based digital violence, and ensure a safe digital space. The system has been evaluated using human evaluators who provided their responses on Chatbot Usability Questionnaire (CUQ) and User Experience Questionnaire (UEQ) to determine overall usability.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.08457v1-abstract-full').style.display = 'none'; document.getElementById('2601.08457v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.08179">arXiv:2601.08179</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.08179">pdf</a>, <a href="https://arxiv.org/ps/2601.08179">ps</a>, <a href="https://arxiv.org/format/2601.08179">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/TMM.2025.3565929">10.1109/TMM.2025.3565929 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Instruction-Driven 3D Facial Expression Generation and Transition
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Vo%2C+A+H">Anh H. Vo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kim%2C+T">Tae-Seok Kim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jin%2C+H">Hulin Jin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Choi%2C+S">Soo-Mi Choi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kim%2C+Y">Yong-Guk Kim</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.08179v1-abstract-short" style="display: inline;">
        &hellip;expressions. This study presents a new framework for instruction-driven facial expression generation that produces a 3D face and, starting from an image of the face, <span class="search-hit mathjax">transforms</span> the facial expression from one designated facial expression to another. The Instruction-driven Facial Expression Decomposer (IFED) module is introduced to facilitate&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.08179v1-abstract-full').style.display = 'inline'; document.getElementById('2601.08179v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.08179v1-abstract-full" style="display: none;">
        A 3D avatar typically has one of six cardinal facial expressions. To simulate realistic emotional variation, we should be able to render a facial transition between two arbitrary expressions. This study presents a new framework for instruction-driven facial expression generation that produces a 3D face and, starting from an image of the face, <span class="search-hit mathjax">transforms</span> the facial expression from one designated facial expression to another. The Instruction-driven Facial Expression Decomposer (IFED) module is introduced to facilitate <span class="search-hit mathjax">multimodal</span> data learning and capture the correlation between textual descriptions and facial expression features. Subsequently, we propose the Instruction to Facial Expression Transition (I2FET) method, which leverages IFED and a vertex reconstruction loss function to refine the semantic comprehension of latent vectors, thus generating a facial expression sequence according to the given instruction. Lastly, we present the Facial Expression Transition model to generate smooth transitions between facial expressions. Extensive evaluation suggests that the proposed model outperforms state-of-the-art methods on the CK+ and CelebV-HQ datasets. The results show that our framework can generate facial expression trajectories according to text instruction. Considering that text prompts allow us to make diverse descriptions of human emotional states, the repertoire of facial expressions and the transitions between them can be expanded greatly. We expect our framework to find various practical applications More information about our project can be found at https://vohoanganh.github.io/tg3dfet/
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.08179v1-abstract-full').style.display = 'none'; document.getElementById('2601.08179v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        IEEE Transactions on Multimedia, 2025
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.08151">arXiv:2601.08151</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.08151">pdf</a>, <a href="https://arxiv.org/ps/2601.08151">ps</a>, <a href="https://arxiv.org/format/2601.08151">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Song%2C+S">Shezheng Song</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+S">Shasha Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+J">Jie Yu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.08151v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Multimodal</span> Large Language Models (MLLMs) have achieved remarkable progress in vision-language understanding, yet how they internally integrate visual and textual information remains poorly understood. To bridge this gap, we perform a systematic layer-wise masking analysis across multiple architectures, revealing how visual-text fusion evolves within MLLMs. T&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.08151v1-abstract-full').style.display = 'inline'; document.getElementById('2601.08151v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.08151v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Multimodal</span> Large Language Models (MLLMs) have achieved remarkable progress in vision-language understanding, yet how they internally integrate visual and textual information remains poorly understood. To bridge this gap, we perform a systematic layer-wise masking analysis across multiple architectures, revealing how visual-text fusion evolves within MLLMs. The results show that fusion emerges at several specific layers rather than being uniformly distributed across the network, and certain models exhibit a late-stage &#34;review&#34; phenomenon where visual signals are reactivated before output generation. Besides, we further analyze layer-wise attention evolution and observe persistent high-attention noise on irrelevant regions, along with gradually increasing attention on text-aligned areas. Guided by these insights, we introduce a training-free contrastive attention framework that models the <span class="search-hit mathjax">transformation</span> between early fusion and final layers to highlight meaningful attention shifts. Extensive experiments across various MLLMs and benchmarks validate our analysis and demonstrate that the proposed approach improves <span class="search-hit mathjax">multimodal</span> reasoning performance. Code will be released.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.08151v1-abstract-full').style.display = 'none'; document.getElementById('2601.08151v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.07581">arXiv:2601.07581</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.07581">pdf</a>, <a href="https://arxiv.org/ps/2601.07581">ps</a>, <a href="https://arxiv.org/format/2601.07581">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        BenchSeg: A Large-Scale Dataset and Benchmark for Multi-View Food Video Segmentation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=AlMughrabi%2C+A">Ahmad AlMughrabi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rivo%2C+G">Guillermo Rivo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jim%C3%A9nez-Farf%C3%A1n%2C+C">Carlos Jiménez-Farfán</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Haroon%2C+U">Umair Haroon</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Al-Areqi%2C+F">Farid Al-Areqi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jung%2C+H">Hyunjun Jung</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Busam%2C+B">Benjamin Busam</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Marques%2C+R">Ricardo Marques</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Radeva%2C+P">Petia Radeva</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.07581v1-abstract-short" style="display: inline;">
        &hellip;284 meticulously annotated frames, capturing each dish under free 360° camera motion. We evaluate a diverse set of 20 state-of-the-art segmentation models (e.g., SAM-based, <span class="search-hit mathjax">transformer</span>, CNN, and large <span class="search-hit mathjax">multimodal</span>) on the existing FoodSeg103 dataset and evaluate them (alone and combined with video-memory modules) on Benc&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.07581v1-abstract-full').style.display = 'inline'; document.getElementById('2601.07581v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.07581v1-abstract-full" style="display: none;">
        Food image segmentation is a critical task for dietary analysis, enabling accurate estimation of food volume and nutrients. However, current methods suffer from limited multi-view data and poor generalization to new viewpoints. We introduce BenchSeg, a novel multi-view food video segmentation dataset and benchmark. BenchSeg aggregates 55 dish scenes (from Nutrition5k, Vegetables &amp; Fruits, MetaFood3D, and FoodKit) with 25,284 meticulously annotated frames, capturing each dish under free 360° camera motion. We evaluate a diverse set of 20 state-of-the-art segmentation models (e.g., SAM-based, <span class="search-hit mathjax">transformer</span>, CNN, and large <span class="search-hit mathjax">multimodal</span>) on the existing FoodSeg103 dataset and evaluate them (alone and combined with video-memory modules) on BenchSeg. Quantitative and qualitative results demonstrate that while standard image segmenters degrade sharply under novel viewpoints, memory-augmented methods maintain temporal consistency across frames. Our best model based on a combination of SeTR-MLA+XMem2 outperforms prior work (e.g., improving over FoodMem by ~2.63% mAP), offering new insights into food segmentation and tracking for dietary analysis. We release BenchSeg to foster future research. The project page including the dataset annotations and the food segmentation models can be found at https://amughrabi.github.io/benchseg.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.07581v1-abstract-full').style.display = 'none'; document.getElementById('2601.07581v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.07516">arXiv:2601.07516</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.07516">pdf</a>, <a href="https://arxiv.org/ps/2601.07516">ps</a>, <a href="https://arxiv.org/format/2601.07516">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Controlling <span class="search-hit mathjax">Multimodal</span> Conversational Agents with Coverage-Enhanced Latent Actions
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Y">Yongqi Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lang%2C+H">Hao Lang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qian%2C+T">Tieyun Qian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Y">Yongbin Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.07516v1-abstract-short" style="display: inline;">
        Vision-language models are increasingly employed as <span class="search-hit mathjax">multimodal</span> conversational agents (MCAs) for diverse conversational tasks. Recently, reinforcement learning (RL) has been widely explored for adapting MCAs to various human-AI interaction scenarios. Despite showing great enhancement in generalization performance, fine-tuning MCAs via RL still faces challenge&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.07516v1-abstract-full').style.display = 'inline'; document.getElementById('2601.07516v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.07516v1-abstract-full" style="display: none;">
        Vision-language models are increasingly employed as <span class="search-hit mathjax">multimodal</span> conversational agents (MCAs) for diverse conversational tasks. Recently, reinforcement learning (RL) has been widely explored for adapting MCAs to various human-AI interaction scenarios. Despite showing great enhancement in generalization performance, fine-tuning MCAs via RL still faces challenges in handling the extremely large text token space. To address this, we learn a compact latent action space for RL fine-tuning instead. Specifically, we adopt the learning from observation mechanism to construct the codebook for the latent action space, where future observations are leveraged to estimate current latent actions that could further be used to reconstruct future observations. However, the scarcity of paired image-text data hinders learning a codebook with sufficient coverage. Thus, we leverage both paired image-text data and text-only data to construct the latent action space, using a cross-modal projector for <span class="search-hit mathjax">transforming</span> text embeddings into image-text embeddings. We initialize the cross-modal projector on paired image-text data, and further train it on massive text-only data with a novel cycle consistency loss to enhance its robustness. We show that our latent action based method outperforms competitive baselines on two conversation tasks across various RL algorithms.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.07516v1-abstract-full').style.display = 'none'; document.getElementById('2601.07516v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.07235">arXiv:2601.07235</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.07235">pdf</a>, <a href="https://arxiv.org/ps/2601.07235">ps</a>, <a href="https://arxiv.org/format/2601.07235">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Gosai%2C+A">Agnivo Gosai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=De%2C+S">Shuvodeep De</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Thankachan%2C+K">Karun Thankachan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.07235v1-abstract-short" style="display: inline;">
        &hellip;widely used datasets such as IMDb, Rotten Tomatoes, and SST-2, and models ranging from Naive Bayes and support vector machines to LSTM networks, BERT, and attention-based <span class="search-hit mathjax">transformers</span>. Beyond summarizing prior work, this survey differentiates itself by offering a comparative, challenge-driven analysis of how these modeling paradigms address domain-specific i&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.07235v1-abstract-full').style.display = 'inline'; document.getElementById('2601.07235v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.07235v1-abstract-full" style="display: none;">
        This paper presents a comprehensive survey of sentiment analysis methods for movie reviews, a benchmark task that has played a central role in advancing natural language processing. We review the evolution of techniques from early lexicon-based and classical machine learning approaches to modern deep learning architectures and large language models, covering widely used datasets such as IMDb, Rotten Tomatoes, and SST-2, and models ranging from Naive Bayes and support vector machines to LSTM networks, BERT, and attention-based <span class="search-hit mathjax">transformers</span>. Beyond summarizing prior work, this survey differentiates itself by offering a comparative, challenge-driven analysis of how these modeling paradigms address domain-specific issues such as sarcasm, negation, contextual ambiguity, and domain shift, which remain open problems in existing literature. Unlike earlier reviews that focus primarily on text-only pipelines, we also synthesize recent advances in <span class="search-hit mathjax">multimodal</span> sentiment analysis that integrate textual, audio, and visual cues from movie trailers and clips. In addition, we examine emerging concerns related to interpretability, fairness, and robustness that are often underexplored in prior surveys, and we outline future research directions including zero-shot and few-shot learning, hybrid symbolic--neural models, and real-time deployment considerations. Overall, this abstract provides a domain-focused roadmap that highlights both established solutions and unresolved challenges toward building more accurate, generalizable, and explainable sentiment analysis systems for movie review data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.07235v1-abstract-full').style.display = 'none'; document.getElementById('2601.07235v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">31 Pages; 1 figure; 108 references; ongoing paper that would be submitted to suitable Wiley journal</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.06874">arXiv:2601.06874</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.06874">pdf</a>, <a href="https://arxiv.org/ps/2601.06874">ps</a>, <a href="https://arxiv.org/format/2601.06874">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MVGGT: <span class="search-hit mathjax">Multimodal</span> Visual Geometry Grounded <span class="search-hit mathjax">Transformer</span> for Multiview 3D Referring Expression Segmentation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+C">Changli Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+H">Haodong Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ji%2C+J">Jiayi Ji</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yao%2C+Y">Yutian Yao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Du%2C+C">Chunsai Du</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kang%2C+J">Jihua Kang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fu%2C+Y">Yanwei Fu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cao%2C+L">Liujuan Cao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.06874v2-abstract-short" style="display: inline;">
        &hellip;first reconstruct a point cloud and then perform segmentation, often yield low-quality geometry, produce coarse or degraded target regions, and run slowly. We propose the <span class="search-hit mathjax">Multimodal</span> Visual Geometry Grounded <span class="search-hit mathjax">Transformer</span> (MVGGT), an efficient end-to-end framework that integrates language information into sparse-view geom&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06874v2-abstract-full').style.display = 'inline'; document.getElementById('2601.06874v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.06874v2-abstract-full" style="display: none;">
        Most existing 3D referring expression segmentation (3DRES) methods rely on dense, high-quality point clouds, while real-world agents such as robots and mobile phones operate with only a few sparse RGB views and strict latency constraints. We introduce Multi-view 3D Referring Expression Segmentation (MV-3DRES), where the model must recover scene structure and segment the referred object directly from sparse multi-view images. Traditional two-stage pipelines, which first reconstruct a point cloud and then perform segmentation, often yield low-quality geometry, produce coarse or degraded target regions, and run slowly. We propose the <span class="search-hit mathjax">Multimodal</span> Visual Geometry Grounded <span class="search-hit mathjax">Transformer</span> (MVGGT), an efficient end-to-end framework that integrates language information into sparse-view geometric reasoning through a dual-branch design. Training in this setting exposes a critical optimization barrier, termed Foreground Gradient Dilution (FGD), where sparse 3D signals lead to weak supervision. To resolve this, we introduce Per-view No-target Suppression Optimization (PVSO), which provides stronger and more balanced gradients across views, enabling stable and efficient learning. To support consistent evaluation, we build MVRefer, a benchmark that defines standardized settings and metrics for MV-3DRES. Experiments show that MVGGT establishes the first strong baseline and achieves both high accuracy and fast inference, outperforming existing alternatives. Code and models are publicly available at https://mvggt.github.io.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06874v2-abstract-full').style.display = 'none'; document.getElementById('2601.06874v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 January, 2026; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 11 January, 2026;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Project Website: https://sosppxo.github.io/mvggt.github.io/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.06847">arXiv:2601.06847</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.06847">pdf</a>, <a href="https://arxiv.org/ps/2601.06847">ps</a>, <a href="https://arxiv.org/format/2601.06847">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+M">Mengmeng Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+X">Xiaoping Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Luo%2C+H">Hao Luo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+F">Fan Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lv%2C+Y">Yisheng Lv</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.06847v1-abstract-short" style="display: inline;">
        &hellip;limitation arises from the scarcity of high-quality, large-scale clinical referring-localization pairs. To address this, we introduce MedGround, an automated pipeline that <span class="search-hit mathjax">transforms</span> segmentation resources into high-quality medical referring grounding data. Leveraging expert masks as spatial anchors, MedGround precisely derives localization targets, extracts&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06847v1-abstract-full').style.display = 'inline'; document.getElementById('2601.06847v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.06847v1-abstract-full" style="display: none;">
        Vision-Language Models (VLMs) can generate convincing clinical narratives, yet frequently struggle to visually ground their statements. We posit this limitation arises from the scarcity of high-quality, large-scale clinical referring-localization pairs. To address this, we introduce MedGround, an automated pipeline that <span class="search-hit mathjax">transforms</span> segmentation resources into high-quality medical referring grounding data. Leveraging expert masks as spatial anchors, MedGround precisely derives localization targets, extracts shape and spatial cues, and guides VLMs to synthesize natural, clinically grounded queries that reflect morphology and location. To ensure data rigor, a multi-stage verification system integrates strict formatting checks, geometry- and medical-prior rules, and image-based visual judging to filter out ambiguous or visually unsupported samples. Finally, we present MedGround-35K, a novel <span class="search-hit mathjax">multimodal</span> medical dataset. Extensive experiments demonstrate that VLMs trained with MedGround-35K consistently achieve improved referring grounding performance, enhance multi-object semantic disambiguation, and exhibit strong generalization to unseen grounding settings. This work highlights MedGround as a scalable, data-driven approach to anchor medical language to verifiable visual evidence. Dataset and code will be released publicly upon acceptance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06847v1-abstract-full').style.display = 'none'; document.getElementById('2601.06847v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">18 pages, 10 figures</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          J.1
        

        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.06753">arXiv:2601.06753</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.06753">pdf</a>, <a href="https://arxiv.org/ps/2601.06753">ps</a>, <a href="https://arxiv.org/format/2601.06753">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards Computational Chinese Paleography
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ma%2C+Y+R">Yiran Rex Ma</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.06753v1-abstract-short" style="display: inline;">
        &hellip;for automated decipherment and human-AI collaboration. We examine the technological shift from classical computer vision to modern deep learning paradigms, including <span class="search-hit mathjax">transformers</span> and large&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06753v1-abstract-full').style.display = 'inline'; document.getElementById('2601.06753v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.06753v1-abstract-full" style="display: none;">
        Chinese paleography, the study of ancient Chinese writing, is undergoing a computational turn powered by artificial intelligence. This position paper charts the trajectory of this emerging field, arguing that it is evolving from automating isolated visual tasks to creating integrated digital ecosystems for scholarly research. We first map the landscape of digital resources, analyzing critical datasets for oracle bone, bronze, and bamboo slip scripts. The core of our analysis follows the field&#39;s methodological pipeline: from foundational visual processing (image restoration, character recognition), through contextual analysis (artifact rejoining, dating), to the advanced reasoning required for automated decipherment and human-AI collaboration. We examine the technological shift from classical computer vision to modern deep learning paradigms, including <span class="search-hit mathjax">transformers</span> and large <span class="search-hit mathjax">multimodal</span> models. Finally, we synthesize the field&#39;s core challenges -- notably data scarcity and a disconnect between current AI capabilities and the holistic nature of humanistic inquiry -- and advocate for a future research agenda focused on creating <span class="search-hit mathjax">multimodal</span>, few-shot, and human-centric systems to augment scholarly expertise.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06753v1-abstract-full').style.display = 'none'; document.getElementById('2601.06753v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">A position paper in progress with Peking University &amp; ByteDance Digital Humanities Open Lab</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.06616">arXiv:2601.06616</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.06616">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        LLM-Driven Accessible Interface: A Model-Based Approach
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Jerry%2C+B">Blessing Jerry</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Moreno%2C+L">Lourdes Moreno</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Francisco%2C+V">Virginia Francisco</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hervas%2C+R">Raquel Hervas</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.06616v1-abstract-short" style="display: inline;">
        &hellip;raises challenges regarding accessibility, explainability, and normative compliance. This paper presents an implemented model-driven architecture for generating personalised, <span class="search-hit mathjax">multimodal</span>, and accessibility-aligned user interfaces. The approach combines structured user profiles, declarative adaptation rules, and validated prompt templates to refine baseline ac&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06616v1-abstract-full').style.display = 'inline'; document.getElementById('2601.06616v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.06616v1-abstract-full" style="display: none;">
        The integration of Large Language Models (LLMs) into interactive systems opens new opportunities for adaptive user experiences, yet it also raises challenges regarding accessibility, explainability, and normative compliance. This paper presents an implemented model-driven architecture for generating personalised, <span class="search-hit mathjax">multimodal</span>, and accessibility-aligned user interfaces. The approach combines structured user profiles, declarative adaptation rules, and validated prompt templates to refine baseline accessible UI templates that conform to WCAG 2.2 and EN 301 549, tailored to cognitive and sensory support needs. LLMs dynamically <span class="search-hit mathjax">transform</span> language complexity, modality, and visual structure, producing outputs such as Plain-Language text, pictograms, and high-contrast layouts aligned with ISO 24495-1 and W3C COGA guidance. A healthcare use case demonstrates how the system generates accessible post-consultation medication instructions tailored to a user profile comprising cognitive disability and hearing impairment. SysML v2 models provide explicit traceability between user needs, adaptation rules, and normative requirements, ensuring explainable and auditable <span class="search-hit mathjax">transformations</span>. Grounded in Human-Centered AI (HCAI), the framework incorporates co-design processes and structured feedback mechanisms to guide iterative refinement and support trustworthy generative behaviour.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06616v1-abstract-full').style.display = 'none'; document.getElementById('2601.06616v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          H.5.2; J.3; K.4.2; H.1.2; I.2.7
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.06475">arXiv:2601.06475</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.06475">pdf</a>, <a href="https://arxiv.org/ps/2601.06475">ps</a>, <a href="https://arxiv.org/format/2601.06475">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        VVTRec: Radio Interferometric Reconstruction through Visual and Textual Modality Enrichment
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Cheng%2C+K">Kai Cheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+R">Ruoqi Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Luo%2C+Q">Qiong Luo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.06475v1-abstract-short" style="display: inline;">
        Radio astronomy is an indispensable discipline for observing distant celestial objects. Measurements of wave signals from radio telescopes, called visibility, need to be <span class="search-hit mathjax">transformed</span> into images for astronomical observations. These dirty images blend information from real sources and artifacts. Therefore, astronomers usually perform reconstruction before imag&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06475v1-abstract-full').style.display = 'inline'; document.getElementById('2601.06475v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.06475v1-abstract-full" style="display: none;">
        Radio astronomy is an indispensable discipline for observing distant celestial objects. Measurements of wave signals from radio telescopes, called visibility, need to be <span class="search-hit mathjax">transformed</span> into images for astronomical observations. These dirty images blend information from real sources and artifacts. Therefore, astronomers usually perform reconstruction before imaging to obtain cleaner images. Existing methods consider only a single modality of sparse visibility data, resulting in images with remaining artifacts and insufficient modeling of correlation. To enhance the extraction of visibility information and emphasize output quality in the image domain, we propose VVTRec, a <span class="search-hit mathjax">multimodal</span> radio interferometric data reconstruction method with visibility-guided visual and textual modality enrichment. In our VVTRec, sparse visibility is <span class="search-hit mathjax">transformed</span> into image-form and text-form features to obtain enhancements in terms of spatial and semantic information, improving the structural integrity and accuracy of images. Also, we leverage Vision-Language Models (VLMs) to achieve additional training-free performance improvements. VVTRec enables sparse visibility, as a foreign modality unseen by VLMs, to accurately extract pre-trained knowledge as a supplement. Our experiments demonstrate that VVTRec effectively enhances imaging results by exploiting <span class="search-hit mathjax">multimodal</span> information without introducing excessive computational overhead.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06475v1-abstract-full').style.display = 'none'; document.getElementById('2601.06475v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.06212">arXiv:2601.06212</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.06212">pdf</a>, <a href="https://arxiv.org/ps/2601.06212">ps</a>, <a href="https://arxiv.org/format/2601.06212">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Akasha 2: Hamiltonian State Space Duality and Visual-Language Joint Embedding Predictive Architectur
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Meziani%2C+Y">Yani Meziani</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.06212v1-abstract-short" style="display: inline;">
        We present Akasha 2, a state-of-the-art <span class="search-hit mathjax">multimodal</span> architecture that integrates Hamiltonian State Space Duality (H-SSD) with Visual-Language Joint Embedding Predictive Architecture (VL-JEPA). The system leverages the Mamba-3 Selective State Space Model (SSM) augmented by a Sparse Mixture of Hamiltonian Experts (SMoE-HE) that enforces latent physical conserva&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06212v1-abstract-full').style.display = 'inline'; document.getElementById('2601.06212v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.06212v1-abstract-full" style="display: none;">
        We present Akasha 2, a state-of-the-art <span class="search-hit mathjax">multimodal</span> architecture that integrates Hamiltonian State Space Duality (H-SSD) with Visual-Language Joint Embedding Predictive Architecture (VL-JEPA). The system leverages the Mamba-3 Selective State Space Model (SSM) augmented by a Sparse Mixture of Hamiltonian Experts (SMoE-HE) that enforces latent physical conservation laws through symplectic integration. For visual synthesis, we introduce Hamiltonian Flow Matching (HFM) and persistent 3D Gaussian Splatting (3DGS), enabling ultra-low latency (&lt;50ms) on mobile hardware. This work establishes a new paradigm in latent world models, achieving unprecedented spatiotemporal coherence through a holographic memory architecture. Our approach demonstrates that incorporating physics-inspired inductive biases into neural architectures yields significant improvements: state-of-the-art video prediction (FVD: 287), 4x faster visual synthesis than diffusion models, and 3-18x inference speedup over <span class="search-hit mathjax">transformer</span> baselines while maintaining energy conservation over extended horizons.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06212v1-abstract-full').style.display = 'none'; document.getElementById('2601.06212v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">12 pages, 6 figures, 3 tables. Includes appendices with pseudocode and implementation details. Supplementary materials eventually at github.com/yanimeziani/akasha</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68T07; 68T45; 70H05
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.6; I.2.10; I.4.8
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.06199">arXiv:2601.06199</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.06199">pdf</a>, <a href="https://arxiv.org/ps/2601.06199">ps</a>, <a href="https://arxiv.org/format/2601.06199">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        FastSLM: Hierarchical Frame Q-Former for Effective Speech Modality Adaptation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+J">Junseok Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lee%2C+S">Sangyong Lee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chun%2C+C">Chang-Jae Chun</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.06199v1-abstract-short" style="display: inline;">
        &hellip;general intelligence (AGI). In particular, there is growing momentum in adapting LLMs to various modalities, including vision, video, and speech, through the development of <span class="search-hit mathjax">multimodal</span> LLMs (MLLMs). However, existing speech-language model (SLM) research has largely overlooked cost-effective adaptation strategies for leveraging LLMs in the speech domain. In th&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06199v1-abstract-full').style.display = 'inline'; document.getElementById('2601.06199v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.06199v1-abstract-full" style="display: none;">
        Recent advances in large language models (LLMs) have demonstrated human-expert-level capabilities, driving significant interest in their potential for achieving artificial general intelligence (AGI). In particular, there is growing momentum in adapting LLMs to various modalities, including vision, video, and speech, through the development of <span class="search-hit mathjax">multimodal</span> LLMs (MLLMs). However, existing speech-language model (SLM) research has largely overlooked cost-effective adaptation strategies for leveraging LLMs in the speech domain. In this paper, we propose FastSLM, a lightweight yet efficient SLM designed for effective understanding and reasoning over long-form speech. To address the challenge of aligning high-frame-rate speech features with LLMs, we introduce the Hierarchical Frame Querying <span class="search-hit mathjax">Transformer</span> (HFQ-Former), which compresses frame-level speech features while capturing both local and global context. Furthermore, we present a novel three-stage training strategy that enhances generalization across a wide range of speech-related tasks. Experimental results demonstrate that FastSLM achieves competitive performance compared to existing state-of-the-art models, despite operating with significantly lower FLOPs and parameter counts, while representing speech with only 1.67 tokens per second. The source code and model checkpoints are available at https://huggingface.co/okestro-ai-lab/FastSLM.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06199v1-abstract-full').style.display = 'none'; document.getElementById('2601.06199v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.06140">arXiv:2601.06140</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.06140">pdf</a>, <a href="https://arxiv.org/ps/2601.06140">ps</a>, <a href="https://arxiv.org/format/2601.06140">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Causal and Federated <span class="search-hit mathjax">Multimodal</span> Learning for Cardiovascular Risk Prediction under Heterogeneous Populations
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Kaushik%2C+R">Rohit Kaushik</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kaushik%2C+E">Eva Kaushik</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.06140v1-abstract-short" style="display: inline;">
        &hellip;calling for predictive models that not only handle diverse and high-dimensional biomedical signals but also maintain interpretability and privacy. We create a single <span class="search-hit mathjax">multimodal</span> learning framework that integrates cross modal <span class="search-hit mathjax">transformers</span> with graph neural networks and causal representation learning to measure personali&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06140v1-abstract-full').style.display = 'inline'; document.getElementById('2601.06140v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.06140v1-abstract-full" style="display: none;">
        Cardiovascular disease (CVD) continues to be the major cause of death globally, calling for predictive models that not only handle diverse and high-dimensional biomedical signals but also maintain interpretability and privacy. We create a single <span class="search-hit mathjax">multimodal</span> learning framework that integrates cross modal <span class="search-hit mathjax">transformers</span> with graph neural networks and causal representation learning to measure personalized CVD risk. The model combines genomic variation, cardiac MRI, ECG waveforms, wearable streams, and structured EHR data to predict risk while also implementing causal invariance constraints across different clinical subpopulations.
  To maintain transparency, we employ SHAP based feature attribution, counterfactual explanations and causal latent alignment for understandable risk factors. Besides, we position the design in a federated, privacy, preserving optimization protocol and establish rules for convergence, calibration and uncertainty quantification under distributional shift. Experimental studies based on large-scale biobank and multi institutional datasets reveal state discrimination and robustness, exhibiting fair performance across demographic strata and clinically distinct cohorts. This study paves the way for a principled approach to clinically trustworthy, interpretable and privacy respecting CVD prediction at the population level.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.06140v1-abstract-full').style.display = 'none'; document.getElementById('2601.06140v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, 5 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.05508">arXiv:2601.05508</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.05508">pdf</a>, <a href="https://arxiv.org/ps/2601.05508">ps</a>, <a href="https://arxiv.org/format/2601.05508">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Enabling Stroke-Level Structural Analysis of Hieroglyphic Scripts without Language-Specific Priors
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Luo%2C+F">Fuwen Luo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wan%2C+Z">Zihao Wan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Z">Ziyue Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Y">Yaluo Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xu%2C+P+T+L">Pau Tong Lin Xu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qiao%2C+X">Xuanjia Qiao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+X">Xiaolong Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+P">Peng Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Y">Yang Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.05508v1-abstract-short" style="display: inline;">
        &hellip;writing systems, encode rich semantic and cultural information within their internal structural composition. Yet, current advanced Large Language Models (LLMs) and <span class="search-hit mathjax">Multimodal</span> LLMs (MLLMs) usually remain structurally blind to this information. LLMs process characters as textual tokens, while MLLMs additionally view them as raw pixel grids. Both fall short to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.05508v1-abstract-full').style.display = 'inline'; document.getElementById('2601.05508v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.05508v1-abstract-full" style="display: none;">
        Hieroglyphs, as logographic writing systems, encode rich semantic and cultural information within their internal structural composition. Yet, current advanced Large Language Models (LLMs) and <span class="search-hit mathjax">Multimodal</span> LLMs (MLLMs) usually remain structurally blind to this information. LLMs process characters as textual tokens, while MLLMs additionally view them as raw pixel grids. Both fall short to model the underlying logic of character strokes. Furthermore, existing structural analysis methods are often script-specific and labor-intensive. In this paper, we propose Hieroglyphic Stroke Analyzer (HieroSA), a novel and generalizable framework that enables MLLMs to automatically derive stroke-level structures from character bitmaps without handcrafted data. It <span class="search-hit mathjax">transforms</span> modern logographic and ancient hieroglyphs character images into explicit, interpretable line-segment representations in a normalized coordinate space, allowing for cross-lingual generalization. Extensive experiments demonstrate that HieroSA effectively captures character-internal structures and semantics, bypassing the need for language-specific priors. Experimental results highlight the potential of our work as a graphematics analysis tool for a deeper understanding of hieroglyphic scripts. View our code at https://github.com/THUNLP-MT/HieroSA.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.05508v1-abstract-full').style.display = 'none'; document.getElementById('2601.05508v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.05470">arXiv:2601.05470</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.05470">pdf</a>, <a href="https://arxiv.org/ps/2601.05470">ps</a>, <a href="https://arxiv.org/format/2601.05470">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ROAP: A Reading-Order and Attention-Prior Pipeline for Optimizing Layout <span class="search-hit mathjax">Transformers</span> in Key Information Extraction
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Xie%2C+T">Tingwei Xie</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+J">Jinxin He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Song%2C+Y">Yonghong Song</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.05470v1-abstract-short" style="display: inline;">
        The efficacy of <span class="search-hit mathjax">Multimodal</span>&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.05470v1-abstract-full').style.display = 'inline'; document.getElementById('2601.05470v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.05470v1-abstract-full" style="display: none;">
        The efficacy of <span class="search-hit mathjax">Multimodal</span> <span class="search-hit mathjax">Transformers</span> in visually-rich document understanding (VrDU) is critically constrained by two inherent limitations: the lack of explicit modeling for logical reading order and the interference of visual tokens that dilutes attention on textual semantics.
  To address these challenges, this paper presents ROAP, a lightweight and architecture-agnostic pipeline designed to optimize attention distributions in Layout <span class="search-hit mathjax">Transformers</span> without altering their pre-trained backbones.
  The proposed pipeline first employs an Adaptive-XY-Gap (AXG-Tree) to robustly extract hierarchical reading sequences from complex layouts. These sequences are then integrated into the attention mechanism via a Reading-Order-Aware Relative Position Bias (RO-RPB). Furthermore, a Textual-Token Sub-block Attention Prior (TT-Prior) is introduced to adaptively suppress visual noise and enhance fine-grained text-text interactions.
  Extensive experiments on the FUNSD and CORD benchmarks demonstrate that ROAP consistently improves the performance of representative backbones, including LayoutLMv3 and GeoLayoutLM.
  These findings confirm that explicitly modeling reading logic and regulating modality interference are critical for robust document understanding, offering a scalable solution for complex layout analysis. The implementation code will be released at https://github.com/KevinYuLei/ROAP.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.05470v1-abstract-full').style.display = 'none'; document.getElementById('2601.05470v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages, 4 figures, 4 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.05353">arXiv:2601.05353</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.05353">pdf</a>, <a href="https://arxiv.org/ps/2601.05353">ps</a>, <a href="https://arxiv.org/format/2601.05353">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Theory">cs.IT</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        GlyRAG: Context-Aware Retrieval-Augmented Framework for Blood Glucose Forecasting
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Soumma%2C+S+B">Shovito Barua Soumma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ghasemzadeh%2C+H">Hassan Ghasemzadeh</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.05353v1-abstract-short" style="display: inline;">
        &hellip;an LLM as a contextualization agent to generate clinical summaries. These summaries are embedded by a language model and fused with patch-based glucose representations in a <span class="search-hit mathjax">multimodal</span> <span class="search-hit mathjax">transformer</span> architecture with a cross translation loss aligining textual and physiological embeddings. A retrieval module then identifie&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.05353v1-abstract-full').style.display = 'inline'; document.getElementById('2601.05353v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.05353v1-abstract-full" style="display: none;">
        Accurate forecasting of blood glucose from CGM is essential for preventing dysglycemic events, thus enabling proactive diabetes management. However, current forecasting models treat blood glucose readings captured using CGMs as a numerical sequence, either ignoring context or relying on additional sensors/modalities that are difficult to collect and deploy at scale. Recently, LLMs have shown promise for time-series forecasting tasks, yet their role as agentic context extractors in diabetes care remains largely unexplored. To address these limitations, we propose GlyRAG, a context-aware, retrieval-augmented forecasting framework that derives semantic understanding of blood glucose dynamics directly from CGM traces without requiring additional sensor modalities. GlyRAG employs an LLM as a contextualization agent to generate clinical summaries. These summaries are embedded by a language model and fused with patch-based glucose representations in a <span class="search-hit mathjax">multimodal</span> <span class="search-hit mathjax">transformer</span> architecture with a cross translation loss aligining textual and physiological embeddings. A retrieval module then identifies similar historical episodes in the learned embedding space and uses cross-attention to integrate these case-based analogues prior to making a forecasting inference. Extensive evaluations on two T1D cohorts show that GlyRAG consistently outperforms state-of-the art methods, achieving up to 39% lower RMSE and a further 1.7% reduction in RMSE over the baseline. Clinical evaluation shows that GlyRAG places 85% predictions in safe zones and achieves 51% improvement in predicting dysglycemic events across both cohorts. These results indicate that LLM-based contextualization and retrieval over CGM traces can enhance the accuracy and clinical reliability of long-horizon glucose forecasting without the need for extra sensors, thus supporting future agentic decision-support tools for diabetes management.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.05353v1-abstract-full').style.display = 'none'; document.getElementById('2601.05353v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.05269">arXiv:2601.05269</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.05269">pdf</a>, <a href="https://arxiv.org/ps/2601.05269">ps</a>, <a href="https://arxiv.org/format/2601.05269">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Studying Illustrations in Manuscripts: An Efficient Deep-Learning Approach
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Evron%2C+Y">Yoav Evron</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Siegal%2C+M+B">Michal Bar-Asher Siegal</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fire%2C+M">Michael Fire</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.05269v2-abstract-short" style="display: inline;">
        The recent Artificial Intelligence (AI) revolution has opened <span class="search-hit mathjax">transformative</span> possibilities for the humanities, particularly in unlocking the visual-artistic content embedded in historical illuminated manuscripts. While digital archives now offer unprecedented access to these materials, the ability to systematically locate, extract, and analyze illustrations&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.05269v2-abstract-full').style.display = 'inline'; document.getElementById('2601.05269v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.05269v2-abstract-full" style="display: none;">
        The recent Artificial Intelligence (AI) revolution has opened <span class="search-hit mathjax">transformative</span> possibilities for the humanities, particularly in unlocking the visual-artistic content embedded in historical illuminated manuscripts. While digital archives now offer unprecedented access to these materials, the ability to systematically locate, extract, and analyze illustrations at scale remains a major challenge. We present a general and scalable AI-based pipeline for large-scale visual analysis of illuminated manuscripts. The framework integrates modern deep-learning models for page-level illustration detection, illustration extraction, and <span class="search-hit mathjax">multimodal</span> description, enabling scholars to search, cluster, and study visual materials and artistic trends across entire corpora. We demonstrate the applicability of this approach on large heterogeneous collections, including the Vatican Library and richly illuminated manuscripts such as the Bible of Borso d&#39;Este. The system reveals meaningful visual patterns and cross-manuscript relationships by embedding illustrations into a shared representation space and analyzing their similarity structure (see figure 4). By harnessing recent advances in computer vision and vision-language models, our framework enables new forms of large-scale visual scholarship in historical studies, art history, and cultural heritage making it possible to explore iconography, stylistic trends, and cultural connections in ways that were previously impractical.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.05269v2-abstract-full').style.display = 'none'; document.getElementById('2601.05269v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 January, 2026; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 November, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">17 pages, 5 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.04571">arXiv:2601.04571</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.04571">pdf</a>, <a href="https://arxiv.org/ps/2601.04571">ps</a>, <a href="https://arxiv.org/format/2601.04571">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Enhancing <span class="search-hit mathjax">Multimodal</span> Retrieval via Complementary Information Extraction and Alignment
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zeng%2C+D">Delong Zeng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xie%2C+Y">Yuexiang Xie</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Y">Yaliang Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shen%2C+Y">Ying Shen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.04571v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Multimodal</span> retrieval has emerged as a promising yet challenging research direction in recent years. Most existing studies in&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.04571v1-abstract-full').style.display = 'inline'; document.getElementById('2601.04571v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.04571v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Multimodal</span> retrieval has emerged as a promising yet challenging research direction in recent years. Most existing studies in <span class="search-hit mathjax">multimodal</span> retrieval focus on capturing information in <span class="search-hit mathjax">multimodal</span> data that is similar to their paired texts, but often ignores the complementary information contained in <span class="search-hit mathjax">multimodal</span> data. In this study, we propose CIEA, a novel <span class="search-hit mathjax">multimodal</span> retrieval approach that employs Complementary Information Extraction and Alignment, which <span class="search-hit mathjax">transforms</span> both text and images in documents into a unified latent space and features a complementary information extractor designed to identify and preserve differences in the image representations. We optimize CIEA using two complementary contrastive losses to ensure semantic integrity and effectively capture the complementary information contained in images. Extensive experiments demonstrate the effectiveness of CIEA, which achieves significant improvements over both divide-and-conquer models and universal dense retrieval models. We provide an ablation study, further discussions, and case studies to highlight the advancements achieved by CIEA. To promote further research in the community, we have released the source code at https://github.com/zengdlong/CIEA.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.04571v1-abstract-full').style.display = 'none'; document.getElementById('2601.04571v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by ACL&#39;2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.04376">arXiv:2601.04376</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.04376">pdf</a>, <a href="https://arxiv.org/ps/2601.04376">ps</a>, <a href="https://arxiv.org/format/2601.04376">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Combining Facial Videos and Biosignals for Stress Estimation During Driving
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Valergaki%2C+P">Paraskevi Valergaki</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nicodemou%2C+V+C">Vassilis C. Nicodemou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Oikonomidis%2C+I">Iason Oikonomidis</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Argyros%2C+A">Antonis Argyros</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Roussos%2C+A">Anastasios Roussos</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.04376v2-abstract-short" style="display: inline;">
        &hellip;physiological signals such as perinasal perspiration and heart rate, facial activity provides complementary cues that can be captured unobtrusively from video. We propose a <span class="search-hit mathjax">multimodal</span> stress estimation framework that combines facial videos and physiological signals, remaining effective even when biosignal acquisition is challenging. Facial behavior is repres&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.04376v2-abstract-full').style.display = 'inline'; document.getElementById('2601.04376v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.04376v2-abstract-full" style="display: none;">
        Reliable stress recognition is critical in applications such as medical monitoring and safety-critical systems, including real-world driving. While stress is commonly detected using physiological signals such as perinasal perspiration and heart rate, facial activity provides complementary cues that can be captured unobtrusively from video. We propose a <span class="search-hit mathjax">multimodal</span> stress estimation framework that combines facial videos and physiological signals, remaining effective even when biosignal acquisition is challenging. Facial behavior is represented using a dense 3D Morphable Model, yielding a 56-dimensional descriptor that captures subtle expression and head-pose dynamics over time. To study how stress modulates facial motion, we perform extensive experiments alongside established physiological markers. Paired hypothesis tests between baseline and stressor phases show that 38 of 56 facial components exhibit consistent, phase-specific stress responses comparable to physiological markers. Building on these findings, we introduce a <span class="search-hit mathjax">Transformer</span>-based temporal modeling framework and evaluate unimodal, early-fusion, and cross-modal attention strategies. Cross-modal attention fusion of 3D-derived facial features with physiological signals substantially improves performance over physiological signals alone, increasing AUROC from 52.7% and accuracy from 51.0% to 92.0% and 86.7%, respectively. Although evaluated on driving data, the proposed framework and protocol may generalize to other stress estimation settings.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.04376v2-abstract-full').style.display = 'none'; document.getElementById('2601.04376v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 January, 2026; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 7 January, 2026;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Under submission to ICPR 2026</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.04359">arXiv:2601.04359</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.04359">pdf</a>, <a href="https://arxiv.org/ps/2601.04359">ps</a>, <a href="https://arxiv.org/format/2601.04359">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PackCache: A Training-Free Acceleration Method for Unified Autoregressive Video Generation via Compact KV-Cache
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+K">Kunyang Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shah%2C+M">Mubarak Shah</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shang%2C+Y">Yuzhang Shang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.04359v1-abstract-short" style="display: inline;">
        A unified autoregressive model is a <span class="search-hit mathjax">Transformer</span>-based framework that addresses diverse <span class="search-hit mathjax">multimodal</span> tasks (e.g., text, image, video) as a single sequence modeling problem under a shared token space. Such models rely on the KV-cache mechanism to reduce attention computation from O(T^2) to O(T); however, KV-cache size grow&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.04359v1-abstract-full').style.display = 'inline'; document.getElementById('2601.04359v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.04359v1-abstract-full" style="display: none;">
        A unified autoregressive model is a <span class="search-hit mathjax">Transformer</span>-based framework that addresses diverse <span class="search-hit mathjax">multimodal</span> tasks (e.g., text, image, video) as a single sequence modeling problem under a shared token space. Such models rely on the KV-cache mechanism to reduce attention computation from O(T^2) to O(T); however, KV-cache size grows linearly with the number of generated tokens, and it rapidly becomes the dominant bottleneck limiting inference efficiency and generative length. Unified autoregressive video generation inherits this limitation. Our analysis reveals that KV-cache tokens exhibit distinct spatiotemporal properties: (i) text and conditioning-image tokens act as persistent semantic anchors that consistently receive high attention, and (ii) attention to previous frames naturally decays with temporal distance. Leveraging these observations, we introduce PackCache, a training-free KV-cache management method that dynamically compacts the KV cache through three coordinated mechanisms: condition anchoring that preserves semantic references, cross-frame decay modeling that allocates cache budget according to temporal distance, and spatially preserving position embedding that maintains coherent 3D structure under cache removal. In terms of efficiency, PackCache accelerates end-to-end generation by 1.7-2.2x on 48-frame long sequences, showcasing its strong potential for enabling longer-sequence video generation. Notably, the final four frames - the portion most impacted by the progressively expanding KV-cache and thus the most expensive segment of the clip - PackCache delivers a 2.6x and 3.7x acceleration on A40 and H200, respectively, for 48-frame videos.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.04359v1-abstract-full').style.display = 'none'; document.getElementById('2601.04359v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.04299">arXiv:2601.04299</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.04299">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Quantitative Methods">q-bio.QM</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        <span class="search-hit mathjax">Transformer</span>-Based Multi-Modal Temporal Embeddings for Explainable Metabolic Phenotyping in Type 1 Diabetes
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Khokhar%2C+P+B">Pir Bakhsh Khokhar</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gravino%2C+C">Carmine Gravino</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Palomba%2C+F">Fabio Palomba</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yayilgan%2C+S+Y">Sule Yildrim Yayilgan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shaikh%2C+S">Sarang Shaikh</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.04299v1-abstract-short" style="display: inline;">
        &hellip;hemoglobin (HbA1c). This study proposes an explainable deep learning framework that integrates continuous glucose monitoring (CGM) data with laboratory profiles to learn <span class="search-hit mathjax">multimodal</span> temporal embeddings of individual metabolic status. Temporal dependencies across modalities are modeled using a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.04299v1-abstract-full').style.display = 'inline'; document.getElementById('2601.04299v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.04299v1-abstract-full" style="display: none;">
        Type 1 diabetes (T1D) is a highly metabolically heterogeneous disease that cannot be adequately characterized by conventional biomarkers such as glycated hemoglobin (HbA1c). This study proposes an explainable deep learning framework that integrates continuous glucose monitoring (CGM) data with laboratory profiles to learn <span class="search-hit mathjax">multimodal</span> temporal embeddings of individual metabolic status. Temporal dependencies across modalities are modeled using a <span class="search-hit mathjax">transformer</span> encoder, while latent metabolic phenotypes are identified via Gaussian mixture modeling. Model interpretability is achieved through <span class="search-hit mathjax">transformer</span> attention visualization and SHAP-based feature attribution. Five latent metabolic phenotypes, ranging from metabolic stability to elevated cardiometabolic risk, were identified among 577 individuals with T1D. These phenotypes exhibit distinct biochemical profiles, including differences in glycemic control, lipid metabolism, renal markers, and thyrotropin (TSH) levels. Attention analysis highlights glucose variability as a dominant temporal factor, while SHAP analysis identifies HbA1c, triglycerides, cholesterol, creatinine, and TSH as key contributors to phenotype differentiation. Phenotype membership shows statistically significant, albeit modest, associations with hypertension, myocardial infarction, and heart failure. Overall, this explainable <span class="search-hit mathjax">multimodal</span> temporal embedding framework reveals physiologically coherent metabolic subgroups in T1D and supports risk stratification beyond single biomarkers.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.04299v1-abstract-full').style.display = 'none'; document.getElementById('2601.04299v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.04219">arXiv:2601.04219</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.04219">pdf</a>, <a href="https://arxiv.org/ps/2601.04219">ps</a>, <a href="https://arxiv.org/format/2601.04219">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multiagent Systems">cs.MA</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AgentTutor: Empowering Personalized Learning with Multi-Turn Interactive Teaching in Intelligent Education Systems
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Y">Yuxin Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Song%2C+Z">Zeqing Song</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lou%2C+J">Jiong Lou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+C">Chentao Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+J">Jie Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.04219v1-abstract-short" style="display: inline;">
        The rapid advancement of large-scale language models (LLMs) has shown their potential to <span class="search-hit mathjax">transform</span> intelligent education systems (IESs) through automated teaching and learning support applications. However, current IESs often rely on single-turn static question-answering, which fails to assess learners&#39; cognitive levels, cannot adjust teaching strategies&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.04219v1-abstract-full').style.display = 'inline'; document.getElementById('2601.04219v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.04219v1-abstract-full" style="display: none;">
        The rapid advancement of large-scale language models (LLMs) has shown their potential to <span class="search-hit mathjax">transform</span> intelligent education systems (IESs) through automated teaching and learning support applications. However, current IESs often rely on single-turn static question-answering, which fails to assess learners&#39; cognitive levels, cannot adjust teaching strategies based on real-time feedback, and is limited to providing simple one-off responses. To address these issues, we introduce AgentTutor, a multi-turn interactive intelligent education system to empower personalized learning. It features an LLM-powered generative multi-agent system and a learner-specific personalized learning profile environment that dynamically optimizes and delivers teaching strategies based on learners&#39; learning status, personalized goals, learning preferences, and <span class="search-hit mathjax">multimodal</span> study materials. It includes five key modules: curriculum decomposition, learner assessment, dynamic strategy, teaching reflection, and knowledge &amp; experience memory. We conducted extensive experiments on multiple benchmark datasets, AgentTutor significantly enhances learners&#39; performance while demonstrating strong effectiveness in multi-turn interactions and competitiveness in teaching quality among other baselines.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.04219v1-abstract-full').style.display = 'none'; document.getElementById('2601.04219v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 December, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">AAAI2026 Workshop AI4EDU</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.04100">arXiv:2601.04100</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.04100">pdf</a>, <a href="https://arxiv.org/ps/2601.04100">ps</a>, <a href="https://arxiv.org/format/2601.04100">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Quantifying the Impact of Modules and Their Interactions in the PSO-X Framework
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Camacho-Villal%C3%B3n%2C+C+L">Christian L. Camacho-Villalón</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nikolikj%2C+A">Ana Nikolikj</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dost%2C+K">Katharina Dost</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tuba%2C+E">Eva Tuba</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=D%C5%BEeroski%2C+S">Sašo Džeroski</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Eftimov%2C+T">Tome Eftimov</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.04100v1-abstract-short" style="display: inline;">
        &hellip;in different problem classes. In practice, this allows us to identify which modules have greater influence on PSO-X performance depending on problem features such as <span class="search-hit mathjax">multimodality</span>, mathematical <span class="search-hit mathjax">transformations</span> and varying dimensionality. We then perform a cluster analysis to identify groups of problem classes that shar&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.04100v1-abstract-full').style.display = 'inline'; document.getElementById('2601.04100v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.04100v1-abstract-full" style="display: none;">
        The PSO-X framework incorporates dozens of modules that have been proposed for solving single-objective continuous optimization problems using particle swarm optimization. While modular frameworks enable users to automatically generate and configure algorithms tailored to specific optimization problems, the complexity of this process increases with the number of modules in the framework and the degrees of freedom defined for their interaction. Understanding how modules affect the performance of algorithms for different problems is critical to making the process of finding effective implementations more efficient and identifying promising areas for further investigation. Despite their practical applications and scientific relevance, there is a lack of empirical studies investigating which modules matter most in modular optimization frameworks and how they interact. In this paper, we analyze the performance of 1424 particle swarm optimization algorithms instantiated from the PSO-X framework on the 25 functions in the CEC&#39;05 benchmark suite with 10 and 30 dimensions. We use functional ANOVA to quantify the impact of modules and their combinations on performance in different problem classes. In practice, this allows us to identify which modules have greater influence on PSO-X performance depending on problem features such as <span class="search-hit mathjax">multimodality</span>, mathematical <span class="search-hit mathjax">transformations</span> and varying dimensionality. We then perform a cluster analysis to identify groups of problem classes that share similar module effect patterns. Our results show low variability in the importance of modules in all problem classes, suggesting that particle swarm optimization performance is driven by a few influential modules.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.04100v1-abstract-full').style.display = 'none'; document.getElementById('2601.04100v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.03660">arXiv:2601.03660</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.03660">pdf</a>, <a href="https://arxiv.org/ps/2601.03660">ps</a>, <a href="https://arxiv.org/format/2601.03660">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MGPC: <span class="search-hit mathjax">Multimodal</span> Network for Generalizable Point Cloud Completion With Modality Dropout and Progressive Decoding
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+J">Jiangyuan Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ma%2C+H">Hongxuan Ma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+Y">Yuhao Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Z">Zhe Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+J">Jian Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zou%2C+W">Wei Zou</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.03660v1-abstract-short" style="display: inline;">
        &hellip;from partial observations caused by limited viewpoints and occlusions. Existing learning-based works, including 3D Convolutional Neural Network (CNN)-based, point-based, and <span class="search-hit mathjax">Transformer</span>-based methods, have achieved strong performance on synthetic benchmarks. However, due to the limitations of modality, scalability, and generative capacity, their generalizati&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.03660v1-abstract-full').style.display = 'inline'; document.getElementById('2601.03660v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.03660v1-abstract-full" style="display: none;">
        Point cloud completion aims to recover complete 3D geometry from partial observations caused by limited viewpoints and occlusions. Existing learning-based works, including 3D Convolutional Neural Network (CNN)-based, point-based, and <span class="search-hit mathjax">Transformer</span>-based methods, have achieved strong performance on synthetic benchmarks. However, due to the limitations of modality, scalability, and generative capacity, their generalization to novel objects and real-world scenarios remains challenging. In this paper, we propose MGPC, a generalizable <span class="search-hit mathjax">multimodal</span> point cloud completion framework that integrates point clouds, RGB images, and text within a unified architecture. MGPC introduces an innovative modality dropout strategy, a <span class="search-hit mathjax">Transformer</span>-based fusion module, and a novel progressive generator to improve robustness, scalability, and geometric modeling capability. We further develop an automatic data generation pipeline and construct MGPC-1M, a large-scale benchmark with over 1,000 categories and one million training pairs. Extensive experiments on MGPC-1M and in-the-wild data demonstrate that the proposed method consistently outperforms prior baselines and exhibits strong generalization under real-world conditions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.03660v1-abstract-full').style.display = 'none'; document.getElementById('2601.03660v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Code and dataset are available at https://github.com/L-J-Yuan/MGPC</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.03482">arXiv:2601.03482</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.03482">pdf</a>, <a href="https://arxiv.org/ps/2601.03482">ps</a>, <a href="https://arxiv.org/format/2601.03482">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Applications">stat.AP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Personalization of Large Foundation Models for Health Interventions
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Konigorski%2C+S">Stefan Konigorski</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Vedder%2C+J+E">Johannes E. Vedder</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Owoyele%2C+B+A">Babajide Alamu Owoyele</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=%C3%96zkan%2C+%C4%B0">İbrahim Özkan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.03482v1-abstract-short" style="display: inline;">
        Large foundation models (LFMs) <span class="search-hit mathjax">transform</span> healthcare AI in prevention, diagnostics, and treatment. However, whether LFMs can provide truly personalized treatment recommendations remains an open question. Recent research has revealed multiple challenges for personalization, including the fundamental generalizability paradox: models achieving high accuracy in o&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.03482v1-abstract-full').style.display = 'inline'; document.getElementById('2601.03482v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.03482v1-abstract-full" style="display: none;">
        Large foundation models (LFMs) <span class="search-hit mathjax">transform</span> healthcare AI in prevention, diagnostics, and treatment. However, whether LFMs can provide truly personalized treatment recommendations remains an open question. Recent research has revealed multiple challenges for personalization, including the fundamental generalizability paradox: models achieving high accuracy in one clinical study perform at chance level in others, demonstrating that personalization and external validity exist in tension. This exemplifies broader contradictions in AI-driven healthcare: the privacy-performance paradox, scale-specificity paradox, and the automation-empathy paradox. As another challenge, the degree of causal understanding required for personalized recommendations, as opposed to mere predictive capacities of LFMs, remains an open question. N-of-1 trials -- crossover self-experiments and the gold standard for individual causal inference in personalized medicine -- resolve these tensions by providing within-person causal evidence while preserving privacy through local experimentation. Despite their impressive capabilities, this paper argues that LFMs cannot replace N-of-1 trials. We argue that LFMs and N-of-1 trials are complementary: LFMs excel at rapid hypothesis generation from population patterns using <span class="search-hit mathjax">multimodal</span> data, while N-of-1 trials excel at causal validation for a given individual. We propose a hybrid framework that combines the strengths of both to enable personalization and navigate the identified paradoxes: LFMs generate ranked intervention candidates with uncertainty estimates, which trigger subsequent N-of-1 trials. Clarifying the boundary between prediction and causation and explicitly addressing the paradoxical tensions are essential for responsible AI integration in personalized medicine.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.03482v1-abstract-full').style.display = 'none'; document.getElementById('2601.03482v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to the AAAI 2026 Workshop on Personalization in the Era of Large Foundation Models (PerFM)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.03464">arXiv:2601.03464</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.03464">pdf</a>, <a href="https://arxiv.org/ps/2601.03464">ps</a>, <a href="https://arxiv.org/format/2601.03464">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Prompting Underestimates LLM Capability for Time Series Classification
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Schumacher%2C+D">Dan Schumacher</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nourbakhsh%2C+E">Erfan Nourbakhsh</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Slavin%2C+R">Rocky Slavin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rios%2C+A">Anthony Rios</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.03464v1-abstract-short" style="display: inline;">
        &hellip;to 0.61-0.67, often matching or exceeding specialized time series models. Layer-wise analyses further show that class-discriminative time series information emerges in early <span class="search-hit mathjax">transformer</span> layers and is amplified by visual and <span class="search-hit mathjax">multimodal</span> inputs. Together, these results demonstrate a systematic mismatch between what LLMs i&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.03464v1-abstract-full').style.display = 'inline'; document.getElementById('2601.03464v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.03464v1-abstract-full" style="display: none;">
        Prompt-based evaluations suggest that large language models (LLMs) perform poorly on time series classification, raising doubts about whether they encode meaningful temporal structure. We show that this conclusion reflects limitations of prompt-based generation rather than the model&#39;s representational capacity by directly comparing prompt outputs with linear probes over the same internal representations. While zero-shot prompting performs near chance, linear probes improve average F1 from 0.15-0.26 to 0.61-0.67, often matching or exceeding specialized time series models. Layer-wise analyses further show that class-discriminative time series information emerges in early <span class="search-hit mathjax">transformer</span> layers and is amplified by visual and <span class="search-hit mathjax">multimodal</span> inputs. Together, these results demonstrate a systematic mismatch between what LLMs internally represent and what prompt-based evaluation reveals, leading current evaluations to underestimate their time series understanding.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.03464v1-abstract-full').style.display = 'none'; document.getElementById('2601.03464v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages + Appendix and References, 9 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.03460">arXiv:2601.03460</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.03460">pdf</a>, <a href="https://arxiv.org/ps/2601.03460">ps</a>, <a href="https://arxiv.org/format/2601.03460">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        FROST-Drive: Scalable and Efficient End-to-End Driving with a Frozen Vision Encoder
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+Z">Zeyu Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+Y">Yimin Zhu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+Y">Yu Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+Y">Yu Sun</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.03460v1-abstract-short" style="display: inline;">
        &hellip;frozen, our approach directly transfers the rich, generalized world knowledge from the VLM to the driving task. Our model architecture combines this frozen encoder with a <span class="search-hit mathjax">transformer</span>-based adapter for <span class="search-hit mathjax">multimodal</span> fusion and a GRU-based decoder for smooth waypoint generation. Furthermore, we introduce a custom loss funct&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.03460v1-abstract-full').style.display = 'inline'; document.getElementById('2601.03460v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.03460v1-abstract-full" style="display: none;">
        End-to-end (E2E) models in autonomous driving aim to directly map sensor inputs to control commands, but their ability to generalize to novel and complex scenarios remains a key challenge. The common practice of fully fine-tuning the vision encoder on driving datasets potentially limits its generalization by causing the model to specialize too heavily in the training data. This work challenges the necessity of this training paradigm. We propose FROST-Drive, a novel E2E architecture designed to preserve and leverage the powerful generalization capabilities of a pretrained vision encoder from a Vision-Language Model (VLM). By keeping the encoder&#39;s weights frozen, our approach directly transfers the rich, generalized world knowledge from the VLM to the driving task. Our model architecture combines this frozen encoder with a <span class="search-hit mathjax">transformer</span>-based adapter for <span class="search-hit mathjax">multimodal</span> fusion and a GRU-based decoder for smooth waypoint generation. Furthermore, we introduce a custom loss function designed to directly optimize for Rater Feedback Score (RFS), a metric that prioritizes robust trajectory planning. We conduct extensive experiments on Waymo Open E2E Dataset, a large-scale datasets deliberately curated to capture the long-tail scenarios, demonstrating that our frozen-encoder approach significantly outperforms models that employ full fine-tuning. Our results provide substantial evidence that preserving the broad knowledge of a capable VLM is a more effective strategy for achieving robust, generalizable driving performance than intensive domain-specific adaptation. This offers a new pathway for developing vision-based models that can better handle the complexities of real-world application domains.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.03460v1-abstract-full').style.display = 'none'; document.getElementById('2601.03460v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.03329">arXiv:2601.03329</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.03329">pdf</a>, <a href="https://arxiv.org/ps/2601.03329">ps</a>, <a href="https://arxiv.org/format/2601.03329">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Attention mechanisms in neural networks
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Hays%2C+H">Hasi Hays</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.03329v1-abstract-short" style="display: inline;">
        &hellip;foundations, computational properties, and practical implementations in contemporary deep learning systems. Applications in natural language processing, computer vision, and <span class="search-hit mathjax">multimodal</span> learning demonstrate the versatility of attention mechanisms. We examine language modeling with autoregressive&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.03329v1-abstract-full').style.display = 'inline'; document.getElementById('2601.03329v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.03329v1-abstract-full" style="display: none;">
        Attention mechanisms represent a fundamental paradigm shift in neural network architectures, enabling models to selectively focus on relevant portions of input sequences through learned weighting functions. This monograph provides a comprehensive and rigorous mathematical treatment of attention mechanisms, encompassing their theoretical foundations, computational properties, and practical implementations in contemporary deep learning systems. Applications in natural language processing, computer vision, and <span class="search-hit mathjax">multimodal</span> learning demonstrate the versatility of attention mechanisms. We examine language modeling with autoregressive <span class="search-hit mathjax">transformers</span>, bidirectional encoders for representation learning, sequence-to-sequence translation, Vision <span class="search-hit mathjax">Transformers</span> for image classification, and cross-modal attention for vision-language tasks. Empirical analysis reveals training characteristics, scaling laws that relate performance to model size and computation, attention pattern visualizations, and performance benchmarks across standard datasets. We discuss the interpretability of learned attention patterns and their relationship to linguistic and visual structures. The monograph concludes with a critical examination of current limitations, including computational scalability, data efficiency, systematic generalization, and interpretability challenges.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.03329v1-abstract-full').style.display = 'none'; document.getElementById('2601.03329v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.02737">arXiv:2601.02737</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.02737">pdf</a>, <a href="https://arxiv.org/ps/2601.02737">ps</a>, <a href="https://arxiv.org/format/2601.02737">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Unveiling and Bridging the Functional Perception Gap in MLLMs: Atomic Visual Alignment and Hierarchical Evaluation via PET-Bench
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ye%2C+Z">Zanting Ye</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Niu%2C+X">Xiaolong Niu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+X">Xuanbin Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Han%2C+X">Xu Han</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+S">Shengyuan Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hao%2C+J">Jing Hao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Peng%2C+Z">Zhihao Peng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+H">Hao Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lv%2C+J">Jieqin Lv</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+F">Fanghu Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+Y">Yanchao Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+H">Hubing Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yuan%2C+Y">Yixuan Yuan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zaidi%2C+H">Habib Zaidi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rahmim%2C+A">Arman Rahmim</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zheng%2C+Y">Yefeng Zheng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lu%2C+L">Lijun Lu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.02737v1-abstract-short" style="display: inline;">
        While <span class="search-hit mathjax">Multimodal</span> Large Language Models (MLLMs) have demonstrated remarkable proficiency in tasks such as abnormality detection and report generation for anatomical modalities, their capability in functional imaging remains largely unexplored. In this work, we identify and quantify a fundamental functional perception gap: the inability of current vision encod&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02737v1-abstract-full').style.display = 'inline'; document.getElementById('2601.02737v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.02737v1-abstract-full" style="display: none;">
        While <span class="search-hit mathjax">Multimodal</span> Large Language Models (MLLMs) have demonstrated remarkable proficiency in tasks such as abnormality detection and report generation for anatomical modalities, their capability in functional imaging remains largely unexplored. In this work, we identify and quantify a fundamental functional perception gap: the inability of current vision encoders to decode functional tracer biodistribution independent of morphological priors. Identifying Positron Emission Tomography (PET) as the quintessential modality to investigate this disconnect, we introduce PET-Bench, the first large-scale functional imaging benchmark comprising 52,308 hierarchical QA pairs from 9,732 multi-site, multi-tracer PET studies. Extensive evaluation of 19 state-of-the-art MLLMs reveals a critical safety hazard termed the Chain-of-Thought (CoT) hallucination trap. We observe that standard CoT prompting, widely considered to enhance reasoning, paradoxically decouples linguistic generation from visual evidence in PET, producing clinically fluent but factually ungrounded diagnoses. To resolve this, we propose Atomic Visual Alignment (AVA), a simple fine-tuning strategy that enforces the mastery of low-level functional perception prior to high-level diagnostic reasoning. Our results demonstrate that AVA effectively bridges the perception gap, <span class="search-hit mathjax">transforming</span> CoT from a source of hallucination into a robust inference tool and improving diagnostic accuracy by up to 14.83%. Code and data are available at https://github.com/yezanting/PET-Bench.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02737v1-abstract-full').style.display = 'none'; document.getElementById('2601.02737v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, 6 figures, 6 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.02677">arXiv:2601.02677</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.02677">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Risk Management">q-fin.RM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Statistical Finance">q-fin.ST</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Uni-FinLLM: A Unified <span class="search-hit mathjax">Multimodal</span> Large Language Model with Modular Task Heads for Micro-Level Stock Prediction and Macro-Level Systemic Risk Assessment
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+G">Gongao Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zeng%2C+H">Haijiang Zeng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jiang%2C+L">Lu Jiang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.02677v1-abstract-short" style="display: inline;">
        &hellip;to systemic vulnerabilities. Existing approaches often treat these tasks in isolation, failing to capture cross-scale dependencies. We propose Uni-FinLLM, a unified <span class="search-hit mathjax">multimodal</span> large language model that uses a shared&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02677v1-abstract-full').style.display = 'inline'; document.getElementById('2601.02677v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.02677v1-abstract-full" style="display: none;">
        Financial institutions and regulators require systems that integrate heterogeneous data to assess risks from stock fluctuations to systemic vulnerabilities. Existing approaches often treat these tasks in isolation, failing to capture cross-scale dependencies. We propose Uni-FinLLM, a unified <span class="search-hit mathjax">multimodal</span> large language model that uses a shared <span class="search-hit mathjax">Transformer</span> backbone and modular task heads to jointly process financial text, numerical time series, fundamentals, and visual data. Through cross-modal attention and multi-task optimization, it learns a coherent representation for micro-, meso-, and macro-level predictions. Evaluated on stock forecasting, credit-risk assessment, and systemic-risk detection, Uni-FinLLM significantly outperforms baselines. It raises stock directional accuracy to 67.4% (from 61.7%), credit-risk accuracy to 84.1% (from 79.6%), and macro early-warning accuracy to 82.3%. Results validate that a unified <span class="search-hit mathjax">multimodal</span> LLM can jointly model asset behavior and systemic vulnerabilities, offering a scalable decision-support engine for finance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02677v1-abstract-full').style.display = 'none'; document.getElementById('2601.02677v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.02456">arXiv:2601.02456</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.02456">pdf</a>, <a href="https://arxiv.org/ps/2601.02456">ps</a>, <a href="https://arxiv.org/format/2601.02456">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        InternVLA-A1: Unifying Understanding, Generation and Action for Robotic Manipulation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Cai%2C+J">Junhao Cai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cai%2C+Z">Zetao Cai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cao%2C+J">Jiafei Cao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+Y">Yilun Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+Z">Zeyu He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jiang%2C+L">Lei Jiang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+H">Hang Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+H">Hengjie Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Y">Yang Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Y">Yufei Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lu%2C+Y">Yanan Lu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lv%2C+Q">Qi Lv</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ma%2C+H">Haoxiang Ma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pang%2C+J">Jiangmiao Pang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qiao%2C+Y">Yu Qiao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qiu%2C+Z">Zherui Qiu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shen%2C+Y">Yanqing Shen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shi%2C+X">Xu Shi</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tian%2C+Y">Yang Tian</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+B">Bolun Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+H">Hanqing Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+J">Jiaheng Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+T">Tai Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wei%2C+X">Xueyuan Wei</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+C">Chao Wu</a>
      , et al. (17 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.02456v1-abstract-short" style="display: inline;">
        Prevalent Vision-Language-Action (VLA) models are typically built upon <span class="search-hit mathjax">Multimodal</span> Large Language Models (MLLMs) and demonstrate exceptional proficiency in semantic understanding, but they inherently lack the capability to deduce physical world dynamics. Consequently, recent approaches have shifted toward World Models, typically formulated via video predictio&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02456v1-abstract-full').style.display = 'inline'; document.getElementById('2601.02456v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.02456v1-abstract-full" style="display: none;">
        Prevalent Vision-Language-Action (VLA) models are typically built upon <span class="search-hit mathjax">Multimodal</span> Large Language Models (MLLMs) and demonstrate exceptional proficiency in semantic understanding, but they inherently lack the capability to deduce physical world dynamics. Consequently, recent approaches have shifted toward World Models, typically formulated via video prediction; however, these methods often suffer from a lack of semantic grounding and exhibit brittleness when handling prediction errors. To synergize semantic understanding with dynamic predictive capabilities, we present InternVLA-A1. This model employs a unified Mixture-of-<span class="search-hit mathjax">Transformers</span> architecture, coordinating three experts for scene understanding, visual foresight generation, and action execution. These components interact seamlessly through a unified masked self-attention mechanism. Building upon InternVL3 and Qwen3-VL, we instantiate InternVLA-A1 at 2B and 3B parameter scales. We pre-train these models on hybrid synthetic-real datasets spanning InternData-A1 and Agibot-World, covering over 533M frames. This hybrid training strategy effectively harnesses the diversity of synthetic simulation data while minimizing the sim-to-real gap. We evaluated InternVLA-A1 across 12 real-world robotic tasks and simulation benchmark. It significantly outperforms leading models like pi0 and GR00T N1.5, achieving a 14.5\% improvement in daily tasks and a 40\%-73.3\% boost in dynamic settings, such as conveyor belt sorting.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02456v1-abstract-full').style.display = 'none'; document.getElementById('2601.02456v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Homepage: https://internrobotics.github.io/internvla-a1.github.io/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.02358">arXiv:2601.02358</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.02358">pdf</a>, <a href="https://arxiv.org/ps/2601.02358">ps</a>, <a href="https://arxiv.org/format/2601.02358">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        VINO: A Unified Visual Generator with Interleaved OmniModal Context
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+J">Junyi Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+T">Tong He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Fu%2C+Z">Zhoujie Fu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wan%2C+P">Pengfei Wan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gai%2C+K">Kun Gai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ye%2C+W">Weicai Ye</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.02358v1-abstract-short" style="display: inline;">
        &hellip;on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a <span class="search-hit mathjax">Multimodal</span> Diffusion <span class="search-hit mathjax">Transformer</span> (MMDiT), where <span class="search-hit mathjax">multimodal</span> inputs are encoded as interleaved conditioning token&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02358v1-abstract-full').style.display = 'inline'; document.getElementById('2601.02358v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.02358v1-abstract-full" style="display: none;">
        We present VINO, a unified visual generator that performs image and video generation and editing within a single framework. Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a <span class="search-hit mathjax">Multimodal</span> Diffusion <span class="search-hit mathjax">Transformer</span> (MMDiT), where <span class="search-hit mathjax">multimodal</span> inputs are encoded as interleaved conditioning tokens, and then used to guide the diffusion process. This design supports multi-reference grounding, long-form instruction following, and coherent identity preservation across static and dynamic content, while avoiding modality-specific architectural components. To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output. Across diverse generation and editing benchmarks, VINO demonstrates strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits. Our results highlight a practical path toward scalable unified visual generation, and the promise of interleaved, in-context computation as a foundation for general-purpose visual creation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02358v1-abstract-full').style.display = 'none'; document.getElementById('2601.02358v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Project page: https://sotamak1r.github.io/VINO-web/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.02356">arXiv:2601.02356</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.02356">pdf</a>, <a href="https://arxiv.org/ps/2601.02356">ps</a>, <a href="https://arxiv.org/format/2601.02356">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric <span class="search-hit mathjax">Transformation</span> in Scenes
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Tan%2C+J">Jing Tan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Z">Zhaoyang Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Shen%2C+Y">Yantao Shen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cai%2C+J">Jiarui Cai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+S">Shuo Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jiajun Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xia%2C+W">Wei Xia</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tu%2C+Z">Zhuowen Tu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Soatto%2C+S">Stefano Soatto</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.02356v2-abstract-short" style="display: inline;">
        We introduce Talk2Move, a reinforcement learning (RL) based diffusion framework for text-instructed spatial <span class="search-hit mathjax">transformation</span> of objects within scenes. Spatially manipulating objects in a scene through natural language poses a challenge for&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02356v2-abstract-full').style.display = 'inline'; document.getElementById('2601.02356v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.02356v2-abstract-full" style="display: none;">
        We introduce Talk2Move, a reinforcement learning (RL) based diffusion framework for text-instructed spatial <span class="search-hit mathjax">transformation</span> of objects within scenes. Spatially manipulating objects in a scene through natural language poses a challenge for <span class="search-hit mathjax">multimodal</span> generation systems. While existing text-based manipulation methods can adjust appearance or style, they struggle to perform object-level geometric <span class="search-hit mathjax">transformations</span>-such as translating, rotating, or resizing objects-due to scarce paired supervision and pixel-level optimization limits. Talk2Move employs Group Relative Policy Optimization (GRPO) to explore geometric actions through diverse rollouts generated from input images and lightweight textual variations, removing the need for costly paired data. A spatial reward guided model aligns geometric <span class="search-hit mathjax">transformations</span> with linguistic description, while off-policy step evaluation and active step sampling improve learning efficiency by focusing on informative <span class="search-hit mathjax">transformation</span> stages. Furthermore, we design object-centric spatial rewards that evaluate displacement, rotation, and scaling behaviors directly, enabling interpretable and coherent <span class="search-hit mathjax">transformations</span>. Experiments on curated benchmarks demonstrate that Talk2Move achieves precise, consistent, and semantically faithful object <span class="search-hit mathjax">transformations</span>, outperforming existing text-guided editing approaches in both spatial accuracy and scene coherence.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02356v2-abstract-full').style.display = 'none'; document.getElementById('2601.02356v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 January, 2026; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 January, 2026;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Project page: https://sparkstj.github.io/talk2move</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.02249">arXiv:2601.02249</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.02249">pdf</a>, <a href="https://arxiv.org/ps/2601.02249">ps</a>, <a href="https://arxiv.org/format/2601.02249">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SLGNet: Synergizing Structural Priors and Language-Guided Modulation for <span class="search-hit mathjax">Multimodal</span> Object Detection
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Xiang%2C+X">Xiantai Xiang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+G">Guangyao Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wen%2C+Z">Zixiao Wen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+W">Wenshuai Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Niu%2C+B">Ben Niu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+F">Feng Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+L">Lijia Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Q">Qiantong Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Y">Yuhan Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Pan%2C+Z">Zongxu Pan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hu%2C+Y">Yuxin Hu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.02249v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Multimodal</span> object detection leveraging RGB and Infrared (IR) images is pivotal for robust perception in all-weather scenarios. While recent adapter-based approaches efficiently transfer RGB-pretrained foundation models to this task, they often prioritize model efficiency at the expense of cross-modal structural consistency. Consequently, critical structural&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02249v1-abstract-full').style.display = 'inline'; document.getElementById('2601.02249v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.02249v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Multimodal</span> object detection leveraging RGB and Infrared (IR) images is pivotal for robust perception in all-weather scenarios. While recent adapter-based approaches efficiently transfer RGB-pretrained foundation models to this task, they often prioritize model efficiency at the expense of cross-modal structural consistency. Consequently, critical structural cues are frequently lost when significant domain gaps arise, such as in high-contrast or nighttime environments. Moreover, conventional static <span class="search-hit mathjax">multimodal</span> fusion mechanisms typically lack environmental awareness, resulting in suboptimal adaptation and constrained detection performance under complex, dynamic scene variations. To address these limitations, we propose SLGNet, a parameter-efficient framework that synergizes hierarchical structural priors and language-guided modulation within a frozen Vision <span class="search-hit mathjax">Transformer</span> (ViT)-based foundation model. Specifically, we design a Structure-Aware Adapter to extract hierarchical structural representations from both modalities and dynamically inject them into the ViT to compensate for structural degradation inherent in ViT-based backbones. Furthermore, we propose a Language-Guided Modulation module that exploits VLM-driven structured captions to dynamically recalibrate visual features, thereby endowing the model with robust environmental awareness. Extensive experiments on the LLVIP, FLIR, KAIST, and DroneVehicle datasets demonstrate that SLGNet establishes new state-of-the-art performance. Notably, on the LLVIP benchmark, our method achieves an mAP of 66.1, while reducing trainable parameters by approximately 87% compared to traditional full fine-tuning. This confirms SLGNet as a robust and efficient solution for <span class="search-hit mathjax">multimodal</span> perception.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02249v1-abstract-full').style.display = 'none'; document.getElementById('2601.02249v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.02211">arXiv:2601.02211</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.02211">pdf</a>, <a href="https://arxiv.org/ps/2601.02211">ps</a>, <a href="https://arxiv.org/format/2601.02211">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Unraveling MMDiT Blocks: Training-free Analysis and Enhancement of Text-conditioned Diffusion
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+B">Binglei Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+M">Mengping Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tan%2C+Z">Zhiyu Tan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+J">Junping Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+H">Hao Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.02211v1-abstract-short" style="display: inline;">
        Recent breakthroughs of <span class="search-hit mathjax">transformer</span>-based diffusion models, particularly with <span class="search-hit mathjax">Multimodal</span> Diffusion <span class="search-hit mathjax">Transformers</span> (MMDiT) driven models like FLUX and Qwen Image, have facilitated thrilling experiences in text-to-image generation and editing. To understand the internal mechanism of&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02211v1-abstract-full').style.display = 'inline'; document.getElementById('2601.02211v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.02211v1-abstract-full" style="display: none;">
        Recent breakthroughs of <span class="search-hit mathjax">transformer</span>-based diffusion models, particularly with <span class="search-hit mathjax">Multimodal</span> Diffusion <span class="search-hit mathjax">Transformers</span> (MMDiT) driven models like FLUX and Qwen Image, have facilitated thrilling experiences in text-to-image generation and editing. To understand the internal mechanism of MMDiT-based models, existing methods tried to analyze the effect of specific components like positional encoding and attention layers. Yet, a comprehensive understanding of how different blocks and their interactions with textual conditions contribute to the synthesis process remains elusive. In this paper, we first develop a systematic pipeline to comprehensively investigate each block&#39;s functionality by removing, disabling and enhancing textual hidden-states at corresponding blocks. Our analysis reveals that 1) semantic information appears in earlier blocks and finer details are rendered in later blocks, 2) removing specific blocks is usually less disruptive than disabling text conditions, and 3) enhancing textual conditions in selective blocks improves semantic attributes. Building on these observations, we further propose novel training-free strategies for improved text alignment, precise editing, and acceleration. Extensive experiments demonstrated that our method outperforms various baselines and remains flexible across text-to-image generation, image editing, and inference acceleration. Our method improves T2I-Combench++ from 56.92% to 63.00% and GenEval from 66.42% to 71.63% on SD3.5, without sacrificing synthesis quality. These results advance understanding of MMDiT models and provide valuable insights to unlock new possibilities for further improvements.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02211v1-abstract-full').style.display = 'none'; document.getElementById('2601.02211v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">11 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.02204">arXiv:2601.02204</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.02204">pdf</a>, <a href="https://arxiv.org/ps/2601.02204">ps</a>, <a href="https://arxiv.org/format/2601.02204">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        NextFlow: Unified Sequential Modeling Activates <span class="search-hit mathjax">Multimodal</span> Understanding and Generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+H">Huichao Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Qu%2C+L">Liao Qu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+Y">Yiheng Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+H">Hang Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Song%2C+Y">Yangyang Song</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dong%2C+Y">Yongsheng Dong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sun%2C+S">Shikun Sun</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+X">Xian Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+X">Xu Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jiang%2C+Y">Yi Jiang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ye%2C+H">Hu Ye</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+B">Bo Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gao%2C+Y">Yiming Gao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+P">Peng Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+A">Akide Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yang%2C+Z">Zhipeng Yang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Deng%2C+Q">Qili Deng</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xing%2C+L">Linjie Xing</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+J">Jiyang Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+Z">Zhao Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+Y">Yang Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Liu%2C+M">Mingcong Liu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Y">Yi Zhang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=He%2C+Q">Qian He</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hu%2C+X">Xiwei Hu</a>
      , et al. (11 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.02204v1-abstract-short" style="display: inline;">
        We present NextFlow, a unified decoder-only autoregressive <span class="search-hit mathjax">transformer</span> trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates <span class="search-hit mathjax">multimodal</span> understanding and generation capabilities, unlocking abi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02204v1-abstract-full').style.display = 'inline'; document.getElementById('2601.02204v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.02204v1-abstract-full" style="display: none;">
        We present NextFlow, a unified decoder-only autoregressive <span class="search-hit mathjax">transformer</span> trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates <span class="search-hit mathjax">multimodal</span> understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02204v1-abstract-full').style.display = 'none'; document.getElementById('2601.02204v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Project page: https://github.com/ByteVisionLab/NextFlow</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.02008">arXiv:2601.02008</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.02008">pdf</a>, <a href="https://arxiv.org/ps/2601.02008">ps</a>, <a href="https://arxiv.org/format/2601.02008">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        XAI-MeD: Explainable Knowledge Guided Neuro-Symbolic Framework for Domain Generalization and Rare Class Detection in Medical Imaging
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Urooj%2C+M">Midhat Urooj</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Banerjee%2C+A">Ayan Banerjee</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gupta%2C+S">Sandeep Gupta</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.02008v1-abstract-short" style="display: inline;">
        &hellip;class sensitivity and deliver transparent clinically aligned interpretations The framework encodes clinical expertise as logical connectives over atomic medical propositions <span class="search-hit mathjax">transforming</span> them into machine checkable class specific rules Their diagnostic utility is quantified through weighted feature satisfaction scores enabling a symbolic reasoning branch tha&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02008v1-abstract-full').style.display = 'inline'; document.getElementById('2601.02008v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.02008v1-abstract-full" style="display: none;">
        Explainability domain generalization and rare class reliability are critical challenges in medical AI where deep models often fail under real world distribution shifts and exhibit bias against infrequent clinical conditions This paper introduces XAIMeD an explainable medical AI framework that integrates clinically accurate expert knowledge into deep learning through a unified neuro symbolic architecture XAIMeD is designed to improve robustness under distribution shift enhance rare class sensitivity and deliver transparent clinically aligned interpretations The framework encodes clinical expertise as logical connectives over atomic medical propositions <span class="search-hit mathjax">transforming</span> them into machine checkable class specific rules Their diagnostic utility is quantified through weighted feature satisfaction scores enabling a symbolic reasoning branch that complements neural predictions A confidence weighted fusion integrates symbolic and deep outputs while a Hunt inspired adaptive routing mechanism guided by Entropy Imbalance Gain EIG and Rare Class Gini mitigates class imbalance high intra class variability and uncertainty We evaluate XAIMeD across diverse modalities on four challenging tasks i Seizure Onset Zone SOZ localization from rs fMRI ii Diabetic Retinopathy grading across 6 multicenter datasets demonstrate substantial performance improvements including 6 percent gains in cross domain generalization and a 10 percent improved rare class F1 score far outperforming state of the art deep learning baselines Ablation studies confirm that the clinically grounded symbolic components act as effective regularizers ensuring robustness to distribution shifts XAIMeD thus provides a principled clinically faithful and interpretable approach to <span class="search-hit mathjax">multimodal</span> medical AI.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.02008v1-abstract-full').style.display = 'none'; document.getElementById('2601.02008v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at AAAI Bridge Program 2026</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.01593">arXiv:2601.01593</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.01593">pdf</a>, <a href="https://arxiv.org/ps/2601.01593">ps</a>, <a href="https://arxiv.org/format/2601.01593">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Beyond Patches: Global-aware Autoregressive Model for <span class="search-hit mathjax">Multimodal</span> Few-Shot Font Generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Cai%2C+H">Haonan Cai</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Luo%2C+Y">Yuxuan Luo</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lian%2C+Z">Zhouhui Lian</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.01593v1-abstract-short" style="display: inline;">
        Manual font design is an intricate process that <span class="search-hit mathjax">transforms</span> a stylistic visual concept into a coherent glyph set. This challenge persists in automated Few-shot Font Generation (FFG), where models often struggle to preserve both the structural integrity and stylistic fidelity from limited references. While autoregressive (AR) models have demonstrated impressiv&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.01593v1-abstract-full').style.display = 'inline'; document.getElementById('2601.01593v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.01593v1-abstract-full" style="display: none;">
        Manual font design is an intricate process that <span class="search-hit mathjax">transforms</span> a stylistic visual concept into a coherent glyph set. This challenge persists in automated Few-shot Font Generation (FFG), where models often struggle to preserve both the structural integrity and stylistic fidelity from limited references. While autoregressive (AR) models have demonstrated impressive generative capabilities, their application to FFG is constrained by conventional patch-level tokenization, which neglects global dependencies crucial for coherent font synthesis. Moreover, existing FFG methods remain within the image-to-image paradigm, relying solely on visual references and overlooking the role of language in conveying stylistic intent during font design. To address these limitations, we propose GAR-Font, a novel AR framework for <span class="search-hit mathjax">multimodal</span> few-shot font generation. GAR-Font introduces a global-aware tokenizer that effectively captures both local structures and global stylistic patterns, a <span class="search-hit mathjax">multimodal</span> style encoder offering flexible style control through a lightweight language-style adapter without requiring intensive <span class="search-hit mathjax">multimodal</span> pretraining, and a post-refinement pipeline that further enhances structural fidelity and style coherence. Extensive experiments show that GAR-Font outperforms existing FFG methods, excelling in maintaining global style faithfulness and achieving higher-quality results with textual stylistic guidance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.01593v1-abstract-full').style.display = 'none'; document.getElementById('2601.01593v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">25 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.01322">arXiv:2601.01322</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.01322">pdf</a>, <a href="https://arxiv.org/ps/2601.01322">ps</a>, <a href="https://arxiv.org/format/2601.01322">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        LinMU: <span class="search-hit mathjax">Multimodal</span> Understanding Made Linear
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+H">Hongjie Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Jha%2C+N+K">Niraj K. Jha</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.01322v1-abstract-short" style="display: inline;">
        &hellip;and makes their understanding of high-resolution images and long-context videos prohibitively expensive. To address this challenge, we introduce LinMU (Linear-complexity <span class="search-hit mathjax">Multimodal</span> Understanding), a VLM design that achieves linear complexity without using any quadratic-complexity modules while maintaining the performance of global-attention-based VLMs. LinMU&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.01322v1-abstract-full').style.display = 'inline'; document.getElementById('2601.01322v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.01322v1-abstract-full" style="display: none;">
        Modern Vision-Language Models (VLMs) achieve impressive performance but are limited by the quadratic complexity of self-attention, which prevents their deployment on edge devices and makes their understanding of high-resolution images and long-context videos prohibitively expensive. To address this challenge, we introduce LinMU (Linear-complexity <span class="search-hit mathjax">Multimodal</span> Understanding), a VLM design that achieves linear complexity without using any quadratic-complexity modules while maintaining the performance of global-attention-based VLMs. LinMU replaces every self-attention layer in the VLM with the M-MATE block: a dual-branch module that combines a bidirectional state-space model for global context (Flex-MA branch) with localized Swin-style window attention (Local-Swin branch) for adjacent correlations. To <span class="search-hit mathjax">transform</span> a pre-trained VLM into the LinMU architecture, we propose a three-stage distillation framework that (i) initializes both branches with self-attention weights and trains the Flex-MA branch alone, (ii) unfreezes the Local-Swin branch and fine-tunes it jointly with the Flex-MA branch, and (iii) unfreezes the remaining blocks and fine-tunes them using LoRA adapters, while regressing on hidden states and token-level logits of the frozen VLM teacher. On MMMU, TextVQA, LongVideoBench, Video-MME, and other benchmarks, LinMU matches the performance of teacher models, yet reduces Time-To-First-Token (TTFT) by up to 2.7$\times$ and improves token throughput by up to 9.0$\times$ on minute-length videos. Ablations confirm the importance of each distillation stage and the necessity of the two branches of the M-MATE block. The proposed framework demonstrates that state-of-the-art <span class="search-hit mathjax">multimodal</span> reasoning can be achieved without quadratic attention, thus opening up avenues for long-context VLMs that can deal with high-resolution images and long videos.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.01322v1-abstract-full').style.display = 'none'; document.getElementById('2601.01322v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">23 pages, 7 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.00907">arXiv:2601.00907</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.00907">pdf</a>, <a href="https://arxiv.org/ps/2601.00907">ps</a>, <a href="https://arxiv.org/format/2601.00907">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Placenta Accreta Spectrum Detection using <span class="search-hit mathjax">Multimodal</span> Deep Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Ali%2C+S">Sumaiya Ali</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Alhothali%2C+A">Areej Alhothali</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Albasri%2C+S">Sameera Albasri</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Alzamzami%2C+O">Ohoud Alzamzami</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Abduljabbar%2C+A">Ahmed Abduljabbar</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Alwazzan%2C+M">Muhammad Alwazzan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.00907v1-abstract-short" style="display: inline;">
        &hellip;maternal and neonatal risks. This study aimed to develop and validate a deep learning framework that enhances PAS detection by integrating multiple imaging modalities. A <span class="search-hit mathjax">multimodal</span> deep learning model was designed using an intermediate feature-level fusion architecture combining 3D Magnetic Resonance Imaging (MRI) and 2D Ultrasound (US) scans. Unimodal featu&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.00907v1-abstract-full').style.display = 'inline'; document.getElementById('2601.00907v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.00907v1-abstract-full" style="display: none;">
        Placenta Accreta Spectrum (PAS) is a life-threatening obstetric complication involving abnormal placental invasion into the uterine wall. Early and accurate prenatal diagnosis is essential to reduce maternal and neonatal risks. This study aimed to develop and validate a deep learning framework that enhances PAS detection by integrating multiple imaging modalities. A <span class="search-hit mathjax">multimodal</span> deep learning model was designed using an intermediate feature-level fusion architecture combining 3D Magnetic Resonance Imaging (MRI) and 2D Ultrasound (US) scans. Unimodal feature extractors, a 3D DenseNet121-Vision <span class="search-hit mathjax">Transformer</span> for MRI and a 2D ResNet50 for US, were selected after systematic comparative analysis. Curated datasets comprising 1,293 MRI and 1,143 US scans were used to train the unimodal models and paired samples of patient-matched MRI-US scans was isolated for <span class="search-hit mathjax">multimodal</span> model development and evaluation. On an independent test set, the <span class="search-hit mathjax">multimodal</span> fusion model achieved superior performance, with an accuracy of 92.5% and an Area Under the Receiver Operating Characteristic Curve (AUC) of 0.927, outperforming the MRI-only (82.5%, AUC 0.825) and US-only (87.5%, AUC 0.879) models. Integrating MRI and US features provides complementary diagnostic information, demonstrating strong potential to enhance prenatal risk assessment and improve patient outcomes.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.00907v1-abstract-full').style.display = 'none'; document.getElementById('2601.00907v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 December, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2601.00670">arXiv:2601.00670</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2601.00670">pdf</a>, <a href="https://arxiv.org/ps/2601.00670">ps</a>, <a href="https://arxiv.org/format/2601.00670">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Wave2Word: A <span class="search-hit mathjax">Multimodal</span> <span class="search-hit mathjax">Transformer</span> Framework for Joint EEG-Text Alignment and Multi-Task Representation Learning in Neurocritical Care
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Samanta%2C+A+K">Argha Kamal Samanta</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mewada%2C+D">Deepak Mewada</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sarma%2C+M">Monalisa Sarma</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Samanta%2C+D">Debasis Samanta</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2601.00670v1-abstract-short" style="display: inline;">
        &hellip;exhibits overlapping patterns, graded expert agreement, and temporal persistence, which are not well captured by classification objectives alone. This work proposes a <span class="search-hit mathjax">multimodal</span> EEG representation learning framework that integrates signal-domain modeling with structured clinical language supervision. First, raw EEG is&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.00670v1-abstract-full').style.display = 'inline'; document.getElementById('2601.00670v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2601.00670v1-abstract-full" style="display: none;">
        Continuous electroencephalography (EEG) is routinely used in neurocritical care to monitor seizures and other harmful brain activity, including rhythmic and periodic patterns that are clinically significant. Although deep learning methods have achieved high accuracy in seizure detection, most existing approaches remain seizure-centric, rely on discrete-label supervision, and are primarily evaluated using accuracy-based metrics. A central limitation of current EEG modeling practice is the weak correspondence between learned representations and how EEG findings are interpreted and summarized in clinical workflows. Harmful EEG activity exhibits overlapping patterns, graded expert agreement, and temporal persistence, which are not well captured by classification objectives alone. This work proposes a <span class="search-hit mathjax">multimodal</span> EEG representation learning framework that integrates signal-domain modeling with structured clinical language supervision. First, raw EEG is <span class="search-hit mathjax">transformed</span> into a longitudinal bipolar montage and time-frequency representations. Second, dual <span class="search-hit mathjax">transformer</span>-based encoders model complementary temporal and frequency-centric dependencies and are fused using an adaptive gating mechanism. Third, EEG embeddings are aligned with structured expert consensus descriptions through a contrastive objective. Finally, an EEG-conditioned text reconstruction loss is introduced as a representation-level constraint alongside standard classification loss. Experimental evaluation using a controlled train-validation-test split achieves a six-class test accuracy of 0.9797. Ablation analyses show that removing contrastive alignment reduces cross-modal retrieval performance from Recall@10 of 0.3390 to 0.0045, despite minimal change in classification accuracy. These findings demonstrate that discriminative accuracy does not reliably reflect representation quality for clinically meaningful EEG modeling.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2601.00670v1-abstract-full').style.display = 'none'; document.getElementById('2601.00670v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 January, 2026; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2026.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2512.24645">arXiv:2512.24645</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2512.24645">pdf</a>, <a href="https://arxiv.org/ps/2512.24645">ps</a>, <a href="https://arxiv.org/format/2512.24645">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3746027.3756869">10.1145/3746027.3756869 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AudioFab: Building A General and Intelligent Audio Factory through Tool Learning
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Zhu%2C+C">Cheng Zhu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Han%2C+J">Jing Han</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Xue%2C+Q">Qianshuai Xue</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+K">Kehan Wang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+H">Huan Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhang%2C+Z">Zixing Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2512.24645v1-abstract-short" style="display: inline;">
        Currently, artificial intelligence is profoundly <span class="search-hit mathjax">transforming</span> the audio domain; however, numerous advanced algorithms and tools remain fragmented, lacking a unified and efficient framework to unlock their full potential. Existing audio agent frameworks often suffer from complex environment configurations and inefficient tool collaboration. To address these l&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.24645v1-abstract-full').style.display = 'inline'; document.getElementById('2512.24645v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2512.24645v1-abstract-full" style="display: none;">
        Currently, artificial intelligence is profoundly <span class="search-hit mathjax">transforming</span> the audio domain; however, numerous advanced algorithms and tools remain fragmented, lacking a unified and efficient framework to unlock their full potential. Existing audio agent frameworks often suffer from complex environment configurations and inefficient tool collaboration. To address these limitations, we introduce AudioFab, an open-source agent framework aimed at establishing an open and intelligent audio-processing ecosystem. Compared to existing solutions, AudioFab&#39;s modular design resolves dependency conflicts, simplifying tool integration and extension. It also optimizes tool learning through intelligent selection and few-shot learning, improving efficiency and accuracy in complex audio tasks. Furthermore, AudioFab provides a user-friendly natural language interface tailored for non-expert users. As a foundational framework, AudioFab&#39;s core contribution lies in offering a stable and extensible platform for future research and development in audio and <span class="search-hit mathjax">multimodal</span> AI. The code is available at https://github.com/SmileHnu/AudioFab.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.24645v1-abstract-full').style.display = 'none'; document.getElementById('2512.24645v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 December, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2025.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        ACM Multimedia 2025
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2512.24271">arXiv:2512.24271</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2512.24271">pdf</a>, <a href="https://arxiv.org/ps/2512.24271">ps</a>, <a href="https://arxiv.org/format/2512.24271">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Taming Hallucinations: Boosting MLLMs&#39; Video Understanding via Counterfactual Video Generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+Z">Zhe Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wen%2C+H">Hao Wen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hao%2C+A">Aiming Hao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Song%2C+B">Bingze Song</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+M">Meiqi Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wu%2C+J">Jiahong Wu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chu%2C+X">Xiangxiang Chu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lu%2C+S">Sheng Lu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wang%2C+H">Haoqian Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2512.24271v1-abstract-short" style="display: inline;">
        <span class="search-hit mathjax">Multimodal</span> Large Language Models (MLLMs) have made remarkable progress in video understanding. However, they suffer from a critical vulnerability: an over-reliance on language priors, which can lead to visual ungrounded hallucinations, especially when processing counterfactual videos that defy common sense. This limitation, stemming from the intrinsic data i&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.24271v1-abstract-full').style.display = 'inline'; document.getElementById('2512.24271v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2512.24271v1-abstract-full" style="display: none;">
        <span class="search-hit mathjax">Multimodal</span> Large Language Models (MLLMs) have made remarkable progress in video understanding. However, they suffer from a critical vulnerability: an over-reliance on language priors, which can lead to visual ungrounded hallucinations, especially when processing counterfactual videos that defy common sense. This limitation, stemming from the intrinsic data imbalance between text and video, is challenging to address due to the substantial cost of collecting and annotating counterfactual data. To address this, we introduce DualityForge, a novel counterfactual data synthesis framework that employs controllable, diffusion-based video editing to <span class="search-hit mathjax">transform</span> real-world videos into counterfactual scenarios. By embedding structured contextual information into the video editing and QA generation processes, the framework automatically produces high-quality QA pairs together with original-edited video pairs for contrastive training. Based on this, we build DualityVidQA, a large-scale video dataset designed to reduce MLLM hallucinations. In addition, to fully exploit the contrastive nature of our paired data, we propose Duality-Normalized Advantage Training (DNA-Train), a two-stage SFT-RL training regime where the RL phase applies pair-wise $\ell_1$ advantage normalization, thereby enabling a more stable and efficient policy optimization. Experiments on DualityVidQA-Test demonstrate that our method substantially reduces model hallucinations on counterfactual videos, yielding a relative improvement of 24.0% over the Qwen2.5-VL-7B baseline. Moreover, our approach achieves significant gains across both hallucination and general-purpose benchmarks, indicating strong generalization capability. We will open-source our dataset and code.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.24271v1-abstract-full').style.display = 'none'; document.getElementById('2512.24271v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 December, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">18 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2512.24243">arXiv:2512.24243</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2512.24243">pdf</a>, <a href="https://arxiv.org/ps/2512.24243">ps</a>, <a href="https://arxiv.org/format/2512.24243">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MambaSeg: Harnessing Mamba for Accurate and Efficient Image-Event Semantic Segmentation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Gu%2C+F">Fuqiang Gu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Li%2C+Y">Yuanke Li</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Long%2C+X">Xianlei Long</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ji%2C+K">Kangping Ji</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Chen%2C+C">Chao Chen</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Gu%2C+Q">Qingyi Gu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Ni%2C+Z">Zhenliang Ni</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2512.24243v1-abstract-short" style="display: inline;">
        &hellip;task in computer vision with wide-ranging applications, including autonomous driving and robotics. While RGB-based methods have achieved strong performance with CNNs and <span class="search-hit mathjax">Transformers</span>, their effectiveness degrades under fast motion, low-light, or high dynamic range conditions due to limitations of frame cameras. Event cameras offer complementary advantages su&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.24243v1-abstract-full').style.display = 'inline'; document.getElementById('2512.24243v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2512.24243v1-abstract-full" style="display: none;">
        Semantic segmentation is a fundamental task in computer vision with wide-ranging applications, including autonomous driving and robotics. While RGB-based methods have achieved strong performance with CNNs and <span class="search-hit mathjax">Transformers</span>, their effectiveness degrades under fast motion, low-light, or high dynamic range conditions due to limitations of frame cameras. Event cameras offer complementary advantages such as high temporal resolution and low latency, yet lack color and texture, making them insufficient on their own. To address this, recent research has explored <span class="search-hit mathjax">multimodal</span> fusion of RGB and event data; however, many existing approaches are computationally expensive and focus primarily on spatial fusion, neglecting the temporal dynamics inherent in event streams. In this work, we propose MambaSeg, a novel dual-branch semantic segmentation framework that employs parallel Mamba encoders to efficiently model RGB images and event streams. To reduce cross-modal ambiguity, we introduce the Dual-Dimensional Interaction Module (DDIM), comprising a Cross-Spatial Interaction Module (CSIM) and a Cross-Temporal Interaction Module (CTIM), which jointly perform fine-grained fusion along both spatial and temporal dimensions. This design improves cross-modal alignment, reduces ambiguity, and leverages the complementary properties of each modality. Extensive experiments on the DDD17 and DSEC datasets demonstrate that MambaSeg achieves state-of-the-art segmentation performance while significantly reducing computational cost, showcasing its promise for efficient, scalable, and robust <span class="search-hit mathjax">multimodal</span> perception.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.24243v1-abstract-full').style.display = 'none'; document.getElementById('2512.24243v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 December, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by AAAI 2026</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2512.23906">arXiv:2512.23906</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2512.23906">pdf</a>, <a href="https://arxiv.org/ps/2512.23906">ps</a>, <a href="https://arxiv.org/format/2512.23906">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A <span class="search-hit mathjax">multimodal</span> <span class="search-hit mathjax">Transformer</span> for InSAR-based ground deformation forecasting with cross-site generalization across Europe
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Yao%2C+W">Wendong Yao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+B">Binhua Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Dev%2C+S">Soumyabrata Dev</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2512.23906v1-abstract-short" style="display: inline;">
        &hellip;of long-term trends, seasonal cycles, and occasional abrupt discontinuities (e.g., co-seismic steps), together with strong spatial heterogeneity. In this study we propose a <span class="search-hit mathjax">multimodal</span> patch-based&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.23906v1-abstract-full').style.display = 'inline'; document.getElementById('2512.23906v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2512.23906v1-abstract-full" style="display: none;">
        Near-real-time regional-scale monitoring of ground deformation is increasingly required to support urban planning, critical infrastructure management, and natural hazard mitigation. While Interferometric Synthetic Aperture Radar (InSAR) and continental-scale services such as the European Ground Motion Service (EGMS) provide dense observations of past motion, predicting the next observation remains challenging due to the superposition of long-term trends, seasonal cycles, and occasional abrupt discontinuities (e.g., co-seismic steps), together with strong spatial heterogeneity. In this study we propose a <span class="search-hit mathjax">multimodal</span> patch-based <span class="search-hit mathjax">Transformer</span> for single-step, fixed-interval next-epoch nowcasting of displacement maps from EGMS time series (resampled to a 64x64 grid over 100 km x 100 km tiles). The model ingests recent displacement snapshots together with (i) static kinematic indicators (mean velocity, acceleration, seasonal amplitude) computed in a leakage-safe manner from the training window only, and (ii) harmonic day-of-year encodings. On the eastern Ireland tile (E32N34), the STGCN is strongest in the displacement-only setting, whereas the <span class="search-hit mathjax">multimodal</span> <span class="search-hit mathjax">Transformer</span> clearly outperforms CNN-LSTM, CNN-LSTM+Attn, and <span class="search-hit mathjax">multimodal</span> STGCN when all models receive the same <span class="search-hit mathjax">multimodal</span> inputs, achieving RMSE = 0.90 mm and $R^2$ = 0.97 on the test set with the best threshold accuracies.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.23906v1-abstract-full').style.display = 'none'; document.getElementById('2512.23906v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 December, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">submitted to ISPRS Journal of Photogrammetry and Remote Sensing for review</span>
    </p>
    

    
      <p class="comments is-size-7">
        
          <span class="has-text-black-bis has-text-weight-semibold">Report number:</span>
          PHOTO-D-25-03411
        

        

        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2512.23903">arXiv:2512.23903</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2512.23903">pdf</a>, <a href="https://arxiv.org/ps/2512.23903">ps</a>, <a href="https://arxiv.org/format/2512.23903">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Scaling Remote Sensing Foundation Models: Data Domain Tradeoffs at the Peta-Scale
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Wickrema%2C+C">Charith Wickrema</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Mace%2C+E">Eliza Mace</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Brown%2C+H">Hunter Brown</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Cabrera%2C+H">Heidys Cabrera</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Krall%2C+N">Nick Krall</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=O%27Neill%2C+M">Matthew O&#39;Neill</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Sarkar%2C+S">Shivangi Sarkar</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Weissman%2C+L">Lowell Weissman</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Hughes%2C+E">Eric Hughes</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zarrella%2C+G">Guido Zarrella</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2512.23903v1-abstract-short" style="display: inline;">
        &hellip;techniques for training foundation models on high-resolution electro-optical (EO) datasets that exceed the current state-of-the-art scale by orders of magnitude. Modern <span class="search-hit mathjax">multimodal</span> machine learning (ML) applications, such as generative artificial intelligence (GenAI) systems for image captioning, search, and reasoning, depend on robust, domain-specialized enc&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.23903v1-abstract-full').style.display = 'inline'; document.getElementById('2512.23903v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2512.23903v1-abstract-full" style="display: none;">
        We explore the scaling behaviors of artificial intelligence to establish practical techniques for training foundation models on high-resolution electro-optical (EO) datasets that exceed the current state-of-the-art scale by orders of magnitude. Modern <span class="search-hit mathjax">multimodal</span> machine learning (ML) applications, such as generative artificial intelligence (GenAI) systems for image captioning, search, and reasoning, depend on robust, domain-specialized encoders for non-text modalities. In natural-image domains where internet-scale data is plentiful, well-established scaling laws help optimize the joint scaling of model capacity, training compute, and dataset size. Unfortunately, these relationships are much less well-understood in high-value domains like remote sensing (RS). Using over a quadrillion pixels of commercial satellite EO data and the MITRE Federal AI Sandbox, we train progressively larger vision <span class="search-hit mathjax">transformer</span> (ViT) backbones, report success and failure modes observed at petascale, and analyze implications for bridging domain gaps across additional RS modalities. We observe that even at this scale, performance is consistent with a data limited regime rather than a model parameter-limited one. These practical insights are intended to inform data-collection strategies, compute budgets, and optimization schedules that advance the future development of frontier-scale RS foundation models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.23903v1-abstract-full').style.display = 'none'; document.getElementById('2512.23903v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 December, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2512.23597">arXiv:2512.23597</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2512.23597">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Scalable Residual Feature Aggregation Framework with Hybrid Metaheuristic Optimization for Robust Early Pancreatic Neoplasm Detection in <span class="search-hit mathjax">Multimodal</span> CT Imaging
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Thiruvengadam%2C+J+A">Janani Annur Thiruvengadam</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Nabigaru%2C+K+M">Kiran Mayee Nabigaru</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Kovi%2C+A">Anusha Kovi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2512.23597v1-abstract-short" style="display: inline;">
        &hellip;be addressed with an effective and scalable system that can assist in enhancing the salience of the subtle visual cues and provide a high level of the generalization on the <span class="search-hit mathjax">multimodal</span> imaging data. A Scalable Residual Feature Aggregation (SRFA) framework is proposed to be used to meet these conditions in this study. The framework integrates a pipeline of pre&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.23597v1-abstract-full').style.display = 'inline'; document.getElementById('2512.23597v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2512.23597v1-abstract-full" style="display: none;">
        The early detection of pancreatic neoplasm is a major clinical dilemma, and it is predominantly so because tumors are likely to occur with minimal contrast margins and a large spread anatomy-wide variation amongst patients on a CT scan. These complexities require to be addressed with an effective and scalable system that can assist in enhancing the salience of the subtle visual cues and provide a high level of the generalization on the <span class="search-hit mathjax">multimodal</span> imaging data. A Scalable Residual Feature Aggregation (SRFA) framework is proposed to be used to meet these conditions in this study. The framework integrates a pipeline of preprocessing followed by the segmentation using the MAGRes-UNet that is effective in making the pancreatic structures and isolating regions of interest more visible. DenseNet-121 performed with residual feature storage is used to extract features to allow deep hierarchical features to be aggregated without properties loss. To go further, hybrid HHO-BA metaheuristic feature selection strategy is used, which guarantees the best feature subset refinement. To be classified, the system is trained based on a new hybrid model that integrates the ability to pay attention on the world, which is the Vision <span class="search-hit mathjax">Transformer</span> (ViT) with the high representational efficiency of EfficientNet-B3. A dual optimization mechanism incorporating SSA and GWO is used to fine-tune hyperparameters to enhance greater robustness and less overfitting. Experimental results support the significant improvement in performance, with the suggested model reaching 96.23% accuracy, 95.58% F1-score and 94.83% specificity, the model is significantly better than the traditional CNNs and contemporary <span class="search-hit mathjax">transformer</span>-based models. Such results highlight the possibility of the SRFA framework as a useful instrument in the early detection of pancreatic tumors.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.23597v1-abstract-full').style.display = 'none'; document.getElementById('2512.23597v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 December, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2512.23568">arXiv:2512.23568</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2512.23568">pdf</a>, <a href="https://arxiv.org/ps/2512.23568">ps</a>, <a href="https://arxiv.org/format/2512.23568">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ThinkGen: Generalized Thinking for Visual Generation
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Jiao%2C+S">Siyu Jiao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lin%2C+Y">Yiheng Lin</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhong%2C+Y">Yujie Zhong</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=She%2C+Q">Qi She</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhou%2C+W">Wei Zhou</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Lan%2C+X">Xiaohan Lan</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Huang%2C+Z">Zilong Huang</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+F">Fei Yu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Yu%2C+Y">Yingchen Yu</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+Y">Yunqing Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Zhao%2C+Y">Yao Zhao</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Wei%2C+Y">Yunchao Wei</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2512.23568v1-abstract-short" style="display: inline;">
        Recent progress in <span class="search-hit mathjax">Multimodal</span> Large Language Models (MLLMs) demonstrates that Chain-of-Thought (CoT) reasoning enables systematic solutions to complex understanding tasks. However, its extension to generation tasks remains nascent and limited by scenario-specific mechanisms that hinder generalization and adaptation. In this work, we present ThinkGen, the fir&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.23568v1-abstract-full').style.display = 'inline'; document.getElementById('2512.23568v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2512.23568v1-abstract-full" style="display: none;">
        Recent progress in <span class="search-hit mathjax">Multimodal</span> Large Language Models (MLLMs) demonstrates that Chain-of-Thought (CoT) reasoning enables systematic solutions to complex understanding tasks. However, its extension to generation tasks remains nascent and limited by scenario-specific mechanisms that hinder generalization and adaptation. In this work, we present ThinkGen, the first think-driven visual generation framework that explicitly leverages MLLM&#39;s CoT reasoning in various generation scenarios. ThinkGen employs a decoupled architecture comprising a pretrained MLLM and a Diffusion <span class="search-hit mathjax">Transformer</span> (DiT), wherein the MLLM generates tailored instructions based on user intent, and DiT produces high-quality images guided by these instructions. We further propose a separable GRPO-based training paradigm (SepGRPO), alternating reinforcement learning between the MLLM and DiT modules. This flexible design enables joint training across diverse datasets, facilitating effective CoT reasoning for a wide range of generative scenarios. Extensive experiments demonstrate that ThinkGen achieves robust, state-of-the-art performance across multiple generation benchmarks. Code is available: https://github.com/jiaosiyuu/ThinkGen
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.23568v1-abstract-full').style.display = 'none'; document.getElementById('2512.23568v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 December, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2512.23380">arXiv:2512.23380</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2512.23380">pdf</a>, <a href="https://arxiv.org/ps/2512.23380">ps</a>, <a href="https://arxiv.org/format/2512.23380">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Operating Systems">cs.OS</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1038/s41598-025-27693-4">10.1038/s41598-025-27693-4 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A unified framework for detecting point and collective anomalies in operating system logs via collaborative <span class="search-hit mathjax">transformers</span>
      
    </p>
    <p class="authors">
      <span class="has-text-black-bis has-text-weight-semibold">Authors:</span>
      
      <a href="/search/cs?searchtype=author&amp;query=Nasirzadeh%2C+M">Mohammad Nasirzadeh</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Tahmoresnezhad%2C+J">Jafar Tahmoresnezhad</a>, 
      
      <a href="/search/cs?searchtype=author&amp;query=Rashidi-Khazaee%2C+P">Parviz Rashidi-Khazaee</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="search-hit">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2512.23380v1-abstract-short" style="display: inline;">
        &hellip;in logs that can be considered log modalities. In light of this intuition, unimodal methods often struggle by ignoring the different modalities of log data. Meanwhile, <span class="search-hit mathjax">multimodal</span> methods fail to handle the interactions between these modalities. Applying&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.23380v1-abstract-full').style.display = 'inline'; document.getElementById('2512.23380v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2512.23380v1-abstract-full" style="display: none;">
        Log anomaly detection is crucial for preserving the security of operating systems. Depending on the source of log data collection, various information is recorded in logs that can be considered log modalities. In light of this intuition, unimodal methods often struggle by ignoring the different modalities of log data. Meanwhile, <span class="search-hit mathjax">multimodal</span> methods fail to handle the interactions between these modalities. Applying <span class="search-hit mathjax">multimodal</span> sentiment analysis to log anomaly detection, we propose CoLog, a framework that collaboratively encodes logs utilizing various modalities. CoLog utilizes collaborative <span class="search-hit mathjax">transformers</span> and multi-head impressed attention to learn interactions among several modalities, ensuring comprehensive anomaly detection. To handle the heterogeneity caused by these interactions, CoLog incorporates a modality adaptation layer, which adapts the representations from different log modalities. This methodology enables CoLog to learn nuanced patterns and dependencies within the data, enhancing its anomaly detection capabilities. Extensive experiments demonstrate CoLog&#39;s superiority over existing state-of-the-art methods. Furthermore, in detecting both point and collective anomalies, CoLog achieves a mean precision of 99.63%, a mean recall of 99.59%, and a mean F1 score of 99.61% across seven benchmark datasets for log-based anomaly detection. The comprehensive detection capabilities of CoLog make it highly suitable for cybersecurity, system monitoring, and operational efficiency. CoLog represents a significant advancement in log anomaly detection, providing a sophisticated and effective solution to point and collective anomaly detection through a unified framework and a solution to the complex challenges automatic log data analysis poses. We also provide the implementation of CoLog at https://github.com/NasirzadehMoh/CoLog.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2512.23380v1-abstract-full').style.display = 'none'; document.getElementById('2512.23380v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 December, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">72 pages, 19 figures, 19 tables, accepted in scientific reports on 5 November 2025</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Scientific Reports 15, 45698 (2025)
      </p>
    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                     
          
          <li>
            <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=50"
              class="pagination-link "
              aria-label="Page 2"
              aria-current="page">2
            </a>
          </li>
          
          <li>
            <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=100"
              class="pagination-link "
              aria-label="Page 3"
              aria-current="page">3
            </a>
          </li>
          
          <li>
            <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=150"
              class="pagination-link "
              aria-label="Page 4"
              aria-current="page">4
            </a>
          </li>
          
          <li>
            <a href="/search/?query=multimodal+transformer&amp;searchtype=all&amp;abstracts=show&amp;size=50&amp;start=200"
              class="pagination-link "
              aria-label="Page 5"
              aria-current="page">5
            </a>
          </li>
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  

  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  <script src="https://static.arxiv.org/static/base/1.0.0a5/js/member_acknowledgement.js"></script>
  </body>
</html>

<!-- ===== END SEARCH ===== -->

<!-- ===== ABS URL: https://arxiv.org/abs/2601.08457 | HTTP 200 ===== -->

<!DOCTYPE html>
<html lang="en">

<head>  <title>[2601.08457] An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="apple-touch-icon" sizes="180x180" href="/static/browse/0.3.4/images/icons/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/static/browse/0.3.4/images/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/static/browse/0.3.4/images/icons/favicon-16x16.png">
  <link rel="manifest" href="/static/browse/0.3.4/images/icons/site.webmanifest">
  <link rel="mask-icon" href="/static/browse/0.3.4/images/icons/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">
  <link rel="stylesheet" type="text/css" media="screen" href="/static/browse/0.3.4/css/arXiv.css?v=20241206" />
  <link rel="stylesheet" type="text/css" media="print" href="/static/browse/0.3.4/css/arXiv-print.css?v=20200611" />
  <link rel="stylesheet" type="text/css" media="screen" href="/static/browse/0.3.4/css/browse_search.css" />
  <script language="javascript" src="/static/browse/0.3.4/js/accordion.js" ></script>
  <script language="javascript" src="/static/browse/0.3.4/js/optin-modal.js?v=20250819"></script>
  
  <link rel="canonical" href="https://arxiv.org/abs/2601.08457"/>
  <meta name="description" content="Abstract page for arXiv paper 2601.08457: An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English"><meta property="og:type" content="website" />
<meta property="og:site_name" content="arXiv.org" />
<meta property="og:title" content="An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English" />
<meta property="og:url" content="https://arxiv.org/abs/2601.08457v1" />
<meta property="og:image" content="/static/browse/0.3.4/images/arxiv-logo-fb.png" />
<meta property="og:image:secure_url" content="/static/browse/0.3.4/images/arxiv-logo-fb.png" />
<meta property="og:image:width" content="1200" />
<meta property="og:image:height" content="700" />
<meta property="og:image:alt" content="arXiv logo"/>
<meta property="og:description" content="Digital platforms have an ever-expanding user base, and act as a hub for communication, business, and connectivity. However, this has also allowed for the spread of hate speech and misogyny. Artificial intelligence models have emerged as an effective solution for countering online hate speech but are under explored for low resource and code-mixed languages and suffer from a lack of interpretability. Explainable Artificial Intelligence (XAI) can enhance transparency in the decisions of deep learning models, which is crucial for a sensitive domain such as hate speech detection. In this paper, we present a multi-modal and explainable web application for detecting misogyny in text and memes in code-mixed Hindi and English. The system leverages state-of-the-art transformer-based models that support multilingual and multimodal settings. For text-based misogyny identification, the system utilizes XLM-RoBERTa (XLM-R) and multilingual Bidirectional Encoder Representations from Transformers (mBERT) on a dataset of approximately 4,193 comments. For multimodal misogyny identification from memes, the system utilizes mBERT + EfficientNet, and mBERT + ResNET trained on a dataset of approximately 4,218 memes. It also provides feature importance scores using explainability techniques including Shapley Additive Values (SHAP) and Local Interpretable Model Agnostic Explanations (LIME). The application aims to serve as a tool for both researchers and content moderators, to promote further research in the field, combat gender based digital violence, and ensure a safe digital space. The system has been evaluated using human evaluators who provided their responses on Chatbot Usability Questionnaire (CUQ) and User Experience Questionnaire (UEQ) to determine overall usability."/>
<meta name="twitter:site" content="@arxiv"/>
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="An Under-Explored Application for Explainable Multimodal Misogyny..."/>
<meta name="twitter:description" content="Digital platforms have an ever-expanding user base, and act as a hub for communication, business, and connectivity. However, this has also allowed for the spread of hate speech and misogyny...."/>
<meta name="twitter:image" content="https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png"/>
<meta name="twitter:image:alt" content="arXiv logo"/>
  <link rel="stylesheet" media="screen" type="text/css" href="/static/browse/0.3.4/css/tooltip.css"/><link rel="stylesheet" media="screen" type="text/css" href="https://static.arxiv.org/js/bibex-dev/bibex.css?20200709"/>  <script src="/static/browse/0.3.4/js/mathjaxToggle.min.js" type="text/javascript"></script>  <script src="//code.jquery.com/jquery-latest.min.js" type="text/javascript"></script>
  <script src="//cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js" type="text/javascript"></script>
  <script src="//cdn.jsdelivr.net/npm/dompurify@2.3.5/dist/purify.min.js"></script>
  <script src="/static/browse/0.3.4/js/toggle-labs.js?20241022" type="text/javascript"></script>
  <script src="/static/browse/0.3.4/js/cite.js" type="text/javascript"></script><meta name="citation_title" content="An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English" /><meta name="citation_author" content="Yadav, Sargam" /><meta name="citation_author" content="Kaushik, Abhishek" /><meta name="citation_author" content="Daid, Kevin Mc" /><meta name="citation_date" content="2026/01/13" /><meta name="citation_online_date" content="2026/01/13" /><meta name="citation_pdf_url" content="https://arxiv.org/pdf/2601.08457" /><meta name="citation_arxiv_id" content="2601.08457" /><meta name="citation_abstract" content="Digital platforms have an ever-expanding user base, and act as a hub for communication, business, and connectivity. However, this has also allowed for the spread of hate speech and misogyny. Artificial intelligence models have emerged as an effective solution for countering online hate speech but are under explored for low resource and code-mixed languages and suffer from a lack of interpretability. Explainable Artificial Intelligence (XAI) can enhance transparency in the decisions of deep learning models, which is crucial for a sensitive domain such as hate speech detection. In this paper, we present a multi-modal and explainable web application for detecting misogyny in text and memes in code-mixed Hindi and English. The system leverages state-of-the-art transformer-based models that support multilingual and multimodal settings. For text-based misogyny identification, the system utilizes XLM-RoBERTa (XLM-R) and multilingual Bidirectional Encoder Representations from Transformers (mBERT) on a dataset of approximately 4,193 comments. For multimodal misogyny identification from memes, the system utilizes mBERT + EfficientNet, and mBERT + ResNET trained on a dataset of approximately 4,218 memes. It also provides feature importance scores using explainability techniques including Shapley Additive Values (SHAP) and Local Interpretable Model Agnostic Explanations (LIME). The application aims to serve as a tool for both researchers and content moderators, to promote further research in the field, combat gender based digital violence, and ensure a safe digital space. The system has been evaluated using human evaluators who provided their responses on Chatbot Usability Questionnaire (CUQ) and User Experience Questionnaire (UEQ) to determine overall usability." />
</head>

<body  class="with-cu-identity">
  
  
  <div class="flex-wrap-footer">
    <header>
      <a href="#content" class="is-sr-only">Skip to main content</a>
      <!-- start desktop header -->
      <div class="columns is-vcentered is-hidden-mobile" id="cu-identity">
        <div class="column" id="cu-logo">
          <a href="https://www.cornell.edu/"><img src="/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" /></a>
        </div><!-- /from April 7 at 1:00 AM to May 29 at 21:40 --><!-- /from May 2 at 1:00 AM to May 5 at 9:45 AM --><div class="column" id="support-ack">
          <span id="support-ack-url">We gratefully acknowledge support from the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors.</span>
          <a href="https://info.arxiv.org/about/donate.html" class="btn-header-donate">Donate</a>
        </div>
      </div>

      <div id="header" class="is-hidden-mobile">
<a aria-hidden="true" tabindex="-1" href="/IgnoreMe"></a>
  <div class="header-breadcrumbs is-hidden-mobile">
    <a href="/"><img src="/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg" alt="arxiv logo" style="height:40px;"/></a> <span>&gt;</span> <a href="/list/cs/recent">cs</a> <span>&gt;</span> arXiv:2601.08457
  </div>

        <div class="columns is-vcentered is-mobile" style="justify-content: flex-end;">
        </div>

          <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
     </div><!-- /end desktop header -->

      <div class="mobile-header">
        <div class="columns is-mobile">
          <div class="column logo-arxiv"><a href="https://arxiv.org/"><img src="/static/browse/0.3.4/images/arxiv-logomark-small-white.svg" alt="arXiv logo" style="height:60px;" /></a></div>
          <div class="column logo-cornell"><a href="https://www.cornell.edu/">
            <picture>
              <source media="(min-width: 501px)"
                srcset="/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg  400w"
                sizes="400w" />
              <source srcset="/static/browse/0.3.4/images/icons/cu/cornell_seal_simple_black.svg 2x" />
              <img src="/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University Logo" />
            </picture>
          </a></div>
          <div class="column nav" id="toggle-container" role="menubar">
            <button class="toggle-control"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-white"><title>open search</title><path d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></button>
            <div class="mobile-toggle-block toggle-target">
              <form class="mobile-search-form" method="GET" action="https://arxiv.org/search">
                <div class="field has-addons">
                  <input class="input" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
                  <input type="hidden" name="source" value="header">
                  <input type="hidden" name="searchtype" value="all">
                  <button class="button">GO</button>
                </div>
              </form>
            </div>

            <button class="toggle-control"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-white" role="menu"><title>open navigation menu</title><path d="M16 132h416c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163 0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"/ ></svg></button>
            <div class="mobile-toggle-block toggle-target">
              <nav class="mobile-menu" aria-labelledby="mobilemenulabel">
                <h2 id="mobilemenulabel">quick links</h2>
                <ul>
                    <li><a href="https://arxiv.org/login">Login</a></li>
                    <li><a href="https://info.arxiv.org/help">Help Pages</a></li>
                    <li><a href="https://info.arxiv.org/about">About</a></li>
                </ul>
              </nav>
            </div>
          </div>
        </div>
      </div><!-- /end mobile-header -->
    </header>

    <main>
      <div id="content">
<!--
rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
         xmlns:dc="http://purl.org/dc/elements/1.1/"
         xmlns:trackback="http://madskills.com/public/xml/rss/module/trackback/">
    <rdf:Description
        rdf:about="/abs/2601.08457"
        dc:identifier="/abs/2601.08457"
        dc:title="An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English"
        trackback:ping="/trackback/2601.08457" />
    </rdf:RDF>
--><div id="abs-outer">

  <div class="leftcolumn">
    <div class="subheader">
      <h1>Computer Science > Artificial Intelligence</h1>
    </div>

    <div class="header-breadcrumbs-mobile">
      <strong>arXiv:2601.08457</strong> (cs)
    </div>
<link rel="stylesheet" type="text/css" href="/static/base/1.0.1/css/abs.css">
<div id="content-inner">
  <div id="abs">
    <div class="dateline">
  [Submitted on 13 Jan 2026]</div>
    <h1 class="title mathjax"><span class="descriptor">Title:</span>An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English</h1>
    <div class="authors"><span class="descriptor">Authors:</span><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yadav,+S" rel="nofollow">Sargam Yadav</a> (1), <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kaushik,+A" rel="nofollow">Abhishek Kaushik</a> (1), <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Daid,+K+M" rel="nofollow">Kevin Mc Daid</a> (1) ((1) Dundalk Institute of Technology)</div>            <div id="download-button-info" hidden>View a PDF of the paper titled An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English, by Sargam Yadav (1) and 2 other authors</div>
    <a class="mobile-submission-download" href="/pdf/2601.08457">View PDF</a>



    <blockquote class="abstract mathjax">
            <span class="descriptor">Abstract:</span>Digital platforms have an ever-expanding user base, and act as a hub for communication, business, and connectivity. However, this has also allowed for the spread of hate speech and misogyny. Artificial intelligence models have emerged as an effective solution for countering online hate speech but are under explored for low resource and code-mixed languages and suffer from a lack of interpretability. Explainable Artificial Intelligence (XAI) can enhance transparency in the decisions of deep learning models, which is crucial for a sensitive domain such as hate speech detection. In this paper, we present a multi-modal and explainable web application for detecting misogyny in text and memes in code-mixed Hindi and English. The system leverages state-of-the-art transformer-based models that support multilingual and multimodal settings. For text-based misogyny identification, the system utilizes XLM-RoBERTa (XLM-R) and multilingual Bidirectional Encoder Representations from Transformers (mBERT) on a dataset of approximately 4,193 comments. For multimodal misogyny identification from memes, the system utilizes mBERT + EfficientNet, and mBERT + ResNET trained on a dataset of approximately 4,218 memes. It also provides feature importance scores using explainability techniques including Shapley Additive Values (SHAP) and Local Interpretable Model Agnostic Explanations (LIME). The application aims to serve as a tool for both researchers and content moderators, to promote further research in the field, combat gender based digital violence, and ensure a safe digital space. The system has been evaluated using human evaluators who provided their responses on Chatbot Usability Questionnaire (CUQ) and User Experience Questionnaire (UEQ) to determine overall usability.
    </blockquote>

    <!--CONTEXT-->
    <div class="metatable">
      <table summary="Additional metadata"><tr>
          <td class="tablecell label">Subjects:</td>
          <td class="tablecell subjects">
            <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)</td>
        </tr><tr>
          <td class="tablecell label">Cite as:</td>
          <td class="tablecell arxivid"><span class="arxivid"><a href="https://arxiv.org/abs/2601.08457">arXiv:2601.08457</a> [cs.AI]</span></td>
        </tr>
        <tr>
          <td class="tablecell label">&nbsp;</td>
          <td class="tablecell arxividv">(or <span class="arxivid">
              <a href="https://arxiv.org/abs/2601.08457v1">arXiv:2601.08457v1</a> [cs.AI]</span> for this version)
          </td>
        </tr>
        <tr>
          <td class="tablecell label">&nbsp;</td>
          <td class="tablecell arxivdoi">              <a href="https://doi.org/10.48550/arXiv.2601.08457" id="arxiv-doi-link">https://doi.org/10.48550/arXiv.2601.08457</a><div class="button-and-tooltip">
              <button class="more-info" aria-describedby="more-info-desc-1">
                <svg height="15" role="presentation" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentColor" d="M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z" class=""></path></svg>
                <span class="visually-hidden">Focus to learn more</span>
              </button>
              <!-- tooltip description -->
              <div role="tooltip" id="more-info-desc-1">
                <span class="left-corner"></span>                  arXiv-issued DOI via DataCite (pending registration)</div>
            </div>
          </td>
        </tr></table>
    </div>
  </div>
</div>
    <div class="submission-history">
      <h2>Submission history</h2> From: Abhishek Kaushik Dr. [<a href="/show-email/5a561cb4/2601.08457" rel="nofollow">view email</a>]      <br/>    <strong>[v1]</strong>
        Tue, 13 Jan 2026 11:31:55 UTC (689 KB)<br/>
</div>
  </div>
  <!--end leftcolumn-->
<div class="extra-services">    <div class="full-text">
      <a name="other"></a>
      <span class="descriptor">Full-text links:</span>
      <h2>Access Paper:</h2>
      <ul>
  <div id="download-button-info" hidden>
View a PDF of the paper titled An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English, by Sargam Yadav (1) and 2 other authors</div><li><a href="/pdf/2601.08457" aria-describedby="download-button-info" accesskey="f" class="abs-button download-pdf">View PDF</a></li></ul>
      <div class="abs-license"><a href="http://creativecommons.org/licenses/by-nc-nd/4.0/" title="Rights to this article" class="has_license">
          <img alt="license icon" role="presentation" src="https://arxiv.org/icons/licenses/by-nc-nd-4.0.png"/>
          <span>view license</span>
        </a></div>
    </div>
    <!--end full-text-->    <div class="browse">
    Current browse context: <div class="current">cs.AI</div>

  <div class="prevnext">
<span class="arrow">
      <a class="abs-button prev-url" href="/prevnext?id=2601.08457&amp;function=prev&amp;context=cs.AI"
         accesskey="p" title="previous in cs.AI (accesskey p)" rel="nofollow">&lt;&nbsp;prev</a>
    </span>
    <span class="is-hidden-mobile">&nbsp; | &nbsp;</span>    <span class="arrow">
      <a class="abs-button next-url" href="/prevnext?id=2601.08457&amp;function=next&amp;context=cs.AI" accesskey="n"
         title="next in cs.AI (accesskey n)"  rel="nofollow">next&nbsp;&gt;</a>
    </span><br/>
  </div><div class="list">
    <a class="abs-button abs-button-grey abs-button-small context-new" href="/list/cs.AI/new"  rel="nofollow">new</a>
    <span class="is-hidden-mobile"> | </span>
    <a class="abs-button abs-button-grey abs-button-small context-recent" href="/list/cs.AI/recent" rel="nofollow">recent</a>
    <span class="is-hidden-mobile"> | </span><a class="abs-button abs-button-grey abs-button-small context-id" href="/list/cs.AI/2026-01" rel="nofollow">2026-01</a>
  </div><div class="abs-switch-cat">
    Change to browse by:
    <div class="switch context-change">
        <a href="/abs/2601.08457?context=cs" rel="nofollow">cs</a><br class="is-hidden-mobile">
        <a class="subclass" href="/abs/2601.08457?context=cs.CL" rel="nofollow">cs.CL</a><br class="is-hidden-mobile">
    </div>
  </div>

    </div>
      <div class="extra-ref-cite">
        <h3>References &amp; Citations</h3>
        <ul>
          <li><a  class="abs-button abs-button-small cite-ads" href="https://ui.adsabs.harvard.edu/abs/arXiv:2601.08457">NASA ADS</a></li><li><a  class="abs-button abs-button-small cite-google-scholar" href="https://scholar.google.com/scholar_lookup?arxiv_id=2601.08457" target="_blank" rel="noopener">Google Scholar</a></li>
          <li><a  class="abs-button abs-button-small cite-semantic-scholar" href="https://api.semanticscholar.org/arXiv:2601.08457" target="_blank" rel="noopener">Semantic Scholar</a></li>
        </ul>
        <div style="clear:both;"></div>
      </div>

<div class='extra-ref-cite'>
    <span id='bib-cite-trigger' class="bib-cite-button abs-button">export BibTeX citation</span>
    <span id='bib-cite-loading' hidden='true'>Loading...</span>
</div>

<div id='bib-cite-modal' class='bib-modal' hidden='true'>
    <div class='bib-modal-content'>
        <div class='bib-modal-title'>
            <h2>BibTeX formatted citation</h2>
            <span class='bib-modal-close' >&times;</span>
        </div>
        <div>
            <textarea id='bib-cite-target' class="bib-citation-content" aria-label="loading the citation">loading...</textarea>
        </div>
        <div>
            <span>Data provided by: </span>
            <a id='bib-cite-source-api'></a>
        </div>
    </div>
</div><div class="bookmarks">
  <div><h3>Bookmark</h3></div><a class="abs-button abs-button-grey abs-button-small" href="http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2601.08457&amp;description=An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English"
     title="Bookmark on BibSonomy">
    <img src="/static/browse/0.3.4/images/icons/social/bibsonomy.png"
         alt="BibSonomy logo"/>
  </a>
  <a class="abs-button abs-button-grey abs-button-small" href="https://reddit.com/submit?url=https://arxiv.org/abs/2601.08457&amp;title=An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English"
     title="Bookmark on Reddit">
    <img src="/static/browse/0.3.4/images/icons/social/reddit.png"
         alt="Reddit logo"/>
  </a>
</div>  </div>
  <!--end extra-services-->
<!-- LABS AREA -->
<div id="labstabs">
  <div class="labstabs"><input type="radio" name="tabs" id="tabone"checked="checked">
    <label for="tabone">Bibliographic Tools</label>
    <div class="tab labs-display-bib">
      <h1>Bibliographic and Citation Tools</h1>
      <div class="toggle">
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input id="bibex-toggle" type="checkbox" class="lab-toggle"
                     data-script-url="/static/browse/0.3.4/bibex/bibex.js?20241202">
              <span class="slider"></span>
              <span class="is-sr-only">Bibliographic Explorer Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-bibex">Bibliographic Explorer</span> <em>(<a href="https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer">What is the Explorer?</a>)</em>
          </div>
        </div>
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="connectedpapers-toggle"
                type="checkbox"
                class="lab-toggle"
                data-script-url="/static/browse/0.3.4/js/connectedpapers.js"
                aria-labelledby="label-for-connected-papers">
              <span class="slider"></span>
              <span class="is-sr-only">Connected Papers Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-connected-papers">Connected Papers</span> <em>(<a href="https://www.connectedpapers.com/about" target="_blank">What is Connected Papers?</a>)</em>
          </div>
        </div><div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="litmaps-toggle"
                type="checkbox"
                class="lab-toggle"
                data-script-url="/static/browse/0.3.4/js/litmaps.js?20210617"
                aria-labelledby="label-for-litmaps">
              <span class="slider"></span>
              <span class="is-sr-only">Litmaps Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-litmaps">Litmaps</span> <em>(<a href="https://www.litmaps.co/" target="_blank">What is Litmaps?</a>)</em>
          </div>
        </div>
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="scite-toggle"
                type="checkbox"
                class="lab-toggle"
                data-script-url="/static/browse/0.3.4/js/scite.js?20210617"
                aria-labelledby="label-for-scite">
              <span class="slider"></span>
              <span class="is-sr-only">scite.ai Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-scite">scite Smart Citations</span> <em>(<a href="https://www.scite.ai/" target="_blank">What are Smart Citations?</a>)</em>
          </div>
        </div>
      </div>
        <div class="labs-content-placeholder labs-display" style="display: none;"></div>
        <div style="min-height: 15px" id="connectedpapers-output"></div>
        <div style="min-height: 15px" id="litmaps-open-in"></div>
        <div style="min-height: 15px" id="scite-open-in"></div>
    </div>


    <input type="radio" name="tabs" id="tabtwo">
    <label for="tabtwo">Code, Data, Media</label>
    <div class="tab">
      <h1>Code, Data and Media Associated with this Article</h1>
      <div class="toggle">
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="alphaxiv-toggle"
                data-script-url="/static/browse/0.3.4/js/alphaxiv.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-alphaxiv">
              <span class="slider"></span>
              <span class="is-sr-only">alphaXiv Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-alphaxiv">alphaXiv</span> <em>(<a href="https://alphaxiv.org/" target="_blank">What is alphaXiv?</a>)</em>
          </div>
        </div>

        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input        
                id="catalyzex-toggle"
                data-script-url="/static/browse/0.3.4/js/catalyzex.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-cx">
              <span class="slider"></span>
              <span class="is-sr-only">Links to Code Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-cx">CatalyzeX Code Finder for Papers</span> <em>(<a href="https://www.catalyzex.com" target="_blank">What is CatalyzeX?</a>)</em>
          </div>
        </div>

        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="dagshub-toggle"
                data-script-url="/static/browse/0.3.4/js/dagshub.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-dagshub">
              <span class="slider"></span>
              <span class="is-sr-only">DagsHub Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-dagshub">DagsHub</span> <em>(<a href="https://dagshub.com/" target="_blank">What is DagsHub?</a>)</em>
          </div>
        </div>
  
        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="gotitpub-toggle"
                data-script-url="/static/browse/0.3.4/js/gotitpub.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-gotitpub">
              <span class="slider"></span>
              <span class="is-sr-only">GotitPub Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-gotitpub">Gotit.pub</span> <em>(<a href="http://gotit.pub/faq" target="_blank">What is GotitPub?</a>)</em>
          </div>
        </div>

        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="huggingface-toggle"
                data-script-url="/static/browse/0.3.4/js/huggingface.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-huggingface">
              <span class="slider"></span>
              <span class="is-sr-only">Huggingface Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-huggingface">Hugging Face</span> <em>(<a href="https://huggingface.co/huggingface" target="_blank">What is Huggingface?</a>)</em>
          </div>
        </div>

        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="paperwithcode-toggle"
                data-script-url="/static/browse/0.3.4/js/paperswithcode.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-pwc">
              <span class="slider"></span>
              <span class="is-sr-only">Links to Code Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-pwc">Papers with Code</span> <em>(<a href="https://paperswithcode.com/" target="_blank">What is Papers with Code?</a>)</em>
          </div>
        </div>


        <div class="columns is-mobile lab-row">
          <div class="column lab-switch">
            <label class="switch">
              <input
                id="sciencecast-toggle"
                data-script-url="/static/browse/0.3.4/js/sciencecast.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-sciencecast">
              <span class="slider"></span>
              <span class="is-sr-only">ScienceCast Toggle</span>
            </label>
          </div>
          <div class="column lab-name">
            <span id="label-for-sciencecast">ScienceCast</span> <em>(<a href="https://sciencecast.org/welcome" target="_blank">What is ScienceCast?</a>)</em>
          </div>
        </div>
      </div>

      <div id="alphaxiv-output" style="display:none"></div>
      <div id="catalyzex-output" style="display:none"></div>
      <div id="dagshub-output" style="display:none"></div>
      <div id="gotitpub-output" style="display:none"></div>
      <div id="pwc-output" style="display:none"></div>
      <div id="pwc-data-output" style="display:none"></div>
      <div id="sciencecast-output" style="display:none"></div>
      <div id="huggingface-output" style="display:none"></div>
    </div>


      <input type="radio" name="tabs" id="labstabs-demos-input">
      <label for="labstabs-demos-input" id="labstabs-demos-label">Demos</label>
      <div class="tab">
        <h1>Demos</h1>
        <div class="toggle">
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input
                  id="replicate-toggle"
                  data-script-url="/static/browse/0.3.4/js/replicate.js"
                  type="checkbox" class="lab-toggle" aria-labelledby="label-for-replicate">
                <span class="slider"></span>
                <span class="is-sr-only">Replicate Toggle</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-replicate">Replicate</span> <em>(<a href="https://replicate.com/docs/arxiv/about" target="_blank">What is Replicate?</a>)</em>
            </div>
          </div>
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input
                  id="spaces-toggle"
                  data-script-url="/static/browse/0.3.4/js/spaces.js"
                  type="checkbox" class="lab-toggle" aria-labelledby="label-for-spaces">
                <span class="slider"></span>
                <span class="is-sr-only">Spaces Toggle</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-spaces">Hugging Face Spaces</span> <em>(<a href="https://huggingface.co/docs/hub/spaces" target="_blank">What is Spaces?</a>)</em>
            </div>
          </div>
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input
                  id="txyz-toggle"
                  data-script-url="/static/browse/0.3.4/js/txyz.js"
                  type="checkbox" class="lab-toggle" aria-labelledby="label-for-txyz">
                <span class="slider"></span>
                <span class="is-sr-only">Spaces Toggle</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-txyz">TXYZ.AI</span> <em>(<a href="https://txyz.ai" target="_blank">What is TXYZ.AI?</a>)</em>
            </div>
          </div>
        </div>
        <div id="replicate-output"></div>
        <div id="spaces-output"></div>
        <div id="txyz-output"></div>
      </div>
      <input type="radio" name="tabs" id="tabfour">
      <label for="tabfour">Related Papers</label>
      <div class="tab">
        <h1>Recommenders and Search Tools</h1>
        <div class="toggle">
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input id="influenceflower-toggle"
                data-script-url="/static/browse/0.3.4/js/influenceflower.js"
                type="checkbox" class="lab-toggle" aria-labelledby="label-for-influenceflower">
                <span class="slider"></span>
                <span class="is-sr-only">Link to Influence Flower</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-influenceflower">Influence Flower</span> <em>(<a href="https://influencemap.cmlab.dev/" target="_blank">What are Influence Flowers?</a>)</em>
            </div>
          </div>
          <div class="columns is-mobile lab-row">
            <div class="column lab-switch">
              <label class="switch">
                <input id="core-recommender-toggle" type="checkbox" class="lab-toggle" aria-labelledby="label-for-core">
                <span class="slider"></span>
                <span class="is-sr-only">Core recommender toggle</span>
              </label>
            </div>
            <div class="column lab-name">
              <span id="label-for-core">CORE Recommender</span> <em>(<a href="https://core.ac.uk/services/recommender">What is CORE?</a>)</em>
            </div>
          </div></div>
        <div id="influenceflower-output"></div>
        <div id="influenceflower-output-graph" style="display:none">
          <ul class="flower-tabs">
            <li class="active"><a class="btn tab-btn" onclick="openTab(event, 'tab-author')">Author</a></li>
            <li><a class="btn tab-btn" onclick="openTab(event, 'tab-venue')">Venue</a></li>
            <li><a class="btn tab-btn" onclick="openTab(event, 'tab-inst')">Institution</a></li>
            <li><a class="btn tab-btn" onclick="openTab(event, 'tab-topic')">Topic</a></li>
          </ul>
          <div class="flower-tab-content">
            <div class="tab-flower active" id="tab-author"><svg id="flower-graph-author"></svg></div>
            <div class="tab-flower" id="tab-venue"><svg id="flower-graph-venue"></svg></div>
            <div class="tab-flower" id="tab-inst"><svg id="flower-graph-inst"></svg></div>
            <div class="tab-flower" id="tab-topic"><svg id="flower-graph-topic"></svg></div>
          </div>
        </div>
        <div id="coreRecommenderOutput"></div>
        <div id="iarxivOutput"></div>
      </div>

      <input type="radio" name="tabs" id="tabfive">
      <label for="tabfive">
        About arXivLabs
      </label>
      <div class="tab">
        <div class="columns">
          <div class="column">
            <h1>arXivLabs: experimental projects with community collaborators</h1>
            <p>arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.</p>
            <p>Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.</p>
            <p>Have an idea for a project that will add value for arXiv's community? <a href="https://info.arxiv.org/labs/index.html"><strong>Learn more about arXivLabs</strong></a>.</p>
          </div>
          <div class="column is-narrow is-full-mobile">
            <p class="icon-labs"><svg xmlns="http://www.w3.org/2000/svg" role="presentation" viewBox="0 0 635.572 811"><path d="M175.6 676v27h-27v-27zm-54 27v27h27v-27zm-27 27v27h27v-27zm396-54v27h-27v-27zm0 27v27h27v-27zm27 27v27h27v-27zm-27-414h27v27h-27zm27 0h27v-27h-27zm27-27h27v-27h-27zm-396 45h-27v-27h27zm-27-54h-27v27h27zm-27-27h-27v27h27z"/><path d="M94.6 730v27h-27v-27zm477 0v27h-27v-27zm-27-495h27v27h-27zm-450 18h-27v-27h27zm477 9h27v27h-27zm-54 495h27v27h-27zm-423 0h27v27h-27zm-54-504h27v27h-27z" fill="#666"/><path d="M67.6 730v27h-27v-27zm54 54v27h-27v-27zm0-108v27h27v-27zm-27 27v27h27v-27zm-81 0v27h27v-27zm585 27v27h-27v-27zm-108-54v27h27v-27zm27 27v27h27v-27zm81 0v27h27v-27zm-54-495h27v27h-27zm-54 108h27v-27h-27zm27-27h27v-27h-27zm0-81h27v-27h-27zm-423 18h-27v-27h27zm54 54h-27v27h27zm-27-27h-27v27h27zm0-81h-27v27h27zm423 612v27h-27v-27zm81-522v27h-27v-27zm-585-9v27h-27v-27z" fill="#999"/><path d="M94.6 784v27h-27v-27zm-27-27v27h27v-27zm-27-54v27h27v-27zm27 0v27h27v-27zm0-27v27h27v-27zm27 0v27h27v-27zm0-27v27h27v-27zm27 0v27h27v-27zm-108 81v27h27v-27zm558 54v27h-27v-27zm-27-27v27h27v-27zm27-54v27h27v-27zm-27 0v27h27v-27zm0-27v27h27v-27zm-27 0v27h27v-27zm0-27v27h27v-27zm-27 0v27h27v-27zm108 81v27h27v-27zm0-495h27v27h-27zm-27 27h27v-27h-27zm-54-27h27v-27h-27zm0 27h27v-27h-27zm-27 0h27v-27h-27zm0 27h27v-27h-27zm-27 0h27v-27h-27zm0 27h27v-27h-27zm81-108h27v-27h-27zm-504 45h-27v-27h27zm27-27h-27v27h27zm54-27h-27v27h27zm0 27h-27v27h27zm27 0h-27v27h27zm0 27h-27v27h27zm27 0h-27v27h27zm0 27h-27v27h27zm-81-108h-27v27h27z" fill="#ccc"/><path d="M598.6 665.1H41.5C-76.5 667 176 280.2 176 280.2h53a46.5 46.5 0 0162.8-56.3 29.2 29.2 0 1128.5 35.9h-1a46.5 46.5 0 01-1.5 20.3l142.5-.1s255.3 387 138.3 385.1zM291 181a29.3 29.3 0 10-29.2-29.3A29.3 29.3 0 00291 181zm65.4-66.8a22.4 22.4 0 10-22.5-22.4 22.4 22.4 0 0022.5 22.4z" fill="#fc0"/><path d="M245.5 172V10h153v162s324 495 198 495h-558c-126 0 207-495 207-495zm126 54h56m-13 72h56m-9 72h56m-20 72h56m-22 72h56m-29 72h56m-457-45c20.8 41.7 87.3 81 160.7 81 72.1 0 142.1-38.2 163.4-81" fill="none" stroke="#000" stroke-miterlimit="10" stroke-width="20"/><path d="M273.3 421.7c0 31-9.8 56.3-21.9 56.3s-21.8-25.2-21.8-56.3 9.8-56.3 21.8-56.3 21.9 25.2 21.9 56.3zm114.4-56.3c-12 0-21.8 25.2-21.8 56.3s9.7 56.3 21.8 56.3 21.9-25.2 21.9-56.3-9.8-56.3-21.9-56.3zM150.1 526.6c-18.2 6.7-27.5 22.9-23.2 30.2s14.8-5.5 33-12.2 37.4-4.9 33-12.2-24.5-12.6-42.8-5.8zm296 5.8c-4.2 7.3 14.9 5.5 33.1 12.2s28.7 19.5 33 12.2-5-23.5-23.2-30.2-38.5-1.5-42.8 5.8z"/></svg></p>
          </div>
        </div>
      </div>

    </div>
</div>
<!-- END LABS AREA -->
  <div class="endorsers">
    <a href="/auth/show-endorsers/2601.08457" class="endorser-who" rel="nofollow">Which authors of this paper are endorsers?</a> |
    <a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="https://info.arxiv.org/help/mathjax.html">What is MathJax?</a>)
    <span class="help" style="font-style: normal; float: right; margin-top: 0; margin-right: 1em;"></span>
  </div>
  <script type="text/javascript" language="javascript">mathjaxToggle();</script>
</div>
      </div>
    </main>

    <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://info.arxiv.org/about">About</a></li>
                <li><a href="https://info.arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
                <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
  </div>

  <script src="/static/base/1.0.1/js/member_acknowledgement.js"></script>

</body>

</html>

<!-- ===== END ABS ===== -->

<!-- ===== HTML URL: https://arxiv.org/html/2601.08457 | HTTP 404 ===== -->

<!DOCTYPE html>
<html lang="en">
  <head>
<meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="/static/base/1.0.1/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/static/base/1.0.1/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/static/base/1.0.1/images/icons/favicon-16x16.png">
<link rel="manifest" href="/static/base/1.0.1/images/icons/site.webmanifest">
<link rel="mask-icon" href="/static/base/1.0.1/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="/static/base/1.0.1/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title> | arXiv e-print repository</title>
<script defer src="/static/base/1.0.1/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="/static/base/1.0.1/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="/static/base/1.0.1/js/notification.js"></script>
  </head>
  <body>
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
<!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="/static/base/1.0.1/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="/static/base/1.0.1/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
    <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
<a href="https://arxiv.org/login">Login</a>    </div>
</div>  </header>
  <main class="container" id="main-container">
<h1>No HTML for '2601.08457'</h1>
<p>HTML is not available for the source.</p>
<p>This could be due to the source files not being HTML, LaTeX, or a conversion failure.</p>
<p>If you are an author, learn how you can help <a href="https://info.arxiv.org/about/accessibility_html_error_messages.html">HTML conversions for your papers</a>.</p>  </main>
  <footer>
<div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>  </footer>
  <script src="/static/base/1.0.1/js/member_acknowledgement.js"></script>
  </body>
</html>

<!-- ===== END HTML ===== -->
