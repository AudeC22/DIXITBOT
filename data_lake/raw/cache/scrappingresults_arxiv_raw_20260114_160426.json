{
  "ok": true,
  "query": "multimodal transformer",
  "sort": "relevance",
  "count": 2,
  "max_results": 2,
  "hit_limit_100": false,
  "message_if_limit": "",
  "items": [
    {
      "arxiv_id": "2601.08457",
      "title": "An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English",
      "authors": [
        "Sargam Yadav",
        "Abhishek Kaushik",
        "Kevin Mc Daid"
      ],
      "abstract": "Digital platforms have an ever-expanding user base, and act as a hub for communication, business, and connectivity. However, this has also allowed for the spread of hate speech and misogyny. Artificial intelligence models have emerged as an effective solution for countering online hate speech but are under explored for low resource and code-mixed languages and suffer from a lack of interpretability. Explainable Artificial Intelligence (XAI) can enhance transparency in the decisions of deep learning models, which is crucial for a sensitive domain such as hate speech detection. In this paper, we present a multi-modal and explainable web application for detecting misogyny in text and memes in code-mixed Hindi and English. The system leverages state-of-the-art transformer -based models that support multilingual and multimodal settings. For text-based misogyny identification, the system utilizes XLM-RoBERTa (XLM-R) and multilingual Bidirectional Encoder Representations from Transformers (mBERT) on a dataset of approximately 4,193 comments. For multimodal misogyny identification from memes, the system utilizes mBERT + EfficientNet, and mBERT + ResNET trained on a dataset of approximately 4,218 memes. It also provides feature importance scores using explainability techniques including Shapley Additive Values (SHAP) and Local Interpretable Model Agnostic Explanations (LIME). The application aims to serve as a tool for both researchers and content moderators, to promote further research in the field, combat gender based digital violence, and ensure a safe digital space. The system has been evaluated using human evaluators who provided their responses on Chatbot Usability Questionnaire (CUQ) and User Experience Questionnaire (UEQ) to determine overall usability.",
      "submitted_date": "13 January, 2026",
      "abs_url": "https://arxiv.org/abs/2601.08457",
      "pdf_url": "https://arxiv.org/pdf/2601.08457",
      "doi": "",
      "versions": [],
      "last_updated_raw": "",
      "html_url": "https://arxiv.org/html/2601.08457",
      "published_date": "",
      "license": "",
      "sections": [],
      "content_text": "",
      "references": [],
      "references_dois": [],
      "fallback_urls": [
        "https://arxiv.org/html/2601.08457"
      ],
      "errors": [
        "html_http_404"
      ],
      "missing_fields": [
        "doi",
        "versions",
        "last_updated_raw",
        "published_date",
        "license",
        "sections",
        "content_text",
        "references",
        "references_dois"
      ],
      "url_hint_if_missing": "Champs manquants: doi, versions, last_updated_raw, published_date, license, sections, content_text, references, references_dois. Tu peux verifier ici: abs=https://arxiv.org/abs/2601.08457 | html=https://arxiv.org/html/2601.08457 | pdf=https://arxiv.org/pdf/2601.08457"
    },
    {
      "arxiv_id": "2601.08179",
      "title": "Instruction-Driven 3D Facial Expression Generation and Transition",
      "authors": [
        "Anh H. Vo",
        "Tae-Seok Kim",
        "Hulin Jin",
        "Soo-Mi Choi",
        "Yong-Guk Kim"
      ],
      "abstract": "A 3D avatar typically has one of six cardinal facial expressions. To simulate realistic emotional variation, we should be able to render a facial transition between two arbitrary expressions. This study presents a new framework for instruction-driven facial expression generation that produces a 3D face and, starting from an image of the face, transforms the facial expression from one designated facial expression to another. The Instruction-driven Facial Expression Decomposer (IFED) module is introduced to facilitate multimodal data learning and capture the correlation between textual descriptions and facial expression features. Subsequently, we propose the Instruction to Facial Expression Transition (I2FET) method, which leverages IFED and a vertex reconstruction loss function to refine the semantic comprehension of latent vectors, thus generating a facial expression sequence according to the given instruction. Lastly, we present the Facial Expression Transition model to generate smooth transitions between facial expressions. Extensive evaluation suggests that the proposed model outperforms state-of-the-art methods on the CK+ and CelebV-HQ datasets. The results show that our framework can generate facial expression trajectories according to text instruction. Considering that text prompts allow us to make diverse descriptions of human emotional states, the repertoire of facial expressions and the transitions between them can be expanded greatly. We expect our framework to find various practical applications More information about our project can be found at https://vohoanganh.github.io/tg3dfet/",
      "submitted_date": "12 January, 2026",
      "abs_url": "https://arxiv.org/abs/2601.08179",
      "pdf_url": "https://arxiv.org/pdf/2601.08179",
      "doi": "https://doi.org/10.1109/TMM.2025.3565929",
      "versions": [],
      "last_updated_raw": "",
      "html_url": "https://arxiv.org/html/2601.08179v1",
      "published_date": "",
      "license": "",
      "sections": [
        {
          "section_index": 1,
          "heading_level": "h1",
          "heading": "Instruction-Driven 3D Facial Expression Generation and Transition",
          "text": "Anh H. Vo, Tae-Seok Kim, Hulin Jin, Soo-Mi Choi, and Yong-Guk Kim A H Vo, T-S Kim, S-M Choi, and Y-G Kim are with the Department of Computer Engineering, Sejong University, Seoul, Republic of Korea.H Jin is with the School of Computer Science and Technology, Anhui University, Hefei, China.* Corresponding Author: ykim@sejong.ac.kr. This is the authorâ€™s accepted manuscript. The final version is published in IEEE Transactions on Multimedia, 2025 Abstract A 3D avatar typically has one of six cardinal facial expressions. To simulate realistic emotional variation, we should be able to render a facial transition between two arbitrary expressions. This study presents a new framework for instruction-driven facial expression generation that produces a 3D face and, starting from an image of the face, transforms the facial expression from one designated facial expression to another. The Instruction-driven Facial Expression Decomposer (IFED) module is introduced to facilitate multimodal data learning and capture the correlation between textual descriptions and facial expression features. Subsequently, we propose the Instruction to Facial Expression Transition (I2FET) method, which leverages IFED and a vertex reconstruction loss function to refine the semantic comprehension of latent vectors, thus generating a facial expression sequence according to the given instruction. Lastly, we present the Facial Expression Transition model to generate smooth transitions between facial expressions. Extensive evaluation suggests that the proposed model outperforms state-of-the-art methods on the CK+ and CelebV-HQ datasets. The results show that our framework can generate facial expression trajectories according to text instruction. Considering that text prompts allow us to make diverse descriptions of human emotional states, the repertoire of facial expressions and the transitions between them can be expanded greatly. We expect our framework to find various practical applications. More information about our project can be found at https://vohoanganh.github.io/tg3dfet/ Index Terms: Instruction-Driven, Facial Expression and Transition, Controllable Avatar, CK+ and CelebV-HQ datasets. I Introduction Face and facial expression generation have attracted much attention in recent years due to their widespread application in many fields such as augmented reality/virtual reality, the gaming industry, movie production, or human-computer interfaces. Face generation can be divided into two categories depending on whether the face is two or three-dimensional (2D or 3D faces). For the 2D case, several methods [ 15 , 41 , 17 , 19 , 36 ] have been proposed to generate facial images using multimodal input, such as text, sketches, and segmentation. Previous studies [ 7 , 41 , 12 , 13 ] have attempted to solve facial expression generation in response to input provided as an emotion label or text prompts. However, these methods have limited ability to control the geometrical information of the output image and are effective only for specific applications. In the case of a 3D face, several studies [ 18 , 45 , 26 , 31 , 4 ] have demonstrated methods for generating 3D faces from 2D ones. The models generate the output image combining geometrical information from the source image, such as the pose, shape, and texture. Figure 1 : Illustration of our framework wherein it accepts a text instruction with a face image (source) as input and generates a 3D face from it. Then, it transforms the facial expression, from disgust (expression 1) to happiness (expression 2) via a neutral expression. Other researchers [ 16 , 1 ] generate a specific 3D facial expression based on text prompts. They utilize CLIP (Contrastive Language-Image Pretraining) or NeRF (Neural Radiance Field) to learn the latent code for manipulating facial expressions. Yet, NeRF requires substantial computational resources and time. For instance, [ 1 ] learns facial parameters and generates facial texture and expression based on text prompts. Although these methods can generate specific facial expressions, they do not address facial expression transitions based on instruction provided. Meanwhile, other studies [ 8 , 2 , 10 , 29 , 32 ] proposed to control facial expressions from an RGB video or generate portrait images [ 40 ] with controllable facial expression, head pose, and shoulder movements. Alternatively, [ 27 , 46 ] utilized the facial landmark trajectories to generate multiple 4D expression transitions on a meshed face. These studies led us to propose a new framework that enables a 3D face to be generated from a source image. Then, the facial expression undergoes transition in response to prompt text provided by the user to request a facial transition between two arbitrary expressions. Recent studies have used text-driven methods to generate a deformed face via NeRF or create an animated face via StyleGAN [ 13 , 16 ] . They generate sequence of facial expressions by specifying a certain emotion, such as â€™the person is happyâ€™. In contrast, our study utilizes a text instruction to control the variation in the facial expression, such as â€™Turn this face from disgust to happinessâ€™, as illustrated in Fig. 1 . Yet, understanding the target expressions specified by text instructions presents a challenge when relying solely on global text embedding. Indeed, even though CLIP is a notable method for the language-conditioning module in the text-to-image task [ 13 , 16 , 11 , 1 ] , it still faces challenges related to bias in understanding the text despite its large-scale training or the provision of global instruction, which may overlook fine-grained sub-instructions. Moreover, a text-only approach could be sensitive in generative models because changes in the structure of instructions can significantly impact their performance. The additional challenge lies therein to ensure that the facial motion described in the instruction is aligned with the semantics of the language. A straightforward approach to generating the facial motion would entail interpolating between the source code and the edited code from StyleClip [ 30 ] . Nevertheless, the additional challenge of controlling the latent code to adjust the head pose of the source image while generating the facial animation makes it difficult to achieve the intended goal. To make the connection between the text and facial expressions, we supplement a text instruction for making a transition between facial expressions into two standard datasets containing facial images, i.e., CelebV-HQ (High-Quality Celebrity Video) and CK+ (Extended Cohn-Kanade). To control the facial model in detail, FLAME (Faces Learned with an Articulated Model and Expressions) [ 22 ] is adopted and then fine-tuned with basic facial expressions and corresponding poses to capture the movement of the head, eyes, and mouth. Furthermore, integrating feature representations from different modalities minimizes ambiguity in redundant resources and effectively handles missing modalities, particularly in generative models, by improving inter-modality relations [ 37 , 38 ] . The proposed approach allows our model, i.e. Instruction to Facial Expression Transition (I2FET), to enhance the relationships between facial expression features and textual descriptions, thereby improving the latent representation for accurately generating target facial expressions and reducing the reliance solely on instructional text. Then, facial expression trajectories are created using the target facial expressions, which are generated by the facial parameters of the source image using I2FET. Using this framework, one can generate a 3D facial expression avatar from a photograph of a face and create any facial transition based on a text prompt instruction that contains the initial and final facial expressions. The main contributions of our study are given as follows: â€¢ We present an instruction-driven 3D facial expression generation and transition framework to generate transitions in facial expressions based on instructions, starting from a photograph of a face. â€¢ We propose an Instruction-Driven Facial Expression Decomposer (IFED), designed to learn multimodal data and capture the correlation between facial expression features and textual descriptions. â€¢ We propose an Instruction to Facial Expression Transition method (I2FET), which utilizes conditional variational autoencoders, integrated with IFED and a vertex reconstruction loss function, to enhance the semantic comprehension of the latent vectors. â€¢ Extensive evaluation of the proposed model on the CK+ and CelebV-HQ datasets suggests that it outperforms SOTA methods."
        },
        {
          "section_index": 2,
          "heading_level": "h6",
          "heading": "Abstract",
          "text": "A 3D avatar typically has one of six cardinal facial expressions. To simulate realistic emotional variation, we should be able to render a facial transition between two arbitrary expressions. This study presents a new framework for instruction-driven facial expression generation that produces a 3D face and, starting from an image of the face, transforms the facial expression from one designated facial expression to another. The Instruction-driven Facial Expression Decomposer (IFED) module is introduced to facilitate multimodal data learning and capture the correlation between textual descriptions and facial expression features. Subsequently, we propose the Instruction to Facial Expression Transition (I2FET) method, which leverages IFED and a vertex reconstruction loss function to refine the semantic comprehension of latent vectors, thus generating a facial expression sequence according to the given instruction. Lastly, we present the Facial Expression Transition model to generate smooth transitions between facial expressions. Extensive evaluation suggests that the proposed model outperforms state-of-the-art methods on the CK+ and CelebV-HQ datasets. The results show that our framework can generate facial expression trajectories according to text instruction. Considering that text prompts allow us to make diverse descriptions of human emotional states, the repertoire of facial expressions and the transitions between them can be expanded greatly. We expect our framework to find various practical applications. More information about our project can be found at https://vohoanganh.github.io/tg3dfet/"
        },
        {
          "section_index": 4,
          "heading_level": "h2",
          "heading": "I Introduction",
          "text": "Face and facial expression generation have attracted much attention in recent years due to their widespread application in many fields such as augmented reality/virtual reality, the gaming industry, movie production, or human-computer interfaces. Face generation can be divided into two categories depending on whether the face is two or three-dimensional (2D or 3D faces). For the 2D case, several methods [ 15 , 41 , 17 , 19 , 36 ] have been proposed to generate facial images using multimodal input, such as text, sketches, and segmentation. Previous studies [ 7 , 41 , 12 , 13 ] have attempted to solve facial expression generation in response to input provided as an emotion label or text prompts. However, these methods have limited ability to control the geometrical information of the output image and are effective only for specific applications. In the case of a 3D face, several studies [ 18 , 45 , 26 , 31 , 4 ] have demonstrated methods for generating 3D faces from 2D ones. The models generate the output image combining geometrical information from the source image, such as the pose, shape, and texture. Figure 1 : Illustration of our framework wherein it accepts a text instruction with a face image (source) as input and generates a 3D face from it. Then, it transforms the facial expression, from disgust (expression 1) to happiness (expression 2) via a neutral expression. Other researchers [ 16 , 1 ] generate a specific 3D facial expression based on text prompts. They utilize CLIP (Contrastive Language-Image Pretraining) or NeRF (Neural Radiance Field) to learn the latent code for manipulating facial expressions. Yet, NeRF requires substantial computational resources and time. For instance, [ 1 ] learns facial parameters and generates facial texture and expression based on text prompts. Although these methods can generate specific facial expressions, they do not address facial expression transitions based on instruction provided. Meanwhile, other studies [ 8 , 2 , 10 , 29 , 32 ] proposed to control facial expressions from an RGB video or generate portrait images [ 40 ] with controllable facial expression, head pose, and shoulder movements. Alternatively, [ 27 , 46 ] utilized the facial landmark trajectories to generate multiple 4D expression transitions on a meshed face. These studies led us to propose a new framework that enables a 3D face to be generated from a source image. Then, the facial expression undergoes transition in response to prompt text provided by the user to request a facial transition between two arbitrary expressions. Recent studies have used text-driven methods to generate a deformed face via NeRF or create an animated face via StyleGAN [ 13 , 16 ] . They generate sequence of facial expressions by specifying a certain emotion, such as â€™the person is happyâ€™. In contrast, our study utilizes a text instruction to control the variation in the facial expression, such as â€™Turn this face from disgust to happinessâ€™, as illustrated in Fig. 1 . Yet, understanding the target expressions specified by text instructions presents a challenge when relying solely on global text embedding. Indeed, even though CLIP is a notable method for the language-conditioning module in the text-to-image task [ 13 , 16 , 11 , 1 ] , it still faces challenges related to bias in understanding the text despite its large-scale training or the provision of global instruction, which may overlook fine-grained sub-instructions. Moreover, a text-only approach could be sensitive in generative models because changes in the structure of instructions can significantly impact their performance. The additional challenge lies therein to ensure that the facial motion described in the instruction is aligned with the semantics of the language. A straightforward approach to generating the facial motion would entail interpolating between the source code and the edited code from StyleClip [ 30 ] . Nevertheless, the additional challenge of controlling the latent code to adjust the head pose of the source image while generating the facial animation makes it difficult to achieve the intended goal. To make the connection between the text and facial expressions, we supplement a text instruction for making a transition between facial expressions into two standard datasets containing facial images, i.e., CelebV-HQ (High-Quality Celebrity Video) and CK+ (Extended Cohn-Kanade). To control the facial model in detail, FLAME (Faces Learned with an Articulated Model and Expressions) [ 22 ] is adopted and then fine-tuned with basic facial expressions and corresponding poses to capture the movement of the head, eyes, and mouth. Furthermore, integrating feature representations from different modalities minimizes ambiguity in redundant resources and effectively handles missing modalities, particularly in generative models, by improving inter-modality relations [ 37 , 38 ] . The proposed approach allows our model, i.e. Instruction to Facial Expression Transition (I2FET), to enhance the relationships between facial expression features and textual descriptions, thereby improving the latent representation for accurately generating target facial expressions and reducing the reliance solely on instructional text. Then, facial expression trajectories are created using the target facial expressions, which are generated by the facial parameters of the source image using I2FET. Using this framework, one can generate a 3D facial expression avatar from a photograph of a face and create any facial transition based on a text prompt instruction that contains the initial and final facial expressions. The main contributions of our study are given as follows: â€¢ We present an instruction-driven 3D facial expression generation and transition framework to generate transitions in facial expressions based on instructions, starting from a photograph of a face. â€¢ We propose an Instruction-Driven Facial Expression Decomposer (IFED), designed to learn multimodal data and capture the correlation between facial expression features and textual descriptions. â€¢ We propose an Instruction to Facial Expression Transition method (I2FET), which utilizes conditional variational autoencoders, integrated with IFED and a vertex reconstruction loss function, to enhance the semantic comprehension of the latent vectors. â€¢ Extensive evaluation of the proposed model on the CK+ and CelebV-HQ datasets suggests that it outperforms SOTA methods."
        },
        {
          "section_index": 5,
          "heading_level": "h2",
          "heading": "II Related Work",
          "text": "II-A Facial Expression Transition Generative Adversarial Networks (GANs) have been favored for generating facial expressions. For instance, Motion3DGAN [ 27 ] , an extension of MotionGAN [ 28 ] , addresses the dynamics of 3D landmarks, whereas WGAN learns the distribution of 3D expression dynamics across the hypersphere space, sampled with a condition to generate landmark sequences. In this case, a neutral 3D face mesh, frame by frame, is deformed by a mesh decoder using the generated landmark sequences although the disadvantage of this work is that the start and end expressions have to be provided to generate a concatenated expression transition. On the other hand, a diffusion model is adopted for facial expression generation [ 46 ] by modeling the input distribution. The deformation of the 3D mesh is guided by a specific landmark, and the original shape of the input facial mesh is considered to apply displacement to each vertex. In this work, MotionClip [ 39 ] , which was originally designed for human motion synthesis, is compared with the task of generating the expression transition conditions given by the text. EMOTE [ 6 ] utilizes emotion labels and various inputs to generate a talking-head avatar that maintains lip-sync from speech as well as facial expression. In particular, the facial motion prior, called FLINT (FLAME IN TIME), uses the expression and pose parameters of FLAME [ 22 ] to represent facial motion sequences. Although several studies have demonstrated methods to produce transitions between facial expressions under diverse conditions, the generated faces are typically conditioned on the front face by neglecting the head pose variations. Inspired by previous works [ 27 , 46 , 6 ] , the present study proposes a model for facial expression transition that learns the expression and pose parameters. The additional merit of our model would be that it can easily be plugged into the models that generate the facial appearance. II-B Face Rendering In the face rendering area, 3D Morphable Model (3DMM) has been widely used for rendering realistic faces [ 2 , 21 , 18 , 26 ] . Among these methods, VariTex [ 2 ] employs a neural texture obtained by a face texture decoder to sample with a UV map. Subsequently, this neural face feature image is merged with an additive feature image to serve as the input for a U-Net to generate the desired image. Exp-GAN [ 21 ] employs a neural volume generator and StyleGAN2 to generate images at various resolutions. This process combines different feature components, such as face features, depth images, and feature volumes. The facial feature is also obtained by sampling the neural texture using the UV map. However, this work is computationally intensive and time-consuming. ROME [ 18 ] can synthesize realistic face images by estimating a specific head mesh and neural texture from an input image, whereupon a neural rendering technique is adapted to render the rigged mesh. Next, the U-net-like neural rendering network, which accepts inputs such as neural texture and surface normals, is utilized to acquire both the target image and its corresponding mask. Similarly, CVTHead [ 26 ] also estimates a neural texture from an input image, and yet, in this case, the neural texture is flattened to a feature vector form before being passed to transformers along with vertex tokens to output a vertex descriptor. Specifically, each of the vertices is considered a query token, and a transformer is employed to learn the canonical vertex feature from the source image. To generate the desired image and its mask, the feature descriptor of each vertex and depth are projected into image space before being processed by a U-Net. In the current study, we employ ROME and CVTHead to generate facial appearances by leveraging DECA for the extraction of facial expression coefficients. This streamlines the integration into a facial expression transition model and enhances the applicability of the proposed framework to practical scenarios."
        },
        {
          "section_index": 6,
          "heading_level": "h3",
          "heading": "II-A Facial Expression Transition",
          "text": "Generative Adversarial Networks (GANs) have been favored for generating facial expressions. For instance, Motion3DGAN [ 27 ] , an extension of MotionGAN [ 28 ] , addresses the dynamics of 3D landmarks, whereas WGAN learns the distribution of 3D expression dynamics across the hypersphere space, sampled with a condition to generate landmark sequences. In this case, a neutral 3D face mesh, frame by frame, is deformed by a mesh decoder using the generated landmark sequences although the disadvantage of this work is that the start and end expressions have to be provided to generate a concatenated expression transition. On the other hand, a diffusion model is adopted for facial expression generation [ 46 ] by modeling the input distribution. The deformation of the 3D mesh is guided by a specific landmark, and the original shape of the input facial mesh is considered to apply displacement to each vertex. In this work, MotionClip [ 39 ] , which was originally designed for human motion synthesis, is compared with the task of generating the expression transition conditions given by the text. EMOTE [ 6 ] utilizes emotion labels and various inputs to generate a talking-head avatar that maintains lip-sync from speech as well as facial expression. In particular, the facial motion prior, called FLINT (FLAME IN TIME), uses the expression and pose parameters of FLAME [ 22 ] to represent facial motion sequences. Although several studies have demonstrated methods to produce transitions between facial expressions under diverse conditions, the generated faces are typically conditioned on the front face by neglecting the head pose variations. Inspired by previous works [ 27 , 46 , 6 ] , the present study proposes a model for facial expression transition that learns the expression and pose parameters. The additional merit of our model would be that it can easily be plugged into the models that generate the facial appearance."
        },
        {
          "section_index": 7,
          "heading_level": "h3",
          "heading": "II-B Face Rendering",
          "text": "In the face rendering area, 3D Morphable Model (3DMM) has been widely used for rendering realistic faces [ 2 , 21 , 18 , 26 ] . Among these methods, VariTex [ 2 ] employs a neural texture obtained by a face texture decoder to sample with a UV map. Subsequently, this neural face feature image is merged with an additive feature image to serve as the input for a U-Net to generate the desired image. Exp-GAN [ 21 ] employs a neural volume generator and StyleGAN2 to generate images at various resolutions. This process combines different feature components, such as face features, depth images, and feature volumes. The facial feature is also obtained by sampling the neural texture using the UV map. However, this work is computationally intensive and time-consuming. ROME [ 18 ] can synthesize realistic face images by estimating a specific head mesh and neural texture from an input image, whereupon a neural rendering technique is adapted to render the rigged mesh. Next, the U-net-like neural rendering network, which accepts inputs such as neural texture and surface normals, is utilized to acquire both the target image and its corresponding mask. Similarly, CVTHead [ 26 ] also estimates a neural texture from an input image, and yet, in this case, the neural texture is flattened to a feature vector form before being passed to transformers along with vertex tokens to output a vertex descriptor. Specifically, each of the vertices is considered a query token, and a transformer is employed to learn the canonical vertex feature from the source image. To generate the desired image and its mask, the feature descriptor of each vertex and depth are projected into image space before being processed by a U-Net. In the current study, we employ ROME and CVTHead to generate facial appearances by leveraging DECA for the extraction of facial expression coefficients. This streamlines the integration into a facial expression transition model and enhances the applicability of the proposed framework to practical scenarios."
        },
        {
          "section_index": 8,
          "heading_level": "h2",
          "heading": "III Method",
          "text": "III-A Problem Statement We aim to develop a model to generate 3D facial expression with two inputs: a face image I s I_{s} and a text instruction t t . The facial expression transition model ğ’¯ w â€‹ ( t , I s ) \\mathcal{T}_{w}(t,I_{s}) , having weights w w , is trained with a training set ğ’³ = { ( t , ( e 0 , Î¸ 0 ) , ( e 1 , Î¸ 1 ) ) ( j ) } j = 1 n \\mathcal{X}=\\{(t,(e_{0},\\theta_{0}),(e_{1},\\theta_{1}))^{(j)}\\}_{j=1}^{n} , where e 0 , e 1 âˆˆ R 1 Ã— 50 e_{0},e_{1}\\in R^{1\\times 50} denote expression vectors with 50 dimensions and Î¸ 0 , Î¸ 1 âˆˆ R 1 Ã— 6 \\theta_{0},\\theta_{1}\\in R^{1\\times 6} represent pose vectors with 6 dimensions corresponding to the first and next expressions in the facial expression sequence specified in instruction t t , respectively. n n is the number of samples in the training dataset. Our network learns ğ’¯ w ( . ) \\mathcal{T}_{w}(.) that approximates the data distribution p â€‹ ( ğ’³ ) p(\\mathcal{X}) and then generates a facial expression coefficient sequence { ( e 0 , Î¸ 0 ) , ( e 1 , Î¸ 1 ) } \\{(e_{0},\\theta_{0}),(e_{1},\\theta_{1})\\} with a given instruction t t . Next, a face rendering module ğ’¢ â€‹ ( I s , ğ’® ) \\mathcal{G}(I_{s},\\mathcal{S}) is used to render a sequence of facial appearances ğ’´ = { y s ( 1 ) , y ( i ) , y e 0 ( k ) , y ( j ) , y e 1 ( T ) } \\mathcal{Y}=\\{y_{s}^{(1)},y^{(i)},y_{e_{0}}^{(k)},y^{(j)},y_{e_{1}}^{(T)}\\} , with inputs, consisting of the texture of source image I s I_{s} and facial expression trajectories S = { s s ( 1 ) , s ( i ) , s e 0 ( k ) , s ( j ) , s e 1 ( T ) } S=\\{s_{s}^{(1)},s^{(i)},s_{e_{0}}^{(k)},s^{(j)},s_{e_{1}}^{(T)}\\} , where { s s ( 1 ) , s e 0 ( k ) , s e 1 ( T ) } \\{s_{s}^{(1)},s_{e_{0}}^{(k)},s_{e_{1}}^{(T)}\\} are anchor facial expression sequences corresponding to the source image, and specific facial expressions, generated by ğ’¯ w ( . ) \\mathcal{T}_{w}(.) . Then, facial expression sequences s ( i ) s^{(i)} and s ( j ) s^{(j)} transitioning between these anchor facial expressions can be inferred by a linear interpolation function to ensure the smoothness of the sequence of facial expressions. III-B The Proposed Method Figure 2 provides an overview of the proposed framework whereby one can control 3D facial expressions of the monocular image I s I_{s} following an input instruction t t . The framework consists of two major modules: (1) Facial Expression Transition (FET), ğ’¯ w â€‹ ( t , I s ) \\mathcal{T}_{w}(t,I_{s}) , for generating diverse expression trajectories; (2) Face Rendering (FR), ğ’¢ â€‹ ( I s , S ) \\mathcal{G}(I_{s},S) , for rendering facial appearances based on the texture of the source image I s I_{s} and the expression trajectories S S generated by the FET module. Figure 2 : Overview of the proposed framework consisting of two major modules: Facial Expression Transition (FET) and Face Rendering (FR). First, textual vector x t x_{t} is obtained from the pre-trained CLIP encoder for the FET module. Simultaneously, latent representations z p z_{p} and z e z_{e} are drawn from ğ’© â€‹ ( 0 , I ) \\mathcal{N}(0,I) . Then, the latent vectors are processed and concatenated to obtain x ^ f \\hat{x}^{f} . Afterward, the IFED module utilizes x t x^{t} and x ^ f \\hat{x}^{f} as inputs to create conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} , and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} for the pose and expression decoders. Subsequently, for smoothness of the facial expression sequence, the facial expression in the source image ( e s , Î¸ s ) (e_{s},\\theta_{s}) is subjected to linear interpolation and the specific facial expressions ( e 0 , Î¸ 0 ) , ( e 1 , Î¸ 1 ) (e_{0},\\theta_{0}),(e_{1},\\theta_{1}) , generated by the I2FET decoders. These sequences are then combined with the shape Ï• s \\phi_{s} and camera c s c_{s} parameters of the source image, obtained from DECA, to form facial expression trajectories { s s ( 1 ) , s ( i ) , s e 0 ( k ) , s ( j ) , s e 1 ( T ) } \\{s_{s}^{(1)},s^{(i)},s^{(k)}_{e_{0}},s^{(j)},s_{e_{1}}^{(T)}\\} . For FR, head mesh reconstruction produces a flame mesh sequence { m s ( 1 ) , m ( i ) , m e 0 ( k ) , m ( j ) , m e 1 ( T ) } \\{m_{s}^{(1)},m^{(i)},m^{(k)}_{e_{0}},m^{(j)},m_{e_{1}}^{(T)}\\} aligned with the facial expression trajectories and the texture encoder extracts the facial appearance from the source image. Finally, the rendering module generates the facial appearances { y s ( 1 ) , y ( i ) , y e 0 ( k ) , y ( j ) , y e 1 ( T ) } \\{y_{s}^{(1)},y^{(i)},y^{(k)}_{e_{0}},y^{(j)},y_{e_{1}}^{(T)}\\} within the facial expression trajectories using the flame mesh sequence and the extracted facial appearance. III-B 1 Facial Expression Transition This module is designed to generate diverse expression trajectories that correspond to the facial parameters of the source face image I s I_{s} and the parameters of specific facial expressions, described in an instruction t t . The module has two primary branches: the first receives an instruction to generate the corresponding sequence of facial expression based on instruction t t , whereas the second takes a 2D face image to encode its facial coefficients and create the facial expression trajectories. Figure 3 : Flow diagram of IFED (Instruction-Driven Facial Expression Decomposer). It consists of CAFT and the decomposition of parameters. The major function of CAFT is to introduce cross-attention between the facial parameters and text instruction. The two outputs from CAFT pass through layer normalization and are combined to create the fused feature vector x f â€‹ u â€‹ s â€‹ e â€‹ d x^{fused} . Then, pose projection function ğ’« p \\mathcal{P}_{p} and expression projection function ğ’« e \\mathcal{P}_{e} are used to decompose x f â€‹ u â€‹ s â€‹ e â€‹ d x^{fused} into two conditional vectors for facial expression ( x e â€‹ m â€‹ b e x_{emb}^{e} ) and pose ( x e â€‹ m â€‹ b p x_{emb}^{p} ). Figure 4 : Detailed architecture of the I2FET (Instruction to Facial Expression Transition) method. Given an instruction and a sequence of facial parameters, including pose and expression, the IFED module utilizes the facial parameters and textual description extracted by CLIP to produce the corresponding conditional feature vectors for pose x e â€‹ m â€‹ b p x_{emb}^{p} and expression x e â€‹ m â€‹ b e x_{emb}^{e} , respectively. The pose and expression encoders then map this sequence of facial parameters and conditional feature vectors to latent representations corresponding to pose and expression. These latent vectors, along with the textual description, are passed through the IFED module to generate the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} . Following this, the expression and pose decoders reconstruct expression e ^ \\hat{e} and pose Î¸ ^ \\hat{\\theta} parameters using these latent representations and the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} , respectively. Finally, the pre-trained FLAME decoder reconstructs the vertex coordinates v ^ \\hat{v} based on e ^ \\hat{e} and Î¸ ^ \\hat{\\theta} . In the former, the pre-trained CLIP encoder [ 33 ] extracts text description x t âˆˆ R 77 Ã— 768 x^{t}\\in R^{77\\times 768} according to the given instruction t t . Then, the I2FET model generates a sequence of expression and pose coefficients for the facial expression trajectories. Instruction-driven Facial Expression Decomposer : A dual-branch vision transformer [ 3 ] is adopted for extracting multi-scale features and fusing cross-attention information at different scales because it is efficient for the image classification task. Here, the Cross-Attention for Facial Expression Parameter and Text (CAFT) is used to learn the relationship between instruction and facial expression parameters by enhancing the understanding of multimodal data and refining the latent representation for the Instruction to Facial Expression Transition model. The proposed IFED module is illustrated in Fig. 3 . In particular, we directly incorporate facial expression parameters and textual descriptions into CAFT instead of adding class tokens as in [ 3 ] . With respect to the transformer encoders, the first branch learns the correlation between the facial expression parameters, while the second branch focuses on learning the textual descriptions. To begin with, each vector representing facial expression parameters comprises two elements: expression and pose parameters, denoted as x f = { e , Î¸ } âˆˆ R m Ã— 56 x^{f}=\\{e,\\theta\\}\\in R^{m\\times 56} , where m m denotes the number of expressions in a given instruction. As [ 24 , 6 ] , we only utilize jaw pose Î¸ j â€‹ a â€‹ w \\theta^{jaw} and expression e e to convey facial expression information. Therefore, we rewrite x f = { e , Î¸ j â€‹ a â€‹ w } âˆˆ R m Ã— 53 x^{f}=\\{e,\\theta^{jaw}\\}\\in R^{m\\times 53} as the input of the first branch. Then, the transformer encoder is utilized as y f = x f + M â€‹ S â€‹ A â€‹ ( L â€‹ N â€‹ ( x f ) ) \\displaystyle y^{f}=x^{f}+MSA(LN(x^{f})) (1) x ^ f = y f + F â€‹ F â€‹ N â€‹ ( L â€‹ N â€‹ ( y f ) ) \\displaystyle\\hat{x}^{f}=y^{f}+FFN(LN(y^{f})) where x ^ f \\hat{x}^{f} denotes the output of the transformer encoder in the facial expression parameters branch â„° P \\mathcal{E}_{P} . L N ( . ) LN(.) , M S A ( . ) MSA(.) , and F F N ( . ) FFN(.) are the layer norm, multi-head self-attention, and feed-forward network, respectively. Likewise, in the text branch, we replicate x t x^{t} to create a vector { x } i = { 0 , 1 } t \\{x\\}^{t}_{i=\\{0,1\\}} for computational purposes. Subsequently, the input for the transformer encoder is generated using a text projection function P t ( . ) P_{t}(.) on { x } i = { 0 , 1 } t \\{x\\}^{t}_{i=\\{0,1\\}} . As a result, the textual description x ^ t \\hat{x}^{t} is obtained through the transformer encoder â„° t \\mathcal{E}_{t} . Both branches are then fused by cross-attention to capture the relationship between the facial expression parameters and textual descriptions. The fusion process can be expressed as y c f = h f â€‹ 2 â€‹ t â€‹ ( x ^ 0 f ) , y c t = h t â€‹ 2 â€‹ f â€‹ ( x ^ 0 t ) , \\displaystyle y_{c}^{f}=h^{f2t}(\\hat{x}_{0}^{f}),\\quad y_{c}^{t}=h^{t2f}(\\hat{x}_{0}^{t}), (2) x o f = g t â€‹ 2 â€‹ f â€‹ ( [ C â€‹ A â€‹ ( y c f âŠ— x 1 t ) + y c f ] ) âŠ— x ^ 1 f \\displaystyle x_{o}^{f}=g^{t2f}([CA(y^{f}_{c}\\otimes x_{1}^{t})+y_{c}^{f}])\\otimes\\hat{x}_{1}^{f} x o t = g f â€‹ 2 â€‹ t â€‹ ( [ C â€‹ A â€‹ ( y c t âŠ— x 1 f ) + y c t ] ) âŠ— x ^ 1 t \\displaystyle x_{o}^{t}=g^{f2t}([CA(y_{c}^{t}\\otimes x_{1}^{f})+y_{c}^{t}])\\otimes\\hat{x}_{1}^{t} where h t â€‹ 2 â€‹ f , h f â€‹ 2 â€‹ t , g t â€‹ 2 â€‹ f h^{t2f},h^{f2t},g^{t2f} , and g f â€‹ 2 â€‹ t g^{f2t} are the projection function and back-projection function between the vectors that represent the text and facial expression parameters, x o f âˆˆ R m Ã— 53 x_{o}^{f}\\in R^{m\\times 53} and x o t âˆˆ R m Ã— 768 x_{o}^{t}\\in R^{m\\times 768} are the outputs of CAFT, C â€‹ A CA denotes cross-attention and âŠ— \\otimes denotes the concatenation operator. Next, the fused feature vector is obtained by concatenating x o f x_{o}^{f} and x o t x_{o}^{t} , after passing through L N ( . ) LN(.) , as follows: x f â€‹ u â€‹ s â€‹ e â€‹ d = L â€‹ N â€‹ ( x o t ) âŠ— L â€‹ N â€‹ ( x o f ) x^{fused}=LN(x_{o}^{t})\\otimes LN(x_{o}^{f}) (3) Finally, the projection functions for pose ğ’« p ( . ) \\mathcal{P}_{p}(.) and expression ğ’« e ( . ) \\mathcal{P}_{e}(.) are applied to the fused feature vector as follows: x e â€‹ m â€‹ b e = ğ’« e â€‹ ( x f â€‹ u â€‹ s â€‹ e â€‹ d ) \\displaystyle x^{e}_{emb}=\\mathcal{P}_{e}(x^{fused}) (4) x e â€‹ m â€‹ b p = ğ’« p â€‹ ( x f â€‹ u â€‹ s â€‹ e â€‹ d ) \\displaystyle x^{p}_{emb}=\\mathcal{P}_{p}(x^{fused}) where x e â€‹ m â€‹ b e âˆˆ R m Ã— 50 x^{e}_{emb}\\in R^{m\\times 50} and x e â€‹ m â€‹ b p âˆˆ R m Ã— 6 x^{p}_{emb}\\in R^{m\\times 6} are the conditional feature vectors for pose and expression, respectively. In this work, ğ’« p ( . ) \\mathcal{P}_{p}(.) and ğ’« e ( . ) \\mathcal{P}_{e}(.) are designed as linear transformation functions to map the feature x f â€‹ u â€‹ s â€‹ e â€‹ d x^{fused} to the corresponding pose and expression feature vectors. Instruction to Facial Expression Transition : The proposed I2FET method is based on a conditional variational autoencoder architecture. The method includes encoders â„° e t ( . ) \\mathcal{E}_{e}^{t}(.) and â„° p t ( . ) \\mathcal{E}_{p}^{t}(.) , as well as decoders ğ’Ÿ e t ( . ) \\mathcal{D}^{t}_{e}(.) and ğ’Ÿ p t ( . ) \\mathcal{D}^{t}_{p}(.) , which are defined using three Multi-Layer Perceptrons (MLPs). These components are conditioned on feature vectors that represent expression and pose information, respectively. An overview of the proposed I2FET architecture is presented in Fig. 4 . Here, the encoders are defined as follows: â„° e t â€‹ ( e âŠ— x e â€‹ m â€‹ b e ) âŸ¶ ( Î¼ e , Ïƒ e ) \\mathcal{E}_{e}^{t}(e\\otimes x^{e}_{emb})\\longrightarrow(\\mu_{e},\\sigma_{e}) (5) and â„° p t â€‹ ( Î¸ âŠ— x e â€‹ m â€‹ b p ) âŸ¶ ( Î¼ p , Ïƒ p ) \\mathcal{E}_{p}^{t}(\\theta\\otimes x^{p}_{emb})\\longrightarrow(\\mu_{p},\\sigma_{p}) (6) where âŠ— \\otimes denotes the concatenation operator and x e â€‹ m â€‹ b e x^{e}_{emb} and x e â€‹ m â€‹ b p x^{p}_{emb} are conditional feature vectors for expression and pose, respectively. For the encoder stage, x e â€‹ m â€‹ b e x^{e}_{emb} and x e â€‹ m â€‹ b p x^{p}_{emb} are created by textual description x t x_{t} and the facial expression parameters ( e , Î¸ ) (e,\\theta) . Î¼ e \\mu_{e} and Î¼ p \\mu_{p} denote the mean and standard deviation of the expression distribution, whereas Ïƒ e \\sigma_{e} , and Ïƒ p \\sigma_{p} are the mean and standard deviation of the pose distribution. In this study, the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}^{e}_{emb} and x ^ e â€‹ m â€‹ b p \\hat{x}^{p}_{emb} in the decoder are computed by textual description x t x_{t} and the latent vectors z ^ p \\hat{z}_{p} and z ^ e \\hat{z}_{e} representing pose and expression, respectively. These latent vectors are calculated as follows: z ~ p = Ïƒ p âˆ— z p + Î¼ p \\displaystyle\\tilde{z}_{p}=\\sigma_{p}*z_{p}+\\mu_{p} (7) z ^ p = ğ’¯ p â€‹ ( z ~ p ) \\displaystyle\\hat{z}_{p}=\\mathcal{T}_{p}(\\tilde{z}_{p}) z ~ e = Ïƒ e âˆ— z e + Î¼ e \\displaystyle\\tilde{z}_{e}=\\sigma_{e}*z_{e}+\\mu_{e} z ^ e = ğ’¯ e â€‹ ( z ~ e ) \\displaystyle\\hat{z}_{e}=\\mathcal{T}_{e}(\\tilde{z}_{e}) where z p z_{p} and z e z_{e} are the latent vectors sampled from ğ’© â€‹ ( 0 , I ) \\mathcal{N}(0,I) . In this case, we utilize linear transformation functions ğ’¯ p ( . ) \\mathcal{T}_{p}(.) and ğ’¯ e ( . ) \\mathcal{T}_{e}(.) to transform z ~ p \\tilde{z}_{p} and z ~ e âˆˆ R m Ã— 16 \\tilde{z}_{e}\\in R^{m\\times 16} into z ^ p âˆˆ R m Ã— 6 \\hat{z}_{p}\\in R^{m\\times 6} and z ^ e âˆˆ R m Ã— 50 \\hat{z}_{e}\\in R^{m\\times 50} , respectively, similar to the expression and pose dimensions. Thus, the latent vectors z ~ e \\tilde{z}_{e} and z ~ p \\tilde{z}_{p} can be used as input for the IFED module. Then, the latent vectors z ^ e \\hat{z}_{e} and z ^ p \\hat{z}_{p} are concatenated together, and along with the text vector x t x^{t} , they are passed through the IFED module to obtain the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} as x ^ e â€‹ m â€‹ b e , x ^ e â€‹ m â€‹ b p = I â€‹ F â€‹ E â€‹ D â€‹ ( z ^ p âŠ— z ^ e , x t ) \\displaystyle\\hat{x}_{emb}^{e},\\hat{x}_{emb}^{p}=IFED(\\hat{z}_{p}\\otimes\\hat{z}_{e},x^{t}) (8) This approach enhances the semantic representation of latent vectors by refining the conditional feature vectors for decoders based on the learned latent vectors throughout the training process. The decoders are defined as follows ğ’Ÿ p t â€‹ ( z ~ p âŠ— x ^ e â€‹ m â€‹ b p ) âŸ¶ ( Î¸ ^ 0 , Î¸ ^ 1 ) \\mathcal{D}^{t}_{p}(\\tilde{z}_{p}\\otimes\\hat{x}^{p}_{emb})\\longrightarrow(\\hat{\\theta}_{0},\\hat{\\theta}_{1}) (9) and ğ’Ÿ e t â€‹ ( z ~ e âŠ— x ^ e â€‹ m â€‹ b e ) âŸ¶ ( e ^ 0 , e ^ 1 ) \\mathcal{D}^{t}_{e}(\\tilde{z}_{e}\\otimes\\hat{x}^{e}_{emb})\\longrightarrow(\\hat{e}_{0},\\hat{e}_{1}) (10) The parameters of the I2FET model are optimized by minimizing the reconstruction loss function â„’ M â€‹ S â€‹ E ( . , . ) \\mathcal{L}_{MSE}(.,.) between the ground truth coefficients e e , Î¸ \\theta , and the predicted coefficients e ^ , Î¸ ^ \\hat{e},\\hat{\\theta} . In addition, the Kullback-Leibler divergence loss function is utilized to minimize the distance between the probability distribution of the ground truth and that of the predicted values. â„’ e = â„’ M â€‹ S â€‹ E â€‹ ( e , e ^ ) \\displaystyle\\mathcal{L}_{e}=\\mathcal{L}_{MSE}(e,\\hat{e}) (11) + 0.5 â€‹ [ âˆ’ âˆ‘ i ( l â€‹ o â€‹ g â€‹ Ïƒ i 2 + 1 ) + âˆ‘ i Ïƒ i 2 + âˆ‘ i Î¼ i 2 ] \\displaystyle+5\\left[-\\sum_{i}(log\\>\\sigma_{i}^{2}+1)+\\sum_{i}\\sigma^{2}_{i}+\\sum_{i}\\mu_{i}^{2}\\right] â„’ p = â„’ M â€‹ S â€‹ E â€‹ ( Î¸ , Î¸ ^ ) \\displaystyle\\mathcal{L}_{p}=\\mathcal{L}_{MSE}(\\theta,\\hat{\\theta}) (12) + 0.5 â€‹ [ âˆ’ âˆ‘ i ( l â€‹ o â€‹ g â€‹ Ïƒ i 2 + 1 ) + âˆ‘ i Ïƒ i 2 + âˆ‘ i Î¼ i 2 ] \\displaystyle+5\\left[-\\sum_{i}(log\\>\\sigma_{i}^{2}+1)+\\sum_{i}\\sigma^{2}_{i}+\\sum_{i}\\mu_{i}^{2}\\right] where â„’ e \\mathcal{L}_{e} and â„’ p \\mathcal{L}_{p} denote the expression and pose loss function, respectively. Furthermore, we utilize the pretrained FLAME [ 22 ] , a parametric model of a 3D head that is integrated into DECA [ 9 ] , which has N = 5023 N=5023 base vertices V b âˆˆ â„› N Ã— 3 V_{b}\\in\\mathcal{R}^{N\\times 3} and two sets of M M and K K basis vectors that encode shape blendshapes ğ’® âˆˆ R N Ã— 3 Ã— M \\mathcal{S}\\in R^{N\\times 3\\times M} , and expression blendshapes B âˆˆ R N Ã— 3 Ã— K B\\in R^{N\\times 3\\times K} . The basic vectors are first blended and then Linear Blend Skinning (LBS) ğ’² ( . ) \\mathcal{W}(.) is applied to rotate the vertices following the pose Î¸ \\theta . The final reconstruction in world coordinates can be computed as follows ğ’± â€‹ ( Ï• , e , Î¸ ) = ğ’² â€‹ ( V b + ğ’® â€‹ Ï• + â„¬ â€‹ e , Î¸ ) \\mathcal{V}(\\phi,e,\\theta)=\\mathcal{W}(V_{b}+\\mathcal{S}\\phi+\\mathcal{B}e,\\theta) (13) where Ï• \\phi , e e , and Î¸ \\theta denote the identity shape, facial expression, and head pose parameters, respectively. In this case, FLAME is used to reconstruct vertices to improve the performance of I2FET. â„’ v = â„’ M â€‹ S â€‹ E â€‹ ( v , v ^ ) \\mathcal{L}_{v}=\\mathcal{L}_{MSE}(v,\\hat{v}) (14) where the vertex coordinates v v and v ^ \\hat{v} are computed by feeding the ground truth { Î¸ , e , Ï• } \\{\\theta,e,\\phi\\} and the reconstructed { Î¸ ^ , e ^ , Ï• } \\{\\hat{\\theta},\\hat{e},\\phi\\} through FLAME. Finally, the total loss function is given as â„’ t â€‹ o â€‹ t â€‹ a â€‹ l = â„’ e + â„’ p + â„’ v \\mathcal{L}_{total}=\\mathcal{L}_{e}+\\mathcal{L}_{p}+\\mathcal{L}_{v} (15) For the second branch, the pre-trained DECA [ 9 ] is used to encode the facial coefficients of the source face I s I_{s} including shape Ï• s âˆˆ R 1 Ã— 100 \\phi_{s}\\in R^{1\\times 100} , expression e s âˆˆ R 1 Ã— 50 e_{s}\\in R^{1\\times 50} , pose Î¸ s âˆˆ R 1 Ã— 6 \\theta_{s}\\in R^{1\\times 6} and camera c s âˆˆ R 1 Ã— 3 c_{s}\\in R^{1\\times 3} . Following this step, { Ï• s , e s , Î¸ s , c s } \\{\\phi_{s},e_{s},\\theta_{s},c_{s}\\} is integrated with the sequence of facial expression coefficients { ( e 0 , Î¸ 0 ) , ( e 1 , Î¸ 1 ) } \\{(e_{0},\\theta_{0}),(e_{1},\\theta_{1})\\} , generated by the first branch to generate the anchor facial expressions { s s ( 1 ) , s e 0 ( k ) , s e 1 ( T ) } \\{s^{(1)}_{s},s^{(k)}_{e_{0}},s^{(T)}_{e_{1}}\\} . Then, a linear interpolation function Î¨ ( . ) \\Psi(.) is applied to smoothen the transition between the two anchored facial expressions. The Î¨ â€‹ ( s ( l ) , s ( n ) ) \\Psi(s^{(l)},s^{(n)}) function for interpolation between facial expression sequences s ( l ) s^{(l)} and s ( n ) s^{(n)} can be defined as e ( k ) = Î´ âˆ— e ( l ) + ( 1 âˆ’ Î´ ) âˆ— e ( n ) \\displaystyle e^{(k)}=\\delta*e^{(l)}+(1-\\delta)*e^{(n)} (16) Î¸ ( k ) = Î´ âˆ— Î¸ ( l ) + ( 1 âˆ’ Î´ ) âˆ— Î¸ ( n ) \\displaystyle\\theta^{(k)}=\\delta*\\theta^{(l)}+(1-\\delta)*\\theta^{(n)} where 0 â‰¤ Î´ â‰¤ 1 0\\leq\\delta\\leq 1 is a linear coefficient and { e ( l ) , Î¸ ( l ) } âˆˆ s ( l ) \\{e^{(l)},\\theta^{(l)}\\}\\in s^{(l)} . This allows our framework to ensure temporal consistency when the motion changes are sampled uniformly. III-B 2 Face Rendering This module renders the facial appearance for the facial expression trajectories, generated by FET. In this work, the face rendering approach is based on the facial parameters of FLAME for integration with FET. Head Mesh Reconstruction: It generates the vertex coordinates corresponding to the facial expressions using Eq. 13 by using the sequence of facial expression transitions as the input. Then, the target vertex coordinates are obtained by combining these expressions with the deformation of hair and shoulder regions f H ( . ) f_{H}(.) from I s I_{s} as follows â„³ t = ğ’± â€‹ ( Ï• , e , Î¸ ) + f H â€‹ ( I s ) \\mathcal{M}^{t}=\\mathcal{V}(\\phi,e,\\theta)+f_{H}(I_{s}) (17) where f H ( . ) f_{H}(.) is the pre-trained linear deformation model [ 18 ] that is used to refine the vertices locations. Rendering : Following the head mesh reconstruction step, facial appearances can be rendered from the reconstructed head vertices â„³ = { m s ( 1 ) , m ( i ) , m e 0 ( k ) , m ( j ) , m e 1 ( T ) } \\mathcal{M}=\\{m_{s}^{(1)},m^{(i)},m_{e_{0}}^{(k)},m^{(j)},m_{e_{1}}^{(T)}\\} and the extracted neutral texture of the source image I s I_{s} by the texture decoder â„° t â€‹ e â€‹ x â€‹ ( I s ) \\mathcal{E}_{tex}(I_{s}) . The present work explores a face-rendering model suitable for facial expression transition. Therefore, we utilize pre-trained facial rendering models, known as state-of-the-art models [ 18 , 26 ] , capable of rendering facial appearances based on the facial parameters calculated with FLAME. Subsequently, the sequence of facial appearances ğ’´ = { y s ( 1 ) , y ( i ) , y e 0 ( k ) , y ( j ) , y e 1 ( T ) } \\mathcal{Y}=\\{y_{s}^{(1)},y^{(i)},y_{e_{0}}^{(k)},y^{(j)},y_{e_{1}}^{(T)}\\} is generated as the output of the proposed framework. (a) (b) (c) Figure 5 : Statistics of the facial expressions utilized in this study for the CK+ (a) and CelebV-HQ (b) datasets, respectively, and a collection of sampled words extracted from the text instructions (c)."
        },
        {
          "section_index": 9,
          "heading_level": "h3",
          "heading": "III-A Problem Statement",
          "text": "We aim to develop a model to generate 3D facial expression with two inputs: a face image I s I_{s} and a text instruction t t . The facial expression transition model ğ’¯ w â€‹ ( t , I s ) \\mathcal{T}_{w}(t,I_{s}) , having weights w w , is trained with a training set ğ’³ = { ( t , ( e 0 , Î¸ 0 ) , ( e 1 , Î¸ 1 ) ) ( j ) } j = 1 n \\mathcal{X}=\\{(t,(e_{0},\\theta_{0}),(e_{1},\\theta_{1}))^{(j)}\\}_{j=1}^{n} , where e 0 , e 1 âˆˆ R 1 Ã— 50 e_{0},e_{1}\\in R^{1\\times 50} denote expression vectors with 50 dimensions and Î¸ 0 , Î¸ 1 âˆˆ R 1 Ã— 6 \\theta_{0},\\theta_{1}\\in R^{1\\times 6} represent pose vectors with 6 dimensions corresponding to the first and next expressions in the facial expression sequence specified in instruction t t , respectively. n n is the number of samples in the training dataset. Our network learns ğ’¯ w ( . ) \\mathcal{T}_{w}(.) that approximates the data distribution p â€‹ ( ğ’³ ) p(\\mathcal{X}) and then generates a facial expression coefficient sequence { ( e 0 , Î¸ 0 ) , ( e 1 , Î¸ 1 ) } \\{(e_{0},\\theta_{0}),(e_{1},\\theta_{1})\\} with a given instruction t t . Next, a face rendering module ğ’¢ â€‹ ( I s , ğ’® ) \\mathcal{G}(I_{s},\\mathcal{S}) is used to render a sequence of facial appearances ğ’´ = { y s ( 1 ) , y ( i ) , y e 0 ( k ) , y ( j ) , y e 1 ( T ) } \\mathcal{Y}=\\{y_{s}^{(1)},y^{(i)},y_{e_{0}}^{(k)},y^{(j)},y_{e_{1}}^{(T)}\\} , with inputs, consisting of the texture of source image I s I_{s} and facial expression trajectories S = { s s ( 1 ) , s ( i ) , s e 0 ( k ) , s ( j ) , s e 1 ( T ) } S=\\{s_{s}^{(1)},s^{(i)},s_{e_{0}}^{(k)},s^{(j)},s_{e_{1}}^{(T)}\\} , where { s s ( 1 ) , s e 0 ( k ) , s e 1 ( T ) } \\{s_{s}^{(1)},s_{e_{0}}^{(k)},s_{e_{1}}^{(T)}\\} are anchor facial expression sequences corresponding to the source image, and specific facial expressions, generated by ğ’¯ w ( . ) \\mathcal{T}_{w}(.) . Then, facial expression sequences s ( i ) s^{(i)} and s ( j ) s^{(j)} transitioning between these anchor facial expressions can be inferred by a linear interpolation function to ensure the smoothness of the sequence of facial expressions."
        },
        {
          "section_index": 10,
          "heading_level": "h3",
          "heading": "III-B The Proposed Method",
          "text": "Figure 2 provides an overview of the proposed framework whereby one can control 3D facial expressions of the monocular image I s I_{s} following an input instruction t t . The framework consists of two major modules: (1) Facial Expression Transition (FET), ğ’¯ w â€‹ ( t , I s ) \\mathcal{T}_{w}(t,I_{s}) , for generating diverse expression trajectories; (2) Face Rendering (FR), ğ’¢ â€‹ ( I s , S ) \\mathcal{G}(I_{s},S) , for rendering facial appearances based on the texture of the source image I s I_{s} and the expression trajectories S S generated by the FET module. Figure 2 : Overview of the proposed framework consisting of two major modules: Facial Expression Transition (FET) and Face Rendering (FR). First, textual vector x t x_{t} is obtained from the pre-trained CLIP encoder for the FET module. Simultaneously, latent representations z p z_{p} and z e z_{e} are drawn from ğ’© â€‹ ( 0 , I ) \\mathcal{N}(0,I) . Then, the latent vectors are processed and concatenated to obtain x ^ f \\hat{x}^{f} . Afterward, the IFED module utilizes x t x^{t} and x ^ f \\hat{x}^{f} as inputs to create conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} , and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} for the pose and expression decoders. Subsequently, for smoothness of the facial expression sequence, the facial expression in the source image ( e s , Î¸ s ) (e_{s},\\theta_{s}) is subjected to linear interpolation and the specific facial expressions ( e 0 , Î¸ 0 ) , ( e 1 , Î¸ 1 ) (e_{0},\\theta_{0}),(e_{1},\\theta_{1}) , generated by the I2FET decoders. These sequences are then combined with the shape Ï• s \\phi_{s} and camera c s c_{s} parameters of the source image, obtained from DECA, to form facial expression trajectories { s s ( 1 ) , s ( i ) , s e 0 ( k ) , s ( j ) , s e 1 ( T ) } \\{s_{s}^{(1)},s^{(i)},s^{(k)}_{e_{0}},s^{(j)},s_{e_{1}}^{(T)}\\} . For FR, head mesh reconstruction produces a flame mesh sequence { m s ( 1 ) , m ( i ) , m e 0 ( k ) , m ( j ) , m e 1 ( T ) } \\{m_{s}^{(1)},m^{(i)},m^{(k)}_{e_{0}},m^{(j)},m_{e_{1}}^{(T)}\\} aligned with the facial expression trajectories and the texture encoder extracts the facial appearance from the source image. Finally, the rendering module generates the facial appearances { y s ( 1 ) , y ( i ) , y e 0 ( k ) , y ( j ) , y e 1 ( T ) } \\{y_{s}^{(1)},y^{(i)},y^{(k)}_{e_{0}},y^{(j)},y_{e_{1}}^{(T)}\\} within the facial expression trajectories using the flame mesh sequence and the extracted facial appearance. III-B 1 Facial Expression Transition This module is designed to generate diverse expression trajectories that correspond to the facial parameters of the source face image I s I_{s} and the parameters of specific facial expressions, described in an instruction t t . The module has two primary branches: the first receives an instruction to generate the corresponding sequence of facial expression based on instruction t t , whereas the second takes a 2D face image to encode its facial coefficients and create the facial expression trajectories. Figure 3 : Flow diagram of IFED (Instruction-Driven Facial Expression Decomposer). It consists of CAFT and the decomposition of parameters. The major function of CAFT is to introduce cross-attention between the facial parameters and text instruction. The two outputs from CAFT pass through layer normalization and are combined to create the fused feature vector x f â€‹ u â€‹ s â€‹ e â€‹ d x^{fused} . Then, pose projection function ğ’« p \\mathcal{P}_{p} and expression projection function ğ’« e \\mathcal{P}_{e} are used to decompose x f â€‹ u â€‹ s â€‹ e â€‹ d x^{fused} into two conditional vectors for facial expression ( x e â€‹ m â€‹ b e x_{emb}^{e} ) and pose ( x e â€‹ m â€‹ b p x_{emb}^{p} ). Figure 4 : Detailed architecture of the I2FET (Instruction to Facial Expression Transition) method. Given an instruction and a sequence of facial parameters, including pose and expression, the IFED module utilizes the facial parameters and textual description extracted by CLIP to produce the corresponding conditional feature vectors for pose x e â€‹ m â€‹ b p x_{emb}^{p} and expression x e â€‹ m â€‹ b e x_{emb}^{e} , respectively. The pose and expression encoders then map this sequence of facial parameters and conditional feature vectors to latent representations corresponding to pose and expression. These latent vectors, along with the textual description, are passed through the IFED module to generate the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} . Following this, the expression and pose decoders reconstruct expression e ^ \\hat{e} and pose Î¸ ^ \\hat{\\theta} parameters using these latent representations and the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} , respectively. Finally, the pre-trained FLAME decoder reconstructs the vertex coordinates v ^ \\hat{v} based on e ^ \\hat{e} and Î¸ ^ \\hat{\\theta} . In the former, the pre-trained CLIP encoder [ 33 ] extracts text description x t âˆˆ R 77 Ã— 768 x^{t}\\in R^{77\\times 768} according to the given instruction t t . Then, the I2FET model generates a sequence of expression and pose coefficients for the facial expression trajectories. Instruction-driven Facial Expression Decomposer : A dual-branch vision transformer [ 3 ] is adopted for extracting multi-scale features and fusing cross-attention information at different scales because it is efficient for the image classification task. Here, the Cross-Attention for Facial Expression Parameter and Text (CAFT) is used to learn the relationship between instruction and facial expression parameters by enhancing the understanding of multimodal data and refining the latent representation for the Instruction to Facial Expression Transition model. The proposed IFED module is illustrated in Fig. 3 . In particular, we directly incorporate facial expression parameters and textual descriptions into CAFT instead of adding class tokens as in [ 3 ] . With respect to the transformer encoders, the first branch learns the correlation between the facial expression parameters, while the second branch focuses on learning the textual descriptions. To begin with, each vector representing facial expression parameters comprises two elements: expression and pose parameters, denoted as x f = { e , Î¸ } âˆˆ R m Ã— 56 x^{f}=\\{e,\\theta\\}\\in R^{m\\times 56} , where m m denotes the number of expressions in a given instruction. As [ 24 , 6 ] , we only utilize jaw pose Î¸ j â€‹ a â€‹ w \\theta^{jaw} and expression e e to convey facial expression information. Therefore, we rewrite x f = { e , Î¸ j â€‹ a â€‹ w } âˆˆ R m Ã— 53 x^{f}=\\{e,\\theta^{jaw}\\}\\in R^{m\\times 53} as the input of the first branch. Then, the transformer encoder is utilized as y f = x f + M â€‹ S â€‹ A â€‹ ( L â€‹ N â€‹ ( x f ) ) \\displaystyle y^{f}=x^{f}+MSA(LN(x^{f})) (1) x ^ f = y f + F â€‹ F â€‹ N â€‹ ( L â€‹ N â€‹ ( y f ) ) \\displaystyle\\hat{x}^{f}=y^{f}+FFN(LN(y^{f})) where x ^ f \\hat{x}^{f} denotes the output of the transformer encoder in the facial expression parameters branch â„° P \\mathcal{E}_{P} . L N ( . ) LN(.) , M S A ( . ) MSA(.) , and F F N ( . ) FFN(.) are the layer norm, multi-head self-attention, and feed-forward network, respectively. Likewise, in the text branch, we replicate x t x^{t} to create a vector { x } i = { 0 , 1 } t \\{x\\}^{t}_{i=\\{0,1\\}} for computational purposes. Subsequently, the input for the transformer encoder is generated using a text projection function P t ( . ) P_{t}(.) on { x } i = { 0 , 1 } t \\{x\\}^{t}_{i=\\{0,1\\}} . As a result, the textual description x ^ t \\hat{x}^{t} is obtained through the transformer encoder â„° t \\mathcal{E}_{t} . Both branches are then fused by cross-attention to capture the relationship between the facial expression parameters and textual descriptions. The fusion process can be expressed as y c f = h f â€‹ 2 â€‹ t â€‹ ( x ^ 0 f ) , y c t = h t â€‹ 2 â€‹ f â€‹ ( x ^ 0 t ) , \\displaystyle y_{c}^{f}=h^{f2t}(\\hat{x}_{0}^{f}),\\quad y_{c}^{t}=h^{t2f}(\\hat{x}_{0}^{t}), (2) x o f = g t â€‹ 2 â€‹ f â€‹ ( [ C â€‹ A â€‹ ( y c f âŠ— x 1 t ) + y c f ] ) âŠ— x ^ 1 f \\displaystyle x_{o}^{f}=g^{t2f}([CA(y^{f}_{c}\\otimes x_{1}^{t})+y_{c}^{f}])\\otimes\\hat{x}_{1}^{f} x o t = g f â€‹ 2 â€‹ t â€‹ ( [ C â€‹ A â€‹ ( y c t âŠ— x 1 f ) + y c t ] ) âŠ— x ^ 1 t \\displaystyle x_{o}^{t}=g^{f2t}([CA(y_{c}^{t}\\otimes x_{1}^{f})+y_{c}^{t}])\\otimes\\hat{x}_{1}^{t} where h t â€‹ 2 â€‹ f , h f â€‹ 2 â€‹ t , g t â€‹ 2 â€‹ f h^{t2f},h^{f2t},g^{t2f} , and g f â€‹ 2 â€‹ t g^{f2t} are the projection function and back-projection function between the vectors that represent the text and facial expression parameters, x o f âˆˆ R m Ã— 53 x_{o}^{f}\\in R^{m\\times 53} and x o t âˆˆ R m Ã— 768 x_{o}^{t}\\in R^{m\\times 768} are the outputs of CAFT, C â€‹ A CA denotes cross-attention and âŠ— \\otimes denotes the concatenation operator. Next, the fused feature vector is obtained by concatenating x o f x_{o}^{f} and x o t x_{o}^{t} , after passing through L N ( . ) LN(.) , as follows: x f â€‹ u â€‹ s â€‹ e â€‹ d = L â€‹ N â€‹ ( x o t ) âŠ— L â€‹ N â€‹ ( x o f ) x^{fused}=LN(x_{o}^{t})\\otimes LN(x_{o}^{f}) (3) Finally, the projection functions for pose ğ’« p ( . ) \\mathcal{P}_{p}(.) and expression ğ’« e ( . ) \\mathcal{P}_{e}(.) are applied to the fused feature vector as follows: x e â€‹ m â€‹ b e = ğ’« e â€‹ ( x f â€‹ u â€‹ s â€‹ e â€‹ d ) \\displaystyle x^{e}_{emb}=\\mathcal{P}_{e}(x^{fused}) (4) x e â€‹ m â€‹ b p = ğ’« p â€‹ ( x f â€‹ u â€‹ s â€‹ e â€‹ d ) \\displaystyle x^{p}_{emb}=\\mathcal{P}_{p}(x^{fused}) where x e â€‹ m â€‹ b e âˆˆ R m Ã— 50 x^{e}_{emb}\\in R^{m\\times 50} and x e â€‹ m â€‹ b p âˆˆ R m Ã— 6 x^{p}_{emb}\\in R^{m\\times 6} are the conditional feature vectors for pose and expression, respectively. In this work, ğ’« p ( . ) \\mathcal{P}_{p}(.) and ğ’« e ( . ) \\mathcal{P}_{e}(.) are designed as linear transformation functions to map the feature x f â€‹ u â€‹ s â€‹ e â€‹ d x^{fused} to the corresponding pose and expression feature vectors. Instruction to Facial Expression Transition : The proposed I2FET method is based on a conditional variational autoencoder architecture. The method includes encoders â„° e t ( . ) \\mathcal{E}_{e}^{t}(.) and â„° p t ( . ) \\mathcal{E}_{p}^{t}(.) , as well as decoders ğ’Ÿ e t ( . ) \\mathcal{D}^{t}_{e}(.) and ğ’Ÿ p t ( . ) \\mathcal{D}^{t}_{p}(.) , which are defined using three Multi-Layer Perceptrons (MLPs). These components are conditioned on feature vectors that represent expression and pose information, respectively. An overview of the proposed I2FET architecture is presented in Fig. 4 . Here, the encoders are defined as follows: â„° e t â€‹ ( e âŠ— x e â€‹ m â€‹ b e ) âŸ¶ ( Î¼ e , Ïƒ e ) \\mathcal{E}_{e}^{t}(e\\otimes x^{e}_{emb})\\longrightarrow(\\mu_{e},\\sigma_{e}) (5) and â„° p t â€‹ ( Î¸ âŠ— x e â€‹ m â€‹ b p ) âŸ¶ ( Î¼ p , Ïƒ p ) \\mathcal{E}_{p}^{t}(\\theta\\otimes x^{p}_{emb})\\longrightarrow(\\mu_{p},\\sigma_{p}) (6) where âŠ— \\otimes denotes the concatenation operator and x e â€‹ m â€‹ b e x^{e}_{emb} and x e â€‹ m â€‹ b p x^{p}_{emb} are conditional feature vectors for expression and pose, respectively. For the encoder stage, x e â€‹ m â€‹ b e x^{e}_{emb} and x e â€‹ m â€‹ b p x^{p}_{emb} are created by textual description x t x_{t} and the facial expression parameters ( e , Î¸ ) (e,\\theta) . Î¼ e \\mu_{e} and Î¼ p \\mu_{p} denote the mean and standard deviation of the expression distribution, whereas Ïƒ e \\sigma_{e} , and Ïƒ p \\sigma_{p} are the mean and standard deviation of the pose distribution. In this study, the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}^{e}_{emb} and x ^ e â€‹ m â€‹ b p \\hat{x}^{p}_{emb} in the decoder are computed by textual description x t x_{t} and the latent vectors z ^ p \\hat{z}_{p} and z ^ e \\hat{z}_{e} representing pose and expression, respectively. These latent vectors are calculated as follows: z ~ p = Ïƒ p âˆ— z p + Î¼ p \\displaystyle\\tilde{z}_{p}=\\sigma_{p}*z_{p}+\\mu_{p} (7) z ^ p = ğ’¯ p â€‹ ( z ~ p ) \\displaystyle\\hat{z}_{p}=\\mathcal{T}_{p}(\\tilde{z}_{p}) z ~ e = Ïƒ e âˆ— z e + Î¼ e \\displaystyle\\tilde{z}_{e}=\\sigma_{e}*z_{e}+\\mu_{e} z ^ e = ğ’¯ e â€‹ ( z ~ e ) \\displaystyle\\hat{z}_{e}=\\mathcal{T}_{e}(\\tilde{z}_{e}) where z p z_{p} and z e z_{e} are the latent vectors sampled from ğ’© â€‹ ( 0 , I ) \\mathcal{N}(0,I) . In this case, we utilize linear transformation functions ğ’¯ p ( . ) \\mathcal{T}_{p}(.) and ğ’¯ e ( . ) \\mathcal{T}_{e}(.) to transform z ~ p \\tilde{z}_{p} and z ~ e âˆˆ R m Ã— 16 \\tilde{z}_{e}\\in R^{m\\times 16} into z ^ p âˆˆ R m Ã— 6 \\hat{z}_{p}\\in R^{m\\times 6} and z ^ e âˆˆ R m Ã— 50 \\hat{z}_{e}\\in R^{m\\times 50} , respectively, similar to the expression and pose dimensions. Thus, the latent vectors z ~ e \\tilde{z}_{e} and z ~ p \\tilde{z}_{p} can be used as input for the IFED module. Then, the latent vectors z ^ e \\hat{z}_{e} and z ^ p \\hat{z}_{p} are concatenated together, and along with the text vector x t x^{t} , they are passed through the IFED module to obtain the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} as x ^ e â€‹ m â€‹ b e , x ^ e â€‹ m â€‹ b p = I â€‹ F â€‹ E â€‹ D â€‹ ( z ^ p âŠ— z ^ e , x t ) \\displaystyle\\hat{x}_{emb}^{e},\\hat{x}_{emb}^{p}=IFED(\\hat{z}_{p}\\otimes\\hat{z}_{e},x^{t}) (8) This approach enhances the semantic representation of latent vectors by refining the conditional feature vectors for decoders based on the learned latent vectors throughout the training process. The decoders are defined as follows ğ’Ÿ p t â€‹ ( z ~ p âŠ— x ^ e â€‹ m â€‹ b p ) âŸ¶ ( Î¸ ^ 0 , Î¸ ^ 1 ) \\mathcal{D}^{t}_{p}(\\tilde{z}_{p}\\otimes\\hat{x}^{p}_{emb})\\longrightarrow(\\hat{\\theta}_{0},\\hat{\\theta}_{1}) (9) and ğ’Ÿ e t â€‹ ( z ~ e âŠ— x ^ e â€‹ m â€‹ b e ) âŸ¶ ( e ^ 0 , e ^ 1 ) \\mathcal{D}^{t}_{e}(\\tilde{z}_{e}\\otimes\\hat{x}^{e}_{emb})\\longrightarrow(\\hat{e}_{0},\\hat{e}_{1}) (10) The parameters of the I2FET model are optimized by minimizing the reconstruction loss function â„’ M â€‹ S â€‹ E ( . , . ) \\mathcal{L}_{MSE}(.,.) between the ground truth coefficients e e , Î¸ \\theta , and the predicted coefficients e ^ , Î¸ ^ \\hat{e},\\hat{\\theta} . In addition, the Kullback-Leibler divergence loss function is utilized to minimize the distance between the probability distribution of the ground truth and that of the predicted values. â„’ e = â„’ M â€‹ S â€‹ E â€‹ ( e , e ^ ) \\displaystyle\\mathcal{L}_{e}=\\mathcal{L}_{MSE}(e,\\hat{e}) (11) + 0.5 â€‹ [ âˆ’ âˆ‘ i ( l â€‹ o â€‹ g â€‹ Ïƒ i 2 + 1 ) + âˆ‘ i Ïƒ i 2 + âˆ‘ i Î¼ i 2 ] \\displaystyle+5\\left[-\\sum_{i}(log\\>\\sigma_{i}^{2}+1)+\\sum_{i}\\sigma^{2}_{i}+\\sum_{i}\\mu_{i}^{2}\\right] â„’ p = â„’ M â€‹ S â€‹ E â€‹ ( Î¸ , Î¸ ^ ) \\displaystyle\\mathcal{L}_{p}=\\mathcal{L}_{MSE}(\\theta,\\hat{\\theta}) (12) + 0.5 â€‹ [ âˆ’ âˆ‘ i ( l â€‹ o â€‹ g â€‹ Ïƒ i 2 + 1 ) + âˆ‘ i Ïƒ i 2 + âˆ‘ i Î¼ i 2 ] \\displaystyle+5\\left[-\\sum_{i}(log\\>\\sigma_{i}^{2}+1)+\\sum_{i}\\sigma^{2}_{i}+\\sum_{i}\\mu_{i}^{2}\\right] where â„’ e \\mathcal{L}_{e} and â„’ p \\mathcal{L}_{p} denote the expression and pose loss function, respectively. Furthermore, we utilize the pretrained FLAME [ 22 ] , a parametric model of a 3D head that is integrated into DECA [ 9 ] , which has N = 5023 N=5023 base vertices V b âˆˆ â„› N Ã— 3 V_{b}\\in\\mathcal{R}^{N\\times 3} and two sets of M M and K K basis vectors that encode shape blendshapes ğ’® âˆˆ R N Ã— 3 Ã— M \\mathcal{S}\\in R^{N\\times 3\\times M} , and expression blendshapes B âˆˆ R N Ã— 3 Ã— K B\\in R^{N\\times 3\\times K} . The basic vectors are first blended and then Linear Blend Skinning (LBS) ğ’² ( . ) \\mathcal{W}(.) is applied to rotate the vertices following the pose Î¸ \\theta . The final reconstruction in world coordinates can be computed as follows ğ’± â€‹ ( Ï• , e , Î¸ ) = ğ’² â€‹ ( V b + ğ’® â€‹ Ï• + â„¬ â€‹ e , Î¸ ) \\mathcal{V}(\\phi,e,\\theta)=\\mathcal{W}(V_{b}+\\mathcal{S}\\phi+\\mathcal{B}e,\\theta) (13) where Ï• \\phi , e e , and Î¸ \\theta denote the identity shape, facial expression, and head pose parameters, respectively. In this case, FLAME is used to reconstruct vertices to improve the performance of I2FET. â„’ v = â„’ M â€‹ S â€‹ E â€‹ ( v , v ^ ) \\mathcal{L}_{v}=\\mathcal{L}_{MSE}(v,\\hat{v}) (14) where the vertex coordinates v v and v ^ \\hat{v} are computed by feeding the ground truth { Î¸ , e , Ï• } \\{\\theta,e,\\phi\\} and the reconstructed { Î¸ ^ , e ^ , Ï• } \\{\\hat{\\theta},\\hat{e},\\phi\\} through FLAME. Finally, the total loss function is given as â„’ t â€‹ o â€‹ t â€‹ a â€‹ l = â„’ e + â„’ p + â„’ v \\mathcal{L}_{total}=\\mathcal{L}_{e}+\\mathcal{L}_{p}+\\mathcal{L}_{v} (15) For the second branch, the pre-trained DECA [ 9 ] is used to encode the facial coefficients of the source face I s I_{s} including shape Ï• s âˆˆ R 1 Ã— 100 \\phi_{s}\\in R^{1\\times 100} , expression e s âˆˆ R 1 Ã— 50 e_{s}\\in R^{1\\times 50} , pose Î¸ s âˆˆ R 1 Ã— 6 \\theta_{s}\\in R^{1\\times 6} and camera c s âˆˆ R 1 Ã— 3 c_{s}\\in R^{1\\times 3} . Following this step, { Ï• s , e s , Î¸ s , c s } \\{\\phi_{s},e_{s},\\theta_{s},c_{s}\\} is integrated with the sequence of facial expression coefficients { ( e 0 , Î¸ 0 ) , ( e 1 , Î¸ 1 ) } \\{(e_{0},\\theta_{0}),(e_{1},\\theta_{1})\\} , generated by the first branch to generate the anchor facial expressions { s s ( 1 ) , s e 0 ( k ) , s e 1 ( T ) } \\{s^{(1)}_{s},s^{(k)}_{e_{0}},s^{(T)}_{e_{1}}\\} . Then, a linear interpolation function Î¨ ( . ) \\Psi(.) is applied to smoothen the transition between the two anchored facial expressions. The Î¨ â€‹ ( s ( l ) , s ( n ) ) \\Psi(s^{(l)},s^{(n)}) function for interpolation between facial expression sequences s ( l ) s^{(l)} and s ( n ) s^{(n)} can be defined as e ( k ) = Î´ âˆ— e ( l ) + ( 1 âˆ’ Î´ ) âˆ— e ( n ) \\displaystyle e^{(k)}=\\delta*e^{(l)}+(1-\\delta)*e^{(n)} (16) Î¸ ( k ) = Î´ âˆ— Î¸ ( l ) + ( 1 âˆ’ Î´ ) âˆ— Î¸ ( n ) \\displaystyle\\theta^{(k)}=\\delta*\\theta^{(l)}+(1-\\delta)*\\theta^{(n)} where 0 â‰¤ Î´ â‰¤ 1 0\\leq\\delta\\leq 1 is a linear coefficient and { e ( l ) , Î¸ ( l ) } âˆˆ s ( l ) \\{e^{(l)},\\theta^{(l)}\\}\\in s^{(l)} . This allows our framework to ensure temporal consistency when the motion changes are sampled uniformly."
        },
        {
          "section_index": 11,
          "heading_level": "h4",
          "heading": "III-B 1 Facial Expression Transition",
          "text": "This module is designed to generate diverse expression trajectories that correspond to the facial parameters of the source face image I s I_{s} and the parameters of specific facial expressions, described in an instruction t t . The module has two primary branches: the first receives an instruction to generate the corresponding sequence of facial expression based on instruction t t , whereas the second takes a 2D face image to encode its facial coefficients and create the facial expression trajectories. Figure 3 : Flow diagram of IFED (Instruction-Driven Facial Expression Decomposer). It consists of CAFT and the decomposition of parameters. The major function of CAFT is to introduce cross-attention between the facial parameters and text instruction. The two outputs from CAFT pass through layer normalization and are combined to create the fused feature vector x f â€‹ u â€‹ s â€‹ e â€‹ d x^{fused} . Then, pose projection function ğ’« p \\mathcal{P}_{p} and expression projection function ğ’« e \\mathcal{P}_{e} are used to decompose x f â€‹ u â€‹ s â€‹ e â€‹ d x^{fused} into two conditional vectors for facial expression ( x e â€‹ m â€‹ b e x_{emb}^{e} ) and pose ( x e â€‹ m â€‹ b p x_{emb}^{p} ). Figure 4 : Detailed architecture of the I2FET (Instruction to Facial Expression Transition) method. Given an instruction and a sequence of facial parameters, including pose and expression, the IFED module utilizes the facial parameters and textual description extracted by CLIP to produce the corresponding conditional feature vectors for pose x e â€‹ m â€‹ b p x_{emb}^{p} and expression x e â€‹ m â€‹ b e x_{emb}^{e} , respectively. The pose and expression encoders then map this sequence of facial parameters and conditional feature vectors to latent representations corresponding to pose and expression. These latent vectors, along with the textual description, are passed through the IFED module to generate the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} . Following this, the expression and pose decoders reconstruct expression e ^ \\hat{e} and pose Î¸ ^ \\hat{\\theta} parameters using these latent representations and the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} , respectively. Finally, the pre-trained FLAME decoder reconstructs the vertex coordinates v ^ \\hat{v} based on e ^ \\hat{e} and Î¸ ^ \\hat{\\theta} . In the former, the pre-trained CLIP encoder [ 33 ] extracts text description x t âˆˆ R 77 Ã— 768 x^{t}\\in R^{77\\times 768} according to the given instruction t t . Then, the I2FET model generates a sequence of expression and pose coefficients for the facial expression trajectories. Instruction-driven Facial Expression Decomposer : A dual-branch vision transformer [ 3 ] is adopted for extracting multi-scale features and fusing cross-attention information at different scales because it is efficient for the image classification task. Here, the Cross-Attention for Facial Expression Parameter and Text (CAFT) is used to learn the relationship between instruction and facial expression parameters by enhancing the understanding of multimodal data and refining the latent representation for the Instruction to Facial Expression Transition model. The proposed IFED module is illustrated in Fig. 3 . In particular, we directly incorporate facial expression parameters and textual descriptions into CAFT instead of adding class tokens as in [ 3 ] . With respect to the transformer encoders, the first branch learns the correlation between the facial expression parameters, while the second branch focuses on learning the textual descriptions. To begin with, each vector representing facial expression parameters comprises two elements: expression and pose parameters, denoted as x f = { e , Î¸ } âˆˆ R m Ã— 56 x^{f}=\\{e,\\theta\\}\\in R^{m\\times 56} , where m m denotes the number of expressions in a given instruction. As [ 24 , 6 ] , we only utilize jaw pose Î¸ j â€‹ a â€‹ w \\theta^{jaw} and expression e e to convey facial expression information. Therefore, we rewrite x f = { e , Î¸ j â€‹ a â€‹ w } âˆˆ R m Ã— 53 x^{f}=\\{e,\\theta^{jaw}\\}\\in R^{m\\times 53} as the input of the first branch. Then, the transformer encoder is utilized as y f = x f + M â€‹ S â€‹ A â€‹ ( L â€‹ N â€‹ ( x f ) ) \\displaystyle y^{f}=x^{f}+MSA(LN(x^{f})) (1) x ^ f = y f + F â€‹ F â€‹ N â€‹ ( L â€‹ N â€‹ ( y f ) ) \\displaystyle\\hat{x}^{f}=y^{f}+FFN(LN(y^{f})) where x ^ f \\hat{x}^{f} denotes the output of the transformer encoder in the facial expression parameters branch â„° P \\mathcal{E}_{P} . L N ( . ) LN(.) , M S A ( . ) MSA(.) , and F F N ( . ) FFN(.) are the layer norm, multi-head self-attention, and feed-forward network, respectively. Likewise, in the text branch, we replicate x t x^{t} to create a vector { x } i = { 0 , 1 } t \\{x\\}^{t}_{i=\\{0,1\\}} for computational purposes. Subsequently, the input for the transformer encoder is generated using a text projection function P t ( . ) P_{t}(.) on { x } i = { 0 , 1 } t \\{x\\}^{t}_{i=\\{0,1\\}} . As a result, the textual description x ^ t \\hat{x}^{t} is obtained through the transformer encoder â„° t \\mathcal{E}_{t} . Both branches are then fused by cross-attention to capture the relationship between the facial expression parameters and textual descriptions. The fusion process can be expressed as y c f = h f â€‹ 2 â€‹ t â€‹ ( x ^ 0 f ) , y c t = h t â€‹ 2 â€‹ f â€‹ ( x ^ 0 t ) , \\displaystyle y_{c}^{f}=h^{f2t}(\\hat{x}_{0}^{f}),\\quad y_{c}^{t}=h^{t2f}(\\hat{x}_{0}^{t}), (2) x o f = g t â€‹ 2 â€‹ f â€‹ ( [ C â€‹ A â€‹ ( y c f âŠ— x 1 t ) + y c f ] ) âŠ— x ^ 1 f \\displaystyle x_{o}^{f}=g^{t2f}([CA(y^{f}_{c}\\otimes x_{1}^{t})+y_{c}^{f}])\\otimes\\hat{x}_{1}^{f} x o t = g f â€‹ 2 â€‹ t â€‹ ( [ C â€‹ A â€‹ ( y c t âŠ— x 1 f ) + y c t ] ) âŠ— x ^ 1 t \\displaystyle x_{o}^{t}=g^{f2t}([CA(y_{c}^{t}\\otimes x_{1}^{f})+y_{c}^{t}])\\otimes\\hat{x}_{1}^{t} where h t â€‹ 2 â€‹ f , h f â€‹ 2 â€‹ t , g t â€‹ 2 â€‹ f h^{t2f},h^{f2t},g^{t2f} , and g f â€‹ 2 â€‹ t g^{f2t} are the projection function and back-projection function between the vectors that represent the text and facial expression parameters, x o f âˆˆ R m Ã— 53 x_{o}^{f}\\in R^{m\\times 53} and x o t âˆˆ R m Ã— 768 x_{o}^{t}\\in R^{m\\times 768} are the outputs of CAFT, C â€‹ A CA denotes cross-attention and âŠ— \\otimes denotes the concatenation operator. Next, the fused feature vector is obtained by concatenating x o f x_{o}^{f} and x o t x_{o}^{t} , after passing through L N ( . ) LN(.) , as follows: x f â€‹ u â€‹ s â€‹ e â€‹ d = L â€‹ N â€‹ ( x o t ) âŠ— L â€‹ N â€‹ ( x o f ) x^{fused}=LN(x_{o}^{t})\\otimes LN(x_{o}^{f}) (3) Finally, the projection functions for pose ğ’« p ( . ) \\mathcal{P}_{p}(.) and expression ğ’« e ( . ) \\mathcal{P}_{e}(.) are applied to the fused feature vector as follows: x e â€‹ m â€‹ b e = ğ’« e â€‹ ( x f â€‹ u â€‹ s â€‹ e â€‹ d ) \\displaystyle x^{e}_{emb}=\\mathcal{P}_{e}(x^{fused}) (4) x e â€‹ m â€‹ b p = ğ’« p â€‹ ( x f â€‹ u â€‹ s â€‹ e â€‹ d ) \\displaystyle x^{p}_{emb}=\\mathcal{P}_{p}(x^{fused}) where x e â€‹ m â€‹ b e âˆˆ R m Ã— 50 x^{e}_{emb}\\in R^{m\\times 50} and x e â€‹ m â€‹ b p âˆˆ R m Ã— 6 x^{p}_{emb}\\in R^{m\\times 6} are the conditional feature vectors for pose and expression, respectively. In this work, ğ’« p ( . ) \\mathcal{P}_{p}(.) and ğ’« e ( . ) \\mathcal{P}_{e}(.) are designed as linear transformation functions to map the feature x f â€‹ u â€‹ s â€‹ e â€‹ d x^{fused} to the corresponding pose and expression feature vectors. Instruction to Facial Expression Transition : The proposed I2FET method is based on a conditional variational autoencoder architecture. The method includes encoders â„° e t ( . ) \\mathcal{E}_{e}^{t}(.) and â„° p t ( . ) \\mathcal{E}_{p}^{t}(.) , as well as decoders ğ’Ÿ e t ( . ) \\mathcal{D}^{t}_{e}(.) and ğ’Ÿ p t ( . ) \\mathcal{D}^{t}_{p}(.) , which are defined using three Multi-Layer Perceptrons (MLPs). These components are conditioned on feature vectors that represent expression and pose information, respectively. An overview of the proposed I2FET architecture is presented in Fig. 4 . Here, the encoders are defined as follows: â„° e t â€‹ ( e âŠ— x e â€‹ m â€‹ b e ) âŸ¶ ( Î¼ e , Ïƒ e ) \\mathcal{E}_{e}^{t}(e\\otimes x^{e}_{emb})\\longrightarrow(\\mu_{e},\\sigma_{e}) (5) and â„° p t â€‹ ( Î¸ âŠ— x e â€‹ m â€‹ b p ) âŸ¶ ( Î¼ p , Ïƒ p ) \\mathcal{E}_{p}^{t}(\\theta\\otimes x^{p}_{emb})\\longrightarrow(\\mu_{p},\\sigma_{p}) (6) where âŠ— \\otimes denotes the concatenation operator and x e â€‹ m â€‹ b e x^{e}_{emb} and x e â€‹ m â€‹ b p x^{p}_{emb} are conditional feature vectors for expression and pose, respectively. For the encoder stage, x e â€‹ m â€‹ b e x^{e}_{emb} and x e â€‹ m â€‹ b p x^{p}_{emb} are created by textual description x t x_{t} and the facial expression parameters ( e , Î¸ ) (e,\\theta) . Î¼ e \\mu_{e} and Î¼ p \\mu_{p} denote the mean and standard deviation of the expression distribution, whereas Ïƒ e \\sigma_{e} , and Ïƒ p \\sigma_{p} are the mean and standard deviation of the pose distribution. In this study, the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}^{e}_{emb} and x ^ e â€‹ m â€‹ b p \\hat{x}^{p}_{emb} in the decoder are computed by textual description x t x_{t} and the latent vectors z ^ p \\hat{z}_{p} and z ^ e \\hat{z}_{e} representing pose and expression, respectively. These latent vectors are calculated as follows: z ~ p = Ïƒ p âˆ— z p + Î¼ p \\displaystyle\\tilde{z}_{p}=\\sigma_{p}*z_{p}+\\mu_{p} (7) z ^ p = ğ’¯ p â€‹ ( z ~ p ) \\displaystyle\\hat{z}_{p}=\\mathcal{T}_{p}(\\tilde{z}_{p}) z ~ e = Ïƒ e âˆ— z e + Î¼ e \\displaystyle\\tilde{z}_{e}=\\sigma_{e}*z_{e}+\\mu_{e} z ^ e = ğ’¯ e â€‹ ( z ~ e ) \\displaystyle\\hat{z}_{e}=\\mathcal{T}_{e}(\\tilde{z}_{e}) where z p z_{p} and z e z_{e} are the latent vectors sampled from ğ’© â€‹ ( 0 , I ) \\mathcal{N}(0,I) . In this case, we utilize linear transformation functions ğ’¯ p ( . ) \\mathcal{T}_{p}(.) and ğ’¯ e ( . ) \\mathcal{T}_{e}(.) to transform z ~ p \\tilde{z}_{p} and z ~ e âˆˆ R m Ã— 16 \\tilde{z}_{e}\\in R^{m\\times 16} into z ^ p âˆˆ R m Ã— 6 \\hat{z}_{p}\\in R^{m\\times 6} and z ^ e âˆˆ R m Ã— 50 \\hat{z}_{e}\\in R^{m\\times 50} , respectively, similar to the expression and pose dimensions. Thus, the latent vectors z ~ e \\tilde{z}_{e} and z ~ p \\tilde{z}_{p} can be used as input for the IFED module. Then, the latent vectors z ^ e \\hat{z}_{e} and z ^ p \\hat{z}_{p} are concatenated together, and along with the text vector x t x^{t} , they are passed through the IFED module to obtain the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} as x ^ e â€‹ m â€‹ b e , x ^ e â€‹ m â€‹ b p = I â€‹ F â€‹ E â€‹ D â€‹ ( z ^ p âŠ— z ^ e , x t ) \\displaystyle\\hat{x}_{emb}^{e},\\hat{x}_{emb}^{p}=IFED(\\hat{z}_{p}\\otimes\\hat{z}_{e},x^{t}) (8)"
        },
        {
          "section_index": 12,
          "heading_level": "h4",
          "heading": "III-B 2 Face Rendering",
          "text": "This module renders the facial appearance for the facial expression trajectories, generated by FET. In this work, the face rendering approach is based on the facial parameters of FLAME for integration with FET. Head Mesh Reconstruction: It generates the vertex coordinates corresponding to the facial expressions using Eq. 13 by using the sequence of facial expression transitions as the input. Then, the target vertex coordinates are obtained by combining these expressions with the deformation of hair and shoulder regions f H ( . ) f_{H}(.) from I s I_{s} as follows â„³ t = ğ’± â€‹ ( Ï• , e , Î¸ ) + f H â€‹ ( I s ) \\mathcal{M}^{t}=\\mathcal{V}(\\phi,e,\\theta)+f_{H}(I_{s}) (17) where f H ( . ) f_{H}(.) is the pre-trained linear deformation model [ 18 ] that is used to refine the vertices locations. Rendering : Following the head mesh reconstruction step, facial appearances can be rendered from the reconstructed head vertices â„³ = { m s ( 1 ) , m ( i ) , m e 0 ( k ) , m ( j ) , m e 1 ( T ) } \\mathcal{M}=\\{m_{s}^{(1)},m^{(i)},m_{e_{0}}^{(k)},m^{(j)},m_{e_{1}}^{(T)}\\} and the extracted neutral texture of the source image I s I_{s} by the texture decoder â„° t â€‹ e â€‹ x â€‹ ( I s ) \\mathcal{E}_{tex}(I_{s}) . The present work explores a face-rendering model suitable for facial expression transition. Therefore, we utilize pre-trained facial rendering models, known as state-of-the-art models [ 18 , 26 ] , capable of rendering facial appearances based on the facial parameters calculated with FLAME. Subsequently, the sequence of facial appearances ğ’´ = { y s ( 1 ) , y ( i ) , y e 0 ( k ) , y ( j ) , y e 1 ( T ) } \\mathcal{Y}=\\{y_{s}^{(1)},y^{(i)},y_{e_{0}}^{(k)},y^{(j)},y_{e_{1}}^{(T)}\\} is generated as the output of the proposed framework. (a) (b) (c) Figure 5 : Statistics of the facial expressions utilized in this study for the CK+ (a) and CelebV-HQ (b) datasets, respectively, and a collection of sampled words extracted from the text instructions (c)."
        },
        {
          "section_index": 13,
          "heading_level": "h2",
          "heading": "IV Experiments",
          "text": "IV-A Dataset TABLE I : N 1 , N 2 âˆˆ { N_{1},N_{2}\\in\\{ HAPPINESS, DISGUST, ANGER, FEAR, SURPRISE, CONTEMPT, SADNESS } \\} IN THE CK+ DATASET AND N 1 , N 2 âˆˆ { N_{1},N_{2}\\in\\{ HAPPINESS, DISGUST, ANGER, FEAR, SURPRISE, CONTEMPT, SADNESS, NEUTRAL } \\} IN THE CELEBV-HQ DATASET. No Instruction/ Prompts 1 Turn this face from [ N 1 N_{1} ] to [ N 2 N_{2} ]. 2 Change this face from [ N 1 N_{1} ] to [ N 2 N_{2} ]. 3 Transform this face from [ N 1 N_{1} ] to [ N 2 N_{2} ]. 4 Modify this face, changing it from [ N 1 N_{1} ] to [ N 2 N_{2} ]. 5 Replace this face from [ N 1 N_{1} to [ N 2 N_{2} ]. IV-A 1 CK+ dataset The CK+ dataset was supplemented with text instructions for facial expression generation and transition. The original CK+ dataset [ 25 ] includes 117 subjects and 7 emotions, such as happiness, disgust, anger, fear, surprise, contempt, and sadness . The instructions are created to describe the transitions between these facial expressions. To extract the facial parameters and the shape image corresponding to the facial expression in the image, we utilize the pre-trained DECA model. For each sample, a tuple is created with an instruction, the target shape image, and facial parameters that correspond to the same subject. The CK+ dataset, in this study, contains a total of 26,352 samples, and the composition of this dataset in terms of the seven facial expressions is shown in Fig. 5(a) . IV-A 2 CelebV-HQ dataset Similar to the CK+ case, text instructions were added to the CelebV-HQ dataset for our experiments [ 44 ] . The original CelebV-HQ dataset included 15,653 subjects and 8 expressions of neutral, happiness, disgust, anger, fear, surprise, contempt, and sadness . The images in the CelebV-HQ dataset were spontaneously collected and presented many challenges such as highly non-frontal head poses, blur, and brightness. This problem was overcome by using a head pose algorithm [ 23 ] to estimate parameters such as the yaw, roll, and pitch , of the head pose ranging from -15 to 15 degrees. Given that the subjects were chosen indiscriminately, we select those with more than two facial expressions. Then, the pre-trained DECA was applied to extract the corresponding shape images and facial parameters. The CelebV-HQ dataset adopted in this study includes 614 subjects and 28,335 samples, and Fig. 5(b) shows a statistical breakdown of the eight facial expressions. Table I lists the template sentences that are used to generate guided instructions for facial expression transitions. We generated five instructions for each sample and depicted the transition from expression N 1 N_{1} to expression N 2 N_{2} . Subsequently, one presentation sentence is randomly chosen from these instructions. Fig. 5(c) illustrates some sampled words used in the text instructions. Figure 6 : The shapes of facial expression are generated using images from the CK+ dataset. The top and bottom rows illustrate surprised and angry expressions with various head poses. IV-B Implementation The I2FET model was trained for 200 epochs with a batch size of 128 and a learning rate of 8 â€‹ e âˆ’ 4 8e^{-4} . The Adam optimizer [ 20 ] was adopted to optimize the model. For the experiments, 10% of the dataset was allocated to the testing set, whereas the remaining portion was used for training. Then, a validation set was drawn from 10% of the training set. In this work, we repeated each experiment ten times and used the average results to evaluate the overall performance of the proposed I2FET. Because our task is new and a specific method that would allow a fair comparison is not available, we chose MotionClip [ 39 ] as a baseline. For the face rendering experiment, the configuration by [ 18 , 26 ] was used. IV-C Classification of Facial Expressions In this study, it was important to employ an appropriate classifier to evaluate the method that was used to generate facial expressions as prior studies [ 27 , 46 ] . Given that the CK+ and CelebV-HQ datasets had skewed distributions, the performance for minor classes could be affected by the facial expression classifiers trained on these imbalanced datasets, in which the results would be biased towards the major class. Moreover, the effectiveness of the classification model could also be affected by head movements, which were commonly encountered as shown in Fig. 6 . These problems were addressed by conducting an experiment to identify an appropriate classifier that can handle not only imbalanced datasets but also invariant rotation transformations. ResNet-101 [ 14 ] , MEK [ 43 ] , and ResNet-101 integrated with Reweighted Focal Loss (RFL) [ 5 ] were utilized to evaluate the performance of facial expression classification. For this purpose, augmented datasets were created by rotating the original CK+ and CelebV-HQ datasets at angles of Â± 10 o , Â± 15 o \\pm 10^{o},\\pm 15^{o} , and Â± 30 o \\pm 30^{o} . Then, the three networks are trained using the original datasets and evaluated on all datasets. The Accuracy was employed to assess whether samples had been correctly classified across all classes, while G-mean was used to evaluate the ability of the model to effectively recognize minor classes that are predicted by the facial expression classification model. The supplementary data in the ablation study in Section IV-E suggests that MEK can address imbalanced datasets but is sensitive to rotational transformations. ResNet101 demonstrates positive results with rotation transformations but performs worse on datasets that are highly imbalanced. ResNet101 integrated with RFL outperforms ResNet101 and MEK in both scenarios. As a result, ResNet101 with RFL was selected as our classifier for evaluating the ability of our models to generate facial expressions. IV-D Results (a) (b) (c) Figure 7 : Visualization of the learned latent space for seven facial expressions by (a) MotionClip, (b) I2FET without IFED, and (c) I2FET with IFED, respectively, using t-SNE. (a) (b) Figure 8 : Confusion matrices for seven facial expressions by (a) MotionClip and (b) our method on CK+. (a) (b) Figure 9 : Confusion matrices for eight facial expressions by (a) MotionClip and (b) our method on CelebV-HQ. TABLE II : COMPARISON OF ACCURACY BETWEEN OUR METHOD AND MOTIONCLIP ON THE CK+ DATASET Method Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow Ground-truth 99.69 Â± \\pm 0.0001 99.39 Â± \\pm 0.0002 99.69 Â± \\pm 9.3e -5 MotionClip 52.00 Â± \\pm 0.0058 20.00 Â± \\pm 0.0082 40.48 Â± \\pm 0.0060 Ours 91.44 Â± \\pm 0.0024 84.03 Â± \\pm 0.0034 80.30 Â± \\pm 0.0064 TABLE III : COMPARISON OF ACCURACY BETWEEN OUR METHOD AND MOTIONCLIP ON THE CELEBV-HQ DATASET Method Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow Ground-truth 93.67 Â± \\pm 0.0000 87.89 Â± \\pm 0.0000 95.04 Â± \\pm 1.1e -16 MotionClip 40.00 Â± \\pm 0.0039 13.73 Â± \\pm 0.0056 34.42 Â± \\pm 0.0069 Ours 58.24 Â± \\pm 0.0028 33.45 Â± \\pm 0.0052 46.47 Â± \\pm 0.0065 TABLE IV : RESULT OBTAINED BY COMBINING OUR METHOD WITH ROME OR CVTHEAD FOR TEXTURE GENERATION ON THE CK+ AND CELEBV-HQ DATASETS, RESPECTIVELY. NOTE THAT â†“ \\downarrow INDICATES THAT LOWER IS BETTER, WHEREAS â†‘ \\uparrow DENOTES THAT THE HIGHER IS BETTER. Dataset Method L â†“ 1 {}_{1}\\downarrow PSNR â†‘ {\\uparrow} LPIPS â†“ {\\downarrow} MS_SSIM â†‘ {\\uparrow} CK+ Ours+ROME 0.199 9.939 0.490 0.347 Ours+CVTHead 0.005 33.74 0.021 0.978 CelebV-HQ Ours+ROME 0.168 11.316 0.505 0.418 Ours+CVTHead 0.003 37.08 0.010 0.990 Figure 10 : Qualitative comparison between 3D expressive heads that were generated by MotionClip and our method, respectively. The input, consisting of a shape image and instruction, was used for generating expressions 1 and 2 using either MotionClip (b) or Ours (c) on CK+ and CelebV-HQ (d, e), respectively. Figure 11 : Qualitative comparison between generated 3D expressive faces when our method is combined with texture generation [ 18 , 26 ] to generate the desired expressions. The input (a), consisting of the source image and instruction, was used for generating expressions 1 and 2 using either Ours+ROME (b) or Ours+CVTHead (c) on CK+ and CelebV-HQ (d, e), respectively. Figure 12 : Successive snapshots of the generated 3D expressive faces that include a neutral expression between two facial expressions instructed by text prompts. As the present study aims to generate an avatar with a 3D facial expression from a 2D image of a face and transform the facial emotion according to the given instructions, two major challenges arise (1) facial expression transition based on expression information given by the instruction and (2) rendering the facial appearance such that it corresponds with the desired facial expression. Two experiments were prepared to evaluate the results obtained with the proposed framework both quantitatively and qualitatively. IV-D 1 Quantitative Comparison Two tasks are prepared for quantitative comparison. Facial Expression Transition The DECA encoder extracts the shape, facial expression, and pose from the source image. At the same time, the CLIP encoder combined with IFED and I2FET was used to interpret the text instruction: the initial facial expression, the final facial expression, and pose variation. To compare our approach with MotionClip, the first accuracy metric (Acc 1 ) was used to count the correctly classified samples for all expression labels in the instructions. The other one (Acc 2 ) was used to measure the correctly classified samples corresponding to both expression labels presented in each input instruction. The result suggests that the accuracy of our model is higher than that of MotionClip on both datasets as summarized in Table II . For example, our model reaches 91.44 % accuracy for generating an independent expression and 84.03 % for generating expression sequences on the CK+ dataset. The Gmean of our model is 80.30%. For the CelebV-HQ dataset, which is the real-world dataset, our model scores 58.24 %, 33.45 %, and 46.47 % for Acc 1 , Acc 2 , and Gmean, respectively, as indicated in Table III . This result suggests that our model outperforms MotionClip for learning the facial expression parameters. Visualization with t-SNE : Fig. 7 shows three learned latent representations, i.e. with MotionClip, I2FET, and I2FET with IFED, for seven facial expressions. These visual representations indicate that segregation between facial expressions is highly distinctive when IFED is combined with I2FET, compared with either MotionClip or I2FET alone. Confusion Matrices Analysis : Fig. 8 and Fig. 9 show that IFED enhances the accuracy across most classes compared to MotionClip on both datasets. Yet, a few case of misclassification occurred between certain classes of expressions, such as anger and sadness, or contempt and anger with sadness on CK+. In real-world datasets such as CelebV-HQ, some misclassification occurs between classes, particularly those with challenging expressions such as fear and surprise, or contempt and neutral. Face Rendering : For the face-rendering task, FET was combined with state-of-the-art models [ 18 , 26 ] for rendering facial appearances that correspond to specific expressions requested by instruction. This was accomplished by integrating whether ROME [ 18 ] or CVTHead [ 26 ] with facial expression transition. In this case, DECA extracts facial parameters to render geometrical information for a specific face. Instead of shapes, the RGB images and instructions are used to evaluate the performance. The L 1 loss, Peak Signal to Noise Ratio (PSNR), Learned Perceptual Image Patch Similarity (LPIPS) [ 42 ] , and Multi-Scale Structured Similarity (MS-SSIM) were used to measure the quality of reconstruction between the target image and synthesized image in the generated expression. As indicated by Table IV , our model with CVTHead produces the best result. IV-D 2 Qualitative Comparison In a similar vein, two tasks were prepared for qualitative comparisons. Facial Expression Transition : Fig. 10 compares sequences of facial expressions, generated by Motionclip and our method on CK+ and CelebV-HQ, respectively. These results clearly show that our method can generate facial expressions that convey more accurate expressions than MotionClip. Face Rendering : The resulting expressions can be qualitatively compared by viewing and deciding which expression looks better and more natural than the other. Fig. 11 shows the generated facial expressions based on learning the latent space of facial parameters on both datasets. The viewer may notice the difference between (b) and (c) for the CK+ dataset, and between (d) and (e) for the CelebV-HQ dataset, respectively, in terms of facial expressions and facial texture. For instance, the facial transition from surprise to happiness seen in the 2nd row is clearer in (c) than in (b), and the facial texture seen in the 4th row is much smoother in (c) than in (b). A similar observation can be made between (d) and (e), suggesting that Ours+CVTHead is superior to than Ours+ROME. For instance, in some cases, those enclosed within dashed boxes in Fig. 11 , a subtle distortion arose either when the facial region of the source image was occluded or when the instructions describe expressions with significant pose changes, such as transitioning from surprise to happiness or from fear to surprise. This led to inconsistencies, and missing details in the generated facial regions compared to Ours+CVTHead. A comparison between the two datasets, such as between (b) and (d) or between (c) and (e), reveals that the visual quality of CK+ is better than that of CelebV-HQ. These results confirm that our model has outstanding ability to transform facial expressions, learned on CK+ or CelebV-HQ, to a specific facial image. In addition, a user study was carried out by requesting 24 subjects to view 60 randomly paired images of various identities selected from either CVTHead + MotionClip or CVTHead + Ours. Subjects were asked to choose the expression closest to the expressions specified in the instructions or indicate whether none of them matched the provided expressions. Naturally, they were unaware as to which algorithm was used to the presented images. Four subjects viewed each video and voted for the one that more closely resembled the expression in the instructions. In cases in which the votes were tied, an additional subject was asked to make the final decision. Based on these outcomes, CVTHead + Ours achieved 73% accuracy, whereas CVTHead + MotionClip yielded 58% accuracy for CK+. For CelebV-HQ, CVTHead + Ours and CVTHead + MotionClip achieved 77% and 70.5% accuracy, respectively. IV-D 3 Inference Time During inference, CVTHead + MotionClip takes 3.97 seconds to generate a video, whereas CVTHead + Ours only takes 3.92 seconds on a single NVIDIA RTX A6000, suggesting that our model is a slightly faster than MotionClip."
        },
        {
          "section_index": 14,
          "heading_level": "h3",
          "heading": "IV-A Dataset",
          "text": "TABLE I : N 1 , N 2 âˆˆ { N_{1},N_{2}\\in\\{ HAPPINESS, DISGUST, ANGER, FEAR, SURPRISE, CONTEMPT, SADNESS } \\} IN THE CK+ DATASET AND N 1 , N 2 âˆˆ { N_{1},N_{2}\\in\\{ HAPPINESS, DISGUST, ANGER, FEAR, SURPRISE, CONTEMPT, SADNESS, NEUTRAL } \\} IN THE CELEBV-HQ DATASET. No Instruction/ Prompts 1 Turn this face from [ N 1 N_{1} ] to [ N 2 N_{2} ]. 2 Change this face from [ N 1 N_{1} ] to [ N 2 N_{2} ]. 3 Transform this face from [ N 1 N_{1} ] to [ N 2 N_{2} ]. 4 Modify this face, changing it from [ N 1 N_{1} ] to [ N 2 N_{2} ]. 5 Replace this face from [ N 1 N_{1} to [ N 2 N_{2} ]. IV-A 1 CK+ dataset The CK+ dataset was supplemented with text instructions for facial expression generation and transition. The original CK+ dataset [ 25 ] includes 117 subjects and 7 emotions, such as happiness, disgust, anger, fear, surprise, contempt, and sadness . The instructions are created to describe the transitions between these facial expressions. To extract the facial parameters and the shape image corresponding to the facial expression in the image, we utilize the pre-trained DECA model. For each sample, a tuple is created with an instruction, the target shape image, and facial parameters that correspond to the same subject. The CK+ dataset, in this study, contains a total of 26,352 samples, and the composition of this dataset in terms of the seven facial expressions is shown in Fig. 5(a) . IV-A 2 CelebV-HQ dataset Similar to the CK+ case, text instructions were added to the CelebV-HQ dataset for our experiments [ 44 ] . The original CelebV-HQ dataset included 15,653 subjects and 8 expressions of neutral, happiness, disgust, anger, fear, surprise, contempt, and sadness . The images in the CelebV-HQ dataset were spontaneously collected and presented many challenges such as highly non-frontal head poses, blur, and brightness. This problem was overcome by using a head pose algorithm [ 23 ] to estimate parameters such as the yaw, roll, and pitch , of the head pose ranging from -15 to 15 degrees. Given that the subjects were chosen indiscriminately, we select those with more than two facial expressions. Then, the pre-trained DECA was applied to extract the corresponding shape images and facial parameters. The CelebV-HQ dataset adopted in this study includes 614 subjects and 28,335 samples, and Fig. 5(b) shows a statistical breakdown of the eight facial expressions. Table I lists the template sentences that are used to generate guided instructions for facial expression transitions. We generated five instructions for each sample and depicted the transition from expression N 1 N_{1} to expression N 2 N_{2} . Subsequently, one presentation sentence is randomly chosen from these instructions. Fig. 5(c) illustrates some sampled words used in the text instructions. Figure 6 : The shapes of facial expression are generated using images from the CK+ dataset. The top and bottom rows illustrate surprised and angry expressions with various head poses."
        },
        {
          "section_index": 15,
          "heading_level": "h4",
          "heading": "IV-A 1 CK+ dataset",
          "text": "The CK+ dataset was supplemented with text instructions for facial expression generation and transition. The original CK+ dataset [ 25 ] includes 117 subjects and 7 emotions, such as happiness, disgust, anger, fear, surprise, contempt, and sadness . The instructions are created to describe the transitions between these facial expressions. To extract the facial parameters and the shape image corresponding to the facial expression in the image, we utilize the pre-trained DECA model. For each sample, a tuple is created with an instruction, the target shape image, and facial parameters that correspond to the same subject. The CK+ dataset, in this study, contains a total of 26,352 samples, and the composition of this dataset in terms of the seven facial expressions is shown in Fig. 5(a) ."
        },
        {
          "section_index": 16,
          "heading_level": "h4",
          "heading": "IV-A 2 CelebV-HQ dataset",
          "text": "Similar to the CK+ case, text instructions were added to the CelebV-HQ dataset for our experiments [ 44 ] . The original CelebV-HQ dataset included 15,653 subjects and 8 expressions of neutral, happiness, disgust, anger, fear, surprise, contempt, and sadness . The images in the CelebV-HQ dataset were spontaneously collected and presented many challenges such as highly non-frontal head poses, blur, and brightness. This problem was overcome by using a head pose algorithm [ 23 ] to estimate parameters such as the yaw, roll, and pitch , of the head pose ranging from -15 to 15 degrees. Given that the subjects were chosen indiscriminately, we select those with more than two facial expressions. Then, the pre-trained DECA was applied to extract the corresponding shape images and facial parameters. The CelebV-HQ dataset adopted in this study includes 614 subjects and 28,335 samples, and Fig. 5(b) shows a statistical breakdown of the eight facial expressions. Table I lists the template sentences that are used to generate guided instructions for facial expression transitions. We generated five instructions for each sample and depicted the transition from expression N 1 N_{1} to expression N 2 N_{2} . Subsequently, one presentation sentence is randomly chosen from these instructions. Fig. 5(c) illustrates some sampled words used in the text instructions. Figure 6 : The shapes of facial expression are generated using images from the CK+ dataset. The top and bottom rows illustrate surprised and angry expressions with various head poses."
        },
        {
          "section_index": 17,
          "heading_level": "h3",
          "heading": "IV-B Implementation",
          "text": "The I2FET model was trained for 200 epochs with a batch size of 128 and a learning rate of 8 â€‹ e âˆ’ 4 8e^{-4} . The Adam optimizer [ 20 ] was adopted to optimize the model. For the experiments, 10% of the dataset was allocated to the testing set, whereas the remaining portion was used for training. Then, a validation set was drawn from 10% of the training set. In this work, we repeated each experiment ten times and used the average results to evaluate the overall performance of the proposed I2FET. Because our task is new and a specific method that would allow a fair comparison is not available, we chose MotionClip [ 39 ] as a baseline. For the face rendering experiment, the configuration by [ 18 , 26 ] was used."
        },
        {
          "section_index": 18,
          "heading_level": "h3",
          "heading": "IV-C Classification of Facial Expressions",
          "text": "In this study, it was important to employ an appropriate classifier to evaluate the method that was used to generate facial expressions as prior studies [ 27 , 46 ] . Given that the CK+ and CelebV-HQ datasets had skewed distributions, the performance for minor classes could be affected by the facial expression classifiers trained on these imbalanced datasets, in which the results would be biased towards the major class. Moreover, the effectiveness of the classification model could also be affected by head movements, which were commonly encountered as shown in Fig. 6 . These problems were addressed by conducting an experiment to identify an appropriate classifier that can handle not only imbalanced datasets but also invariant rotation transformations. ResNet-101 [ 14 ] , MEK [ 43 ] , and ResNet-101 integrated with Reweighted Focal Loss (RFL) [ 5 ] were utilized to evaluate the performance of facial expression classification. For this purpose, augmented datasets were created by rotating the original CK+ and CelebV-HQ datasets at angles of Â± 10 o , Â± 15 o \\pm 10^{o},\\pm 15^{o} , and Â± 30 o \\pm 30^{o} . Then, the three networks are trained using the original datasets and evaluated on all datasets. The Accuracy was employed to assess whether samples had been correctly classified across all classes, while G-mean was used to evaluate the ability of the model to effectively recognize minor classes that are predicted by the facial expression classification model. The supplementary data in the ablation study in Section IV-E suggests that MEK can address imbalanced datasets but is sensitive to rotational transformations. ResNet101 demonstrates positive results with rotation transformations but performs worse on datasets that are highly imbalanced. ResNet101 integrated with RFL outperforms ResNet101 and MEK in both scenarios. As a result, ResNet101 with RFL was selected as our classifier for evaluating the ability of our models to generate facial expressions."
        },
        {
          "section_index": 19,
          "heading_level": "h3",
          "heading": "IV-D Results",
          "text": "(a) (b) (c) Figure 7 : Visualization of the learned latent space for seven facial expressions by (a) MotionClip, (b) I2FET without IFED, and (c) I2FET with IFED, respectively, using t-SNE. (a) (b) Figure 8 : Confusion matrices for seven facial expressions by (a) MotionClip and (b) our method on CK+. (a) (b) Figure 9 : Confusion matrices for eight facial expressions by (a) MotionClip and (b) our method on CelebV-HQ. TABLE II : COMPARISON OF ACCURACY BETWEEN OUR METHOD AND MOTIONCLIP ON THE CK+ DATASET Method Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow Ground-truth 99.69 Â± \\pm 0.0001 99.39 Â± \\pm 0.0002 99.69 Â± \\pm 9.3e -5 MotionClip 52.00 Â± \\pm 0.0058 20.00 Â± \\pm 0.0082 40.48 Â± \\pm 0.0060 Ours 91.44 Â± \\pm 0.0024 84.03 Â± \\pm 0.0034 80.30 Â± \\pm 0.0064 TABLE III : COMPARISON OF ACCURACY BETWEEN OUR METHOD AND MOTIONCLIP ON THE CELEBV-HQ DATASET Method Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow Ground-truth 93.67 Â± \\pm 0.0000 87.89 Â± \\pm 0.0000 95.04 Â± \\pm 1.1e -16 MotionClip 40.00 Â± \\pm 0.0039 13.73 Â± \\pm 0.0056 34.42 Â± \\pm 0.0069 Ours 58.24 Â± \\pm 0.0028 33.45 Â± \\pm 0.0052 46.47 Â± \\pm 0.0065 TABLE IV : RESULT OBTAINED BY COMBINING OUR METHOD WITH ROME OR CVTHEAD FOR TEXTURE GENERATION ON THE CK+ AND CELEBV-HQ DATASETS, RESPECTIVELY. NOTE THAT â†“ \\downarrow INDICATES THAT LOWER IS BETTER, WHEREAS â†‘ \\uparrow DENOTES THAT THE HIGHER IS BETTER. Dataset Method L â†“ 1 {}_{1}\\downarrow PSNR â†‘ {\\uparrow} LPIPS â†“ {\\downarrow} MS_SSIM â†‘ {\\uparrow} CK+ Ours+ROME 0.199 9.939 0.490 0.347 Ours+CVTHead 0.005 33.74 0.021 0.978 CelebV-HQ Ours+ROME 0.168 11.316 0.505 0.418 Ours+CVTHead 0.003 37.08 0.010 0.990 Figure 10 : Qualitative comparison between 3D expressive heads that were generated by MotionClip and our method, respectively. The input, consisting of a shape image and instruction, was used for generating expressions 1 and 2 using either MotionClip (b) or Ours (c) on CK+ and CelebV-HQ (d, e), respectively. Figure 11 : Qualitative comparison between generated 3D expressive faces when our method is combined with texture generation [ 18 , 26 ] to generate the desired expressions. The input (a), consisting of the source image and instruction, was used for generating expressions 1 and 2 using either Ours+ROME (b) or Ours+CVTHead (c) on CK+ and CelebV-HQ (d, e), respectively. Figure 12 : Successive snapshots of the generated 3D expressive faces that include a neutral expression between two facial expressions instructed by text prompts. As the present study aims to generate an avatar with a 3D facial expression from a 2D image of a face and transform the facial emotion according to the given instructions, two major challenges arise (1) facial expression transition based on expression information given by the instruction and (2) rendering the facial appearance such that it corresponds with the desired facial expression. Two experiments were prepared to evaluate the results obtained with the proposed framework both quantitatively and qualitatively. IV-D 1 Quantitative Comparison Two tasks are prepared for quantitative comparison. Facial Expression Transition The DECA encoder extracts the shape, facial expression, and pose from the source image. At the same time, the CLIP encoder combined with IFED and I2FET was used to interpret the text instruction: the initial facial expression, the final facial expression, and pose variation. To compare our approach with MotionClip, the first accuracy metric (Acc 1 ) was used to count the correctly classified samples for all expression labels in the instructions. The other one (Acc 2 ) was used to measure the correctly classified samples corresponding to both expression labels presented in each input instruction. The result suggests that the accuracy of our model is higher than that of MotionClip on both datasets as summarized in Table II . For example, our model reaches 91.44 % accuracy for generating an independent expression and 84.03 % for generating expression sequences on the CK+ dataset. The Gmean of our model is 80.30%. For the CelebV-HQ dataset, which is the real-world dataset, our model scores 58.24 %, 33.45 %, and 46.47 % for Acc 1 , Acc 2 , and Gmean, respectively, as indicated in Table III . This result suggests that our model outperforms MotionClip for learning the facial expression parameters. Visualization with t-SNE : Fig. 7 shows three learned latent representations, i.e. with MotionClip, I2FET, and I2FET with IFED, for seven facial expressions. These visual representations indicate that segregation between facial expressions is highly distinctive when IFED is combined with I2FET, compared with either MotionClip or I2FET alone. Confusion Matrices Analysis : Fig. 8 and Fig. 9 show that IFED enhances the accuracy across most classes compared to MotionClip on both datasets. Yet, a few case of misclassification occurred between certain classes of expressions, such as anger and sadness, or contempt and anger with sadness on CK+. In real-world datasets such as CelebV-HQ, some misclassification occurs between classes, particularly those with challenging expressions such as fear and surprise, or contempt and neutral. Face Rendering : For the face-rendering task, FET was combined with state-of-the-art models [ 18 , 26 ] for rendering facial appearances that correspond to specific expressions requested by instruction. This was accomplished by integrating whether ROME [ 18 ] or CVTHead [ 26 ] with facial expression transition. In this case, DECA extracts facial parameters to render geometrical information for a specific face. Instead of shapes, the RGB images and instructions are used to evaluate the performance. The L 1 loss, Peak Signal to Noise Ratio (PSNR), Learned Perceptual Image Patch Similarity (LPIPS) [ 42 ] , and Multi-Scale Structured Similarity (MS-SSIM) were used to measure the quality of reconstruction between the target image and synthesized image in the generated expression. As indicated by Table IV , our model with CVTHead produces the best result. IV-D 2 Qualitative Comparison In a similar vein, two tasks were prepared for qualitative comparisons. Facial Expression Transition : Fig. 10 compares sequences of facial expressions, generated by Motionclip and our method on CK+ and CelebV-HQ, respectively. These results clearly show that our method can generate facial expressions that convey more accurate expressions than MotionClip. Face Rendering : The resulting expressions can be qualitatively compared by viewing and deciding which expression looks better and more natural than the other. Fig. 11 shows the generated facial expressions based on learning the latent space of facial parameters on both datasets. The viewer may notice the difference between (b) and (c) for the CK+ dataset, and between (d) and (e) for the CelebV-HQ dataset, respectively, in terms of facial expressions and facial texture. For instance, the facial transition from surprise to happiness seen in the 2nd row is clearer in (c) than in (b), and the facial texture seen in the 4th row is much smoother in (c) than in (b). A similar observation can be made between (d) and (e), suggesting that Ours+CVTHead is superior to than Ours+ROME. For instance, in some cases, those enclosed within dashed boxes in Fig. 11 , a subtle distortion arose either when the facial region of the source image was occluded or when the instructions describe expressions with significant pose changes, such as transitioning from surprise to happiness or from fear to surprise. This led to inconsistencies, and missing details in the generated facial regions compared to Ours+CVTHead. A comparison between the two datasets, such as between (b) and (d) or between (c) and (e), reveals that the visual quality of CK+ is better than that of CelebV-HQ. These results confirm that our model has outstanding ability to transform facial expressions, learned on CK+ or CelebV-HQ, to a specific facial image. In addition, a user study was carried out by requesting 24 subjects to view 60 randomly paired images of various identities selected from either CVTHead + MotionClip or CVTHead + Ours. Subjects were asked to choose the expression closest to the expressions specified in the instructions or indicate whether none of them matched the provided expressions. Naturally, they were unaware as to which algorithm was used to the presented images. Four subjects viewed each video and voted for the one that more closely resembled the expression in the instructions. In cases in which the votes were tied, an additional subject was asked to make the final decision. Based on these outcomes, CVTHead + Ours achieved 73% accuracy, whereas CVTHead + MotionClip yielded 58% accuracy for CK+. For CelebV-HQ, CVTHead + Ours and CVTHead + MotionClip achieved 77% and 70.5% accuracy, respectively."
        },
        {
          "section_index": 20,
          "heading_level": "h4",
          "heading": "IV-D 1 Quantitative Comparison",
          "text": "Two tasks are prepared for quantitative comparison. Facial Expression Transition The DECA encoder extracts the shape, facial expression, and pose from the source image. At the same time, the CLIP encoder combined with IFED and I2FET was used to interpret the text instruction: the initial facial expression, the final facial expression, and pose variation. To compare our approach with MotionClip, the first accuracy metric (Acc 1 ) was used to count the correctly classified samples for all expression labels in the instructions. The other one (Acc 2 ) was used to measure the correctly classified samples corresponding to both expression labels presented in each input instruction. The result suggests that the accuracy of our model is higher than that of MotionClip on both datasets as summarized in Table II . For example, our model reaches 91.44 % accuracy for generating an independent expression and 84.03 % for generating expression sequences on the CK+ dataset. The Gmean of our model is 80.30%. For the CelebV-HQ dataset, which is the real-world dataset, our model scores 58.24 %, 33.45 %, and 46.47 % for Acc 1 , Acc 2 , and Gmean, respectively, as indicated in Table III . This result suggests that our model outperforms MotionClip for learning the facial expression parameters. Visualization with t-SNE : Fig. 7 shows three learned latent representations, i.e. with MotionClip, I2FET, and I2FET with IFED, for seven facial expressions. These visual representations indicate that segregation between facial expressions is highly distinctive when IFED is combined with I2FET, compared with either MotionClip or I2FET alone. Confusion Matrices Analysis : Fig. 8 and Fig. 9 show that IFED enhances the accuracy across most classes compared to MotionClip on both datasets. Yet, a few case of misclassification occurred between certain classes of expressions, such as anger and sadness, or contempt and anger with sadness on CK+. In real-world datasets such as CelebV-HQ, some misclassification occurs between classes, particularly those with challenging expressions such as fear and surprise, or contempt and neutral. Face Rendering : For the face-rendering task, FET was combined with state-of-the-art models [ 18 , 26 ] for rendering facial appearances that correspond to specific expressions requested by instruction. This was accomplished by integrating whether ROME [ 18 ] or CVTHead [ 26 ] with facial expression transition. In this case, DECA extracts facial parameters to render geometrical information for a specific face. Instead of shapes, the RGB images and instructions are used to evaluate the performance. The L 1 loss, Peak Signal to Noise Ratio (PSNR), Learned Perceptual Image Patch Similarity (LPIPS) [ 42 ] , and Multi-Scale Structured Similarity (MS-SSIM) were used to measure the quality of reconstruction between the target image and synthesized image in the generated expression. As indicated by Table IV , our model with CVTHead produces the best result."
        },
        {
          "section_index": 21,
          "heading_level": "h4",
          "heading": "IV-D 2 Qualitative Comparison",
          "text": "In a similar vein, two tasks were prepared for qualitative comparisons. Facial Expression Transition : Fig. 10 compares sequences of facial expressions, generated by Motionclip and our method on CK+ and CelebV-HQ, respectively. These results clearly show that our method can generate facial expressions that convey more accurate expressions than MotionClip. Face Rendering : The resulting expressions can be qualitatively compared by viewing and deciding which expression looks better and more natural than the other. Fig. 11 shows the generated facial expressions based on learning the latent space of facial parameters on both datasets. The viewer may notice the difference between (b) and (c) for the CK+ dataset, and between (d) and (e) for the CelebV-HQ dataset, respectively, in terms of facial expressions and facial texture. For instance, the facial transition from surprise to happiness seen in the 2nd row is clearer in (c) than in (b), and the facial texture seen in the 4th row is much smoother in (c) than in (b). A similar observation can be made between (d) and (e), suggesting that Ours+CVTHead is superior to than Ours+ROME. For instance, in some cases, those enclosed within dashed boxes in Fig. 11 , a subtle distortion arose either when the facial region of the source image was occluded or when the instructions describe expressions with significant pose changes, such as transitioning from surprise to happiness or from fear to surprise. This led to inconsistencies, and missing details in the generated facial regions compared to Ours+CVTHead. A comparison between the two datasets, such as between (b) and (d) or between (c) and (e), reveals that the visual quality of CK+ is better than that of CelebV-HQ. These results confirm that our model has outstanding ability to transform facial expressions, learned on CK+ or CelebV-HQ, to a specific facial image. In addition, a user study was carried out by requesting 24 subjects to view 60 randomly paired images of various identities selected from either CVTHead + MotionClip or CVTHead + Ours. Subjects were asked to choose the expression closest to the expressions specified in the instructions or indicate whether none of them matched the provided expressions. Naturally, they were unaware as to which algorithm was used to the presented images. Four subjects viewed each video and voted for the one that more closely resembled the expression in the instructions. In cases in which the votes were tied, an additional subject was asked to make the final decision. Based on these outcomes, CVTHead + Ours achieved 73% accuracy, whereas CVTHead + MotionClip yielded 58% accuracy for CK+. For CelebV-HQ, CVTHead + Ours and CVTHead + MotionClip achieved 77% and 70.5% accuracy, respectively."
        },
        {
          "section_index": 22,
          "heading_level": "h4",
          "heading": "IV-D 3 Inference Time",
          "text": "During inference, CVTHead + MotionClip takes 3.97 seconds to generate a video, whereas CVTHead + Ours only takes 3.92 seconds on a single NVIDIA RTX A6000, suggesting that our model is a slightly faster than MotionClip."
        },
        {
          "section_index": 23,
          "heading_level": "h3",
          "heading": "IV-E Ablation Study",
          "text": "This section presents a series of ablation studies that were carried out to evaluate the performance of our model. IV-E 1 Effectiveness of the Proposed Components To demonstrate the effectiveness of each proposed component, we conducted a comparative experiment, as shown in Table V . It can be observed that the IFED module significantly improves the performance of I2FET compared to I2FET without IFED on both CK+ and CelebV-HQ datasets. Additionally, integration of the vertex loss function with IFED increases Acc 1 and Gmean on both datasets. Despite a slight decrease in Acc 2 on CK+, it still performs well on CelebV-HQ. TABLE V : COMPARISON OF THE ACCURACY BETWEEN THE PROPOSED DIFFERENT COMPONENTS FOR OUR METHOD (I2FET) ON THE TWO DATASETS Dataset Method Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow w/o IFED 76.89 Â± \\pm 0.0029 63.23 Â± \\pm 0.0047 63.59 Â± \\pm 0.0059 w/ IFED 91.30 Â± \\pm 0.0017 84.06 Â± \\pm 0.0026 79.41 Â± \\pm 0.0061 CK+ w/ IFED 91.44 Â± \\pm 0.0024 84.03 Â± \\pm 0.0034 80.30 Â± \\pm 0.0064 + w/ â„’ v \\mathcal{L}_{v} w/o IFED 48.35 Â± \\pm 0.0037 22.1 Â± \\pm 0.0040 32.88 Â± \\pm 0.0058 w/ IFED 57.80 Â± \\pm 0.0020 32.60 Â± \\pm 0.0037 46.04 Â± \\pm 0.0062 CelebV-HQ w/ IFED 58.24 Â± \\pm 0.0028 33.45 Â± \\pm 0.0052 46.47 Â± \\pm 0.0065 + w/ â„’ v \\mathcal{L}_{v} IV-E 2 Impact of Three Loss Functions To analyze the impact of each loss function in I2FET, an experiment was carried out to compare the performance of the model in various scenarios. Specifically, IFED was integrated with the I2FET model and each loss function was incrementally added to determine its impact on the model performance of the model. The results in Table VI indicate that the performance improved when the pose loss function integrated with the expression loss function, whereas the vertex loss function yielded a slight improvement when used alongside both the pose and expression loss functions. In this study, the vertex loss was integrated with other loss functions to capture the geometry of the face, to enhance the accuracy of facial expression recognition. Fig 13 and 14 are the confusion matrix of our model when the total loss function integrated with â„’ v \\mathcal{L}_{v} or without â„’ v \\mathcal{L}_{v} on the CK+ and CelebV-HQ datasets. Note that the fear accuracies for both datasets rather decrease slightly when the vertex loss is applied. It is known that the fear expression often overlaps with other expressions (e.g., surprise and happiness), thereby making it challenging to distinguish. Moreover, since the fear portion of CK+ is much larger than that of CelebV-HQ in Fig. 5, it appears that confusion during the facial expression transition has a reverse impact on Acc 2 as shown in Table VI (a) (b) Figure 13 : Confusion matrices for seven facial expressions by I2FET (a) without â„’ v \\mathcal{L}_{v} and (b) with â„’ v \\mathcal{L}_{v} on CK+. (a) (b) Figure 14 : Confusion matrices for eight facial expressions by I2FET (a) without â„’ v \\mathcal{L}_{v} and (b) with â„’ v \\mathcal{L}_{v} on CelebV-HQ. TABLE VI : COMPARISON OF THE IMPACT OF LOSS FUNCTIONS ON THE PERFORMANCE OF OUR METHOD (I2FET) ACROSS THE TWO DATASETS Dataset Method Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow â„’ e \\mathcal{L}_{e} 66.80 Â± \\pm 0.0056 45.96 Â± \\pm 0.0112 52.42 Â± \\pm 0.0730 â„’ e \\mathcal{L}_{e} + â„’ p \\mathcal{L}_{p} 91.30 Â± \\pm 0.0017 84.06 Â± \\pm 0.0026 79.41 Â± \\pm 0.0061 CK+ â„’ e \\mathcal{L}_{e} + â„’ p \\mathcal{L}_{p} + â„’ v \\mathcal{L}_{v} 91.44 Â± \\pm 0.0024 84.03 Â± \\pm 0.0034 80.30 Â± \\pm 0.0064 â„’ e \\mathcal{L}_{e} 37.57 Â± \\pm 0.0052 11.99 Â± \\pm 0.0326 25.82 Â± \\pm 0.0114 â„’ e \\mathcal{L}_{e} + â„’ p \\mathcal{L}_{p} 57.80 Â± \\pm 0.0020 32.60 Â± \\pm 0.0037 46.04 Â± \\pm 0.0062 CelebV-HQ â„’ e \\mathcal{L}_{e} + â„’ p \\mathcal{L}_{p} + â„’ v \\mathcal{L}_{v} 58.24 Â± \\pm 0.0028 33.45 Â± \\pm 0.0052 46.47 Â± \\pm 0.0065 IV-E 3 Impact of IFED on the Training Process This experiment was conducted to compare the two loss functions with and without incorporating the IFED module in I2FET. Fig. 15(a) and Fig. 15(b) show the results for CK+ and CelebV-HQ, respectively, confirming that I2FED plays an important role. IV-E 4 Effect of IFED on Intensity of Facial Expression Fig. 16 demonstrates that IFED helps I2FED to generate high-intensity facial expressions. (a) (b) Figure 15 : Comparison of the validation loss during the training process of I2FET w/ and w/o IFED, respectively, on (a) CK+ and (b) CelebV-HQ. The green curves represent I2FET without IFED, whereas the blue curves are the results for I2FET with IFED. Figure 16 : Qualitative comparison between 3D expressive faces generated (a) w/o and (b) w/ IFED. IV-E 5 Impact of Quantity of CAFT Layer The experiments were conducted to evaluate the results of the proposed I2FET model by designing the IFED module with varying numbers of CAFT layers. We fixed the number of transformer layers to 1 in both branches and adjusted the number of CAFT layers. The results in Table VII indicate that increasing the quantity of CAFT layers does not improve the performance of the proposed I2FET. IV-E 6 Impact of Quantity of Transformer Encoder in Facial Parameter Branch and Text Branch In this experiment, we utilized different configurations by adjusting the number of transformer encoder layers of IFED. The findings suggest that the I2FET model performs better when N = 2 and M = 1 compared to other configurations, particularly in terms of Acc 1 and G-mean, as shown in Table VIII . TABLE VII : COMPARISON OF THE ACCURACY OF THE I2FET METHOD ON BOTH DATASETS BY VARYING THE NUMBER OF CAFT LAYERS IN I2FED Dataset #Layers Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow 1 91.36 Â± \\pm 0.0023 84.13 Â± \\pm 0.0046 79.65 Â± \\pm 0.0080 CK+ 2 88.95 Â± \\pm 0.0019 80.95 Â± \\pm 0.0038 78.72 Â± \\pm 0.0071 1 58.25 Â± \\pm 0.0034 33.03 Â± \\pm 0.0056 46.46 Â± \\pm 0.0066 CelebV-HQ 2 55.68 Â± \\pm 0.0034 29.82 Â± \\pm 0.0066 42.59 Â± \\pm 0.0085 TABLE VIII : COMPARISON OF THE ACCURACY OF THE PROPOSED METHOD WITH THE NUMBER OF TRANSFORMER LAYERS FOR FACIAL PARAMETERS BRANCH ( N ) AND TEXT BRANCH ( M ) IN TFED ON CK+ Mode N M Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow A 1 1 91.36 Â± \\pm 0.0023 84.13 Â± \\pm 0.0046 79.67 Â± \\pm 0.008 B 2 1 91.44 Â± \\pm 0.0024 84.03 Â± \\pm 0.0034 80.30 Â± \\pm 0.0064 C 1 2 91.27 Â± \\pm 0.0019 83.84 Â± \\pm 0.0037 79.69 Â± \\pm 0.0052 D 2 2 89.07 Â± \\pm 0.0021 81.13 Â± \\pm 0.0034 78.71 Â± \\pm 0.0070"
        },
        {
          "section_index": 24,
          "heading_level": "h4",
          "heading": "IV-E 1 Effectiveness of the Proposed Components",
          "text": "To demonstrate the effectiveness of each proposed component, we conducted a comparative experiment, as shown in Table V . It can be observed that the IFED module significantly improves the performance of I2FET compared to I2FET without IFED on both CK+ and CelebV-HQ datasets. Additionally, integration of the vertex loss function with IFED increases Acc 1 and Gmean on both datasets. Despite a slight decrease in Acc 2 on CK+, it still performs well on CelebV-HQ. TABLE V : COMPARISON OF THE ACCURACY BETWEEN THE PROPOSED DIFFERENT COMPONENTS FOR OUR METHOD (I2FET) ON THE TWO DATASETS Dataset Method Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow w/o IFED 76.89 Â± \\pm 0.0029 63.23 Â± \\pm 0.0047 63.59 Â± \\pm 0.0059 w/ IFED 91.30 Â± \\pm 0.0017 84.06 Â± \\pm 0.0026 79.41 Â± \\pm 0.0061 CK+ w/ IFED 91.44 Â± \\pm 0.0024 84.03 Â± \\pm 0.0034 80.30 Â± \\pm 0.0064 + w/ â„’ v \\mathcal{L}_{v} w/o IFED 48.35 Â± \\pm 0.0037 22.1 Â± \\pm 0.0040 32.88 Â± \\pm 0.0058 w/ IFED 57.80 Â± \\pm 0.0020 32.60 Â± \\pm 0.0037 46.04 Â± \\pm 0.0062 CelebV-HQ w/ IFED 58.24 Â± \\pm 0.0028 33.45 Â± \\pm 0.0052 46.47 Â± \\pm 0.0065 + w/ â„’ v \\mathcal{L}_{v}"
        },
        {
          "section_index": 25,
          "heading_level": "h4",
          "heading": "IV-E 2 Impact of Three Loss Functions",
          "text": "To analyze the impact of each loss function in I2FET, an experiment was carried out to compare the performance of the model in various scenarios. Specifically, IFED was integrated with the I2FET model and each loss function was incrementally added to determine its impact on the model performance of the model. The results in Table VI indicate that the performance improved when the pose loss function integrated with the expression loss function, whereas the vertex loss function yielded a slight improvement when used alongside both the pose and expression loss functions. In this study, the vertex loss was integrated with other loss functions to capture the geometry of the face, to enhance the accuracy of facial expression recognition. Fig 13 and 14 are the confusion matrix of our model when the total loss function integrated with â„’ v \\mathcal{L}_{v} or without â„’ v \\mathcal{L}_{v} on the CK+ and CelebV-HQ datasets. Note that the fear accuracies for both datasets rather decrease slightly when the vertex loss is applied. It is known that the fear expression often overlaps with other expressions (e.g., surprise and happiness), thereby making it challenging to distinguish. Moreover, since the fear portion of CK+ is much larger than that of CelebV-HQ in Fig. 5, it appears that confusion during the facial expression transition has a reverse impact on Acc 2 as shown in Table VI (a) (b) Figure 13 : Confusion matrices for seven facial expressions by I2FET (a) without â„’ v \\mathcal{L}_{v} and (b) with â„’ v \\mathcal{L}_{v} on CK+. (a) (b) Figure 14 : Confusion matrices for eight facial expressions by I2FET (a) without â„’ v \\mathcal{L}_{v} and (b) with â„’ v \\mathcal{L}_{v} on CelebV-HQ. TABLE VI : COMPARISON OF THE IMPACT OF LOSS FUNCTIONS ON THE PERFORMANCE OF OUR METHOD (I2FET) ACROSS THE TWO DATASETS Dataset Method Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow â„’ e \\mathcal{L}_{e} 66.80 Â± \\pm 0.0056 45.96 Â± \\pm 0.0112 52.42 Â± \\pm 0.0730 â„’ e \\mathcal{L}_{e} + â„’ p \\mathcal{L}_{p} 91.30 Â± \\pm 0.0017 84.06 Â± \\pm 0.0026 79.41 Â± \\pm 0.0061 CK+ â„’ e \\mathcal{L}_{e} + â„’ p \\mathcal{L}_{p} + â„’ v \\mathcal{L}_{v} 91.44 Â± \\pm 0.0024 84.03 Â± \\pm 0.0034 80.30 Â± \\pm 0.0064 â„’ e \\mathcal{L}_{e} 37.57 Â± \\pm 0.0052 11.99 Â± \\pm 0.0326 25.82 Â± \\pm 0.0114 â„’ e \\mathcal{L}_{e} + â„’ p \\mathcal{L}_{p} 57.80 Â± \\pm 0.0020 32.60 Â± \\pm 0.0037 46.04 Â± \\pm 0.0062 CelebV-HQ â„’ e \\mathcal{L}_{e} + â„’ p \\mathcal{L}_{p} + â„’ v \\mathcal{L}_{v} 58.24 Â± \\pm 0.0028 33.45 Â± \\pm 0.0052 46.47 Â± \\pm 0.0065"
        },
        {
          "section_index": 26,
          "heading_level": "h4",
          "heading": "IV-E 3 Impact of IFED on the Training Process",
          "text": "This experiment was conducted to compare the two loss functions with and without incorporating the IFED module in I2FET. Fig. 15(a) and Fig. 15(b) show the results for CK+ and CelebV-HQ, respectively, confirming that I2FED plays an important role."
        },
        {
          "section_index": 27,
          "heading_level": "h4",
          "heading": "IV-E 4 Effect of IFED on Intensity of Facial Expression",
          "text": "Fig. 16 demonstrates that IFED helps I2FED to generate high-intensity facial expressions. (a) (b) Figure 15 : Comparison of the validation loss during the training process of I2FET w/ and w/o IFED, respectively, on (a) CK+ and (b) CelebV-HQ. The green curves represent I2FET without IFED, whereas the blue curves are the results for I2FET with IFED. Figure 16 : Qualitative comparison between 3D expressive faces generated (a) w/o and (b) w/ IFED."
        },
        {
          "section_index": 28,
          "heading_level": "h4",
          "heading": "IV-E 5 Impact of Quantity of CAFT Layer",
          "text": "The experiments were conducted to evaluate the results of the proposed I2FET model by designing the IFED module with varying numbers of CAFT layers. We fixed the number of transformer layers to 1 in both branches and adjusted the number of CAFT layers. The results in Table VII indicate that increasing the quantity of CAFT layers does not improve the performance of the proposed I2FET."
        },
        {
          "section_index": 29,
          "heading_level": "h4",
          "heading": "IV-E 6 Impact of Quantity of Transformer Encoder in Facial Parameter Branch and Text Branch",
          "text": "In this experiment, we utilized different configurations by adjusting the number of transformer encoder layers of IFED. The findings suggest that the I2FET model performs better when N = 2 and M = 1 compared to other configurations, particularly in terms of Acc 1 and G-mean, as shown in Table VIII . TABLE VII : COMPARISON OF THE ACCURACY OF THE I2FET METHOD ON BOTH DATASETS BY VARYING THE NUMBER OF CAFT LAYERS IN I2FED Dataset #Layers Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow 1 91.36 Â± \\pm 0.0023 84.13 Â± \\pm 0.0046 79.65 Â± \\pm 0.0080 CK+ 2 88.95 Â± \\pm 0.0019 80.95 Â± \\pm 0.0038 78.72 Â± \\pm 0.0071 1 58.25 Â± \\pm 0.0034 33.03 Â± \\pm 0.0056 46.46 Â± \\pm 0.0066 CelebV-HQ 2 55.68 Â± \\pm 0.0034 29.82 Â± \\pm 0.0066 42.59 Â± \\pm 0.0085 TABLE VIII : COMPARISON OF THE ACCURACY OF THE PROPOSED METHOD WITH THE NUMBER OF TRANSFORMER LAYERS FOR FACIAL PARAMETERS BRANCH ( N ) AND TEXT BRANCH ( M ) IN TFED ON CK+ Mode N M Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow A 1 1 91.36 Â± \\pm 0.0023 84.13 Â± \\pm 0.0046 79.67 Â± \\pm 0.008 B 2 1 91.44 Â± \\pm 0.0024 84.03 Â± \\pm 0.0034 80.30 Â± \\pm 0.0064 C 1 2 91.27 Â± \\pm 0.0019 83.84 Â± \\pm 0.0037 79.69 Â± \\pm 0.0052 D 2 2 89.07 Â± \\pm 0.0021 81.13 Â± \\pm 0.0034 78.71 Â± \\pm 0.0070"
        },
        {
          "section_index": 30,
          "heading_level": "h3",
          "heading": "IV-F Longer Facial Expression Generation with Neutral Expressions",
          "text": "According to appraisal theory [ 35 ] , which is one of the major emotion theories, the dynamic of emotion is described as the sequential check theory of emotion differentiation. In other words, the differentiation of emotional states of humans is the result of a sequence of specific stimulus evaluation appraisal checks, rather than an action of hopping from one emotional state to another in a discrete manner. In our framework, the transition of the facial expression from A to B is similar to the differentiation of emotional states in a sequence: starting from A facial expression with strong emotion a neutral expression and then moving to strong facial expression B. In addition to appraisal theory, the present approach is suitable for investigating facial behaviors within the pleasure-arousal (P-A) space [ 34 ] , where the neutral expression serves as a transitional expression between two specific expressions. For this purpose, a neutral encoder-decoder (NED) was adopted to generate neutral faces. The aim thereof was to improve the performance of our framework when a neutral expression is not available in the instruction to increase its flexibility for extending facial expression sequences. IV-F 1 Implementation Details In this scenario, two datasets containing the parameters of neutral expressions and poses were constructed the pretrained DECA to extract the data from the CelebV-HQ and CK+ datasets. As the CK+ dataset does not contain neutral expressions, the first frame of each video was extracted and considered as the representative neutral case. Afterward, NED was trained on these two neutral datasets. IV-F 2 Network Architecture The neutral encoder-decoder model was constructed with fully connected layers. The encoder consisted of four fully connected layers, whereas the decoder was structured with four fully connected layers dedicated to pose and expression, respectively. IV-F 3 Analysis Similarly, we also conducted experiments to evaluate neutral expression generation for both quantitative and qualitative comparisons. Quantitative comparison : Specifically, the accuracy of NED on the CK+ and CelebV-HQ datasets was 49.5% and 51.33% for generating the neutral expression, respectively. These results are presented in Table IX , which confirms that NED + CVTHead outperforms NED + ROME. Qualitative comparison : Fig. 17 visualizes neutral expressions generated by NED + CVTHead on CK+ and CelebV-HQ. Moreover, we also illustrate neutral expressions generated by FLAME with the expression parameter set to zero, as shown in Fig. 17 c. The results show that the facial units differ, depending on whether the environment is controllable or non-controllable, or whether FLAME was used. This indicates that neutral expressions are more complex in real-world scenarios, leading us to prefer using a neural encoder-decoder instead of the expression parameter settings provided by FLAME Figure 17 : Samples of facial images with a neutral expression generated by NED+CVTHead. The input images are shown in the leftmost column (Source). The images generated using CK+ dataset (controllable environment) are shown in (a), whereas those using CelebV-HQ dataset (non-controllable environment) are shown in (b). On the other hand, the generated 3D face with FLAME is depicted by setting the expression parameter to zero in (c). Note that subtle differences exist in the image quality between the three generated cases. TABLE IX : RESULT OBTAINED WHEN NED IS COMBINED WITH ROME OR CVTHEAD FOR TEXTURE GENERATION ON CK+ AND CELEBV-HQ, RESPECTIVELY. Dataset Method L â†“ 1 {}_{1}\\downarrow PSNR â†‘ {\\uparrow} LPIPS â†“ {\\downarrow} MS_SSIM â†‘ {\\uparrow} CK+ NED + ROME 0.193 10.207 0.516 0.348 NED + CVTHead 0.004 35.158 0.014 0.987 CelebV-HQ NED + ROME 0.183 10.818 0.498 0.407 NED + CVTHead 0.004 36.347 0.0117 0.989 In a user study similar to that described in Section IV-D 2 , we asked subjects to determine whether the generated images display neutral expressions. Most subjects succeeded in identifying the neutral expressions produced by our model on CK+, and they had difficulties with only 4% of the samples generated by the learned model on CelebV-HQ. TABLE X : COMPARISON OF FACIAL EXPRESSION CLASSIFIERS ON CK+ AND CELEBV-HQ WITH SAMPLES ROTATED AT VARIOUS ANGLES { 0 o , Â± 10 o , Â± 15 o , Â± 30 o } \\{0^{o},\\pm 10^{o},\\pm 15^{o},\\pm 30^{o}\\} 0 o Â± \\pm 10 o Â± \\pm 15 o Â± \\pm 30 o Dataset Method Acc 1 â†‘ \\uparrow G-mean â†‘ \\uparrow Acc 1 â†‘ \\uparrow G-mean â†‘ \\uparrow Acc 1 â†‘ \\uparrow G-mean â†‘ \\uparrow Acc 1 â†‘ \\uparrow G-mean â†‘ \\uparrow MEK [ 43 ] 100 100 86.41 76.20 76.01 52.16 32.56 0.0 CK+ ResNet-101 [ 14 ] 99.56 99.4 84.40 73.28 79.50 67.6 75.29 33.39 ResNet-101 with RFL 99.69 99.69 87.18 75.85 83.66 66.13 77.48 31.5 MEK [ 43 ] 99.75 99.83 60.49 55.07 49.08 33.84 32.05 9.64 CelebV-HQ ResNet-101 [ 14 ] 37 12.77 35.77 13.39 34.97 0.0 28.92 0.0 ResNet-101 with RFL 93.67 95.04 58.54 49.92 51.53 41.22 40.71 26.80"
        },
        {
          "section_index": 31,
          "heading_level": "h4",
          "heading": "IV-F 1 Implementation Details",
          "text": "In this scenario, two datasets containing the parameters of neutral expressions and poses were constructed the pretrained DECA to extract the data from the CelebV-HQ and CK+ datasets. As the CK+ dataset does not contain neutral expressions, the first frame of each video was extracted and considered as the representative neutral case. Afterward, NED was trained on these two neutral datasets."
        },
        {
          "section_index": 32,
          "heading_level": "h4",
          "heading": "IV-F 2 Network Architecture",
          "text": "The neutral encoder-decoder model was constructed with fully connected layers. The encoder consisted of four fully connected layers, whereas the decoder was structured with four fully connected layers dedicated to pose and expression, respectively."
        },
        {
          "section_index": 33,
          "heading_level": "h4",
          "heading": "IV-F 3 Analysis",
          "text": "Similarly, we also conducted experiments to evaluate neutral expression generation for both quantitative and qualitative comparisons. Quantitative comparison : Specifically, the accuracy of NED on the CK+ and CelebV-HQ datasets was 49.5% and 51.33% for generating the neutral expression, respectively. These results are presented in Table IX , which confirms that NED + CVTHead outperforms NED + ROME. Qualitative comparison : Fig. 17 visualizes neutral expressions generated by NED + CVTHead on CK+ and CelebV-HQ. Moreover, we also illustrate neutral expressions generated by FLAME with the expression parameter set to zero, as shown in Fig. 17 c. The results show that the facial units differ, depending on whether the environment is controllable or non-controllable, or whether FLAME was used. This indicates that neutral expressions are more complex in real-world scenarios, leading us to prefer using a neural encoder-decoder instead of the expression parameter settings provided by FLAME Figure 17 : Samples of facial images with a neutral expression generated by NED+CVTHead. The input images are shown in the leftmost column (Source). The images generated using CK+ dataset (controllable environment) are shown in (a), whereas those using CelebV-HQ dataset (non-controllable environment) are shown in (b). On the other hand, the generated 3D face with FLAME is depicted by setting the expression parameter to zero in (c). Note that subtle differences exist in the image quality between the three generated cases. TABLE IX : RESULT OBTAINED WHEN NED IS COMBINED WITH ROME OR CVTHEAD FOR TEXTURE GENERATION ON CK+ AND CELEBV-HQ, RESPECTIVELY. Dataset Method L â†“ 1 {}_{1}\\downarrow PSNR â†‘ {\\uparrow} LPIPS â†“ {\\downarrow} MS_SSIM â†‘ {\\uparrow} CK+ NED + ROME 0.193 10.207 0.516 0.348 NED + CVTHead 0.004 35.158 0.014 0.987 CelebV-HQ NED + ROME 0.183 10.818 0.498 0.407 NED + CVTHead 0.004 36.347 0.0117 0.989 In a user study similar to that described in Section IV-D 2 , we asked subjects to determine whether the generated images display neutral expressions. Most subjects succeeded in identifying the neutral expressions produced by our model on CK+, and they had difficulties with only 4% of the samples generated by the learned model on CelebV-HQ. TABLE X : COMPARISON OF FACIAL EXPRESSION CLASSIFIERS ON CK+ AND CELEBV-HQ WITH SAMPLES ROTATED AT VARIOUS ANGLES { 0 o , Â± 10 o , Â± 15 o , Â± 30 o } \\{0^{o},\\pm 10^{o},\\pm 15^{o},\\pm 30^{o}\\} 0 o Â± \\pm 10 o Â± \\pm 15 o Â± \\pm 30 o Dataset Method Acc 1 â†‘ \\uparrow G-mean â†‘ \\uparrow Acc 1 â†‘ \\uparrow G-mean â†‘ \\uparrow Acc 1 â†‘ \\uparrow G-mean â†‘ \\uparrow Acc 1 â†‘ \\uparrow G-mean â†‘ \\uparrow MEK [ 43 ] 100 100 86.41 76.20 76.01 52.16 32.56 0.0 CK+ ResNet-101 [ 14 ] 99.56 99.4 84.40 73.28 79.50 67.6 75.29 33.39 ResNet-101 with RFL 99.69 99.69 87.18 75.85 83.66 66.13 77.48 31.5 MEK [ 43 ] 99.75 99.83 60.49 55.07 49.08 33.84 32.05 9.64 CelebV-HQ ResNet-101 [ 14 ] 37 12.77 35.77 13.39 34.97 0.0 28.92 0.0 ResNet-101 with RFL 93.67 95.04 58.54 49.92 51.53 41.22 40.71 26.80"
        },
        {
          "section_index": 34,
          "heading_level": "h3",
          "heading": "IV-G Facial Expression Classification",
          "text": "IV-G 1 Implementation For ResNet-101 [ 14 ] and ResNet-101 with RFL [ 5 ] , the learning rate was 1 â€‹ e âˆ’ 4 1e^{-4} and the batch size was 64. Adam was used to optimize the models with 50 epochs per cycle. For the MEK model, the configuration in [ 43 ] was used to optimize the training process for both the CK+ and CelebV-HQ datasets. IV-G 2 Analysis Based on the results in Table X , ResNet-101 with RFL was more stable compared to ResNet-101 or MEK in terms of their classification performance, suggesting that ResNet-101 with RFL can handle both imbalanced datasets and rotational transformations fairly well. In particular, MEK achieved the best result on the original images although its performance was affected by rotated images. ResNet-101 with RFL demonstrated more stable performance than ResNet-101 and MEK on CK+. For real-world scenarios such as those in the CelebV-HQ dataset, we obtained similar results. These findings indicate that the performance of facial expression classifiers deteriorates for faces that are rotated by Â± 15 \\pm 15 degrees or more. In addition, MEK performs better for the original datasets, but worse for the augmented datasets. The performance of ResNet-101 is high on datasets with low imbalance rates, whereas it is low on datasets with high imbalance rates. Meanwhile, ResNet-101 with RFL delivers stable performance on imbalanced and rotated datasets. Based on this result, we adopted ResNet-101 by default during evaluation. (a) (b) Figure 18 : Generated 3D faces (a) by changing the template sentences and (b) a few unsuccessful cases."
        },
        {
          "section_index": 35,
          "heading_level": "h4",
          "heading": "IV-G 1 Implementation",
          "text": "For ResNet-101 [ 14 ] and ResNet-101 with RFL [ 5 ] , the learning rate was 1 â€‹ e âˆ’ 4 1e^{-4} and the batch size was 64. Adam was used to optimize the models with 50 epochs per cycle. For the MEK model, the configuration in [ 43 ] was used to optimize the training process for both the CK+ and CelebV-HQ datasets."
        },
        {
          "section_index": 36,
          "heading_level": "h4",
          "heading": "IV-G 2 Analysis",
          "text": "Based on the results in Table X , ResNet-101 with RFL was more stable compared to ResNet-101 or MEK in terms of their classification performance, suggesting that ResNet-101 with RFL can handle both imbalanced datasets and rotational transformations fairly well. In particular, MEK achieved the best result on the original images although its performance was affected by rotated images. ResNet-101 with RFL demonstrated more stable performance than ResNet-101 and MEK on CK+. For real-world scenarios such as those in the CelebV-HQ dataset, we obtained similar results. These findings indicate that the performance of facial expression classifiers deteriorates for faces that are rotated by Â± 15 \\pm 15 degrees or more. In addition, MEK performs better for the original datasets, but worse for the augmented datasets. The performance of ResNet-101 is high on datasets with low imbalance rates, whereas it is low on datasets with high imbalance rates. Meanwhile, ResNet-101 with RFL delivers stable performance on imbalanced and rotated datasets. Based on this result, we adopted ResNet-101 by default during evaluation. (a) (b) Figure 18 : Generated 3D faces (a) by changing the template sentences and (b) a few unsuccessful cases."
        },
        {
          "section_index": 37,
          "heading_level": "h3",
          "heading": "IV-H Discussion",
          "text": "IV-H 1 Diversity of Facial Expression Sequences Our framework is capable of animating various facial expressions guided by instruction and facial images with pose variation, as illustrated in Fig 12 . Moreover, it can generate facial expressions corresponding to both controlled and real-world environments, enabling the generation of a diverse range of expressions for creating a long video. IV-H 2 Results on Instructions with Changes in Template Sentences Fig. 18(a) shows the outcomes obtained by changing the template sentences. The result suggests that our model can successfully generate facial expressions that closely resemble specific emotions described in the instruction. IV-H 3 Failure Cases Although our model performs well in many cases, the model failed in certain instances as exemplified in Fig 18(b) . In the top row, significant pose variation in the input image leads to regions of the shoulders being missing from the resulting image due to constraints within the pre-trained face-rendering model. Likewise, images in which regions of the head and those containing hair were missing from the source image also led to the lack of information in the face rendering process because of this constraint, as shown in the bottom row. In the second row, the generated result is restricted by the size of the expression vocabulary, causing a scared face to be transformed into an angry face."
        },
        {
          "section_index": 38,
          "heading_level": "h4",
          "heading": "IV-H 1 Diversity of Facial Expression Sequences",
          "text": "Our framework is capable of animating various facial expressions guided by instruction and facial images with pose variation, as illustrated in Fig 12 . Moreover, it can generate facial expressions corresponding to both controlled and real-world environments, enabling the generation of a diverse range of expressions for creating a long video."
        },
        {
          "section_index": 39,
          "heading_level": "h4",
          "heading": "IV-H 2 Results on Instructions with Changes in Template Sentences",
          "text": "Fig. 18(a) shows the outcomes obtained by changing the template sentences. The result suggests that our model can successfully generate facial expressions that closely resemble specific emotions described in the instruction."
        },
        {
          "section_index": 40,
          "heading_level": "h4",
          "heading": "IV-H 3 Failure Cases",
          "text": "Although our model performs well in many cases, the model failed in certain instances as exemplified in Fig 18(b) . In the top row, significant pose variation in the input image leads to regions of the shoulders being missing from the resulting image due to constraints within the pre-trained face-rendering model. Likewise, images in which regions of the head and those containing hair were missing from the source image also led to the lack of information in the face rendering process because of this constraint, as shown in the bottom row. In the second row, the generated result is restricted by the size of the expression vocabulary, causing a scared face to be transformed into an angry face."
        },
        {
          "section_index": 41,
          "heading_level": "h2",
          "heading": "V Conclusion",
          "text": "In this study, we present a novel framework for generating 3D facial expressions from an RGB image and animating facial transitions between two expressions as specified by entering text instructions. First, FET was introduced to produce a sequence of facial expressions. The Instruction-Driven Facial Expression Decomposer was designed to learn from multimodal data and capture correlations between textual descriptions and facial expression features. We proposed an Instruction to Facial Expression Transition model to generate target facial expressions guided by a single instruction. Our framework integrates FET and a pre-trained face rendering model to generate facial appearances aligned with the expected expression sequence. Furthermore, our framework can be expanded to include the generation of a neutral expression, thereby enabling the creation of diverse expressions in a video, thus closely simulating facial expression behaviors in real-world scenarios. Extensive experiments were conducted on the CK+ and CelebV-HQ datasets to demonstrate the effectiveness of our framework. Although our proposed approach demonstrates positive outcomes in terms of controlling the facial expression of an RGB image according to the provided instructions, it does have certain limitations. Similar to previous studies [ 18 , 26 ] in which facial parameters were used, the performance of our model relies on the precision of 3D face coefficients, especially using DECA [ 9 ] within our configurations since it may encounter difficulty, often seen in challenging scenarios, in disentangling facial factors. Also, even though linear interpolation can maintain temporal consistency, it often compromises the realism of the synthesized videos. Although the present work uses either basic facial expressions or two designated expressions, the vocabulary number can be easily expanded by combining a Large Language Model (LLM) with our model. Given that text prompting is a powerful tool by which various emotional states can be expressed on a 3D avatar via our framework, we expect it to find various applications in the future."
        },
        {
          "section_index": 42,
          "heading_level": "h2",
          "heading": "Acknowledgment",
          "text": "This work was supported by the Information Technology Research Center (ITRC) support program (IITP-2022-RS-2022-00156354) and a Korean government grant (MSIT) (No.RS-2019-II190231) from the Institute of Information & Communications Technology Planning & Evaluation (IITP) as well as by the Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (2020R1A6A1A03038540)."
        },
        {
          "section_index": 43,
          "heading_level": "h2",
          "heading": "References",
          "text": "[1] S. Aneja, J. Thies, A. Dai, and M. NieÃŸner (2023) ClipFace: text-guided editing of textured 3d morphable models . In SIGGRAPH â€™23 Conference Proceedings , Cited by: Â§I , Â§I . [2] M. C. Buehler, A. Meka, G. Li, T. Beeler, and O. Hilliges (2021) VariTex: variational neural face textures . In Proceedings of the IEEE/CVF International Conference on Computer Vision , Cited by: Â§I , Â§ II-B . [3] C. Chen, Q. Fan, and R. Panda (2021) CrossViT: cross-attention multi-scale vision transformer for image classification . 2021 IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 347â€“356 . Cited by: Â§ III-B 1 . [4] Y. Chen and S. Xiong (2023) Rethinking one-shot face reenactment: a spatialâ€“temporal reconstruction view . Knowledge Based Systems 277 ( C ). Cited by: Â§I . [5] Y. Cui, M. Jia, T. Lin, Y. Song, and S. J. Belongie (2019) Class-balanced loss based on effective number of samples . 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 9260â€“9269 . Cited by: Â§ IV-C , Â§ IV-G 1 . [6] R. DanÄ›Äek, K. Chhatre, S. Tripathi, Y. Wen, M. Black, and T. Bolkart (2023) Emotional speech-driven animation with content-emotion disentanglement . In SIGGRAPH Asia 2023 Conference Papers , Cited by: Â§ II-A , Â§ II-A , Â§ III-B 1 . [7] H. Ding, K. Sricharan, and R. Chellappa (2018) Exprgan: facial expression editing with controllable expression intensity . AAAI . Cited by: Â§I . [8] M. C. Doukas, S. Zafeiriou, and V. Sharmanska (2021-10) HeadGAN: one-shot neural head synthesis and editing . In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 14398â€“14407 . Cited by: Â§I . [9] Y. Feng, H. Feng, M. J. Black, and T. Bolkart (2021) Learning an animatable detailed 3D face model from in-the-wild images . In ACM Transactions on Graphics, (Proc. SIGGRAPH) , Vol. 40 . Cited by: Â§ III-B 1 , Â§ III-B 1 , Â§V . [10] P. Grassal, M. Prinzler, T. Leistner, C. Rother, M. NieÃŸner, and J. Thies (2022) Neural head avatars from monocular rgb videos . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 18653â€“18664 . Cited by: Â§I . [11] X. Gu, C. Wen, W. Ye, J. Song, and Y. Gao (2023) Seer: language instructed video prediction with latent diffusion models . ICLR 2024 . Cited by: Â§I . [12] J. Guo, H. Manukyan, C. Yang, C. Wang, L. Khachatryan, S. Navasardyan, S. Song, H. Shi, and G. Huang (2023) FaceCLIP: facial image-to-video translation via a brief text description . IEEE Transactions on Circuits and Systems for Video Technology , pp. 1â€“1 . Cited by: Â§I . [13] T. Hang, H. Yang, B. Liu, J. Fu, X. Geng, and B. Guo (2023) Language-guided face animation by recurrent stylegan-based generator . IEEE Transactions on Multimedia 25 ( ), pp. 1â€“12 . Cited by: Â§I , Â§I . [14] K. He, X. Zhang, S. Ren, and J. Sun (2016) Deep Residual Learning for Image Recognition . In Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition , CVPR â€™16 , pp. 770â€“778 . External Links: Document , ISSN 1063-6919 Cited by: Â§ IV-C , Â§ IV-G 1 , TABLE X , TABLE X . [15] Z. Huang, K. C.K. Chan, Y. Jiang, and Z. Liu (2023) Collaborative diffusion for multi-modal face generation and editing . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , Cited by: Â§I . [16] S. Hwang, J. Hyung, D. Kim, M. Kim, and J. Choo (2023-10) FaceCLIPNeRF: text-driven 3d face manipulation using deformable neural radiance fields . In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 3469â€“3479 . Cited by: Â§I , Â§I . [17] Y. Jiang, Z. Huang, X. Pan, C. C. Loy, and Z. Liu (2021) Talk-to-edit: fine-grained facial editing via dialog . In Proceedings of the IEEE/CVF International Conference on Computer Vision , Cited by: Â§I . [18] T. Khakhulin, V. Sklyarova, V. Lempitsky, and E. Zakharov (2022) Realistic one-shot mesh-based head avatars . In European Conference of Computer vision (ECCV) , Cited by: Â§I , Â§ II-B , Â§ III-B 2 , Â§ III-B 2 , Figure 11 , Figure 11 , Â§ IV-B , Â§ IV-D 1 , Â§V . [19] M. Kim, F. Liu, A. Jain, and X. Liu (2023-06) DCFace: synthetic face generation with dual condition diffusion model . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 12715â€“12725 . Cited by: Â§I . [20] D. P. Kingma and J. Ba (2014) Adam: a method for stochastic optimization . ICLR abs/1412.6980 . Cited by: Â§ IV-B . [21] Y. Lee, T. Choi, H. Go, H. Lee, S. Cho, and J. Kim (2023) Exp-gan: 3d-aware facial image generation with expression control . In Computer Vision â€“ ACCV 2022: 16th Asian Conference on Computer Vision , pp. 151â€“167 . External Links: ISBN 978-3-031-26292-0 Cited by: Â§ II-B . [22] T. Li, T. Bolkart, M. J. Black, H. Li, and J. Romero (2017) Learning a model of facial shape and expression from 4d scans . ACM Transactions on Graphics (TOG) 36 , pp. 1 â€“ 17 . Cited by: Â§I , Â§ II-A , Â§ III-B 1 . [23] X. Li, D. Zhang, M. Li, and D. Lee (2023) Accurate head pose estimation using image rectification and a lightweight convolutional neural network . IEEE Transactions on Multimedia 25 ( ), pp. 2239â€“2251 . Cited by: Â§ IV-A 2 . [24] K. Lin, A. Trevithick, K. Cheng, M. Sarkis, M. Ghafoorian, N. Bi, G. Reitmayr, and R. Ramamoorthi (2023) PVP: Personalized Video Prior for Editable Dynamic Portraits using StyleGAN . Computer Graphics Forum 42 ( 4 ), pp. 13 pages . Cited by: Â§ III-B 1 . [25] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and I. Matthews (2010) The extended cohn-kanade dataset (ck+): a complete dataset for action unit and emotion-specified expression . In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops , Vol. , pp. 94â€“101 . Cited by: Â§ IV-A 1 . [26] H. Ma, T. Zhang, S. Sun, X. Yan, K. Han, and X. Xie (2024) CVTHead: one-shot controllable head avatar with vertex-feature transformer . IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) . Cited by: Â§I , Â§ II-B , Â§ III-B 2 , Figure 11 , Figure 11 , Â§ IV-B , Â§ IV-D 1 , Â§V . [27] N. Otberdout, C. Ferrari, M. Daoudi, S. Berretti, and A. Bimbo (2023) Generating complex 4d expression transitions by learning face landmark trajectories . IEEE Transactions on Affective Computing . Cited by: Â§I , Â§ II-A , Â§ II-A , Â§ IV-C . [28] N. Otberdout, C. Ferrari, M. Daoudi, S. Berritti, and A. Del Bimbo (2022-06) Sparse to dense dynamic 3d facial expression generation . In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , Cited by: Â§ II-A . [29] F. Paraperas Papantoniou, P. P. Filntisis, P. Maragos, and A. Roussos (2022) Neural emotion director: speech-preserving semantic control of facial expressions in â€in-the-wildâ€ videos . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , Cited by: Â§I . [30] O. Patashnik, Z. Wu, E. Shechtman, D. Cohen-Or, and D. Lischinski (2021-10) StyleCLIP: text-driven manipulation of stylegan imagery . In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 2085â€“2094 . Cited by: Â§I . [31] B. Peng, H. Fan, W. Wang, J. Dong, and S. Lyu (2022) A unified framework for high fidelity face swap and expression reenactment . IEEE Transactions on Circuits and Systems for Video Technology 32 ( 6 ), pp. 3673â€“3684 . Cited by: Â§I . [32] S. Qian, T. Kirschstein, L. Schoneveld, D. Davoli, S. Giebenhain, and M. NieÃŸner (2024) GaussianAvatars: photorealistic head avatars with rigged 3d gaussians . In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , Cited by: Â§I . [33] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. (2021) Learning transferable visual models from natural language supervision . In International conference on machine learning , pp. 8748â€“8763 . Cited by: Â§ III-B 1 . [34] J. Russell and J. M. F. dez Dols (1997) Reading emotions from and into faces: resurrecting dimensional-contextual perspective . The psychology of facial expression , pp. 295â€“320 . Note: Cambridge University Press Cited by: Â§ IV-F . [35] K. R. Scherer (2001) Appraisal considered as a process of multilevel sequential checking. . Oxford University Press , pp. 92 â€“ 120 . Cited by: Â§ IV-F . [36] S. Su, J. Zhu, L. Gao, and J. Song (2024) Utilizing greedy nature for multimodal conditional image synthesis in transformers . IEEE Transactions on Multimedia 26 ( ), pp. 2354â€“2366 . Cited by: Â§I . [37] T. Sutter, I. Daunhawer, and J. Vogt (2020) Multimodal generative learning utilizing jensen-shannon-divergence . Vol. 33 , pp. 6100â€“6110 . Cited by: Â§I . [38] M. Suzuki and Y. Matsuo (2022) A survey of multimodal deep generative models . Advanced Robotics 36 , pp. 261 â€“ 278 . Cited by: Â§I . [39] G. Tevet, B. Gordon, A. Hertz, A. H. Bermano, and D. Cohen-Or (2022) Motionclip: exposing human motion generation to clip space . In Computer Visionâ€“ECCV 2022: 17th European Conference , pp. 358â€“374 . Cited by: Â§ II-A , Â§ IV-B . [40] Y. Wu, S. Xu, J. Xiang, F. Wei, Q. Chen, J. Yang, and X. Tong (2023) AniPortraitGAN: animatable 3d portrait generation from 2d image collections . In SIGGRAPH Asia 2023 Conference Proceedings , Cited by: Â§I . [41] W. Xia, Y. Yang, J. Xue, and B. Wu (2021) TediGAN: text-guided diverse face image generation and manipulation . In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , Cited by: Â§I . [42] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang (2018) The unreasonable effectiveness of deep features as a perceptual metric . In CVPR , Cited by: Â§ IV-D 1 . [43] Y. Zhang, Y. Li, lixiong Qin, X. Liu, and W. Deng (2023) Leave no stone unturned: mine extra knowledge for imbalanced facial expression recognition . In Thirty-seventh Conference on Neural Information Processing Systems , Cited by: Â§ IV-C , Â§ IV-G 1 , TABLE X , TABLE X . [44] H. Zhu, W. Wu, W. Zhu, L. Jiang, S. Tang, L. Zhang, Z. Liu, and C. C. Loy (2022) CelebV-HQ: a large-scale video facial attributes dataset . In ECCV , Cited by: Â§ IV-A 2 . [45] W. Zielonka, T. Bolkart, and J. Thies (2023-06) Instant volumetric head avatars . In IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) , External Links: Document Cited by: Â§I . [46] K. Zou, S. Faisan, B. Yu, S. Valette, and H. Seo (2023) 4D facial expression diffusion model . External Links: 2303.16611 Cited by: Â§I , Â§ II-A , Â§ II-A , Â§ IV-C ."
        }
      ],
      "content_text": "Instruction-Driven 3D Facial Expression Generation and Transition\nAnh H. Vo, Tae-Seok Kim, Hulin Jin, Soo-Mi Choi, and Yong-Guk Kim A H Vo, T-S Kim, S-M Choi, and Y-G Kim are with the Department of Computer Engineering, Sejong University, Seoul, Republic of Korea.H Jin is with the School of Computer Science and Technology, Anhui University, Hefei, China.* Corresponding Author: ykim@sejong.ac.kr. This is the authorâ€™s accepted manuscript. The final version is published in IEEE Transactions on Multimedia, 2025 Abstract A 3D avatar typically has one of six cardinal facial expressions. To simulate realistic emotional variation, we should be able to render a facial transition between two arbitrary expressions. This study presents a new framework for instruction-driven facial expression generation that produces a 3D face and, starting from an image of the face, transforms the facial expression from one designated facial expression to another. The Instruction-driven Facial Expression Decomposer (IFED) module is introduced to facilitate multimodal data learning and capture the correlation between textual descriptions and facial expression features. Subsequently, we propose the Instruction to Facial Expression Transition (I2FET) method, which leverages IFED and a vertex reconstruction loss function to refine the semantic comprehension of latent vectors, thus generating a facial expression sequence according to the given instruction. Lastly, we present the Facial Expression Transition model to generate smooth transitions between facial expressions. Extensive evaluation suggests that the proposed model outperforms state-of-the-art methods on the CK+ and CelebV-HQ datasets. The results show that our framework can generate facial expression trajectories according to text instruction. Considering that text prompts allow us to make diverse descriptions of human emotional states, the repertoire of facial expressions and the transitions between them can be expanded greatly. We expect our framework to find various practical applications. More information about our project can be found at https://vohoanganh.github.io/tg3dfet/ Index Terms: Instruction-Driven, Facial Expression and Transition, Controllable Avatar, CK+ and CelebV-HQ datasets. I Introduction Face and facial expression generation have attracted much attention in recent years due to their widespread application in many fields such as augmented reality/virtual reality, the gaming industry, movie production, or human-computer interfaces. Face generation can be divided into two categories depending on whether the face is two or three-dimensional (2D or 3D faces). For the 2D case, several methods [ 15 , 41 , 17 , 19 , 36 ] have been proposed to generate facial images using multimodal input, such as text, sketches, and segmentation. Previous studies [ 7 , 41 , 12 , 13 ] have attempted to solve facial expression generation in response to input provided as an emotion label or text prompts. However, these methods have limited ability to control the geometrical information of the output image and are effective only for specific applications. In the case of a 3D face, several studies [ 18 , 45 , 26 , 31 , 4 ] have demonstrated methods for generating 3D faces from 2D ones. The models generate the output image combining geometrical information from the source image, such as the pose, shape, and texture. Figure 1 : Illustration of our framework wherein it accepts a text instruction with a face image (source) as input and generates a 3D face from it. Then, it transforms the facial expression, from disgust (expression 1) to happiness (expression 2) via a neutral expression. Other researchers [ 16 , 1 ] generate a specific 3D facial expression based on text prompts. They utilize CLIP (Contrastive Language-Image Pretraining) or NeRF (Neural Radiance Field) to learn the latent code for manipulating facial expressions. Yet, NeRF requires substantial computational resources and time. For instance, [ 1 ] learns facial parameters and generates facial texture and expression based on text prompts. Although these methods can generate specific facial expressions, they do not address facial expression transitions based on instruction provided. Meanwhile, other studies [ 8 , 2 , 10 , 29 , 32 ] proposed to control facial expressions from an RGB video or generate portrait images [ 40 ] with controllable facial expression, head pose, and shoulder movements. Alternatively, [ 27 , 46 ] utilized the facial landmark trajectories to generate multiple 4D expression transitions on a meshed face. These studies led us to propose a new framework that enables a 3D face to be generated from a source image. Then, the facial expression undergoes transition in response to prompt text provided by the user to request a facial transition between two arbitrary expressions. Recent studies have used text-driven methods to generate a deformed face via NeRF or create an animated face via StyleGAN [ 13 , 16 ] . They generate sequence of facial expressions by specifying a certain emotion, such as â€™the person is happyâ€™. In contrast, our study utilizes a text instruction to control the variation in the facial expression, such as â€™Turn this face from disgust to happinessâ€™, as illustrated in Fig. 1 . Yet, understanding the target expressions specified by text instructions presents a challenge when relying solely on global text embedding. Indeed, even though CLIP is a notable method for the language-conditioning module in the text-to-image task [ 13 , 16 , 11 , 1 ] , it still faces challenges related to bias in understanding the text despite its large-scale training or the provision of global instruction, which may overlook fine-grained sub-instructions. Moreover, a text-only approach could be sensitive in generative models because changes in the structure of instructions can significantly impact their performance. The additional challenge lies therein to ensure that the facial motion described in the instruction is aligned with the semantics of the language. A straightforward approach to generating the facial motion would entail interpolating between the source code and the edited code from StyleClip [ 30 ] . Nevertheless, the additional challenge of controlling the latent code to adjust the head pose of the source image while generating the facial animation makes it difficult to achieve the intended goal. To make the connection between the text and facial expressions, we supplement a text instruction for making a transition between facial expressions into two standard datasets containing facial images, i.e., CelebV-HQ (High-Quality Celebrity Video) and CK+ (Extended Cohn-Kanade). To control the facial model in detail, FLAME (Faces Learned with an Articulated Model and Expressions) [ 22 ] is adopted and then fine-tuned with basic facial expressions and corresponding poses to capture the movement of the head, eyes, and mouth. Furthermore, integrating feature representations from different modalities minimizes ambiguity in redundant resources and effectively handles missing modalities, particularly in generative models, by improving inter-modality relations [ 37 , 38 ] . The proposed approach allows our model, i.e. Instruction to Facial Expression Transition (I2FET), to enhance the relationships between facial expression features and textual descriptions, thereby improving the latent representation for accurately generating target facial expressions and reducing the reliance solely on instructional text. Then, facial expression trajectories are created using the target facial expressions, which are generated by the facial parameters of the source image using I2FET. Using this framework, one can generate a 3D facial expression avatar from a photograph of a face and create any facial transition based on a text prompt instruction that contains the initial and final facial expressions. The main contributions of our study are given as follows: â€¢ We present an instruction-driven 3D facial expression generation and transition framework to generate transitions in facial expressions based on instructions, starting from a photograph of a face. â€¢ We propose an Instruction-Driven Facial Expression Decomposer (IFED), designed to learn multimodal data and capture the correlation between facial expression features and textual descriptions. â€¢ We propose an Instruction to Facial Expression Transition method (I2FET), which utilizes conditional variational autoencoders, integrated with IFED and a vertex reconstruction loss function, to enhance the semantic comprehension of the latent vectors. â€¢ Extensive evaluation of the proposed model on the CK+ and CelebV-HQ datasets suggests that it outperforms SOTA methods.\n\nAbstract\nA 3D avatar typically has one of six cardinal facial expressions. To simulate realistic emotional variation, we should be able to render a facial transition between two arbitrary expressions. This study presents a new framework for instruction-driven facial expression generation that produces a 3D face and, starting from an image of the face, transforms the facial expression from one designated facial expression to another. The Instruction-driven Facial Expression Decomposer (IFED) module is introduced to facilitate multimodal data learning and capture the correlation between textual descriptions and facial expression features. Subsequently, we propose the Instruction to Facial Expression Transition (I2FET) method, which leverages IFED and a vertex reconstruction loss function to refine the semantic comprehension of latent vectors, thus generating a facial expression sequence according to the given instruction. Lastly, we present the Facial Expression Transition model to generate smooth transitions between facial expressions. Extensive evaluation suggests that the proposed model outperforms state-of-the-art methods on the CK+ and CelebV-HQ datasets. The results show that our framework can generate facial expression trajectories according to text instruction. Considering that text prompts allow us to make diverse descriptions of human emotional states, the repertoire of facial expressions and the transitions between them can be expanded greatly. We expect our framework to find various practical applications. More information about our project can be found at https://vohoanganh.github.io/tg3dfet/\n\nI Introduction\nFace and facial expression generation have attracted much attention in recent years due to their widespread application in many fields such as augmented reality/virtual reality, the gaming industry, movie production, or human-computer interfaces. Face generation can be divided into two categories depending on whether the face is two or three-dimensional (2D or 3D faces). For the 2D case, several methods [ 15 , 41 , 17 , 19 , 36 ] have been proposed to generate facial images using multimodal input, such as text, sketches, and segmentation. Previous studies [ 7 , 41 , 12 , 13 ] have attempted to solve facial expression generation in response to input provided as an emotion label or text prompts. However, these methods have limited ability to control the geometrical information of the output image and are effective only for specific applications. In the case of a 3D face, several studies [ 18 , 45 , 26 , 31 , 4 ] have demonstrated methods for generating 3D faces from 2D ones. The models generate the output image combining geometrical information from the source image, such as the pose, shape, and texture. Figure 1 : Illustration of our framework wherein it accepts a text instruction with a face image (source) as input and generates a 3D face from it. Then, it transforms the facial expression, from disgust (expression 1) to happiness (expression 2) via a neutral expression. Other researchers [ 16 , 1 ] generate a specific 3D facial expression based on text prompts. They utilize CLIP (Contrastive Language-Image Pretraining) or NeRF (Neural Radiance Field) to learn the latent code for manipulating facial expressions. Yet, NeRF requires substantial computational resources and time. For instance, [ 1 ] learns facial parameters and generates facial texture and expression based on text prompts. Although these methods can generate specific facial expressions, they do not address facial expression transitions based on instruction provided. Meanwhile, other studies [ 8 , 2 , 10 , 29 , 32 ] proposed to control facial expressions from an RGB video or generate portrait images [ 40 ] with controllable facial expression, head pose, and shoulder movements. Alternatively, [ 27 , 46 ] utilized the facial landmark trajectories to generate multiple 4D expression transitions on a meshed face. These studies led us to propose a new framework that enables a 3D face to be generated from a source image. Then, the facial expression undergoes transition in response to prompt text provided by the user to request a facial transition between two arbitrary expressions. Recent studies have used text-driven methods to generate a deformed face via NeRF or create an animated face via StyleGAN [ 13 , 16 ] . They generate sequence of facial expressions by specifying a certain emotion, such as â€™the person is happyâ€™. In contrast, our study utilizes a text instruction to control the variation in the facial expression, such as â€™Turn this face from disgust to happinessâ€™, as illustrated in Fig. 1 . Yet, understanding the target expressions specified by text instructions presents a challenge when relying solely on global text embedding. Indeed, even though CLIP is a notable method for the language-conditioning module in the text-to-image task [ 13 , 16 , 11 , 1 ] , it still faces challenges related to bias in understanding the text despite its large-scale training or the provision of global instruction, which may overlook fine-grained sub-instructions. Moreover, a text-only approach could be sensitive in generative models because changes in the structure of instructions can significantly impact their performance. The additional challenge lies therein to ensure that the facial motion described in the instruction is aligned with the semantics of the language. A straightforward approach to generating the facial motion would entail interpolating between the source code and the edited code from StyleClip [ 30 ] . Nevertheless, the additional challenge of controlling the latent code to adjust the head pose of the source image while generating the facial animation makes it difficult to achieve the intended goal. To make the connection between the text and facial expressions, we supplement a text instruction for making a transition between facial expressions into two standard datasets containing facial images, i.e., CelebV-HQ (High-Quality Celebrity Video) and CK+ (Extended Cohn-Kanade). To control the facial model in detail, FLAME (Faces Learned with an Articulated Model and Expressions) [ 22 ] is adopted and then fine-tuned with basic facial expressions and corresponding poses to capture the movement of the head, eyes, and mouth. Furthermore, integrating feature representations from different modalities minimizes ambiguity in redundant resources and effectively handles missing modalities, particularly in generative models, by improving inter-modality relations [ 37 , 38 ] . The proposed approach allows our model, i.e. Instruction to Facial Expression Transition (I2FET), to enhance the relationships between facial expression features and textual descriptions, thereby improving the latent representation for accurately generating target facial expressions and reducing the reliance solely on instructional text. Then, facial expression trajectories are created using the target facial expressions, which are generated by the facial parameters of the source image using I2FET. Using this framework, one can generate a 3D facial expression avatar from a photograph of a face and create any facial transition based on a text prompt instruction that contains the initial and final facial expressions. The main contributions of our study are given as follows: â€¢ We present an instruction-driven 3D facial expression generation and transition framework to generate transitions in facial expressions based on instructions, starting from a photograph of a face. â€¢ We propose an Instruction-Driven Facial Expression Decomposer (IFED), designed to learn multimodal data and capture the correlation between facial expression features and textual descriptions. â€¢ We propose an Instruction to Facial Expression Transition method (I2FET), which utilizes conditional variational autoencoders, integrated with IFED and a vertex reconstruction loss function, to enhance the semantic comprehension of the latent vectors. â€¢ Extensive evaluation of the proposed model on the CK+ and CelebV-HQ datasets suggests that it outperforms SOTA methods.\n\nII Related Work\nII-A Facial Expression Transition Generative Adversarial Networks (GANs) have been favored for generating facial expressions. For instance, Motion3DGAN [ 27 ] , an extension of MotionGAN [ 28 ] , addresses the dynamics of 3D landmarks, whereas WGAN learns the distribution of 3D expression dynamics across the hypersphere space, sampled with a condition to generate landmark sequences. In this case, a neutral 3D face mesh, frame by frame, is deformed by a mesh decoder using the generated landmark sequences although the disadvantage of this work is that the start and end expressions have to be provided to generate a concatenated expression transition. On the other hand, a diffusion model is adopted for facial expression generation [ 46 ] by modeling the input distribution. The deformation of the 3D mesh is guided by a specific landmark, and the original shape of the input facial mesh is considered to apply displacement to each vertex. In this work, MotionClip [ 39 ] , which was originally designed for human motion synthesis, is compared with the task of generating the expression transition conditions given by the text. EMOTE [ 6 ] utilizes emotion labels and various inputs to generate a talking-head avatar that maintains lip-sync from speech as well as facial expression. In particular, the facial motion prior, called FLINT (FLAME IN TIME), uses the expression and pose parameters of FLAME [ 22 ] to represent facial motion sequences. Although several studies have demonstrated methods to produce transitions between facial expressions under diverse conditions, the generated faces are typically conditioned on the front face by neglecting the head pose variations. Inspired by previous works [ 27 , 46 , 6 ] , the present study proposes a model for facial expression transition that learns the expression and pose parameters. The additional merit of our model would be that it can easily be plugged into the models that generate the facial appearance. II-B Face Rendering In the face rendering area, 3D Morphable Model (3DMM) has been widely used for rendering realistic faces [ 2 , 21 , 18 , 26 ] . Among these methods, VariTex [ 2 ] employs a neural texture obtained by a face texture decoder to sample with a UV map. Subsequently, this neural face feature image is merged with an additive feature image to serve as the input for a U-Net to generate the desired image. Exp-GAN [ 21 ] employs a neural volume generator and StyleGAN2 to generate images at various resolutions. This process combines different feature components, such as face features, depth images, and feature volumes. The facial feature is also obtained by sampling the neural texture using the UV map. However, this work is computationally intensive and time-consuming. ROME [ 18 ] can synthesize realistic face images by estimating a specific head mesh and neural texture from an input image, whereupon a neural rendering technique is adapted to render the rigged mesh. Next, the U-net-like neural rendering network, which accepts inputs such as neural texture and surface normals, is utilized to acquire both the target image and its corresponding mask. Similarly, CVTHead [ 26 ] also estimates a neural texture from an input image, and yet, in this case, the neural texture is flattened to a feature vector form before being passed to transformers along with vertex tokens to output a vertex descriptor. Specifically, each of the vertices is considered a query token, and a transformer is employed to learn the canonical vertex feature from the source image. To generate the desired image and its mask, the feature descriptor of each vertex and depth are projected into image space before being processed by a U-Net. In the current study, we employ ROME and CVTHead to generate facial appearances by leveraging DECA for the extraction of facial expression coefficients. This streamlines the integration into a facial expression transition model and enhances the applicability of the proposed framework to practical scenarios.\n\nII-A Facial Expression Transition\nGenerative Adversarial Networks (GANs) have been favored for generating facial expressions. For instance, Motion3DGAN [ 27 ] , an extension of MotionGAN [ 28 ] , addresses the dynamics of 3D landmarks, whereas WGAN learns the distribution of 3D expression dynamics across the hypersphere space, sampled with a condition to generate landmark sequences. In this case, a neutral 3D face mesh, frame by frame, is deformed by a mesh decoder using the generated landmark sequences although the disadvantage of this work is that the start and end expressions have to be provided to generate a concatenated expression transition. On the other hand, a diffusion model is adopted for facial expression generation [ 46 ] by modeling the input distribution. The deformation of the 3D mesh is guided by a specific landmark, and the original shape of the input facial mesh is considered to apply displacement to each vertex. In this work, MotionClip [ 39 ] , which was originally designed for human motion synthesis, is compared with the task of generating the expression transition conditions given by the text. EMOTE [ 6 ] utilizes emotion labels and various inputs to generate a talking-head avatar that maintains lip-sync from speech as well as facial expression. In particular, the facial motion prior, called FLINT (FLAME IN TIME), uses the expression and pose parameters of FLAME [ 22 ] to represent facial motion sequences. Although several studies have demonstrated methods to produce transitions between facial expressions under diverse conditions, the generated faces are typically conditioned on the front face by neglecting the head pose variations. Inspired by previous works [ 27 , 46 , 6 ] , the present study proposes a model for facial expression transition that learns the expression and pose parameters. The additional merit of our model would be that it can easily be plugged into the models that generate the facial appearance.\n\nII-B Face Rendering\nIn the face rendering area, 3D Morphable Model (3DMM) has been widely used for rendering realistic faces [ 2 , 21 , 18 , 26 ] . Among these methods, VariTex [ 2 ] employs a neural texture obtained by a face texture decoder to sample with a UV map. Subsequently, this neural face feature image is merged with an additive feature image to serve as the input for a U-Net to generate the desired image. Exp-GAN [ 21 ] employs a neural volume generator and StyleGAN2 to generate images at various resolutions. This process combines different feature components, such as face features, depth images, and feature volumes. The facial feature is also obtained by sampling the neural texture using the UV map. However, this work is computationally intensive and time-consuming. ROME [ 18 ] can synthesize realistic face images by estimating a specific head mesh and neural texture from an input image, whereupon a neural rendering technique is adapted to render the rigged mesh. Next, the U-net-like neural rendering network, which accepts inputs such as neural texture and surface normals, is utilized to acquire both the target image and its corresponding mask. Similarly, CVTHead [ 26 ] also estimates a neural texture from an input image, and yet, in this case, the neural texture is flattened to a feature vector form before being passed to transformers along with vertex tokens to output a vertex descriptor. Specifically, each of the vertices is considered a query token, and a transformer is employed to learn the canonical vertex feature from the source image. To generate the desired image and its mask, the feature descriptor of each vertex and depth are projected into image space before being processed by a U-Net. In the current study, we employ ROME and CVTHead to generate facial appearances by leveraging DECA for the extraction of facial expression coefficients. This streamlines the integration into a facial expression transition model and enhances the applicability of the proposed framework to practical scenarios.\n\nIII Method\nIII-A Problem Statement We aim to develop a model to generate 3D facial expression with two inputs: a face image I s I_{s} and a text instruction t t . The facial expression transition model ğ’¯ w â€‹ ( t , I s ) \\mathcal{T}_{w}(t,I_{s}) , having weights w w , is trained with a training set ğ’³ = { ( t , ( e 0 , Î¸ 0 ) , ( e 1 , Î¸ 1 ) ) ( j ) } j = 1 n \\mathcal{X}=\\{(t,(e_{0},\\theta_{0}),(e_{1},\\theta_{1}))^{(j)}\\}_{j=1}^{n} , where e 0 , e 1 âˆˆ R 1 Ã— 50 e_{0},e_{1}\\in R^{1\\times 50} denote expression vectors with 50 dimensions and Î¸ 0 , Î¸ 1 âˆˆ R 1 Ã— 6 \\theta_{0},\\theta_{1}\\in R^{1\\times 6} represent pose vectors with 6 dimensions corresponding to the first and next expressions in the facial expression sequence specified in instruction t t , respectively. n n is the number of samples in the training dataset. Our network learns ğ’¯ w ( . ) \\mathcal{T}_{w}(.) that approximates the data distribution p â€‹ ( ğ’³ ) p(\\mathcal{X}) and then generates a facial expression coefficient sequence { ( e 0 , Î¸ 0 ) , ( e 1 , Î¸ 1 ) } \\{(e_{0},\\theta_{0}),(e_{1},\\theta_{1})\\} with a given instruction t t . Next, a face rendering module ğ’¢ â€‹ ( I s , ğ’® ) \\mathcal{G}(I_{s},\\mathcal{S}) is used to render a sequence of facial appearances ğ’´ = { y s ( 1 ) , y ( i ) , y e 0 ( k ) , y ( j ) , y e 1 ( T ) } \\mathcal{Y}=\\{y_{s}^{(1)},y^{(i)},y_{e_{0}}^{(k)},y^{(j)},y_{e_{1}}^{(T)}\\} , with inputs, consisting of the texture of source image I s I_{s} and facial expression trajectories S = { s s ( 1 ) , s ( i ) , s e 0 ( k ) , s ( j ) , s e 1 ( T ) } S=\\{s_{s}^{(1)},s^{(i)},s_{e_{0}}^{(k)},s^{(j)},s_{e_{1}}^{(T)}\\} , where { s s ( 1 ) , s e 0 ( k ) , s e 1 ( T ) } \\{s_{s}^{(1)},s_{e_{0}}^{(k)},s_{e_{1}}^{(T)}\\} are anchor facial expression sequences corresponding to the source image, and specific facial expressions, generated by ğ’¯ w ( . ) \\mathcal{T}_{w}(.) . Then, facial expression sequences s ( i ) s^{(i)} and s ( j ) s^{(j)} transitioning between these anchor facial expressions can be inferred by a linear interpolation function to ensure the smoothness of the sequence of facial expressions. III-B The Proposed Method Figure 2 provides an overview of the proposed framework whereby one can control 3D facial expressions of the monocular image I s I_{s} following an input instruction t t . The framework consists of two major modules: (1) Facial Expression Transition (FET), ğ’¯ w â€‹ ( t , I s ) \\mathcal{T}_{w}(t,I_{s}) , for generating diverse expression trajectories; (2) Face Rendering (FR), ğ’¢ â€‹ ( I s , S ) \\mathcal{G}(I_{s},S) , for rendering facial appearances based on the texture of the source image I s I_{s} and the expression trajectories S S generated by the FET module. Figure 2 : Overview of the proposed framework consisting of two major modules: Facial Expression Transition (FET) and Face Rendering (FR). First, textual vector x t x_{t} is obtained from the pre-trained CLIP encoder for the FET module. Simultaneously, latent representations z p z_{p} and z e z_{e} are drawn from ğ’© â€‹ ( 0 , I ) \\mathcal{N}(0,I) . Then, the latent vectors are processed and concatenated to obtain x ^ f \\hat{x}^{f} . Afterward, the IFED module utilizes x t x^{t} and x ^ f \\hat{x}^{f} as inputs to create conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} , and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} for the pose and expression decoders. Subsequently, for smoothness of the facial expression sequence, the facial expression in the source image ( e s , Î¸ s ) (e_{s},\\theta_{s}) is subjected to linear interpolation and the specific facial expressions ( e 0 , Î¸ 0 ) , ( e 1 , Î¸ 1 ) (e_{0},\\theta_{0}),(e_{1},\\theta_{1}) , generated by the I2FET decoders. These sequences are then combined with the shape Ï• s \\phi_{s} and camera c s c_{s} parameters of the source image, obtained from DECA, to form facial expression trajectories { s s ( 1 ) , s ( i ) , s e 0 ( k ) , s ( j ) , s e 1 ( T ) } \\{s_{s}^{(1)},s^{(i)},s^{(k)}_{e_{0}},s^{(j)},s_{e_{1}}^{(T)}\\} . For FR, head mesh reconstruction produces a flame mesh sequence { m s ( 1 ) , m ( i ) , m e 0 ( k ) , m ( j ) , m e 1 ( T ) } \\{m_{s}^{(1)},m^{(i)},m^{(k)}_{e_{0}},m^{(j)},m_{e_{1}}^{(T)}\\} aligned with the facial expression trajectories and the texture encoder extracts the facial appearance from the source image. Finally, the rendering module generates the facial appearances { y s ( 1 ) , y ( i ) , y e 0 ( k ) , y ( j ) , y e 1 ( T ) } \\{y_{s}^{(1)},y^{(i)},y^{(k)}_{e_{0}},y^{(j)},y_{e_{1}}^{(T)}\\} within the facial expression trajectories using the flame mesh sequence and the extracted facial appearance. III-B 1 Facial Expression Transition This module is designed to generate diverse expression trajectories that correspond to the facial parameters of the source face image I s I_{s} and the parameters of specific facial expressions, described in an instruction t t . The module has two primary branches: the first receives an instruction to generate the corresponding sequence of facial expression based on instruction t t , whereas the second takes a 2D face image to encode its facial coefficients and create the facial expression trajectories. Figure 3 : Flow diagram of IFED (Instruction-Driven Facial Expression Decomposer). It consists of CAFT and the decomposition of parameters. The major function of CAFT is to introduce cross-attention between the facial parameters and text instruction. The two outputs from CAFT pass through layer normalization and are combined to create the fused feature vector x f â€‹ u â€‹ s â€‹ e â€‹ d x^{fused} . Then, pose projection function ğ’« p \\mathcal{P}_{p} and expression projection function ğ’« e \\mathcal{P}_{e} are used to decompose x f â€‹ u â€‹ s â€‹ e â€‹ d x^{fused} into two conditional vectors for facial expression ( x e â€‹ m â€‹ b e x_{emb}^{e} ) and pose ( x e â€‹ m â€‹ b p x_{emb}^{p} ). Figure 4 : Detailed architecture of the I2FET (Instruction to Facial Expression Transition) method. Given an instruction and a sequence of facial parameters, including pose and expression, the IFED module utilizes the facial parameters and textual description extracted by CLIP to produce the corresponding conditional feature vectors for pose x e â€‹ m â€‹ b p x_{emb}^{p} and expression x e â€‹ m â€‹ b e x_{emb}^{e} , respectively. The pose and expression encoders then map this sequence of facial parameters and conditional feature vectors to latent representations corresponding to pose and expression. These latent vectors, along with the textual description, are passed through the IFED module to generate the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} . Following this, the expression and pose decoders reconstruct expression e ^ \\hat{e} and pose Î¸ ^ \\hat{\\theta} parameters using these latent representations and the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} , respectively. Finally, the pre-trained FLAME decoder reconstructs the vertex coordinates v ^ \\hat{v} based on e ^ \\hat{e} and Î¸ ^ \\hat{\\theta} . In the former, the pre-trained CLIP encoder [ 33 ] extracts text description x t âˆˆ R 77 Ã— 768 x^{t}\\in R^{77\\times 768} according to the given instruction t t . Then, the I2FET model generates a sequence of expression and pose coefficients for the facial expression trajectories. Instruction-driven Facial Expression Decomposer : A dual-branch vision transformer [ 3 ] is adopted for extracting multi-scale features and fusing cross-attention information at different scales because it is efficient for the image classification task. Here, the Cross-Attention for Facial Expression Parameter and Text (CAFT) is used to learn the relationship between instruction and facial expression parameters by enhancing the understanding of multimodal data and refining the latent representation for the Instruction to Facial Expression Transition model. The proposed IFED module is illustrated in Fig. 3 . In particular, we directly incorporate facial expression parameters and textual descriptions into CAFT instead of adding class tokens as in [ 3 ] . With respect to the transformer encoders, the first branch learns the correlation between the facial expression parameters, while the second branch focuses on learning the textual descriptions. To begin with, each vector representing facial expression parameters comprises two elements: expression and pose parameters, denoted as x f = { e , Î¸ } âˆˆ R m Ã— 56 x^{f}=\\{e,\\theta\\}\\in R^{m\\times 56} , where m m denotes the number of expressions in a given instruction. As [ 24 , 6 ] , we only utilize jaw pose Î¸ j â€‹ a â€‹ w \\theta^{jaw} and expression e e to convey facial expression information. Therefore, we rewrite x f = { e , Î¸ j â€‹ a â€‹ w } âˆˆ R m Ã— 53 x^{f}=\\{e,\\theta^{jaw}\\}\\in R^{m\\times 53} as the input of the first branch. Then, the transformer encoder is utilized as y f = x f + M â€‹ S â€‹ A â€‹ ( L â€‹ N â€‹ ( x f ) ) \\displaystyle y^{f}=x^{f}+MSA(LN(x^{f})) (1) x ^ f = y f + F â€‹ F â€‹ N â€‹ ( L â€‹ N â€‹ ( y f ) ) \\displaystyle\\hat{x}^{f}=y^{f}+FFN(LN(y^{f})) where x ^ f \\hat{x}^{f} denotes the output of the transformer encoder in the facial expression parameters branch â„° P \\mathcal{E}_{P} . L N ( . ) LN(.) , M S A ( . ) MSA(.) , and F F N ( . ) FFN(.) are the layer norm, multi-head self-attention, and feed-forward network, respectively. Likewise, in the text branch, we replicate x t x^{t} to create a vector { x } i = { 0 , 1 } t \\{x\\}^{t}_{i=\\{0,1\\}} for computational purposes. Subsequently, the input for the transformer encoder is generated using a text projection function P t ( . ) P_{t}(.) on { x } i = { 0 , 1 } t \\{x\\}^{t}_{i=\\{0,1\\}} . As a result, the textual description x ^ t \\hat{x}^{t} is obtained through the transformer encoder â„° t \\mathcal{E}_{t} . Both branches are then fused by cross-attention to capture the relationship between the facial expression parameters and textual descriptions. The fusion process can be expressed as y c f = h f â€‹ 2 â€‹ t â€‹ ( x ^ 0 f ) , y c t = h t â€‹ 2 â€‹ f â€‹ ( x ^ 0 t ) , \\displaystyle y_{c}^{f}=h^{f2t}(\\hat{x}_{0}^{f}),\\quad y_{c}^{t}=h^{t2f}(\\hat{x}_{0}^{t}), (2) x o f = g t â€‹ 2 â€‹ f â€‹ ( [ C â€‹ A â€‹ ( y c f âŠ— x 1 t ) + y c f ] ) âŠ— x ^ 1 f \\displaystyle x_{o}^{f}=g^{t2f}([CA(y^{f}_{c}\\otimes x_{1}^{t})+y_{c}^{f}])\\otimes\\hat{x}_{1}^{f} x o t = g f â€‹ 2 â€‹ t â€‹ ( [ C â€‹ A â€‹ ( y c t âŠ— x 1 f ) + y c t ] ) âŠ— x ^ 1 t \\displaystyle x_{o}^{t}=g^{f2t}([CA(y_{c}^{t}\\otimes x_{1}^{f})+y_{c}^{t}])\\otimes\\hat{x}_{1}^{t} where h t â€‹ 2 â€‹ f , h f â€‹ 2 â€‹ t , g t â€‹ 2 â€‹ f h^{t2f},h^{f2t},g^{t2f} , and g f â€‹ 2 â€‹ t g^{f2t} are the projection function and back-projection function between the vectors that represent the text and facial expression parameters, x o f âˆˆ R m Ã— 53 x_{o}^{f}\\in R^{m\\times 53} and x o t âˆˆ R m Ã— 768 x_{o}^{t}\\in R^{m\\times 768} are the outputs of CAFT, C â€‹ A CA denotes cross-attention and âŠ— \\otimes denotes the concatenation operator. Next, the fused feature vector is obtained by concatenating x o f x_{o}^{f} and x o t x_{o}^{t} , after passing through L N ( . ) LN(.) , as follows: x f â€‹ u â€‹ s â€‹ e â€‹ d = L â€‹ N â€‹ ( x o t ) âŠ— L â€‹ N â€‹ ( x o f ) x^{fused}=LN(x_{o}^{t})\\otimes LN(x_{o}^{f}) (3) Finally, the projection functions for pose ğ’« p ( . ) \\mathcal{P}_{p}(.) and expression ğ’« e ( . ) \\mathcal{P}_{e}(.) are applied to the fused feature vector as follows: x e â€‹ m â€‹ b e = ğ’« e â€‹ ( x f â€‹ u â€‹ s â€‹ e â€‹ d ) \\displaystyle x^{e}_{emb}=\\mathcal{P}_{e}(x^{fused}) (4) x e â€‹ m â€‹ b p = ğ’« p â€‹ ( x f â€‹ u â€‹ s â€‹ e â€‹ d ) \\displaystyle x^{p}_{emb}=\\mathcal{P}_{p}(x^{fused}) where x e â€‹ m â€‹ b e âˆˆ R m Ã— 50 x^{e}_{emb}\\in R^{m\\times 50} and x e â€‹ m â€‹ b p âˆˆ R m Ã— 6 x^{p}_{emb}\\in R^{m\\times 6} are the conditional feature vectors for pose and expression, respectively. In this work, ğ’« p ( . ) \\mathcal{P}_{p}(.) and ğ’« e ( . ) \\mathcal{P}_{e}(.) are designed as linear transformation functions to map the feature x f â€‹ u â€‹ s â€‹ e â€‹ d x^{fused} to the corresponding pose and expression feature vectors. Instruction to Facial Expression Transition : The proposed I2FET method is based on a conditional variational autoencoder architecture. The method includes encoders â„° e t ( . ) \\mathcal{E}_{e}^{t}(.) and â„° p t ( . ) \\mathcal{E}_{p}^{t}(.) , as well as decoders ğ’Ÿ e t ( . ) \\mathcal{D}^{t}_{e}(.) and ğ’Ÿ p t ( . ) \\mathcal{D}^{t}_{p}(.) , which are defined using three Multi-Layer Perceptrons (MLPs). These components are conditioned on feature vectors that represent expression and pose information, respectively. An overview of the proposed I2FET architecture is presented in Fig. 4 . Here, the encoders are defined as follows: â„° e t â€‹ ( e âŠ— x e â€‹ m â€‹ b e ) âŸ¶ ( Î¼ e , Ïƒ e ) \\mathcal{E}_{e}^{t}(e\\otimes x^{e}_{emb})\\longrightarrow(\\mu_{e},\\sigma_{e}) (5) and â„° p t â€‹ ( Î¸ âŠ— x e â€‹ m â€‹ b p ) âŸ¶ ( Î¼ p , Ïƒ p ) \\mathcal{E}_{p}^{t}(\\theta\\otimes x^{p}_{emb})\\longrightarrow(\\mu_{p},\\sigma_{p}) (6) where âŠ— \\otimes denotes the concatenation operator and x e â€‹ m â€‹ b e x^{e}_{emb} and x e â€‹ m â€‹ b p x^{p}_{emb} are conditional feature vectors for expression and pose, respectively. For the encoder stage, x e â€‹ m â€‹ b e x^{e}_{emb} and x e â€‹ m â€‹ b p x^{p}_{emb} are created by textual description x t x_{t} and the facial expression parameters ( e , Î¸ ) (e,\\theta) . Î¼ e \\mu_{e} and Î¼ p \\mu_{p} denote the mean and standard deviation of the expression distribution, whereas Ïƒ e \\sigma_{e} , and Ïƒ p \\sigma_{p} are the mean and standard deviation of the pose distribution. In this study, the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}^{e}_{emb} and x ^ e â€‹ m â€‹ b p \\hat{x}^{p}_{emb} in the decoder are computed by textual description x t x_{t} and the latent vectors z ^ p \\hat{z}_{p} and z ^ e \\hat{z}_{e} representing pose and expression, respectively. These latent vectors are calculated as follows: z ~ p = Ïƒ p âˆ— z p + Î¼ p \\displaystyle\\tilde{z}_{p}=\\sigma_{p}*z_{p}+\\mu_{p} (7) z ^ p = ğ’¯ p â€‹ ( z ~ p ) \\displaystyle\\hat{z}_{p}=\\mathcal{T}_{p}(\\tilde{z}_{p}) z ~ e = Ïƒ e âˆ— z e + Î¼ e \\displaystyle\\tilde{z}_{e}=\\sigma_{e}*z_{e}+\\mu_{e} z ^ e = ğ’¯ e â€‹ ( z ~ e ) \\displaystyle\\hat{z}_{e}=\\mathcal{T}_{e}(\\tilde{z}_{e}) where z p z_{p} and z e z_{e} are the latent vectors sampled from ğ’© â€‹ ( 0 , I ) \\mathcal{N}(0,I) . In this case, we utilize linear transformation functions ğ’¯ p ( . ) \\mathcal{T}_{p}(.) and ğ’¯ e ( . ) \\mathcal{T}_{e}(.) to transform z ~ p \\tilde{z}_{p} and z ~ e âˆˆ R m Ã— 16 \\tilde{z}_{e}\\in R^{m\\times 16} into z ^ p âˆˆ R m Ã— 6 \\hat{z}_{p}\\in R^{m\\times 6} and z ^ e âˆˆ R m Ã— 50 \\hat{z}_{e}\\in R^{m\\times 50} , respectively, similar to the expression and pose dimensions. Thus, the latent vectors z ~ e \\tilde{z}_{e} and z ~ p \\tilde{z}_{p} can be used as input for the IFED module. Then, the latent vectors z ^ e \\hat{z}_{e} and z ^ p \\hat{z}_{p} are concatenated together, and along with the text vector x t x^{t} , they are passed through the IFED module to obtain the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} as x ^ e â€‹ m â€‹ b e , x ^ e â€‹ m â€‹ b p = I â€‹ F â€‹ E â€‹ D â€‹ ( z ^ p âŠ— z ^ e , x t ) \\displaystyle\\hat{x}_{emb}^{e},\\hat{x}_{emb}^{p}=IFED(\\hat{z}_{p}\\otimes\\hat{z}_{e},x^{t}) (8) This approach enhances the semantic representation of latent vectors by refining the conditional feature vectors for decoders based on the learned latent vectors throughout the training process. The decoders are defined as follows ğ’Ÿ p t â€‹ ( z ~ p âŠ— x ^ e â€‹ m â€‹ b p ) âŸ¶ ( Î¸ ^ 0 , Î¸ ^ 1 ) \\mathcal{D}^{t}_{p}(\\tilde{z}_{p}\\otimes\\hat{x}^{p}_{emb})\\longrightarrow(\\hat{\\theta}_{0},\\hat{\\theta}_{1}) (9) and ğ’Ÿ e t â€‹ ( z ~ e âŠ— x ^ e â€‹ m â€‹ b e ) âŸ¶ ( e ^ 0 , e ^ 1 ) \\mathcal{D}^{t}_{e}(\\tilde{z}_{e}\\otimes\\hat{x}^{e}_{emb})\\longrightarrow(\\hat{e}_{0},\\hat{e}_{1}) (10) The parameters of the I2FET model are optimized by minimizing the reconstruction loss function â„’ M â€‹ S â€‹ E ( . , . ) \\mathcal{L}_{MSE}(.,.) between the ground truth coefficients e e , Î¸ \\theta , and the predicted coefficients e ^ , Î¸ ^ \\hat{e},\\hat{\\theta} . In addition, the Kullback-Leibler divergence loss function is utilized to minimize the distance between the probability distribution of the ground truth and that of the predicted values. â„’ e = â„’ M â€‹ S â€‹ E â€‹ ( e , e ^ ) \\displaystyle\\mathcal{L}_{e}=\\mathcal{L}_{MSE}(e,\\hat{e}) (11) + 0.5 â€‹ [ âˆ’ âˆ‘ i ( l â€‹ o â€‹ g â€‹ Ïƒ i 2 + 1 ) + âˆ‘ i Ïƒ i 2 + âˆ‘ i Î¼ i 2 ] \\displaystyle+5\\left[-\\sum_{i}(log\\>\\sigma_{i}^{2}+1)+\\sum_{i}\\sigma^{2}_{i}+\\sum_{i}\\mu_{i}^{2}\\right] â„’ p = â„’ M â€‹ S â€‹ E â€‹ ( Î¸ , Î¸ ^ ) \\displaystyle\\mathcal{L}_{p}=\\mathcal{L}_{MSE}(\\theta,\\hat{\\theta}) (12) + 0.5 â€‹ [ âˆ’ âˆ‘ i ( l â€‹ o â€‹ g â€‹ Ïƒ i 2 + 1 ) + âˆ‘ i Ïƒ i 2 + âˆ‘ i Î¼ i 2 ] \\displaystyle+5\\left[-\\sum_{i}(log\\>\\sigma_{i}^{2}+1)+\\sum_{i}\\sigma^{2}_{i}+\\sum_{i}\\mu_{i}^{2}\\right] where â„’ e \\mathcal{L}_{e} and â„’ p \\mathcal{L}_{p} denote the expression and pose loss function, respectively. Furthermore, we utilize the pretrained FLAME [ 22 ] , a parametric model of a 3D head that is integrated into DECA [ 9 ] , which has N = 5023 N=5023 base vertices V b âˆˆ â„› N Ã— 3 V_{b}\\in\\mathcal{R}^{N\\times 3} and two sets of M M and K K basis vectors that encode shape blendshapes ğ’® âˆˆ R N Ã— 3 Ã— M \\mathcal{S}\\in R^{N\\times 3\\times M} , and expression blendshapes B âˆˆ R N Ã— 3 Ã— K B\\in R^{N\\times 3\\times K} . The basic vectors are first blended and then Linear Blend Skinning (LBS) ğ’² ( . ) \\mathcal{W}(.) is applied to rotate the vertices following the pose Î¸ \\theta . The final reconstruction in world coordinates can be computed as follows ğ’± â€‹ ( Ï• , e , Î¸ ) = ğ’² â€‹ ( V b + ğ’® â€‹ Ï• + â„¬ â€‹ e , Î¸ ) \\mathcal{V}(\\phi,e,\\theta)=\\mathcal{W}(V_{b}+\\mathcal{S}\\phi+\\mathcal{B}e,\\theta) (13) where Ï• \\phi , e e , and Î¸ \\theta denote the identity shape, facial expression, and head pose parameters, respectively. In this case, FLAME is used to reconstruct vertices to improve the performance of I2FET. â„’ v = â„’ M â€‹ S â€‹ E â€‹ ( v , v ^ ) \\mathcal{L}_{v}=\\mathcal{L}_{MSE}(v,\\hat{v}) (14) where the vertex coordinates v v and v ^ \\hat{v} are computed by feeding the ground truth { Î¸ , e , Ï• } \\{\\theta,e,\\phi\\} and the reconstructed { Î¸ ^ , e ^ , Ï• } \\{\\hat{\\theta},\\hat{e},\\phi\\} through FLAME. Finally, the total loss function is given as â„’ t â€‹ o â€‹ t â€‹ a â€‹ l = â„’ e + â„’ p + â„’ v \\mathcal{L}_{total}=\\mathcal{L}_{e}+\\mathcal{L}_{p}+\\mathcal{L}_{v} (15) For the second branch, the pre-trained DECA [ 9 ] is used to encode the facial coefficients of the source face I s I_{s} including shape Ï• s âˆˆ R 1 Ã— 100 \\phi_{s}\\in R^{1\\times 100} , expression e s âˆˆ R 1 Ã— 50 e_{s}\\in R^{1\\times 50} , pose Î¸ s âˆˆ R 1 Ã— 6 \\theta_{s}\\in R^{1\\times 6} and camera c s âˆˆ R 1 Ã— 3 c_{s}\\in R^{1\\times 3} . Following this step, { Ï• s , e s , Î¸ s , c s } \\{\\phi_{s},e_{s},\\theta_{s},c_{s}\\} is integrated with the sequence of facial expression coefficients { ( e 0 , Î¸ 0 ) , ( e 1 , Î¸ 1 ) } \\{(e_{0},\\theta_{0}),(e_{1},\\theta_{1})\\} , generated by the first branch to generate the anchor facial expressions { s s ( 1 ) , s e 0 ( k ) , s e 1 ( T ) } \\{s^{(1)}_{s},s^{(k)}_{e_{0}},s^{(T)}_{e_{1}}\\} . Then, a linear interpolation function Î¨ ( . ) \\Psi(.) is applied to smoothen the transition between the two anchored facial expressions. The Î¨ â€‹ ( s ( l ) , s ( n ) ) \\Psi(s^{(l)},s^{(n)}) function for interpolation between facial expression sequences s ( l ) s^{(l)} and s ( n ) s^{(n)} can be defined as e ( k ) = Î´ âˆ— e ( l ) + ( 1 âˆ’ Î´ ) âˆ— e ( n ) \\displaystyle e^{(k)}=\\delta*e^{(l)}+(1-\\delta)*e^{(n)} (16) Î¸ ( k ) = Î´ âˆ— Î¸ ( l ) + ( 1 âˆ’ Î´ ) âˆ— Î¸ ( n ) \\displaystyle\\theta^{(k)}=\\delta*\\theta^{(l)}+(1-\\delta)*\\theta^{(n)} where 0 â‰¤ Î´ â‰¤ 1 0\\leq\\delta\\leq 1 is a linear coefficient and { e ( l ) , Î¸ ( l ) } âˆˆ s ( l ) \\{e^{(l)},\\theta^{(l)}\\}\\in s^{(l)} . This allows our framework to ensure temporal consistency when the motion changes are sampled uniformly. III-B 2 Face Rendering This module renders the facial appearance for the facial expression trajectories, generated by FET. In this work, the face rendering approach is based on the facial parameters of FLAME for integration with FET. Head Mesh Reconstruction: It generates the vertex coordinates corresponding to the facial expressions using Eq. 13 by using the sequence of facial expression transitions as the input. Then, the target vertex coordinates are obtained by combining these expressions with the deformation of hair and shoulder regions f H ( . ) f_{H}(.) from I s I_{s} as follows â„³ t = ğ’± â€‹ ( Ï• , e , Î¸ ) + f H â€‹ ( I s ) \\mathcal{M}^{t}=\\mathcal{V}(\\phi,e,\\theta)+f_{H}(I_{s}) (17) where f H ( . ) f_{H}(.) is the pre-trained linear deformation model [ 18 ] that is used to refine the vertices locations. Rendering : Following the head mesh reconstruction step, facial appearances can be rendered from the reconstructed head vertices â„³ = { m s ( 1 ) , m ( i ) , m e 0 ( k ) , m ( j ) , m e 1 ( T ) } \\mathcal{M}=\\{m_{s}^{(1)},m^{(i)},m_{e_{0}}^{(k)},m^{(j)},m_{e_{1}}^{(T)}\\} and the extracted neutral texture of the source image I s I_{s} by the texture decoder â„° t â€‹ e â€‹ x â€‹ ( I s ) \\mathcal{E}_{tex}(I_{s}) . The present work explores a face-rendering model suitable for facial expression transition. Therefore, we utilize pre-trained facial rendering models, known as state-of-the-art models [ 18 , 26 ] , capable of rendering facial appearances based on the facial parameters calculated with FLAME. Subsequently, the sequence of facial appearances ğ’´ = { y s ( 1 ) , y ( i ) , y e 0 ( k ) , y ( j ) , y e 1 ( T ) } \\mathcal{Y}=\\{y_{s}^{(1)},y^{(i)},y_{e_{0}}^{(k)},y^{(j)},y_{e_{1}}^{(T)}\\} is generated as the output of the proposed framework. (a) (b) (c) Figure 5 : Statistics of the facial expressions utilized in this study for the CK+ (a) and CelebV-HQ (b) datasets, respectively, and a collection of sampled words extracted from the text instructions (c).\n\nIII-A Problem Statement\nWe aim to develop a model to generate 3D facial expression with two inputs: a face image I s I_{s} and a text instruction t t . The facial expression transition model ğ’¯ w â€‹ ( t , I s ) \\mathcal{T}_{w}(t,I_{s}) , having weights w w , is trained with a training set ğ’³ = { ( t , ( e 0 , Î¸ 0 ) , ( e 1 , Î¸ 1 ) ) ( j ) } j = 1 n \\mathcal{X}=\\{(t,(e_{0},\\theta_{0}),(e_{1},\\theta_{1}))^{(j)}\\}_{j=1}^{n} , where e 0 , e 1 âˆˆ R 1 Ã— 50 e_{0},e_{1}\\in R^{1\\times 50} denote expression vectors with 50 dimensions and Î¸ 0 , Î¸ 1 âˆˆ R 1 Ã— 6 \\theta_{0},\\theta_{1}\\in R^{1\\times 6} represent pose vectors with 6 dimensions corresponding to the first and next expressions in the facial expression sequence specified in instruction t t , respectively. n n is the number of samples in the training dataset. Our network learns ğ’¯ w ( . ) \\mathcal{T}_{w}(.) that approximates the data distribution p â€‹ ( ğ’³ ) p(\\mathcal{X}) and then generates a facial expression coefficient sequence { ( e 0 , Î¸ 0 ) , ( e 1 , Î¸ 1 ) } \\{(e_{0},\\theta_{0}),(e_{1},\\theta_{1})\\} with a given instruction t t . Next, a face rendering module ğ’¢ â€‹ ( I s , ğ’® ) \\mathcal{G}(I_{s},\\mathcal{S}) is used to render a sequence of facial appearances ğ’´ = { y s ( 1 ) , y ( i ) , y e 0 ( k ) , y ( j ) , y e 1 ( T ) } \\mathcal{Y}=\\{y_{s}^{(1)},y^{(i)},y_{e_{0}}^{(k)},y^{(j)},y_{e_{1}}^{(T)}\\} , with inputs, consisting of the texture of source image I s I_{s} and facial expression trajectories S = { s s ( 1 ) , s ( i ) , s e 0 ( k ) , s ( j ) , s e 1 ( T ) } S=\\{s_{s}^{(1)},s^{(i)},s_{e_{0}}^{(k)},s^{(j)},s_{e_{1}}^{(T)}\\} , where { s s ( 1 ) , s e 0 ( k ) , s e 1 ( T ) } \\{s_{s}^{(1)},s_{e_{0}}^{(k)},s_{e_{1}}^{(T)}\\} are anchor facial expression sequences corresponding to the source image, and specific facial expressions, generated by ğ’¯ w ( . ) \\mathcal{T}_{w}(.) . Then, facial expression sequences s ( i ) s^{(i)} and s ( j ) s^{(j)} transitioning between these anchor facial expressions can be inferred by a linear interpolation function to ensure the smoothness of the sequence of facial expressions.\n\nIII-B The Proposed Method\nFigure 2 provides an overview of the proposed framework whereby one can control 3D facial expressions of the monocular image I s I_{s} following an input instruction t t . The framework consists of two major modules: (1) Facial Expression Transition (FET), ğ’¯ w â€‹ ( t , I s ) \\mathcal{T}_{w}(t,I_{s}) , for generating diverse expression trajectories; (2) Face Rendering (FR), ğ’¢ â€‹ ( I s , S ) \\mathcal{G}(I_{s},S) , for rendering facial appearances based on the texture of the source image I s I_{s} and the expression trajectories S S generated by the FET module. Figure 2 : Overview of the proposed framework consisting of two major modules: Facial Expression Transition (FET) and Face Rendering (FR). First, textual vector x t x_{t} is obtained from the pre-trained CLIP encoder for the FET module. Simultaneously, latent representations z p z_{p} and z e z_{e} are drawn from ğ’© â€‹ ( 0 , I ) \\mathcal{N}(0,I) . Then, the latent vectors are processed and concatenated to obtain x ^ f \\hat{x}^{f} . Afterward, the IFED module utilizes x t x^{t} and x ^ f \\hat{x}^{f} as inputs to create conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} , and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} for the pose and expression decoders. Subsequently, for smoothness of the facial expression sequence, the facial expression in the source image ( e s , Î¸ s ) (e_{s},\\theta_{s}) is subjected to linear interpolation and the specific facial expressions ( e 0 , Î¸ 0 ) , ( e 1 , Î¸ 1 ) (e_{0},\\theta_{0}),(e_{1},\\theta_{1}) , generated by the I2FET decoders. These sequences are then combined with the shape Ï• s \\phi_{s} and camera c s c_{s} parameters of the source image, obtained from DECA, to form facial expression trajectories { s s ( 1 ) , s ( i ) , s e 0 ( k ) , s ( j ) , s e 1 ( T ) } \\{s_{s}^{(1)},s^{(i)},s^{(k)}_{e_{0}},s^{(j)},s_{e_{1}}^{(T)}\\} . For FR, head mesh reconstruction produces a flame mesh sequence { m s ( 1 ) , m ( i ) , m e 0 ( k ) , m ( j ) , m e 1 ( T ) } \\{m_{s}^{(1)},m^{(i)},m^{(k)}_{e_{0}},m^{(j)},m_{e_{1}}^{(T)}\\} aligned with the facial expression trajectories and the texture encoder extracts the facial appearance from the source image. Finally, the rendering module generates the facial appearances { y s ( 1 ) , y ( i ) , y e 0 ( k ) , y ( j ) , y e 1 ( T ) } \\{y_{s}^{(1)},y^{(i)},y^{(k)}_{e_{0}},y^{(j)},y_{e_{1}}^{(T)}\\} within the facial expression trajectories using the flame mesh sequence and the extracted facial appearance. III-B 1 Facial Expression Transition This module is designed to generate diverse expression trajectories that correspond to the facial parameters of the source face image I s I_{s} and the parameters of specific facial expressions, described in an instruction t t . The module has two primary branches: the first receives an instruction to generate the corresponding sequence of facial expression based on instruction t t , whereas the second takes a 2D face image to encode its facial coefficients and create the facial expression trajectories. Figure 3 : Flow diagram of IFED (Instruction-Driven Facial Expression Decomposer). It consists of CAFT and the decomposition of parameters. The major function of CAFT is to introduce cross-attention between the facial parameters and text instruction. The two outputs from CAFT pass through layer normalization and are combined to create the fused feature vector x f â€‹ u â€‹ s â€‹ e â€‹ d x^{fused} . Then, pose projection function ğ’« p \\mathcal{P}_{p} and expression projection function ğ’« e \\mathcal{P}_{e} are used to decompose x f â€‹ u â€‹ s â€‹ e â€‹ d x^{fused} into two conditional vectors for facial expression ( x e â€‹ m â€‹ b e x_{emb}^{e} ) and pose ( x e â€‹ m â€‹ b p x_{emb}^{p} ). Figure 4 : Detailed architecture of the I2FET (Instruction to Facial Expression Transition) method. Given an instruction and a sequence of facial parameters, including pose and expression, the IFED module utilizes the facial parameters and textual description extracted by CLIP to produce the corresponding conditional feature vectors for pose x e â€‹ m â€‹ b p x_{emb}^{p} and expression x e â€‹ m â€‹ b e x_{emb}^{e} , respectively. The pose and expression encoders then map this sequence of facial parameters and conditional feature vectors to latent representations corresponding to pose and expression. These latent vectors, along with the textual description, are passed through the IFED module to generate the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} . Following this, the expression and pose decoders reconstruct expression e ^ \\hat{e} and pose Î¸ ^ \\hat{\\theta} parameters using these latent representations and the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} , respectively. Finally, the pre-trained FLAME decoder reconstructs the vertex coordinates v ^ \\hat{v} based on e ^ \\hat{e} and Î¸ ^ \\hat{\\theta} . In the former, the pre-trained CLIP encoder [ 33 ] extracts text description x t âˆˆ R 77 Ã— 768 x^{t}\\in R^{77\\times 768} according to the given instruction t t . Then, the I2FET model generates a sequence of expression and pose coefficients for the facial expression trajectories. Instruction-driven Facial Expression Decomposer : A dual-branch vision transformer [ 3 ] is adopted for extracting multi-scale features and fusing cross-attention information at different scales because it is efficient for the image classification task. Here, the Cross-Attention for Facial Expression Parameter and Text (CAFT) is used to learn the relationship between instruction and facial expression parameters by enhancing the understanding of multimodal data and refining the latent representation for the Instruction to Facial Expression Transition model. The proposed IFED module is illustrated in Fig. 3 . In particular, we directly incorporate facial expression parameters and textual descriptions into CAFT instead of adding class tokens as in [ 3 ] . With respect to the transformer encoders, the first branch learns the correlation between the facial expression parameters, while the second branch focuses on learning the textual descriptions. To begin with, each vector representing facial expression parameters comprises two elements: expression and pose parameters, denoted as x f = { e , Î¸ } âˆˆ R m Ã— 56 x^{f}=\\{e,\\theta\\}\\in R^{m\\times 56} , where m m denotes the number of expressions in a given instruction. As [ 24 , 6 ] , we only utilize jaw pose Î¸ j â€‹ a â€‹ w \\theta^{jaw} and expression e e to convey facial expression information. Therefore, we rewrite x f = { e , Î¸ j â€‹ a â€‹ w } âˆˆ R m Ã— 53 x^{f}=\\{e,\\theta^{jaw}\\}\\in R^{m\\times 53} as the input of the first branch. Then, the transformer encoder is utilized as y f = x f + M â€‹ S â€‹ A â€‹ ( L â€‹ N â€‹ ( x f ) ) \\displaystyle y^{f}=x^{f}+MSA(LN(x^{f})) (1) x ^ f = y f + F â€‹ F â€‹ N â€‹ ( L â€‹ N â€‹ ( y f ) ) \\displaystyle\\hat{x}^{f}=y^{f}+FFN(LN(y^{f})) where x ^ f \\hat{x}^{f} denotes the output of the transformer encoder in the facial expression parameters branch â„° P \\mathcal{E}_{P} . L N ( . ) LN(.) , M S A ( . ) MSA(.) , and F F N ( . ) FFN(.) are the layer norm, multi-head self-attention, and feed-forward network, respectively. Likewise, in the text branch, we replicate x t x^{t} to create a vector { x } i = { 0 , 1 } t \\{x\\}^{t}_{i=\\{0,1\\}} for computational purposes. Subsequently, the input for the transformer encoder is generated using a text projection function P t ( . ) P_{t}(.) on { x } i = { 0 , 1 } t \\{x\\}^{t}_{i=\\{0,1\\}} . As a result, the textual description x ^ t \\hat{x}^{t} is obtained through the transformer encoder â„° t \\mathcal{E}_{t} . Both branches are then fused by cross-attention to capture the relationship between the facial expression parameters and textual descriptions. The fusion process can be expressed as y c f = h f â€‹ 2 â€‹ t â€‹ ( x ^ 0 f ) , y c t = h t â€‹ 2 â€‹ f â€‹ ( x ^ 0 t ) , \\displaystyle y_{c}^{f}=h^{f2t}(\\hat{x}_{0}^{f}),\\quad y_{c}^{t}=h^{t2f}(\\hat{x}_{0}^{t}), (2) x o f = g t â€‹ 2 â€‹ f â€‹ ( [ C â€‹ A â€‹ ( y c f âŠ— x 1 t ) + y c f ] ) âŠ— x ^ 1 f \\displaystyle x_{o}^{f}=g^{t2f}([CA(y^{f}_{c}\\otimes x_{1}^{t})+y_{c}^{f}])\\otimes\\hat{x}_{1}^{f} x o t = g f â€‹ 2 â€‹ t â€‹ ( [ C â€‹ A â€‹ ( y c t âŠ— x 1 f ) + y c t ] ) âŠ— x ^ 1 t \\displaystyle x_{o}^{t}=g^{f2t}([CA(y_{c}^{t}\\otimes x_{1}^{f})+y_{c}^{t}])\\otimes\\hat{x}_{1}^{t} where h t â€‹ 2 â€‹ f , h f â€‹ 2 â€‹ t , g t â€‹ 2 â€‹ f h^{t2f},h^{f2t},g^{t2f} , and g f â€‹ 2 â€‹ t g^{f2t} are the projection function and back-projection function between the vectors that represent the text and facial expression parameters, x o f âˆˆ R m Ã— 53 x_{o}^{f}\\in R^{m\\times 53} and x o t âˆˆ R m Ã— 768 x_{o}^{t}\\in R^{m\\times 768} are the outputs of CAFT, C â€‹ A CA denotes cross-attention and âŠ— \\otimes denotes the concatenation operator. Next, the fused feature vector is obtained by concatenating x o f x_{o}^{f} and x o t x_{o}^{t} , after passing through L N ( . ) LN(.) , as follows: x f â€‹ u â€‹ s â€‹ e â€‹ d = L â€‹ N â€‹ ( x o t ) âŠ— L â€‹ N â€‹ ( x o f ) x^{fused}=LN(x_{o}^{t})\\otimes LN(x_{o}^{f}) (3) Finally, the projection functions for pose ğ’« p ( . ) \\mathcal{P}_{p}(.) and expression ğ’« e ( . ) \\mathcal{P}_{e}(.) are applied to the fused feature vector as follows: x e â€‹ m â€‹ b e = ğ’« e â€‹ ( x f â€‹ u â€‹ s â€‹ e â€‹ d ) \\displaystyle x^{e}_{emb}=\\mathcal{P}_{e}(x^{fused}) (4) x e â€‹ m â€‹ b p = ğ’« p â€‹ ( x f â€‹ u â€‹ s â€‹ e â€‹ d ) \\displaystyle x^{p}_{emb}=\\mathcal{P}_{p}(x^{fused}) where x e â€‹ m â€‹ b e âˆˆ R m Ã— 50 x^{e}_{emb}\\in R^{m\\times 50} and x e â€‹ m â€‹ b p âˆˆ R m Ã— 6 x^{p}_{emb}\\in R^{m\\times 6} are the conditional feature vectors for pose and expression, respectively. In this work, ğ’« p ( . ) \\mathcal{P}_{p}(.) and ğ’« e ( . ) \\mathcal{P}_{e}(.) are designed as linear transformation functions to map the feature x f â€‹ u â€‹ s â€‹ e â€‹ d x^{fused} to the corresponding pose and expression feature vectors. Instruction to Facial Expression Transition : The proposed I2FET method is based on a conditional variational autoencoder architecture. The method includes encoders â„° e t ( . ) \\mathcal{E}_{e}^{t}(.) and â„° p t ( . ) \\mathcal{E}_{p}^{t}(.) , as well as decoders ğ’Ÿ e t ( . ) \\mathcal{D}^{t}_{e}(.) and ğ’Ÿ p t ( . ) \\mathcal{D}^{t}_{p}(.) , which are defined using three Multi-Layer Perceptrons (MLPs). These components are conditioned on feature vectors that represent expression and pose information, respectively. An overview of the proposed I2FET architecture is presented in Fig. 4 . Here, the encoders are defined as follows: â„° e t â€‹ ( e âŠ— x e â€‹ m â€‹ b e ) âŸ¶ ( Î¼ e , Ïƒ e ) \\mathcal{E}_{e}^{t}(e\\otimes x^{e}_{emb})\\longrightarrow(\\mu_{e},\\sigma_{e}) (5) and â„° p t â€‹ ( Î¸ âŠ— x e â€‹ m â€‹ b p ) âŸ¶ ( Î¼ p , Ïƒ p ) \\mathcal{E}_{p}^{t}(\\theta\\otimes x^{p}_{emb})\\longrightarrow(\\mu_{p},\\sigma_{p}) (6) where âŠ— \\otimes denotes the concatenation operator and x e â€‹ m â€‹ b e x^{e}_{emb} and x e â€‹ m â€‹ b p x^{p}_{emb} are conditional feature vectors for expression and pose, respectively. For the encoder stage, x e â€‹ m â€‹ b e x^{e}_{emb} and x e â€‹ m â€‹ b p x^{p}_{emb} are created by textual description x t x_{t} and the facial expression parameters ( e , Î¸ ) (e,\\theta) . Î¼ e \\mu_{e} and Î¼ p \\mu_{p} denote the mean and standard deviation of the expression distribution, whereas Ïƒ e \\sigma_{e} , and Ïƒ p \\sigma_{p} are the mean and standard deviation of the pose distribution. In this study, the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}^{e}_{emb} and x ^ e â€‹ m â€‹ b p \\hat{x}^{p}_{emb} in the decoder are computed by textual description x t x_{t} and the latent vectors z ^ p \\hat{z}_{p} and z ^ e \\hat{z}_{e} representing pose and expression, respectively. These latent vectors are calculated as follows: z ~ p = Ïƒ p âˆ— z p + Î¼ p \\displaystyle\\tilde{z}_{p}=\\sigma_{p}*z_{p}+\\mu_{p} (7) z ^ p = ğ’¯ p â€‹ ( z ~ p ) \\displaystyle\\hat{z}_{p}=\\mathcal{T}_{p}(\\tilde{z}_{p}) z ~ e = Ïƒ e âˆ— z e + Î¼ e \\displaystyle\\tilde{z}_{e}=\\sigma_{e}*z_{e}+\\mu_{e} z ^ e = ğ’¯ e â€‹ ( z ~ e ) \\displaystyle\\hat{z}_{e}=\\mathcal{T}_{e}(\\tilde{z}_{e}) where z p z_{p} and z e z_{e} are the latent vectors sampled from ğ’© â€‹ ( 0 , I ) \\mathcal{N}(0,I) . In this case, we utilize linear transformation functions ğ’¯ p ( . ) \\mathcal{T}_{p}(.) and ğ’¯ e ( . ) \\mathcal{T}_{e}(.) to transform z ~ p \\tilde{z}_{p} and z ~ e âˆˆ R m Ã— 16 \\tilde{z}_{e}\\in R^{m\\times 16} into z ^ p âˆˆ R m Ã— 6 \\hat{z}_{p}\\in R^{m\\times 6} and z ^ e âˆˆ R m Ã— 50 \\hat{z}_{e}\\in R^{m\\times 50} , respectively, similar to the expression and pose dimensions. Thus, the latent vectors z ~ e \\tilde{z}_{e} and z ~ p \\tilde{z}_{p} can be used as input for the IFED module. Then, the latent vectors z ^ e \\hat{z}_{e} and z ^ p \\hat{z}_{p} are concatenated together, and along with the text vector x t x^{t} , they are passed through the IFED module to obtain the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} as x ^ e â€‹ m â€‹ b e , x ^ e â€‹ m â€‹ b p = I â€‹ F â€‹ E â€‹ D â€‹ ( z ^ p âŠ— z ^ e , x t ) \\displaystyle\\hat{x}_{emb}^{e},\\hat{x}_{emb}^{p}=IFED(\\hat{z}_{p}\\otimes\\hat{z}_{e},x^{t}) (8) This approach enhances the semantic representation of latent vectors by refining the conditional feature vectors for decoders based on the learned latent vectors throughout the training process. The decoders are defined as follows ğ’Ÿ p t â€‹ ( z ~ p âŠ— x ^ e â€‹ m â€‹ b p ) âŸ¶ ( Î¸ ^ 0 , Î¸ ^ 1 ) \\mathcal{D}^{t}_{p}(\\tilde{z}_{p}\\otimes\\hat{x}^{p}_{emb})\\longrightarrow(\\hat{\\theta}_{0},\\hat{\\theta}_{1}) (9) and ğ’Ÿ e t â€‹ ( z ~ e âŠ— x ^ e â€‹ m â€‹ b e ) âŸ¶ ( e ^ 0 , e ^ 1 ) \\mathcal{D}^{t}_{e}(\\tilde{z}_{e}\\otimes\\hat{x}^{e}_{emb})\\longrightarrow(\\hat{e}_{0},\\hat{e}_{1}) (10) The parameters of the I2FET model are optimized by minimizing the reconstruction loss function â„’ M â€‹ S â€‹ E ( . , . ) \\mathcal{L}_{MSE}(.,.) between the ground truth coefficients e e , Î¸ \\theta , and the predicted coefficients e ^ , Î¸ ^ \\hat{e},\\hat{\\theta} . In addition, the Kullback-Leibler divergence loss function is utilized to minimize the distance between the probability distribution of the ground truth and that of the predicted values. â„’ e = â„’ M â€‹ S â€‹ E â€‹ ( e , e ^ ) \\displaystyle\\mathcal{L}_{e}=\\mathcal{L}_{MSE}(e,\\hat{e}) (11) + 0.5 â€‹ [ âˆ’ âˆ‘ i ( l â€‹ o â€‹ g â€‹ Ïƒ i 2 + 1 ) + âˆ‘ i Ïƒ i 2 + âˆ‘ i Î¼ i 2 ] \\displaystyle+5\\left[-\\sum_{i}(log\\>\\sigma_{i}^{2}+1)+\\sum_{i}\\sigma^{2}_{i}+\\sum_{i}\\mu_{i}^{2}\\right] â„’ p = â„’ M â€‹ S â€‹ E â€‹ ( Î¸ , Î¸ ^ ) \\displaystyle\\mathcal{L}_{p}=\\mathcal{L}_{MSE}(\\theta,\\hat{\\theta}) (12) + 0.5 â€‹ [ âˆ’ âˆ‘ i ( l â€‹ o â€‹ g â€‹ Ïƒ i 2 + 1 ) + âˆ‘ i Ïƒ i 2 + âˆ‘ i Î¼ i 2 ] \\displaystyle+5\\left[-\\sum_{i}(log\\>\\sigma_{i}^{2}+1)+\\sum_{i}\\sigma^{2}_{i}+\\sum_{i}\\mu_{i}^{2}\\right] where â„’ e \\mathcal{L}_{e} and â„’ p \\mathcal{L}_{p} denote the expression and pose loss function, respectively. Furthermore, we utilize the pretrained FLAME [ 22 ] , a parametric model of a 3D head that is integrated into DECA [ 9 ] , which has N = 5023 N=5023 base vertices V b âˆˆ â„› N Ã— 3 V_{b}\\in\\mathcal{R}^{N\\times 3} and two sets of M M and K K basis vectors that encode shape blendshapes ğ’® âˆˆ R N Ã— 3 Ã— M \\mathcal{S}\\in R^{N\\times 3\\times M} , and expression blendshapes B âˆˆ R N Ã— 3 Ã— K B\\in R^{N\\times 3\\times K} . The basic vectors are first blended and then Linear Blend Skinning (LBS) ğ’² ( . ) \\mathcal{W}(.) is applied to rotate the vertices following the pose Î¸ \\theta . The final reconstruction in world coordinates can be computed as follows ğ’± â€‹ ( Ï• , e , Î¸ ) = ğ’² â€‹ ( V b + ğ’® â€‹ Ï• + â„¬ â€‹ e , Î¸ ) \\mathcal{V}(\\phi,e,\\theta)=\\mathcal{W}(V_{b}+\\mathcal{S}\\phi+\\mathcal{B}e,\\theta) (13) where Ï• \\phi , e e , and Î¸ \\theta denote the identity shape, facial expression, and head pose parameters, respectively. In this case, FLAME is used to reconstruct vertices to improve the performance of I2FET. â„’ v = â„’ M â€‹ S â€‹ E â€‹ ( v , v ^ ) \\mathcal{L}_{v}=\\mathcal{L}_{MSE}(v,\\hat{v}) (14) where the vertex coordinates v v and v ^ \\hat{v} are computed by feeding the ground truth { Î¸ , e , Ï• } \\{\\theta,e,\\phi\\} and the reconstructed { Î¸ ^ , e ^ , Ï• } \\{\\hat{\\theta},\\hat{e},\\phi\\} through FLAME. Finally, the total loss function is given as â„’ t â€‹ o â€‹ t â€‹ a â€‹ l = â„’ e + â„’ p + â„’ v \\mathcal{L}_{total}=\\mathcal{L}_{e}+\\mathcal{L}_{p}+\\mathcal{L}_{v} (15) For the second branch, the pre-trained DECA [ 9 ] is used to encode the facial coefficients of the source face I s I_{s} including shape Ï• s âˆˆ R 1 Ã— 100 \\phi_{s}\\in R^{1\\times 100} , expression e s âˆˆ R 1 Ã— 50 e_{s}\\in R^{1\\times 50} , pose Î¸ s âˆˆ R 1 Ã— 6 \\theta_{s}\\in R^{1\\times 6} and camera c s âˆˆ R 1 Ã— 3 c_{s}\\in R^{1\\times 3} . Following this step, { Ï• s , e s , Î¸ s , c s } \\{\\phi_{s},e_{s},\\theta_{s},c_{s}\\} is integrated with the sequence of facial expression coefficients { ( e 0 , Î¸ 0 ) , ( e 1 , Î¸ 1 ) } \\{(e_{0},\\theta_{0}),(e_{1},\\theta_{1})\\} , generated by the first branch to generate the anchor facial expressions { s s ( 1 ) , s e 0 ( k ) , s e 1 ( T ) } \\{s^{(1)}_{s},s^{(k)}_{e_{0}},s^{(T)}_{e_{1}}\\} . Then, a linear interpolation function Î¨ ( . ) \\Psi(.) is applied to smoothen the transition between the two anchored facial expressions. The Î¨ â€‹ ( s ( l ) , s ( n ) ) \\Psi(s^{(l)},s^{(n)}) function for interpolation between facial expression sequences s ( l ) s^{(l)} and s ( n ) s^{(n)} can be defined as e ( k ) = Î´ âˆ— e ( l ) + ( 1 âˆ’ Î´ ) âˆ— e ( n ) \\displaystyle e^{(k)}=\\delta*e^{(l)}+(1-\\delta)*e^{(n)} (16) Î¸ ( k ) = Î´ âˆ— Î¸ ( l ) + ( 1 âˆ’ Î´ ) âˆ— Î¸ ( n ) \\displaystyle\\theta^{(k)}=\\delta*\\theta^{(l)}+(1-\\delta)*\\theta^{(n)} where 0 â‰¤ Î´ â‰¤ 1 0\\leq\\delta\\leq 1 is a linear coefficient and { e ( l ) , Î¸ ( l ) } âˆˆ s ( l ) \\{e^{(l)},\\theta^{(l)}\\}\\in s^{(l)} . This allows our framework to ensure temporal consistency when the motion changes are sampled uniformly.\n\nIII-B 1 Facial Expression Transition\nThis module is designed to generate diverse expression trajectories that correspond to the facial parameters of the source face image I s I_{s} and the parameters of specific facial expressions, described in an instruction t t . The module has two primary branches: the first receives an instruction to generate the corresponding sequence of facial expression based on instruction t t , whereas the second takes a 2D face image to encode its facial coefficients and create the facial expression trajectories. Figure 3 : Flow diagram of IFED (Instruction-Driven Facial Expression Decomposer). It consists of CAFT and the decomposition of parameters. The major function of CAFT is to introduce cross-attention between the facial parameters and text instruction. The two outputs from CAFT pass through layer normalization and are combined to create the fused feature vector x f â€‹ u â€‹ s â€‹ e â€‹ d x^{fused} . Then, pose projection function ğ’« p \\mathcal{P}_{p} and expression projection function ğ’« e \\mathcal{P}_{e} are used to decompose x f â€‹ u â€‹ s â€‹ e â€‹ d x^{fused} into two conditional vectors for facial expression ( x e â€‹ m â€‹ b e x_{emb}^{e} ) and pose ( x e â€‹ m â€‹ b p x_{emb}^{p} ). Figure 4 : Detailed architecture of the I2FET (Instruction to Facial Expression Transition) method. Given an instruction and a sequence of facial parameters, including pose and expression, the IFED module utilizes the facial parameters and textual description extracted by CLIP to produce the corresponding conditional feature vectors for pose x e â€‹ m â€‹ b p x_{emb}^{p} and expression x e â€‹ m â€‹ b e x_{emb}^{e} , respectively. The pose and expression encoders then map this sequence of facial parameters and conditional feature vectors to latent representations corresponding to pose and expression. These latent vectors, along with the textual description, are passed through the IFED module to generate the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} . Following this, the expression and pose decoders reconstruct expression e ^ \\hat{e} and pose Î¸ ^ \\hat{\\theta} parameters using these latent representations and the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} , respectively. Finally, the pre-trained FLAME decoder reconstructs the vertex coordinates v ^ \\hat{v} based on e ^ \\hat{e} and Î¸ ^ \\hat{\\theta} . In the former, the pre-trained CLIP encoder [ 33 ] extracts text description x t âˆˆ R 77 Ã— 768 x^{t}\\in R^{77\\times 768} according to the given instruction t t . Then, the I2FET model generates a sequence of expression and pose coefficients for the facial expression trajectories. Instruction-driven Facial Expression Decomposer : A dual-branch vision transformer [ 3 ] is adopted for extracting multi-scale features and fusing cross-attention information at different scales because it is efficient for the image classification task. Here, the Cross-Attention for Facial Expression Parameter and Text (CAFT) is used to learn the relationship between instruction and facial expression parameters by enhancing the understanding of multimodal data and refining the latent representation for the Instruction to Facial Expression Transition model. The proposed IFED module is illustrated in Fig. 3 . In particular, we directly incorporate facial expression parameters and textual descriptions into CAFT instead of adding class tokens as in [ 3 ] . With respect to the transformer encoders, the first branch learns the correlation between the facial expression parameters, while the second branch focuses on learning the textual descriptions. To begin with, each vector representing facial expression parameters comprises two elements: expression and pose parameters, denoted as x f = { e , Î¸ } âˆˆ R m Ã— 56 x^{f}=\\{e,\\theta\\}\\in R^{m\\times 56} , where m m denotes the number of expressions in a given instruction. As [ 24 , 6 ] , we only utilize jaw pose Î¸ j â€‹ a â€‹ w \\theta^{jaw} and expression e e to convey facial expression information. Therefore, we rewrite x f = { e , Î¸ j â€‹ a â€‹ w } âˆˆ R m Ã— 53 x^{f}=\\{e,\\theta^{jaw}\\}\\in R^{m\\times 53} as the input of the first branch. Then, the transformer encoder is utilized as y f = x f + M â€‹ S â€‹ A â€‹ ( L â€‹ N â€‹ ( x f ) ) \\displaystyle y^{f}=x^{f}+MSA(LN(x^{f})) (1) x ^ f = y f + F â€‹ F â€‹ N â€‹ ( L â€‹ N â€‹ ( y f ) ) \\displaystyle\\hat{x}^{f}=y^{f}+FFN(LN(y^{f})) where x ^ f \\hat{x}^{f} denotes the output of the transformer encoder in the facial expression parameters branch â„° P \\mathcal{E}_{P} . L N ( . ) LN(.) , M S A ( . ) MSA(.) , and F F N ( . ) FFN(.) are the layer norm, multi-head self-attention, and feed-forward network, respectively. Likewise, in the text branch, we replicate x t x^{t} to create a vector { x } i = { 0 , 1 } t \\{x\\}^{t}_{i=\\{0,1\\}} for computational purposes. Subsequently, the input for the transformer encoder is generated using a text projection function P t ( . ) P_{t}(.) on { x } i = { 0 , 1 } t \\{x\\}^{t}_{i=\\{0,1\\}} . As a result, the textual description x ^ t \\hat{x}^{t} is obtained through the transformer encoder â„° t \\mathcal{E}_{t} . Both branches are then fused by cross-attention to capture the relationship between the facial expression parameters and textual descriptions. The fusion process can be expressed as y c f = h f â€‹ 2 â€‹ t â€‹ ( x ^ 0 f ) , y c t = h t â€‹ 2 â€‹ f â€‹ ( x ^ 0 t ) , \\displaystyle y_{c}^{f}=h^{f2t}(\\hat{x}_{0}^{f}),\\quad y_{c}^{t}=h^{t2f}(\\hat{x}_{0}^{t}), (2) x o f = g t â€‹ 2 â€‹ f â€‹ ( [ C â€‹ A â€‹ ( y c f âŠ— x 1 t ) + y c f ] ) âŠ— x ^ 1 f \\displaystyle x_{o}^{f}=g^{t2f}([CA(y^{f}_{c}\\otimes x_{1}^{t})+y_{c}^{f}])\\otimes\\hat{x}_{1}^{f} x o t = g f â€‹ 2 â€‹ t â€‹ ( [ C â€‹ A â€‹ ( y c t âŠ— x 1 f ) + y c t ] ) âŠ— x ^ 1 t \\displaystyle x_{o}^{t}=g^{f2t}([CA(y_{c}^{t}\\otimes x_{1}^{f})+y_{c}^{t}])\\otimes\\hat{x}_{1}^{t} where h t â€‹ 2 â€‹ f , h f â€‹ 2 â€‹ t , g t â€‹ 2 â€‹ f h^{t2f},h^{f2t},g^{t2f} , and g f â€‹ 2 â€‹ t g^{f2t} are the projection function and back-projection function between the vectors that represent the text and facial expression parameters, x o f âˆˆ R m Ã— 53 x_{o}^{f}\\in R^{m\\times 53} and x o t âˆˆ R m Ã— 768 x_{o}^{t}\\in R^{m\\times 768} are the outputs of CAFT, C â€‹ A CA denotes cross-attention and âŠ— \\otimes denotes the concatenation operator. Next, the fused feature vector is obtained by concatenating x o f x_{o}^{f} and x o t x_{o}^{t} , after passing through L N ( . ) LN(.) , as follows: x f â€‹ u â€‹ s â€‹ e â€‹ d = L â€‹ N â€‹ ( x o t ) âŠ— L â€‹ N â€‹ ( x o f ) x^{fused}=LN(x_{o}^{t})\\otimes LN(x_{o}^{f}) (3) Finally, the projection functions for pose ğ’« p ( . ) \\mathcal{P}_{p}(.) and expression ğ’« e ( . ) \\mathcal{P}_{e}(.) are applied to the fused feature vector as follows: x e â€‹ m â€‹ b e = ğ’« e â€‹ ( x f â€‹ u â€‹ s â€‹ e â€‹ d ) \\displaystyle x^{e}_{emb}=\\mathcal{P}_{e}(x^{fused}) (4) x e â€‹ m â€‹ b p = ğ’« p â€‹ ( x f â€‹ u â€‹ s â€‹ e â€‹ d ) \\displaystyle x^{p}_{emb}=\\mathcal{P}_{p}(x^{fused}) where x e â€‹ m â€‹ b e âˆˆ R m Ã— 50 x^{e}_{emb}\\in R^{m\\times 50} and x e â€‹ m â€‹ b p âˆˆ R m Ã— 6 x^{p}_{emb}\\in R^{m\\times 6} are the conditional feature vectors for pose and expression, respectively. In this work, ğ’« p ( . ) \\mathcal{P}_{p}(.) and ğ’« e ( . ) \\mathcal{P}_{e}(.) are designed as linear transformation functions to map the feature x f â€‹ u â€‹ s â€‹ e â€‹ d x^{fused} to the corresponding pose and expression feature vectors. Instruction to Facial Expression Transition : The proposed I2FET method is based on a conditional variational autoencoder architecture. The method includes encoders â„° e t ( . ) \\mathcal{E}_{e}^{t}(.) and â„° p t ( . ) \\mathcal{E}_{p}^{t}(.) , as well as decoders ğ’Ÿ e t ( . ) \\mathcal{D}^{t}_{e}(.) and ğ’Ÿ p t ( . ) \\mathcal{D}^{t}_{p}(.) , which are defined using three Multi-Layer Perceptrons (MLPs). These components are conditioned on feature vectors that represent expression and pose information, respectively. An overview of the proposed I2FET architecture is presented in Fig. 4 . Here, the encoders are defined as follows: â„° e t â€‹ ( e âŠ— x e â€‹ m â€‹ b e ) âŸ¶ ( Î¼ e , Ïƒ e ) \\mathcal{E}_{e}^{t}(e\\otimes x^{e}_{emb})\\longrightarrow(\\mu_{e},\\sigma_{e}) (5) and â„° p t â€‹ ( Î¸ âŠ— x e â€‹ m â€‹ b p ) âŸ¶ ( Î¼ p , Ïƒ p ) \\mathcal{E}_{p}^{t}(\\theta\\otimes x^{p}_{emb})\\longrightarrow(\\mu_{p},\\sigma_{p}) (6) where âŠ— \\otimes denotes the concatenation operator and x e â€‹ m â€‹ b e x^{e}_{emb} and x e â€‹ m â€‹ b p x^{p}_{emb} are conditional feature vectors for expression and pose, respectively. For the encoder stage, x e â€‹ m â€‹ b e x^{e}_{emb} and x e â€‹ m â€‹ b p x^{p}_{emb} are created by textual description x t x_{t} and the facial expression parameters ( e , Î¸ ) (e,\\theta) . Î¼ e \\mu_{e} and Î¼ p \\mu_{p} denote the mean and standard deviation of the expression distribution, whereas Ïƒ e \\sigma_{e} , and Ïƒ p \\sigma_{p} are the mean and standard deviation of the pose distribution. In this study, the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}^{e}_{emb} and x ^ e â€‹ m â€‹ b p \\hat{x}^{p}_{emb} in the decoder are computed by textual description x t x_{t} and the latent vectors z ^ p \\hat{z}_{p} and z ^ e \\hat{z}_{e} representing pose and expression, respectively. These latent vectors are calculated as follows: z ~ p = Ïƒ p âˆ— z p + Î¼ p \\displaystyle\\tilde{z}_{p}=\\sigma_{p}*z_{p}+\\mu_{p} (7) z ^ p = ğ’¯ p â€‹ ( z ~ p ) \\displaystyle\\hat{z}_{p}=\\mathcal{T}_{p}(\\tilde{z}_{p}) z ~ e = Ïƒ e âˆ— z e + Î¼ e \\displaystyle\\tilde{z}_{e}=\\sigma_{e}*z_{e}+\\mu_{e} z ^ e = ğ’¯ e â€‹ ( z ~ e ) \\displaystyle\\hat{z}_{e}=\\mathcal{T}_{e}(\\tilde{z}_{e}) where z p z_{p} and z e z_{e} are the latent vectors sampled from ğ’© â€‹ ( 0 , I ) \\mathcal{N}(0,I) . In this case, we utilize linear transformation functions ğ’¯ p ( . ) \\mathcal{T}_{p}(.) and ğ’¯ e ( . ) \\mathcal{T}_{e}(.) to transform z ~ p \\tilde{z}_{p} and z ~ e âˆˆ R m Ã— 16 \\tilde{z}_{e}\\in R^{m\\times 16} into z ^ p âˆˆ R m Ã— 6 \\hat{z}_{p}\\in R^{m\\times 6} and z ^ e âˆˆ R m Ã— 50 \\hat{z}_{e}\\in R^{m\\times 50} , respectively, similar to the expression and pose dimensions. Thus, the latent vectors z ~ e \\tilde{z}_{e} and z ~ p \\tilde{z}_{p} can be used as input for the IFED module. Then, the latent vectors z ^ e \\hat{z}_{e} and z ^ p \\hat{z}_{p} are concatenated together, and along with the text vector x t x^{t} , they are passed through the IFED module to obtain the conditional feature vectors x ^ e â€‹ m â€‹ b e \\hat{x}_{emb}^{e} and x ^ e â€‹ m â€‹ b p \\hat{x}_{emb}^{p} as x ^ e â€‹ m â€‹ b e , x ^ e â€‹ m â€‹ b p = I â€‹ F â€‹ E â€‹ D â€‹ ( z ^ p âŠ— z ^ e , x t ) \\displaystyle\\hat{x}_{emb}^{e},\\hat{x}_{emb}^{p}=IFED(\\hat{z}_{p}\\otimes\\hat{z}_{e},x^{t}) (8)\n\nIII-B 2 Face Rendering\nThis module renders the facial appearance for the facial expression trajectories, generated by FET. In this work, the face rendering approach is based on the facial parameters of FLAME for integration with FET. Head Mesh Reconstruction: It generates the vertex coordinates corresponding to the facial expressions using Eq. 13 by using the sequence of facial expression transitions as the input. Then, the target vertex coordinates are obtained by combining these expressions with the deformation of hair and shoulder regions f H ( . ) f_{H}(.) from I s I_{s} as follows â„³ t = ğ’± â€‹ ( Ï• , e , Î¸ ) + f H â€‹ ( I s ) \\mathcal{M}^{t}=\\mathcal{V}(\\phi,e,\\theta)+f_{H}(I_{s}) (17) where f H ( . ) f_{H}(.) is the pre-trained linear deformation model [ 18 ] that is used to refine the vertices locations. Rendering : Following the head mesh reconstruction step, facial appearances can be rendered from the reconstructed head vertices â„³ = { m s ( 1 ) , m ( i ) , m e 0 ( k ) , m ( j ) , m e 1 ( T ) } \\mathcal{M}=\\{m_{s}^{(1)},m^{(i)},m_{e_{0}}^{(k)},m^{(j)},m_{e_{1}}^{(T)}\\} and the extracted neutral texture of the source image I s I_{s} by the texture decoder â„° t â€‹ e â€‹ x â€‹ ( I s ) \\mathcal{E}_{tex}(I_{s}) . The present work explores a face-rendering model suitable for facial expression transition. Therefore, we utilize pre-trained facial rendering models, known as state-of-the-art models [ 18 , 26 ] , capable of rendering facial appearances based on the facial parameters calculated with FLAME. Subsequently, the sequence of facial appearances ğ’´ = { y s ( 1 ) , y ( i ) , y e 0 ( k ) , y ( j ) , y e 1 ( T ) } \\mathcal{Y}=\\{y_{s}^{(1)},y^{(i)},y_{e_{0}}^{(k)},y^{(j)},y_{e_{1}}^{(T)}\\} is generated as the output of the proposed framework. (a) (b) (c) Figure 5 : Statistics of the facial expressions utilized in this study for the CK+ (a) and CelebV-HQ (b) datasets, respectively, and a collection of sampled words extracted from the text instructions (c).\n\nIV Experiments\nIV-A Dataset TABLE I : N 1 , N 2 âˆˆ { N_{1},N_{2}\\in\\{ HAPPINESS, DISGUST, ANGER, FEAR, SURPRISE, CONTEMPT, SADNESS } \\} IN THE CK+ DATASET AND N 1 , N 2 âˆˆ { N_{1},N_{2}\\in\\{ HAPPINESS, DISGUST, ANGER, FEAR, SURPRISE, CONTEMPT, SADNESS, NEUTRAL } \\} IN THE CELEBV-HQ DATASET. No Instruction/ Prompts 1 Turn this face from [ N 1 N_{1} ] to [ N 2 N_{2} ]. 2 Change this face from [ N 1 N_{1} ] to [ N 2 N_{2} ]. 3 Transform this face from [ N 1 N_{1} ] to [ N 2 N_{2} ]. 4 Modify this face, changing it from [ N 1 N_{1} ] to [ N 2 N_{2} ]. 5 Replace this face from [ N 1 N_{1} to [ N 2 N_{2} ]. IV-A 1 CK+ dataset The CK+ dataset was supplemented with text instructions for facial expression generation and transition. The original CK+ dataset [ 25 ] includes 117 subjects and 7 emotions, such as happiness, disgust, anger, fear, surprise, contempt, and sadness . The instructions are created to describe the transitions between these facial expressions. To extract the facial parameters and the shape image corresponding to the facial expression in the image, we utilize the pre-trained DECA model. For each sample, a tuple is created with an instruction, the target shape image, and facial parameters that correspond to the same subject. The CK+ dataset, in this study, contains a total of 26,352 samples, and the composition of this dataset in terms of the seven facial expressions is shown in Fig. 5(a) . IV-A 2 CelebV-HQ dataset Similar to the CK+ case, text instructions were added to the CelebV-HQ dataset for our experiments [ 44 ] . The original CelebV-HQ dataset included 15,653 subjects and 8 expressions of neutral, happiness, disgust, anger, fear, surprise, contempt, and sadness . The images in the CelebV-HQ dataset were spontaneously collected and presented many challenges such as highly non-frontal head poses, blur, and brightness. This problem was overcome by using a head pose algorithm [ 23 ] to estimate parameters such as the yaw, roll, and pitch , of the head pose ranging from -15 to 15 degrees. Given that the subjects were chosen indiscriminately, we select those with more than two facial expressions. Then, the pre-trained DECA was applied to extract the corresponding shape images and facial parameters. The CelebV-HQ dataset adopted in this study includes 614 subjects and 28,335 samples, and Fig. 5(b) shows a statistical breakdown of the eight facial expressions. Table I lists the template sentences that are used to generate guided instructions for facial expression transitions. We generated five instructions for each sample and depicted the transition from expression N 1 N_{1} to expression N 2 N_{2} . Subsequently, one presentation sentence is randomly chosen from these instructions. Fig. 5(c) illustrates some sampled words used in the text instructions. Figure 6 : The shapes of facial expression are generated using images from the CK+ dataset. The top and bottom rows illustrate surprised and angry expressions with various head poses. IV-B Implementation The I2FET model was trained for 200 epochs with a batch size of 128 and a learning rate of 8 â€‹ e âˆ’ 4 8e^{-4} . The Adam optimizer [ 20 ] was adopted to optimize the model. For the experiments, 10% of the dataset was allocated to the testing set, whereas the remaining portion was used for training. Then, a validation set was drawn from 10% of the training set. In this work, we repeated each experiment ten times and used the average results to evaluate the overall performance of the proposed I2FET. Because our task is new and a specific method that would allow a fair comparison is not available, we chose MotionClip [ 39 ] as a baseline. For the face rendering experiment, the configuration by [ 18 , 26 ] was used. IV-C Classification of Facial Expressions In this study, it was important to employ an appropriate classifier to evaluate the method that was used to generate facial expressions as prior studies [ 27 , 46 ] . Given that the CK+ and CelebV-HQ datasets had skewed distributions, the performance for minor classes could be affected by the facial expression classifiers trained on these imbalanced datasets, in which the results would be biased towards the major class. Moreover, the effectiveness of the classification model could also be affected by head movements, which were commonly encountered as shown in Fig. 6 . These problems were addressed by conducting an experiment to identify an appropriate classifier that can handle not only imbalanced datasets but also invariant rotation transformations. ResNet-101 [ 14 ] , MEK [ 43 ] , and ResNet-101 integrated with Reweighted Focal Loss (RFL) [ 5 ] were utilized to evaluate the performance of facial expression classification. For this purpose, augmented datasets were created by rotating the original CK+ and CelebV-HQ datasets at angles of Â± 10 o , Â± 15 o \\pm 10^{o},\\pm 15^{o} , and Â± 30 o \\pm 30^{o} . Then, the three networks are trained using the original datasets and evaluated on all datasets. The Accuracy was employed to assess whether samples had been correctly classified across all classes, while G-mean was used to evaluate the ability of the model to effectively recognize minor classes that are predicted by the facial expression classification model. The supplementary data in the ablation study in Section IV-E suggests that MEK can address imbalanced datasets but is sensitive to rotational transformations. ResNet101 demonstrates positive results with rotation transformations but performs worse on datasets that are highly imbalanced. ResNet101 integrated with RFL outperforms ResNet101 and MEK in both scenarios. As a result, ResNet101 with RFL was selected as our classifier for evaluating the ability of our models to generate facial expressions. IV-D Results (a) (b) (c) Figure 7 : Visualization of the learned latent space for seven facial expressions by (a) MotionClip, (b) I2FET without IFED, and (c) I2FET with IFED, respectively, using t-SNE. (a) (b) Figure 8 : Confusion matrices for seven facial expressions by (a) MotionClip and (b) our method on CK+. (a) (b) Figure 9 : Confusion matrices for eight facial expressions by (a) MotionClip and (b) our method on CelebV-HQ. TABLE II : COMPARISON OF ACCURACY BETWEEN OUR METHOD AND MOTIONCLIP ON THE CK+ DATASET Method Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow Ground-truth 99.69 Â± \\pm 0.0001 99.39 Â± \\pm 0.0002 99.69 Â± \\pm 9.3e -5 MotionClip 52.00 Â± \\pm 0.0058 20.00 Â± \\pm 0.0082 40.48 Â± \\pm 0.0060 Ours 91.44 Â± \\pm 0.0024 84.03 Â± \\pm 0.0034 80.30 Â± \\pm 0.0064 TABLE III : COMPARISON OF ACCURACY BETWEEN OUR METHOD AND MOTIONCLIP ON THE CELEBV-HQ DATASET Method Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow Ground-truth 93.67 Â± \\pm 0.0000 87.89 Â± \\pm 0.0000 95.04 Â± \\pm 1.1e -16 MotionClip 40.00 Â± \\pm 0.0039 13.73 Â± \\pm 0.0056 34.42 Â± \\pm 0.0069 Ours 58.24 Â± \\pm 0.0028 33.45 Â± \\pm 0.0052 46.47 Â± \\pm 0.0065 TABLE IV : RESULT OBTAINED BY COMBINING OUR METHOD WITH ROME OR CVTHEAD FOR TEXTURE GENERATION ON THE CK+ AND CELEBV-HQ DATASETS, RESPECTIVELY. NOTE THAT â†“ \\downarrow INDICATES THAT LOWER IS BETTER, WHEREAS â†‘ \\uparrow DENOTES THAT THE HIGHER IS BETTER. Dataset Method L â†“ 1 {}_{1}\\downarrow PSNR â†‘ {\\uparrow} LPIPS â†“ {\\downarrow} MS_SSIM â†‘ {\\uparrow} CK+ Ours+ROME 0.199 9.939 0.490 0.347 Ours+CVTHead 0.005 33.74 0.021 0.978 CelebV-HQ Ours+ROME 0.168 11.316 0.505 0.418 Ours+CVTHead 0.003 37.08 0.010 0.990 Figure 10 : Qualitative comparison between 3D expressive heads that were generated by MotionClip and our method, respectively. The input, consisting of a shape image and instruction, was used for generating expressions 1 and 2 using either MotionClip (b) or Ours (c) on CK+ and CelebV-HQ (d, e), respectively. Figure 11 : Qualitative comparison between generated 3D expressive faces when our method is combined with texture generation [ 18 , 26 ] to generate the desired expressions. The input (a), consisting of the source image and instruction, was used for generating expressions 1 and 2 using either Ours+ROME (b) or Ours+CVTHead (c) on CK+ and CelebV-HQ (d, e), respectively. Figure 12 : Successive snapshots of the generated 3D expressive faces that include a neutral expression between two facial expressions instructed by text prompts. As the present study aims to generate an avatar with a 3D facial expression from a 2D image of a face and transform the facial emotion according to the given instructions, two major challenges arise (1) facial expression transition based on expression information given by the instruction and (2) rendering the facial appearance such that it corresponds with the desired facial expression. Two experiments were prepared to evaluate the results obtained with the proposed framework both quantitatively and qualitatively. IV-D 1 Quantitative Comparison Two tasks are prepared for quantitative comparison. Facial Expression Transition The DECA encoder extracts the shape, facial expression, and pose from the source image. At the same time, the CLIP encoder combined with IFED and I2FET was used to interpret the text instruction: the initial facial expression, the final facial expression, and pose variation. To compare our approach with MotionClip, the first accuracy metric (Acc 1 ) was used to count the correctly classified samples for all expression labels in the instructions. The other one (Acc 2 ) was used to measure the correctly classified samples corresponding to both expression labels presented in each input instruction. The result suggests that the accuracy of our model is higher than that of MotionClip on both datasets as summarized in Table II . For example, our model reaches 91.44 % accuracy for generating an independent expression and 84.03 % for generating expression sequences on the CK+ dataset. The Gmean of our model is 80.30%. For the CelebV-HQ dataset, which is the real-world dataset, our model scores 58.24 %, 33.45 %, and 46.47 % for Acc 1 , Acc 2 , and Gmean, respectively, as indicated in Table III . This result suggests that our model outperforms MotionClip for learning the facial expression parameters. Visualization with t-SNE : Fig. 7 shows three learned latent representations, i.e. with MotionClip, I2FET, and I2FET with IFED, for seven facial expressions. These visual representations indicate that segregation between facial expressions is highly distinctive when IFED is combined with I2FET, compared with either MotionClip or I2FET alone. Confusion Matrices Analysis : Fig. 8 and Fig. 9 show that IFED enhances the accuracy across most classes compared to MotionClip on both datasets. Yet, a few case of misclassification occurred between certain classes of expressions, such as anger and sadness, or contempt and anger with sadness on CK+. In real-world datasets such as CelebV-HQ, some misclassification occurs between classes, particularly those with challenging expressions such as fear and surprise, or contempt and neutral. Face Rendering : For the face-rendering task, FET was combined with state-of-the-art models [ 18 , 26 ] for rendering facial appearances that correspond to specific expressions requested by instruction. This was accomplished by integrating whether ROME [ 18 ] or CVTHead [ 26 ] with facial expression transition. In this case, DECA extracts facial parameters to render geometrical information for a specific face. Instead of shapes, the RGB images and instructions are used to evaluate the performance. The L 1 loss, Peak Signal to Noise Ratio (PSNR), Learned Perceptual Image Patch Similarity (LPIPS) [ 42 ] , and Multi-Scale Structured Similarity (MS-SSIM) were used to measure the quality of reconstruction between the target image and synthesized image in the generated expression. As indicated by Table IV , our model with CVTHead produces the best result. IV-D 2 Qualitative Comparison In a similar vein, two tasks were prepared for qualitative comparisons. Facial Expression Transition : Fig. 10 compares sequences of facial expressions, generated by Motionclip and our method on CK+ and CelebV-HQ, respectively. These results clearly show that our method can generate facial expressions that convey more accurate expressions than MotionClip. Face Rendering : The resulting expressions can be qualitatively compared by viewing and deciding which expression looks better and more natural than the other. Fig. 11 shows the generated facial expressions based on learning the latent space of facial parameters on both datasets. The viewer may notice the difference between (b) and (c) for the CK+ dataset, and between (d) and (e) for the CelebV-HQ dataset, respectively, in terms of facial expressions and facial texture. For instance, the facial transition from surprise to happiness seen in the 2nd row is clearer in (c) than in (b), and the facial texture seen in the 4th row is much smoother in (c) than in (b). A similar observation can be made between (d) and (e), suggesting that Ours+CVTHead is superior to than Ours+ROME. For instance, in some cases, those enclosed within dashed boxes in Fig. 11 , a subtle distortion arose either when the facial region of the source image was occluded or when the instructions describe expressions with significant pose changes, such as transitioning from surprise to happiness or from fear to surprise. This led to inconsistencies, and missing details in the generated facial regions compared to Ours+CVTHead. A comparison between the two datasets, such as between (b) and (d) or between (c) and (e), reveals that the visual quality of CK+ is better than that of CelebV-HQ. These results confirm that our model has outstanding ability to transform facial expressions, learned on CK+ or CelebV-HQ, to a specific facial image. In addition, a user study was carried out by requesting 24 subjects to view 60 randomly paired images of various identities selected from either CVTHead + MotionClip or CVTHead + Ours. Subjects were asked to choose the expression closest to the expressions specified in the instructions or indicate whether none of them matched the provided expressions. Naturally, they were unaware as to which algorithm was used to the presented images. Four subjects viewed each video and voted for the one that more closely resembled the expression in the instructions. In cases in which the votes were tied, an additional subject was asked to make the final decision. Based on these outcomes, CVTHead + Ours achieved 73% accuracy, whereas CVTHead + MotionClip yielded 58% accuracy for CK+. For CelebV-HQ, CVTHead + Ours and CVTHead + MotionClip achieved 77% and 70.5% accuracy, respectively. IV-D 3 Inference Time During inference, CVTHead + MotionClip takes 3.97 seconds to generate a video, whereas CVTHead + Ours only takes 3.92 seconds on a single NVIDIA RTX A6000, suggesting that our model is a slightly faster than MotionClip.\n\nIV-A Dataset\nTABLE I : N 1 , N 2 âˆˆ { N_{1},N_{2}\\in\\{ HAPPINESS, DISGUST, ANGER, FEAR, SURPRISE, CONTEMPT, SADNESS } \\} IN THE CK+ DATASET AND N 1 , N 2 âˆˆ { N_{1},N_{2}\\in\\{ HAPPINESS, DISGUST, ANGER, FEAR, SURPRISE, CONTEMPT, SADNESS, NEUTRAL } \\} IN THE CELEBV-HQ DATASET. No Instruction/ Prompts 1 Turn this face from [ N 1 N_{1} ] to [ N 2 N_{2} ]. 2 Change this face from [ N 1 N_{1} ] to [ N 2 N_{2} ]. 3 Transform this face from [ N 1 N_{1} ] to [ N 2 N_{2} ]. 4 Modify this face, changing it from [ N 1 N_{1} ] to [ N 2 N_{2} ]. 5 Replace this face from [ N 1 N_{1} to [ N 2 N_{2} ]. IV-A 1 CK+ dataset The CK+ dataset was supplemented with text instructions for facial expression generation and transition. The original CK+ dataset [ 25 ] includes 117 subjects and 7 emotions, such as happiness, disgust, anger, fear, surprise, contempt, and sadness . The instructions are created to describe the transitions between these facial expressions. To extract the facial parameters and the shape image corresponding to the facial expression in the image, we utilize the pre-trained DECA model. For each sample, a tuple is created with an instruction, the target shape image, and facial parameters that correspond to the same subject. The CK+ dataset, in this study, contains a total of 26,352 samples, and the composition of this dataset in terms of the seven facial expressions is shown in Fig. 5(a) . IV-A 2 CelebV-HQ dataset Similar to the CK+ case, text instructions were added to the CelebV-HQ dataset for our experiments [ 44 ] . The original CelebV-HQ dataset included 15,653 subjects and 8 expressions of neutral, happiness, disgust, anger, fear, surprise, contempt, and sadness . The images in the CelebV-HQ dataset were spontaneously collected and presented many challenges such as highly non-frontal head poses, blur, and brightness. This problem was overcome by using a head pose algorithm [ 23 ] to estimate parameters such as the yaw, roll, and pitch , of the head pose ranging from -15 to 15 degrees. Given that the subjects were chosen indiscriminately, we select those with more than two facial expressions. Then, the pre-trained DECA was applied to extract the corresponding shape images and facial parameters. The CelebV-HQ dataset adopted in this study includes 614 subjects and 28,335 samples, and Fig. 5(b) shows a statistical breakdown of the eight facial expressions. Table I lists the template sentences that are used to generate guided instructions for facial expression transitions. We generated five instructions for each sample and depicted the transition from expression N 1 N_{1} to expression N 2 N_{2} . Subsequently, one presentation sentence is randomly chosen from these instructions. Fig. 5(c) illustrates some sampled words used in the text instructions. Figure 6 : The shapes of facial expression are generated using images from the CK+ dataset. The top and bottom rows illustrate surprised and angry expressions with various head poses.\n\nIV-A 1 CK+ dataset\nThe CK+ dataset was supplemented with text instructions for facial expression generation and transition. The original CK+ dataset [ 25 ] includes 117 subjects and 7 emotions, such as happiness, disgust, anger, fear, surprise, contempt, and sadness . The instructions are created to describe the transitions between these facial expressions. To extract the facial parameters and the shape image corresponding to the facial expression in the image, we utilize the pre-trained DECA model. For each sample, a tuple is created with an instruction, the target shape image, and facial parameters that correspond to the same subject. The CK+ dataset, in this study, contains a total of 26,352 samples, and the composition of this dataset in terms of the seven facial expressions is shown in Fig. 5(a) .\n\nIV-A 2 CelebV-HQ dataset\nSimilar to the CK+ case, text instructions were added to the CelebV-HQ dataset for our experiments [ 44 ] . The original CelebV-HQ dataset included 15,653 subjects and 8 expressions of neutral, happiness, disgust, anger, fear, surprise, contempt, and sadness . The images in the CelebV-HQ dataset were spontaneously collected and presented many challenges such as highly non-frontal head poses, blur, and brightness. This problem was overcome by using a head pose algorithm [ 23 ] to estimate parameters such as the yaw, roll, and pitch , of the head pose ranging from -15 to 15 degrees. Given that the subjects were chosen indiscriminately, we select those with more than two facial expressions. Then, the pre-trained DECA was applied to extract the corresponding shape images and facial parameters. The CelebV-HQ dataset adopted in this study includes 614 subjects and 28,335 samples, and Fig. 5(b) shows a statistical breakdown of the eight facial expressions. Table I lists the template sentences that are used to generate guided instructions for facial expression transitions. We generated five instructions for each sample and depicted the transition from expression N 1 N_{1} to expression N 2 N_{2} . Subsequently, one presentation sentence is randomly chosen from these instructions. Fig. 5(c) illustrates some sampled words used in the text instructions. Figure 6 : The shapes of facial expression are generated using images from the CK+ dataset. The top and bottom rows illustrate surprised and angry expressions with various head poses.\n\nIV-B Implementation\nThe I2FET model was trained for 200 epochs with a batch size of 128 and a learning rate of 8 â€‹ e âˆ’ 4 8e^{-4} . The Adam optimizer [ 20 ] was adopted to optimize the model. For the experiments, 10% of the dataset was allocated to the testing set, whereas the remaining portion was used for training. Then, a validation set was drawn from 10% of the training set. In this work, we repeated each experiment ten times and used the average results to evaluate the overall performance of the proposed I2FET. Because our task is new and a specific method that would allow a fair comparison is not available, we chose MotionClip [ 39 ] as a baseline. For the face rendering experiment, the configuration by [ 18 , 26 ] was used.\n\nIV-C Classification of Facial Expressions\nIn this study, it was important to employ an appropriate classifier to evaluate the method that was used to generate facial expressions as prior studies [ 27 , 46 ] . Given that the CK+ and CelebV-HQ datasets had skewed distributions, the performance for minor classes could be affected by the facial expression classifiers trained on these imbalanced datasets, in which the results would be biased towards the major class. Moreover, the effectiveness of the classification model could also be affected by head movements, which were commonly encountered as shown in Fig. 6 . These problems were addressed by conducting an experiment to identify an appropriate classifier that can handle not only imbalanced datasets but also invariant rotation transformations. ResNet-101 [ 14 ] , MEK [ 43 ] , and ResNet-101 integrated with Reweighted Focal Loss (RFL) [ 5 ] were utilized to evaluate the performance of facial expression classification. For this purpose, augmented datasets were created by rotating the original CK+ and CelebV-HQ datasets at angles of Â± 10 o , Â± 15 o \\pm 10^{o},\\pm 15^{o} , and Â± 30 o \\pm 30^{o} . Then, the three networks are trained using the original datasets and evaluated on all datasets. The Accuracy was employed to assess whether samples had been correctly classified across all classes, while G-mean was used to evaluate the ability of the model to effectively recognize minor classes that are predicted by the facial expression classification model. The supplementary data in the ablation study in Section IV-E suggests that MEK can address imbalanced datasets but is sensitive to rotational transformations. ResNet101 demonstrates positive results with rotation transformations but performs worse on datasets that are highly imbalanced. ResNet101 integrated with RFL outperforms ResNet101 and MEK in both scenarios. As a result, ResNet101 with RFL was selected as our classifier for evaluating the ability of our models to generate facial expressions.\n\nIV-D Results\n(a) (b) (c) Figure 7 : Visualization of the learned latent space for seven facial expressions by (a) MotionClip, (b) I2FET without IFED, and (c) I2FET with IFED, respectively, using t-SNE. (a) (b) Figure 8 : Confusion matrices for seven facial expressions by (a) MotionClip and (b) our method on CK+. (a) (b) Figure 9 : Confusion matrices for eight facial expressions by (a) MotionClip and (b) our method on CelebV-HQ. TABLE II : COMPARISON OF ACCURACY BETWEEN OUR METHOD AND MOTIONCLIP ON THE CK+ DATASET Method Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow Ground-truth 99.69 Â± \\pm 0.0001 99.39 Â± \\pm 0.0002 99.69 Â± \\pm 9.3e -5 MotionClip 52.00 Â± \\pm 0.0058 20.00 Â± \\pm 0.0082 40.48 Â± \\pm 0.0060 Ours 91.44 Â± \\pm 0.0024 84.03 Â± \\pm 0.0034 80.30 Â± \\pm 0.0064 TABLE III : COMPARISON OF ACCURACY BETWEEN OUR METHOD AND MOTIONCLIP ON THE CELEBV-HQ DATASET Method Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow Ground-truth 93.67 Â± \\pm 0.0000 87.89 Â± \\pm 0.0000 95.04 Â± \\pm 1.1e -16 MotionClip 40.00 Â± \\pm 0.0039 13.73 Â± \\pm 0.0056 34.42 Â± \\pm 0.0069 Ours 58.24 Â± \\pm 0.0028 33.45 Â± \\pm 0.0052 46.47 Â± \\pm 0.0065 TABLE IV : RESULT OBTAINED BY COMBINING OUR METHOD WITH ROME OR CVTHEAD FOR TEXTURE GENERATION ON THE CK+ AND CELEBV-HQ DATASETS, RESPECTIVELY. NOTE THAT â†“ \\downarrow INDICATES THAT LOWER IS BETTER, WHEREAS â†‘ \\uparrow DENOTES THAT THE HIGHER IS BETTER. Dataset Method L â†“ 1 {}_{1}\\downarrow PSNR â†‘ {\\uparrow} LPIPS â†“ {\\downarrow} MS_SSIM â†‘ {\\uparrow} CK+ Ours+ROME 0.199 9.939 0.490 0.347 Ours+CVTHead 0.005 33.74 0.021 0.978 CelebV-HQ Ours+ROME 0.168 11.316 0.505 0.418 Ours+CVTHead 0.003 37.08 0.010 0.990 Figure 10 : Qualitative comparison between 3D expressive heads that were generated by MotionClip and our method, respectively. The input, consisting of a shape image and instruction, was used for generating expressions 1 and 2 using either MotionClip (b) or Ours (c) on CK+ and CelebV-HQ (d, e), respectively. Figure 11 : Qualitative comparison between generated 3D expressive faces when our method is combined with texture generation [ 18 , 26 ] to generate the desired expressions. The input (a), consisting of the source image and instruction, was used for generating expressions 1 and 2 using either Ours+ROME (b) or Ours+CVTHead (c) on CK+ and CelebV-HQ (d, e), respectively. Figure 12 : Successive snapshots of the generated 3D expressive faces that include a neutral expression between two facial expressions instructed by text prompts. As the present study aims to generate an avatar with a 3D facial expression from a 2D image of a face and transform the facial emotion according to the given instructions, two major challenges arise (1) facial expression transition based on expression information given by the instruction and (2) rendering the facial appearance such that it corresponds with the desired facial expression. Two experiments were prepared to evaluate the results obtained with the proposed framework both quantitatively and qualitatively. IV-D 1 Quantitative Comparison Two tasks are prepared for quantitative comparison. Facial Expression Transition The DECA encoder extracts the shape, facial expression, and pose from the source image. At the same time, the CLIP encoder combined with IFED and I2FET was used to interpret the text instruction: the initial facial expression, the final facial expression, and pose variation. To compare our approach with MotionClip, the first accuracy metric (Acc 1 ) was used to count the correctly classified samples for all expression labels in the instructions. The other one (Acc 2 ) was used to measure the correctly classified samples corresponding to both expression labels presented in each input instruction. The result suggests that the accuracy of our model is higher than that of MotionClip on both datasets as summarized in Table II . For example, our model reaches 91.44 % accuracy for generating an independent expression and 84.03 % for generating expression sequences on the CK+ dataset. The Gmean of our model is 80.30%. For the CelebV-HQ dataset, which is the real-world dataset, our model scores 58.24 %, 33.45 %, and 46.47 % for Acc 1 , Acc 2 , and Gmean, respectively, as indicated in Table III . This result suggests that our model outperforms MotionClip for learning the facial expression parameters. Visualization with t-SNE : Fig. 7 shows three learned latent representations, i.e. with MotionClip, I2FET, and I2FET with IFED, for seven facial expressions. These visual representations indicate that segregation between facial expressions is highly distinctive when IFED is combined with I2FET, compared with either MotionClip or I2FET alone. Confusion Matrices Analysis : Fig. 8 and Fig. 9 show that IFED enhances the accuracy across most classes compared to MotionClip on both datasets. Yet, a few case of misclassification occurred between certain classes of expressions, such as anger and sadness, or contempt and anger with sadness on CK+. In real-world datasets such as CelebV-HQ, some misclassification occurs between classes, particularly those with challenging expressions such as fear and surprise, or contempt and neutral. Face Rendering : For the face-rendering task, FET was combined with state-of-the-art models [ 18 , 26 ] for rendering facial appearances that correspond to specific expressions requested by instruction. This was accomplished by integrating whether ROME [ 18 ] or CVTHead [ 26 ] with facial expression transition. In this case, DECA extracts facial parameters to render geometrical information for a specific face. Instead of shapes, the RGB images and instructions are used to evaluate the performance. The L 1 loss, Peak Signal to Noise Ratio (PSNR), Learned Perceptual Image Patch Similarity (LPIPS) [ 42 ] , and Multi-Scale Structured Similarity (MS-SSIM) were used to measure the quality of reconstruction between the target image and synthesized image in the generated expression. As indicated by Table IV , our model with CVTHead produces the best result. IV-D 2 Qualitative Comparison In a similar vein, two tasks were prepared for qualitative comparisons. Facial Expression Transition : Fig. 10 compares sequences of facial expressions, generated by Motionclip and our method on CK+ and CelebV-HQ, respectively. These results clearly show that our method can generate facial expressions that convey more accurate expressions than MotionClip. Face Rendering : The resulting expressions can be qualitatively compared by viewing and deciding which expression looks better and more natural than the other. Fig. 11 shows the generated facial expressions based on learning the latent space of facial parameters on both datasets. The viewer may notice the difference between (b) and (c) for the CK+ dataset, and between (d) and (e) for the CelebV-HQ dataset, respectively, in terms of facial expressions and facial texture. For instance, the facial transition from surprise to happiness seen in the 2nd row is clearer in (c) than in (b), and the facial texture seen in the 4th row is much smoother in (c) than in (b). A similar observation can be made between (d) and (e), suggesting that Ours+CVTHead is superior to than Ours+ROME. For instance, in some cases, those enclosed within dashed boxes in Fig. 11 , a subtle distortion arose either when the facial region of the source image was occluded or when the instructions describe expressions with significant pose changes, such as transitioning from surprise to happiness or from fear to surprise. This led to inconsistencies, and missing details in the generated facial regions compared to Ours+CVTHead. A comparison between the two datasets, such as between (b) and (d) or between (c) and (e), reveals that the visual quality of CK+ is better than that of CelebV-HQ. These results confirm that our model has outstanding ability to transform facial expressions, learned on CK+ or CelebV-HQ, to a specific facial image. In addition, a user study was carried out by requesting 24 subjects to view 60 randomly paired images of various identities selected from either CVTHead + MotionClip or CVTHead + Ours. Subjects were asked to choose the expression closest to the expressions specified in the instructions or indicate whether none of them matched the provided expressions. Naturally, they were unaware as to which algorithm was used to the presented images. Four subjects viewed each video and voted for the one that more closely resembled the expression in the instructions. In cases in which the votes were tied, an additional subject was asked to make the final decision. Based on these outcomes, CVTHead + Ours achieved 73% accuracy, whereas CVTHead + MotionClip yielded 58% accuracy for CK+. For CelebV-HQ, CVTHead + Ours and CVTHead + MotionClip achieved 77% and 70.5% accuracy, respectively.\n\nIV-D 1 Quantitative Comparison\nTwo tasks are prepared for quantitative comparison. Facial Expression Transition The DECA encoder extracts the shape, facial expression, and pose from the source image. At the same time, the CLIP encoder combined with IFED and I2FET was used to interpret the text instruction: the initial facial expression, the final facial expression, and pose variation. To compare our approach with MotionClip, the first accuracy metric (Acc 1 ) was used to count the correctly classified samples for all expression labels in the instructions. The other one (Acc 2 ) was used to measure the correctly classified samples corresponding to both expression labels presented in each input instruction. The result suggests that the accuracy of our model is higher than that of MotionClip on both datasets as summarized in Table II . For example, our model reaches 91.44 % accuracy for generating an independent expression and 84.03 % for generating expression sequences on the CK+ dataset. The Gmean of our model is 80.30%. For the CelebV-HQ dataset, which is the real-world dataset, our model scores 58.24 %, 33.45 %, and 46.47 % for Acc 1 , Acc 2 , and Gmean, respectively, as indicated in Table III . This result suggests that our model outperforms MotionClip for learning the facial expression parameters. Visualization with t-SNE : Fig. 7 shows three learned latent representations, i.e. with MotionClip, I2FET, and I2FET with IFED, for seven facial expressions. These visual representations indicate that segregation between facial expressions is highly distinctive when IFED is combined with I2FET, compared with either MotionClip or I2FET alone. Confusion Matrices Analysis : Fig. 8 and Fig. 9 show that IFED enhances the accuracy across most classes compared to MotionClip on both datasets. Yet, a few case of misclassification occurred between certain classes of expressions, such as anger and sadness, or contempt and anger with sadness on CK+. In real-world datasets such as CelebV-HQ, some misclassification occurs between classes, particularly those with challenging expressions such as fear and surprise, or contempt and neutral. Face Rendering : For the face-rendering task, FET was combined with state-of-the-art models [ 18 , 26 ] for rendering facial appearances that correspond to specific expressions requested by instruction. This was accomplished by integrating whether ROME [ 18 ] or CVTHead [ 26 ] with facial expression transition. In this case, DECA extracts facial parameters to render geometrical information for a specific face. Instead of shapes, the RGB images and instructions are used to evaluate the performance. The L 1 loss, Peak Signal to Noise Ratio (PSNR), Learned Perceptual Image Patch Similarity (LPIPS) [ 42 ] , and Multi-Scale Structured Similarity (MS-SSIM) were used to measure the quality of reconstruction between the target image and synthesized image in the generated expression. As indicated by Table IV , our model with CVTHead produces the best result.\n\nIV-D 2 Qualitative Comparison\nIn a similar vein, two tasks were prepared for qualitative comparisons. Facial Expression Transition : Fig. 10 compares sequences of facial expressions, generated by Motionclip and our method on CK+ and CelebV-HQ, respectively. These results clearly show that our method can generate facial expressions that convey more accurate expressions than MotionClip. Face Rendering : The resulting expressions can be qualitatively compared by viewing and deciding which expression looks better and more natural than the other. Fig. 11 shows the generated facial expressions based on learning the latent space of facial parameters on both datasets. The viewer may notice the difference between (b) and (c) for the CK+ dataset, and between (d) and (e) for the CelebV-HQ dataset, respectively, in terms of facial expressions and facial texture. For instance, the facial transition from surprise to happiness seen in the 2nd row is clearer in (c) than in (b), and the facial texture seen in the 4th row is much smoother in (c) than in (b). A similar observation can be made between (d) and (e), suggesting that Ours+CVTHead is superior to than Ours+ROME. For instance, in some cases, those enclosed within dashed boxes in Fig. 11 , a subtle distortion arose either when the facial region of the source image was occluded or when the instructions describe expressions with significant pose changes, such as transitioning from surprise to happiness or from fear to surprise. This led to inconsistencies, and missing details in the generated facial regions compared to Ours+CVTHead. A comparison between the two datasets, such as between (b) and (d) or between (c) and (e), reveals that the visual quality of CK+ is better than that of CelebV-HQ. These results confirm that our model has outstanding ability to transform facial expressions, learned on CK+ or CelebV-HQ, to a specific facial image. In addition, a user study was carried out by requesting 24 subjects to view 60 randomly paired images of various identities selected from either CVTHead + MotionClip or CVTHead + Ours. Subjects were asked to choose the expression closest to the expressions specified in the instructions or indicate whether none of them matched the provided expressions. Naturally, they were unaware as to which algorithm was used to the presented images. Four subjects viewed each video and voted for the one that more closely resembled the expression in the instructions. In cases in which the votes were tied, an additional subject was asked to make the final decision. Based on these outcomes, CVTHead + Ours achieved 73% accuracy, whereas CVTHead + MotionClip yielded 58% accuracy for CK+. For CelebV-HQ, CVTHead + Ours and CVTHead + MotionClip achieved 77% and 70.5% accuracy, respectively.\n\nIV-D 3 Inference Time\nDuring inference, CVTHead + MotionClip takes 3.97 seconds to generate a video, whereas CVTHead + Ours only takes 3.92 seconds on a single NVIDIA RTX A6000, suggesting that our model is a slightly faster than MotionClip.\n\nIV-E Ablation Study\nThis section presents a series of ablation studies that were carried out to evaluate the performance of our model. IV-E 1 Effectiveness of the Proposed Components To demonstrate the effectiveness of each proposed component, we conducted a comparative experiment, as shown in Table V . It can be observed that the IFED module significantly improves the performance of I2FET compared to I2FET without IFED on both CK+ and CelebV-HQ datasets. Additionally, integration of the vertex loss function with IFED increases Acc 1 and Gmean on both datasets. Despite a slight decrease in Acc 2 on CK+, it still performs well on CelebV-HQ. TABLE V : COMPARISON OF THE ACCURACY BETWEEN THE PROPOSED DIFFERENT COMPONENTS FOR OUR METHOD (I2FET) ON THE TWO DATASETS Dataset Method Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow w/o IFED 76.89 Â± \\pm 0.0029 63.23 Â± \\pm 0.0047 63.59 Â± \\pm 0.0059 w/ IFED 91.30 Â± \\pm 0.0017 84.06 Â± \\pm 0.0026 79.41 Â± \\pm 0.0061 CK+ w/ IFED 91.44 Â± \\pm 0.0024 84.03 Â± \\pm 0.0034 80.30 Â± \\pm 0.0064 + w/ â„’ v \\mathcal{L}_{v} w/o IFED 48.35 Â± \\pm 0.0037 22.1 Â± \\pm 0.0040 32.88 Â± \\pm 0.0058 w/ IFED 57.80 Â± \\pm 0.0020 32.60 Â± \\pm 0.0037 46.04 Â± \\pm 0.0062 CelebV-HQ w/ IFED 58.24 Â± \\pm 0.0028 33.45 Â± \\pm 0.0052 46.47 Â± \\pm 0.0065 + w/ â„’ v \\mathcal{L}_{v} IV-E 2 Impact of Three Loss Functions To analyze the impact of each loss function in I2FET, an experiment was carried out to compare the performance of the model in various scenarios. Specifically, IFED was integrated with the I2FET model and each loss function was incrementally added to determine its impact on the model performance of the model. The results in Table VI indicate that the performance improved when the pose loss function integrated with the expression loss function, whereas the vertex loss function yielded a slight improvement when used alongside both the pose and expression loss functions. In this study, the vertex loss was integrated with other loss functions to capture the geometry of the face, to enhance the accuracy of facial expression recognition. Fig 13 and 14 are the confusion matrix of our model when the total loss function integrated with â„’ v \\mathcal{L}_{v} or without â„’ v \\mathcal{L}_{v} on the CK+ and CelebV-HQ datasets. Note that the fear accuracies for both datasets rather decrease slightly when the vertex loss is applied. It is known that the fear expression often overlaps with other expressions (e.g., surprise and happiness), thereby making it challenging to distinguish. Moreover, since the fear portion of CK+ is much larger than that of CelebV-HQ in Fig. 5, it appears that confusion during the facial expression transition has a reverse impact on Acc 2 as shown in Table VI (a) (b) Figure 13 : Confusion matrices for seven facial expressions by I2FET (a) without â„’ v \\mathcal{L}_{v} and (b) with â„’ v \\mathcal{L}_{v} on CK+. (a) (b) Figure 14 : Confusion matrices for eight facial expressions by I2FET (a) without â„’ v \\mathcal{L}_{v} and (b) with â„’ v \\mathcal{L}_{v} on CelebV-HQ. TABLE VI : COMPARISON OF THE IMPACT OF LOSS FUNCTIONS ON THE PERFORMANCE OF OUR METHOD (I2FET) ACROSS THE TWO DATASETS Dataset Method Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow â„’ e \\mathcal{L}_{e} 66.80 Â± \\pm 0.0056 45.96 Â± \\pm 0.0112 52.42 Â± \\pm 0.0730 â„’ e \\mathcal{L}_{e} + â„’ p \\mathcal{L}_{p} 91.30 Â± \\pm 0.0017 84.06 Â± \\pm 0.0026 79.41 Â± \\pm 0.0061 CK+ â„’ e \\mathcal{L}_{e} + â„’ p \\mathcal{L}_{p} + â„’ v \\mathcal{L}_{v} 91.44 Â± \\pm 0.0024 84.03 Â± \\pm 0.0034 80.30 Â± \\pm 0.0064 â„’ e \\mathcal{L}_{e} 37.57 Â± \\pm 0.0052 11.99 Â± \\pm 0.0326 25.82 Â± \\pm 0.0114 â„’ e \\mathcal{L}_{e} + â„’ p \\mathcal{L}_{p} 57.80 Â± \\pm 0.0020 32.60 Â± \\pm 0.0037 46.04 Â± \\pm 0.0062 CelebV-HQ â„’ e \\mathcal{L}_{e} + â„’ p \\mathcal{L}_{p} + â„’ v \\mathcal{L}_{v} 58.24 Â± \\pm 0.0028 33.45 Â± \\pm 0.0052 46.47 Â± \\pm 0.0065 IV-E 3 Impact of IFED on the Training Process This experiment was conducted to compare the two loss functions with and without incorporating the IFED module in I2FET. Fig. 15(a) and Fig. 15(b) show the results for CK+ and CelebV-HQ, respectively, confirming that I2FED plays an important role. IV-E 4 Effect of IFED on Intensity of Facial Expression Fig. 16 demonstrates that IFED helps I2FED to generate high-intensity facial expressions. (a) (b) Figure 15 : Comparison of the validation loss during the training process of I2FET w/ and w/o IFED, respectively, on (a) CK+ and (b) CelebV-HQ. The green curves represent I2FET without IFED, whereas the blue curves are the results for I2FET with IFED. Figure 16 : Qualitative comparison between 3D expressive faces generated (a) w/o and (b) w/ IFED. IV-E 5 Impact of Quantity of CAFT Layer The experiments were conducted to evaluate the results of the proposed I2FET model by designing the IFED module with varying numbers of CAFT layers. We fixed the number of transformer layers to 1 in both branches and adjusted the number of CAFT layers. The results in Table VII indicate that increasing the quantity of CAFT layers does not improve the performance of the proposed I2FET. IV-E 6 Impact of Quantity of Transformer Encoder in Facial Parameter Branch and Text Branch In this experiment, we utilized different configurations by adjusting the number of transformer encoder layers of IFED. The findings suggest that the I2FET model performs better when N = 2 and M = 1 compared to other configurations, particularly in terms of Acc 1 and G-mean, as shown in Table VIII . TABLE VII : COMPARISON OF THE ACCURACY OF THE I2FET METHOD ON BOTH DATASETS BY VARYING THE NUMBER OF CAFT LAYERS IN I2FED Dataset #Layers Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow 1 91.36 Â± \\pm 0.0023 84.13 Â± \\pm 0.0046 79.65 Â± \\pm 0.0080 CK+ 2 88.95 Â± \\pm 0.0019 80.95 Â± \\pm 0.0038 78.72 Â± \\pm 0.0071 1 58.25 Â± \\pm 0.0034 33.03 Â± \\pm 0.0056 46.46 Â± \\pm 0.0066 CelebV-HQ 2 55.68 Â± \\pm 0.0034 29.82 Â± \\pm 0.0066 42.59 Â± \\pm 0.0085 TABLE VIII : COMPARISON OF THE ACCURACY OF THE PROPOSED METHOD WITH THE NUMBER OF TRANSFORMER LAYERS FOR FACIAL PARAMETERS BRANCH ( N ) AND TEXT BRANCH ( M ) IN TFED ON CK+ Mode N M Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow A 1 1 91.36 Â± \\pm 0.0023 84.13 Â± \\pm 0.0046 79.67 Â± \\pm 0.008 B 2 1 91.44 Â± \\pm 0.0024 84.03 Â± \\pm 0.0034 80.30 Â± \\pm 0.0064 C 1 2 91.27 Â± \\pm 0.0019 83.84 Â± \\pm 0.0037 79.69 Â± \\pm 0.0052 D 2 2 89.07 Â± \\pm 0.0021 81.13 Â± \\pm 0.0034 78.71 Â± \\pm 0.0070\n\nIV-E 1 Effectiveness of the Proposed Components\nTo demonstrate the effectiveness of each proposed component, we conducted a comparative experiment, as shown in Table V . It can be observed that the IFED module significantly improves the performance of I2FET compared to I2FET without IFED on both CK+ and CelebV-HQ datasets. Additionally, integration of the vertex loss function with IFED increases Acc 1 and Gmean on both datasets. Despite a slight decrease in Acc 2 on CK+, it still performs well on CelebV-HQ. TABLE V : COMPARISON OF THE ACCURACY BETWEEN THE PROPOSED DIFFERENT COMPONENTS FOR OUR METHOD (I2FET) ON THE TWO DATASETS Dataset Method Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow w/o IFED 76.89 Â± \\pm 0.0029 63.23 Â± \\pm 0.0047 63.59 Â± \\pm 0.0059 w/ IFED 91.30 Â± \\pm 0.0017 84.06 Â± \\pm 0.0026 79.41 Â± \\pm 0.0061 CK+ w/ IFED 91.44 Â± \\pm 0.0024 84.03 Â± \\pm 0.0034 80.30 Â± \\pm 0.0064 + w/ â„’ v \\mathcal{L}_{v} w/o IFED 48.35 Â± \\pm 0.0037 22.1 Â± \\pm 0.0040 32.88 Â± \\pm 0.0058 w/ IFED 57.80 Â± \\pm 0.0020 32.60 Â± \\pm 0.0037 46.04 Â± \\pm 0.0062 CelebV-HQ w/ IFED 58.24 Â± \\pm 0.0028 33.45 Â± \\pm 0.0052 46.47 Â± \\pm 0.0065 + w/ â„’ v \\mathcal{L}_{v}\n\nIV-E 2 Impact of Three Loss Functions\nTo analyze the impact of each loss function in I2FET, an experiment was carried out to compare the performance of the model in various scenarios. Specifically, IFED was integrated with the I2FET model and each loss function was incrementally added to determine its impact on the model performance of the model. The results in Table VI indicate that the performance improved when the pose loss function integrated with the expression loss function, whereas the vertex loss function yielded a slight improvement when used alongside both the pose and expression loss functions. In this study, the vertex loss was integrated with other loss functions to capture the geometry of the face, to enhance the accuracy of facial expression recognition. Fig 13 and 14 are the confusion matrix of our model when the total loss function integrated with â„’ v \\mathcal{L}_{v} or without â„’ v \\mathcal{L}_{v} on the CK+ and CelebV-HQ datasets. Note that the fear accuracies for both datasets rather decrease slightly when the vertex loss is applied. It is known that the fear expression often overlaps with other expressions (e.g., surprise and happiness), thereby making it challenging to distinguish. Moreover, since the fear portion of CK+ is much larger than that of CelebV-HQ in Fig. 5, it appears that confusion during the facial expression transition has a reverse impact on Acc 2 as shown in Table VI (a) (b) Figure 13 : Confusion matrices for seven facial expressions by I2FET (a) without â„’ v \\mathcal{L}_{v} and (b) with â„’ v \\mathcal{L}_{v} on CK+. (a) (b) Figure 14 : Confusion matrices for eight facial expressions by I2FET (a) without â„’ v \\mathcal{L}_{v} and (b) with â„’ v \\mathcal{L}_{v} on CelebV-HQ. TABLE VI : COMPARISON OF THE IMPACT OF LOSS FUNCTIONS ON THE PERFORMANCE OF OUR METHOD (I2FET) ACROSS THE TWO DATASETS Dataset Method Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow â„’ e \\mathcal{L}_{e} 66.80 Â± \\pm 0.0056 45.96 Â± \\pm 0.0112 52.42 Â± \\pm 0.0730 â„’ e \\mathcal{L}_{e} + â„’ p \\mathcal{L}_{p} 91.30 Â± \\pm 0.0017 84.06 Â± \\pm 0.0026 79.41 Â± \\pm 0.0061 CK+ â„’ e \\mathcal{L}_{e} + â„’ p \\mathcal{L}_{p} + â„’ v \\mathcal{L}_{v} 91.44 Â± \\pm 0.0024 84.03 Â± \\pm 0.0034 80.30 Â± \\pm 0.0064 â„’ e \\mathcal{L}_{e} 37.57 Â± \\pm 0.0052 11.99 Â± \\pm 0.0326 25.82 Â± \\pm 0.0114 â„’ e \\mathcal{L}_{e} + â„’ p \\mathcal{L}_{p} 57.80 Â± \\pm 0.0020 32.60 Â± \\pm 0.0037 46.04 Â± \\pm 0.0062 CelebV-HQ â„’ e \\mathcal{L}_{e} + â„’ p \\mathcal{L}_{p} + â„’ v \\mathcal{L}_{v} 58.24 Â± \\pm 0.0028 33.45 Â± \\pm 0.0052 46.47 Â± \\pm 0.0065\n\nIV-E 3 Impact of IFED on the Training Process\nThis experiment was conducted to compare the two loss functions with and without incorporating the IFED module in I2FET. Fig. 15(a) and Fig. 15(b) show the results for CK+ and CelebV-HQ, respectively, confirming that I2FED plays an important role.\n\nIV-E 4 Effect of IFED on Intensity of Facial Expression\nFig. 16 demonstrates that IFED helps I2FED to generate high-intensity facial expressions. (a) (b) Figure 15 : Comparison of the validation loss during the training process of I2FET w/ and w/o IFED, respectively, on (a) CK+ and (b) CelebV-HQ. The green curves represent I2FET without IFED, whereas the blue curves are the results for I2FET with IFED. Figure 16 : Qualitative comparison between 3D expressive faces generated (a) w/o and (b) w/ IFED.\n\nIV-E 5 Impact of Quantity of CAFT Layer\nThe experiments were conducted to evaluate the results of the proposed I2FET model by designing the IFED module with varying numbers of CAFT layers. We fixed the number of transformer layers to 1 in both branches and adjusted the number of CAFT layers. The results in Table VII indicate that increasing the quantity of CAFT layers does not improve the performance of the proposed I2FET.\n\nIV-E 6 Impact of Quantity of Transformer Encoder in Facial Parameter Branch and Text Branch\nIn this experiment, we utilized different configurations by adjusting the number of transformer encoder layers of IFED. The findings suggest that the I2FET model performs better when N = 2 and M = 1 compared to other configurations, particularly in terms of Acc 1 and G-mean, as shown in Table VIII . TABLE VII : COMPARISON OF THE ACCURACY OF THE I2FET METHOD ON BOTH DATASETS BY VARYING THE NUMBER OF CAFT LAYERS IN I2FED Dataset #Layers Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow 1 91.36 Â± \\pm 0.0023 84.13 Â± \\pm 0.0046 79.65 Â± \\pm 0.0080 CK+ 2 88.95 Â± \\pm 0.0019 80.95 Â± \\pm 0.0038 78.72 Â± \\pm 0.0071 1 58.25 Â± \\pm 0.0034 33.03 Â± \\pm 0.0056 46.46 Â± \\pm 0.0066 CelebV-HQ 2 55.68 Â± \\pm 0.0034 29.82 Â± \\pm 0.0066 42.59 Â± \\pm 0.0085 TABLE VIII : COMPARISON OF THE ACCURACY OF THE PROPOSED METHOD WITH THE NUMBER OF TRANSFORMER LAYERS FOR FACIAL PARAMETERS BRANCH ( N ) AND TEXT BRANCH ( M ) IN TFED ON CK+ Mode N M Acc 1 (%) â†‘ \\uparrow Acc 2 (%) â†‘ \\uparrow Gmean (%) â†‘ \\uparrow A 1 1 91.36 Â± \\pm 0.0023 84.13 Â± \\pm 0.0046 79.67 Â± \\pm 0.008 B 2 1 91.44 Â± \\pm 0.0024 84.03 Â± \\pm 0.0034 80.30 Â± \\pm 0.0064 C 1 2 91.27 Â± \\pm 0.0019 83.84 Â± \\pm 0.0037 79.69 Â± \\pm 0.0052 D 2 2 89.07 Â± \\pm 0.0021 81.13 Â± \\pm 0.0034 78.71 Â± \\pm 0.0070\n\nIV-F Longer Facial Expression Generation with Neutral Expressions\nAccording to appraisal theory [ 35 ] , which is one of the major emotion theories, the dynamic of emotion is described as the sequential check theory of emotion differentiation. In other words, the differentiation of emotional states of humans is the result of a sequence of specific stimulus evaluation appraisal checks, rather than an action of hopping from one emotional state to another in a discrete manner. In our framework, the transition of the facial expression from A to B is similar to the differentiation of emotional states in a sequence: starting from A facial expression with strong emotion a neutral expression and then moving to strong facial expression B. In addition to appraisal theory, the present approach is suitable for investigating facial behaviors within the pleasure-arousal (P-A) space [ 34 ] , where the neutral expression serves as a transitional expression between two specific expressions. For this purpose, a neutral encoder-decoder (NED) was adopted to generate neutral faces. The aim thereof was to improve the performance of our framework when a neutral expression is not available in the instruction to increase its flexibility for extending facial expression sequences. IV-F 1 Implementation Details In this scenario, two datasets containing the parameters of neutral expressions and poses were constructed the pretrained DECA to extract the data from the CelebV-HQ and CK+ datasets. As the CK+ dataset does not contain neutral expressions, the first frame of each video was extracted and considered as the representative neutral case. Afterward, NED was trained on these two neutral datasets. IV-F 2 Network Architecture The neutral encoder-decoder model was constructed with fully connected layers. The encoder consisted of four fully connected layers, whereas the decoder was structured with four fully connected layers dedicated to pose and expression, respectively. IV-F 3 Analysis Similarly, we also conducted experiments to evaluate neutral expression generation for both quantitative and qualitative comparisons. Quantitative comparison : Specifically, the accuracy of NED on the CK+ and CelebV-HQ datasets was 49.5% and 51.33% for generating the neutral expression, respectively. These results are presented in Table IX , which confirms that NED + CVTHead outperforms NED + ROME. Qualitative comparison : Fig. 17 visualizes neutral expressions generated by NED + CVTHead on CK+ and CelebV-HQ. Moreover, we also illustrate neutral expressions generated by FLAME with the expression parameter set to zero, as shown in Fig. 17 c. The results show that the facial units differ, depending on whether the environment is controllable or non-controllable, or whether FLAME was used. This indicates that neutral expressions are more complex in real-world scenarios, leading us to prefer using a neural encoder-decoder instead of the expression parameter settings provided by FLAME Figure 17 : Samples of facial images with a neutral expression generated by NED+CVTHead. The input images are shown in the leftmost column (Source). The images generated using CK+ dataset (controllable environment) are shown in (a), whereas those using CelebV-HQ dataset (non-controllable environment) are shown in (b). On the other hand, the generated 3D face with FLAME is depicted by setting the expression parameter to zero in (c). Note that subtle differences exist in the image quality between the three generated cases. TABLE IX : RESULT OBTAINED WHEN NED IS COMBINED WITH ROME OR CVTHEAD FOR TEXTURE GENERATION ON CK+ AND CELEBV-HQ, RESPECTIVELY. Dataset Method L â†“ 1 {}_{1}\\downarrow PSNR â†‘ {\\uparrow} LPIPS â†“ {\\downarrow} MS_SSIM â†‘ {\\uparrow} CK+ NED + ROME 0.193 10.207 0.516 0.348 NED + CVTHead 0.004 35.158 0.014 0.987 CelebV-HQ NED + ROME 0.183 10.818 0.498 0.407 NED + CVTHead 0.004 36.347 0.0117 0.989 In a user study similar to that described in Section IV-D 2 , we asked subjects to determine whether the generated images display neutral expressions. Most subjects succeeded in identifying the neutral expressions produced by our model on CK+, and they had difficulties with only 4% of the samples generated by the learned model on CelebV-HQ. TABLE X : COMPARISON OF FACIAL EXPRESSION CLASSIFIERS ON CK+ AND CELEBV-HQ WITH SAMPLES ROTATED AT VARIOUS ANGLES { 0 o , Â± 10 o , Â± 15 o , Â± 30 o } \\{0^{o},\\pm 10^{o},\\pm 15^{o},\\pm 30^{o}\\} 0 o Â± \\pm 10 o Â± \\pm 15 o Â± \\pm 30 o Dataset Method Acc 1 â†‘ \\uparrow G-mean â†‘ \\uparrow Acc 1 â†‘ \\uparrow G-mean â†‘ \\uparrow Acc 1 â†‘ \\uparrow G-mean â†‘ \\uparrow Acc 1 â†‘ \\uparrow G-mean â†‘ \\uparrow MEK [ 43 ] 100 100 86.41 76.20 76.01 52.16 32.56 0.0 CK+ ResNet-101 [ 14 ] 99.56 99.4 84.40 73.28 79.50 67.6 75.29 33.39 ResNet-101 with RFL 99.69 99.69 87.18 75.85 83.66 66.13 77.48 31.5 MEK [ 43 ] 99.75 99.83 60.49 55.07 49.08 33.84 32.05 9.64 CelebV-HQ ResNet-101 [ 14 ] 37 12.77 35.77 13.39 34.97 0.0 28.92 0.0 ResNet-101 with RFL 93.67 95.04 58.54 49.92 51.53 41.22 40.71 26.80\n\nIV-F 1 Implementation Details\nIn this scenario, two datasets containing the parameters of neutral expressions and poses were constructed the pretrained DECA to extract the data from the CelebV-HQ and CK+ datasets. As the CK+ dataset does not contain neutral expressions, the first frame of each video was extracted and considered as the representative neutral case. Afterward, NED was trained on these two neutral datasets.\n\nIV-F 2 Network Architecture\nThe neutral encoder-decoder model was constructed with fully connected layers. The encoder consisted of four fully connected layers, whereas the decoder was structured with four fully connected layers dedicated to pose and expression, respectively.\n\nIV-F 3 Analysis\nSimilarly, we also conducted experiments to evaluate neutral expression generation for both quantitative and qualitative comparisons. Quantitative comparison : Specifically, the accuracy of NED on the CK+ and CelebV-HQ datasets was 49.5% and 51.33% for generating the neutral expression, respectively. These results are presented in Table IX , which confirms that NED + CVTHead outperforms NED + ROME. Qualitative comparison : Fig. 17 visualizes neutral expressions generated by NED + CVTHead on CK+ and CelebV-HQ. Moreover, we also illustrate neutral expressions generated by FLAME with the expression parameter set to zero, as shown in Fig. 17 c. The results show that the facial units differ, depending on whether the environment is controllable or non-controllable, or whether FLAME was used. This indicates that neutral expressions are more complex in real-world scenarios, leading us to prefer using a neural encoder-decoder instead of the expression parameter settings provided by FLAME Figure 17 : Samples of facial images with a neutral expression generated by NED+CVTHead. The input images are shown in the leftmost column (Source). The images generated using CK+ dataset (controllable environment) are shown in (a), whereas those using CelebV-HQ dataset (non-controllable environment) are shown in (b). On the other hand, the generated 3D face with FLAME is depicted by setting the expression parameter to zero in (c). Note that subtle differences exist in the image quality between the three generated cases. TABLE IX : RESULT OBTAINED WHEN NED IS COMBINED WITH ROME OR CVTHEAD FOR TEXTURE GENERATION ON CK+ AND CELEBV-HQ, RESPECTIVELY. Dataset Method L â†“ 1 {}_{1}\\downarrow PSNR â†‘ {\\uparrow} LPIPS â†“ {\\downarrow} MS_SSIM â†‘ {\\uparrow} CK+ NED + ROME 0.193 10.207 0.516 0.348 NED + CVTHead 0.004 35.158 0.014 0.987 CelebV-HQ NED + ROME 0.183 10.818 0.498 0.407 NED + CVTHead 0.004 36.347 0.0117 0.989 In a user study similar to that described in Section IV-D 2 , we asked subjects to determine whether the generated images display neutral expressions. Most subjects succeeded in identifying the neutral expressions produced by our model on CK+, and they had difficulties with only 4% of the samples generated by the learned model on CelebV-HQ. TABLE X : COMPARISON OF FACIAL EXPRESSION CLASSIFIERS ON CK+ AND CELEBV-HQ WITH SAMPLES ROTATED AT VARIOUS ANGLES { 0 o , Â± 10 o , Â± 15 o , Â± 30 o } \\{0^{o},\\pm 10^{o},\\pm 15^{o},\\pm 30^{o}\\} 0 o Â± \\pm 10 o Â± \\pm 15 o Â± \\pm 30 o Dataset Method Acc 1 â†‘ \\uparrow G-mean â†‘ \\uparrow Acc 1 â†‘ \\uparrow G-mean â†‘ \\uparrow Acc 1 â†‘ \\uparrow G-mean â†‘ \\uparrow Acc 1 â†‘ \\uparrow G-mean â†‘ \\uparrow MEK [ 43 ] 100 100 86.41 76.20 76.01 52.16 32.56 0.0 CK+ ResNet-101 [ 14 ] 99.56 99.4 84.40 73.28 79.50 67.6 75.29 33.39 ResNet-101 with RFL 99.69 99.69 87.18 75.85 83.66 66.13 77.48 31.5 MEK [ 43 ] 99.75 99.83 60.49 55.07 49.08 33.84 32.05 9.64 CelebV-HQ ResNet-101 [ 14 ] 37 12.77 35.77 13.39 34.97 0.0 28.92 0.0 ResNet-101 with RFL 93.67 95.04 58.54 49.92 51.53 41.22 40.71 26.80\n\nIV-G Facial Expression Classification\nIV-G 1 Implementation For ResNet-101 [ 14 ] and ResNet-101 with RFL [ 5 ] , the learning rate was 1 â€‹ e âˆ’ 4 1e^{-4} and the batch size was 64. Adam was used to optimize the models with 50 epochs per cycle. For the MEK model, the configuration in [ 43 ] was used to optimize the training process for both the CK+ and CelebV-HQ datasets. IV-G 2 Analysis Based on the results in Table X , ResNet-101 with RFL was more stable compared to ResNet-101 or MEK in terms of their classification performance, suggesting that ResNet-101 with RFL can handle both imbalanced datasets and rotational transformations fairly well. In particular, MEK achieved the best result on the original images although its performance was affected by rotated images. ResNet-101 with RFL demonstrated more stable performance than ResNet-101 and MEK on CK+. For real-world scenarios such as those in the CelebV-HQ dataset, we obtained similar results. These findings indicate that the performance of facial expression classifiers deteriorates for faces that are rotated by Â± 15 \\pm 15 degrees or more. In addition, MEK performs better for the original datasets, but worse for the augmented datasets. The performance of ResNet-101 is high on datasets with low imbalance rates, whereas it is low on datasets with high imbalance rates. Meanwhile, ResNet-101 with RFL delivers stable performance on imbalanced and rotated datasets. Based on this result, we adopted ResNet-101 by default during evaluation. (a) (b) Figure 18 : Generated 3D faces (a) by changing the template sentences and (b) a few unsuccessful cases.\n\nIV-G 1 Implementation\nFor ResNet-101 [ 14 ] and ResNet-101 with RFL [ 5 ] , the learning rate was 1 â€‹ e âˆ’ 4 1e^{-4} and the batch size was 64. Adam was used to optimize the models with 50 epochs per cycle. For the MEK model, the configuration in [ 43 ] was used to optimize the training process for both the CK+ and CelebV-HQ datasets.\n\nIV-G 2 Analysis\nBased on the results in Table X , ResNet-101 with RFL was more stable compared to ResNet-101 or MEK in terms of their classification performance, suggesting that ResNet-101 with RFL can handle both imbalanced datasets and rotational transformations fairly well. In particular, MEK achieved the best result on the original images although its performance was affected by rotated images. ResNet-101 with RFL demonstrated more stable performance than ResNet-101 and MEK on CK+. For real-world scenarios such as those in the CelebV-HQ dataset, we obtained similar results. These findings indicate that the performance of facial expression classifiers deteriorates for faces that are rotated by Â± 15 \\pm 15 degrees or more. In addition, MEK performs better for the original datasets, but worse for the augmented datasets. The performance of ResNet-101 is high on datasets with low imbalance rates, whereas it is low on datasets with high imbalance rates. Meanwhile, ResNet-101 with RFL delivers stable performance on imbalanced and rotated datasets. Based on this result, we adopted ResNet-101 by default during evaluation. (a) (b) Figure 18 : Generated 3D faces (a) by changing the template sentences and (b) a few unsuccessful cases.\n\nIV-H Discussion\nIV-H 1 Diversity of Facial Expression Sequences Our framework is capable of animating various facial expressions guided by instruction and facial images with pose variation, as illustrated in Fig 12 . Moreover, it can generate facial expressions corresponding to both controlled and real-world environments, enabling the generation of a diverse range of expressions for creating a long video. IV-H 2 Results on Instructions with Changes in Template Sentences Fig. 18(a) shows the outcomes obtained by changing the template sentences. The result suggests that our model can successfully generate facial expressions that closely resemble specific emotions described in the instruction. IV-H 3 Failure Cases Although our model performs well in many cases, the model failed in certain instances as exemplified in Fig 18(b) . In the top row, significant pose variation in the input image leads to regions of the shoulders being missing from the resulting image due to constraints within the pre-trained face-rendering model. Likewise, images in which regions of the head and those containing hair were missing from the source image also led to the lack of information in the face rendering process because of this constraint, as shown in the bottom row. In the second row, the generated result is restricted by the size of the expression vocabulary, causing a scared face to be transformed into an angry face.\n\nIV-H 1 Diversity of Facial Expression Sequences\nOur framework is capable of animating various facial expressions guided by instruction and facial images with pose variation, as illustrated in Fig 12 . Moreover, it can generate facial expressions corresponding to both controlled and real-world environments, enabling the generation of a diverse range of expressions for creating a long video.\n\nIV-H 2 Results on Instructions with Changes in Template Sentences\nFig. 18(a) shows the outcomes obtained by changing the template sentences. The result suggests that our model can successfully generate facial expressions that closely resemble specific emotions described in the instruction.\n\nIV-H 3 Failure Cases\nAlthough our model performs well in many cases, the model failed in certain instances as exemplified in Fig 18(b) . In the top row, significant pose variation in the input image leads to regions of the shoulders being missing from the resulting image due to constraints within the pre-trained face-rendering model. Likewise, images in which regions of the head and those containing hair were missing from the source image also led to the lack of information in the face rendering process because of this constraint, as shown in the bottom row. In the second row, the generated result is restricted by the size of the expression vocabulary, causing a scared face to be transformed into an angry face.\n\nV Conclusion\nIn this study, we present a novel framework for generating 3D facial expressions from an RGB image and animating facial transitions between two expressions as specified by entering text instructions. First, FET was introduced to produce a sequence of facial expressions. The Instruction-Driven Facial Expression Decomposer was designed to learn from multimodal data and capture correlations between textual descriptions and facial expression features. We proposed an Instruction to Facial Expression Transition model to generate target facial expressions guided by a single instruction. Our framework integrates FET and a pre-trained face rendering model to generate facial appearances aligned with the expected expression sequence. Furthermore, our framework can be expanded to include the generation of a neutral expression, thereby enabling the creation of diverse expressions in a video, thus closely simulating facial expression behaviors in real-world scenarios. Extensive experiments were conducted on the CK+ and CelebV-HQ datasets to demonstrate the effectiveness of our framework. Although our proposed approach demonstrates positive outcomes in terms of controlling the facial expression of an RGB image according to the provided instructions, it does have certain limitations. Similar to previous studies [ 18 , 26 ] in which facial parameters were used, the performance of our model relies on the precision of 3D face coefficients, especially using DECA [ 9 ] within our configurations since it may encounter difficulty, often seen in challenging scenarios, in disentangling facial factors. Also, even though linear interpolation can maintain temporal consistency, it often compromises the realism of the synthesized videos. Although the present work uses either basic facial expressions or two designated expressions, the vocabulary number can be easily expanded by combining a Large Language Model (LLM) with our model. Given that text prompting is a powerful tool by which various emotional states can be expressed on a 3D avatar via our framework, we expect it to find various applications in the future.\n\nAcknowledgment\nThis work was supported by the Information Technology Research Center (ITRC) support program (IITP-2022-RS-2022-00156354) and a Korean government grant (MSIT) (No.RS-2019-II190231) from the Institute of Information & Communications Technology Planning & Evaluation (IITP) as well as by the Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (2020R1A6A1A03038540).\n\nReferences\n[1] S. Aneja, J. Thies, A. Dai, and M. NieÃŸner (2023) ClipFace: text-guided editing of textured 3d morphable models . In SIGGRAPH â€™23 Conference Proceedings , Cited by: Â§I , Â§I . [2] M. C. Buehler, A. Meka, G. Li, T. Beeler, and O. Hilliges (2021) VariTex: variational neural face textures . In Proceedings of the IEEE/CVF International Conference on Computer Vision , Cited by: Â§I , Â§ II-B . [3] C. Chen, Q. Fan, and R. Panda (2021) CrossViT: cross-attention multi-scale vision transformer for image classification . 2021 IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 347â€“356 . Cited by: Â§ III-B 1 . [4] Y. Chen and S. Xiong (2023) Rethinking one-shot face reenactment: a spatialâ€“temporal reconstruction view . Knowledge Based Systems 277 ( C ). Cited by: Â§I . [5] Y. Cui, M. Jia, T. Lin, Y. Song, and S. J. Belongie (2019) Class-balanced loss based on effective number of samples . 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 9260â€“9269 . Cited by: Â§ IV-C , Â§ IV-G 1 . [6] R. DanÄ›Äek, K. Chhatre, S. Tripathi, Y. Wen, M. Black, and T. Bolkart (2023) Emotional speech-driven animation with content-emotion disentanglement . In SIGGRAPH Asia 2023 Conference Papers , Cited by: Â§ II-A , Â§ II-A , Â§ III-B 1 . [7] H. Ding, K. Sricharan, and R. Chellappa (2018) Exprgan: facial expression editing with controllable expression intensity . AAAI . Cited by: Â§I . [8] M. C. Doukas, S. Zafeiriou, and V. Sharmanska (2021-10) HeadGAN: one-shot neural head synthesis and editing . In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 14398â€“14407 . Cited by: Â§I . [9] Y. Feng, H. Feng, M. J. Black, and T. Bolkart (2021) Learning an animatable detailed 3D face model from in-the-wild images . In ACM Transactions on Graphics, (Proc. SIGGRAPH) , Vol. 40 . Cited by: Â§ III-B 1 , Â§ III-B 1 , Â§V . [10] P. Grassal, M. Prinzler, T. Leistner, C. Rother, M. NieÃŸner, and J. Thies (2022) Neural head avatars from monocular rgb videos . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 18653â€“18664 . Cited by: Â§I . [11] X. Gu, C. Wen, W. Ye, J. Song, and Y. Gao (2023) Seer: language instructed video prediction with latent diffusion models . ICLR 2024 . Cited by: Â§I . [12] J. Guo, H. Manukyan, C. Yang, C. Wang, L. Khachatryan, S. Navasardyan, S. Song, H. Shi, and G. Huang (2023) FaceCLIP: facial image-to-video translation via a brief text description . IEEE Transactions on Circuits and Systems for Video Technology , pp. 1â€“1 . Cited by: Â§I . [13] T. Hang, H. Yang, B. Liu, J. Fu, X. Geng, and B. Guo (2023) Language-guided face animation by recurrent stylegan-based generator . IEEE Transactions on Multimedia 25 ( ), pp. 1â€“12 . Cited by: Â§I , Â§I . [14] K. He, X. Zhang, S. Ren, and J. Sun (2016) Deep Residual Learning for Image Recognition . In Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition , CVPR â€™16 , pp. 770â€“778 . External Links: Document , ISSN 1063-6919 Cited by: Â§ IV-C , Â§ IV-G 1 , TABLE X , TABLE X . [15] Z. Huang, K. C.K. Chan, Y. Jiang, and Z. Liu (2023) Collaborative diffusion for multi-modal face generation and editing . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , Cited by: Â§I . [16] S. Hwang, J. Hyung, D. Kim, M. Kim, and J. Choo (2023-10) FaceCLIPNeRF: text-driven 3d face manipulation using deformable neural radiance fields . In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 3469â€“3479 . Cited by: Â§I , Â§I . [17] Y. Jiang, Z. Huang, X. Pan, C. C. Loy, and Z. Liu (2021) Talk-to-edit: fine-grained facial editing via dialog . In Proceedings of the IEEE/CVF International Conference on Computer Vision , Cited by: Â§I . [18] T. Khakhulin, V. Sklyarova, V. Lempitsky, and E. Zakharov (2022) Realistic one-shot mesh-based head avatars . In European Conference of Computer vision (ECCV) , Cited by: Â§I , Â§ II-B , Â§ III-B 2 , Â§ III-B 2 , Figure 11 , Figure 11 , Â§ IV-B , Â§ IV-D 1 , Â§V . [19] M. Kim, F. Liu, A. Jain, and X. Liu (2023-06) DCFace: synthetic face generation with dual condition diffusion model . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 12715â€“12725 . Cited by: Â§I . [20] D. P. Kingma and J. Ba (2014) Adam: a method for stochastic optimization . ICLR abs/1412.6980 . Cited by: Â§ IV-B . [21] Y. Lee, T. Choi, H. Go, H. Lee, S. Cho, and J. Kim (2023) Exp-gan: 3d-aware facial image generation with expression control . In Computer Vision â€“ ACCV 2022: 16th Asian Conference on Computer Vision , pp. 151â€“167 . External Links: ISBN 978-3-031-26292-0 Cited by: Â§ II-B . [22] T. Li, T. Bolkart, M. J. Black, H. Li, and J. Romero (2017) Learning a model of facial shape and expression from 4d scans . ACM Transactions on Graphics (TOG) 36 , pp. 1 â€“ 17 . Cited by: Â§I , Â§ II-A , Â§ III-B 1 . [23] X. Li, D. Zhang, M. Li, and D. Lee (2023) Accurate head pose estimation using image rectification and a lightweight convolutional neural network . IEEE Transactions on Multimedia 25 ( ), pp. 2239â€“2251 . Cited by: Â§ IV-A 2 . [24] K. Lin, A. Trevithick, K. Cheng, M. Sarkis, M. Ghafoorian, N. Bi, G. Reitmayr, and R. Ramamoorthi (2023) PVP: Personalized Video Prior for Editable Dynamic Portraits using StyleGAN . Computer Graphics Forum 42 ( 4 ), pp. 13 pages . Cited by: Â§ III-B 1 . [25] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and I. Matthews (2010) The extended cohn-kanade dataset (ck+): a complete dataset for action unit and emotion-specified expression . In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops , Vol. , pp. 94â€“101 . Cited by: Â§ IV-A 1 . [26] H. Ma, T. Zhang, S. Sun, X. Yan, K. Han, and X. Xie (2024) CVTHead: one-shot controllable head avatar with vertex-feature transformer . IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) . Cited by: Â§I , Â§ II-B , Â§ III-B 2 , Figure 11 , Figure 11 , Â§ IV-B , Â§ IV-D 1 , Â§V . [27] N. Otberdout, C. Ferrari, M. Daoudi, S. Berretti, and A. Bimbo (2023) Generating complex 4d expression transitions by learning face landmark trajectories . IEEE Transactions on Affective Computing . Cited by: Â§I , Â§ II-A , Â§ II-A , Â§ IV-C . [28] N. Otberdout, C. Ferrari, M. Daoudi, S. Berritti, and A. Del Bimbo (2022-06) Sparse to dense dynamic 3d facial expression generation . In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , Cited by: Â§ II-A . [29] F. Paraperas Papantoniou, P. P. Filntisis, P. Maragos, and A. Roussos (2022) Neural emotion director: speech-preserving semantic control of facial expressions in â€in-the-wildâ€ videos . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , Cited by: Â§I . [30] O. Patashnik, Z. Wu, E. Shechtman, D. Cohen-Or, and D. Lischinski (2021-10) StyleCLIP: text-driven manipulation of stylegan imagery . In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 2085â€“2094 . Cited by: Â§I . [31] B. Peng, H. Fan, W. Wang, J. Dong, and S. Lyu (2022) A unified framework for high fidelity face swap and expression reenactment . IEEE Transactions on Circuits and Systems for Video Technology 32 ( 6 ), pp. 3673â€“3684 . Cited by: Â§I . [32] S. Qian, T. Kirschstein, L. Schoneveld, D. Davoli, S. Giebenhain, and M. NieÃŸner (2024) GaussianAvatars: photorealistic head avatars with rigged 3d gaussians . In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , Cited by: Â§I . [33] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. (2021) Learning transferable visual models from natural language supervision . In International conference on machine learning , pp. 8748â€“8763 . Cited by: Â§ III-B 1 . [34] J. Russell and J. M. F. dez Dols (1997) Reading emotions from and into faces: resurrecting dimensional-contextual perspective . The psychology of facial expression , pp. 295â€“320 . Note: Cambridge University Press Cited by: Â§ IV-F . [35] K. R. Scherer (2001) Appraisal considered as a process of multilevel sequential checking. . Oxford University Press , pp. 92 â€“ 120 . Cited by: Â§ IV-F . [36] S. Su, J. Zhu, L. Gao, and J. Song (2024) Utilizing greedy nature for multimodal conditional image synthesis in transformers . IEEE Transactions on Multimedia 26 ( ), pp. 2354â€“2366 . Cited by: Â§I . [37] T. Sutter, I. Daunhawer, and J. Vogt (2020) Multimodal generative learning utilizing jensen-shannon-divergence . Vol. 33 , pp. 6100â€“6110 . Cited by: Â§I . [38] M. Suzuki and Y. Matsuo (2022) A survey of multimodal deep generative models . Advanced Robotics 36 , pp. 261 â€“ 278 . Cited by: Â§I . [39] G. Tevet, B. Gordon, A. Hertz, A. H. Bermano, and D. Cohen-Or (2022) Motionclip: exposing human motion generation to clip space . In Computer Visionâ€“ECCV 2022: 17th European Conference , pp. 358â€“374 . Cited by: Â§ II-A , Â§ IV-B . [40] Y. Wu, S. Xu, J. Xiang, F. Wei, Q. Chen, J. Yang, and X. Tong (2023) AniPortraitGAN: animatable 3d portrait generation from 2d image collections . In SIGGRAPH Asia 2023 Conference Proceedings , Cited by: Â§I . [41] W. Xia, Y. Yang, J. Xue, and B. Wu (2021) TediGAN: text-guided diverse face image generation and manipulation . In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , Cited by: Â§I . [42] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang (2018) The unreasonable effectiveness of deep features as a perceptual metric . In CVPR , Cited by: Â§ IV-D 1 . [43] Y. Zhang, Y. Li, lixiong Qin, X. Liu, and W. Deng (2023) Leave no stone unturned: mine extra knowledge for imbalanced facial expression recognition . In Thirty-seventh Conference on Neural Information Processing Systems , Cited by: Â§ IV-C , Â§ IV-G 1 , TABLE X , TABLE X . [44] H. Zhu, W. Wu, W. Zhu, L. Jiang, S. Tang, L. Zhang, Z. Liu, and C. C. Loy (2022) CelebV-HQ: a large-scale video facial attributes dataset . In ECCV , Cited by: Â§ IV-A 2 . [45] W. Zielonka, T. Bolkart, and J. Thies (2023-06) Instant volumetric head avatars . In IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) , External Links: Document Cited by: Â§I . [46] K. Zou, S. Faisan, B. Yu, S. Valette, and H. Seo (2023) 4D facial expression diffusion model . External Links: 2303.16611 Cited by: Â§I , Â§ II-A , Â§ II-A , Â§ IV-C .",
      "references": [
        {
          "raw_text": "[1] S. Aneja, J. Thies, A. Dai, and M. NieÃŸner (2023) ClipFace: text-guided editing of textured 3d morphable models . In SIGGRAPH â€™23 Conference Proceedings , Cited by: Â§I , Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p2.1",
            "https://arxiv.org/html/2601.08179v1#S1.p3.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[2] M. C. Buehler, A. Meka, G. Li, T. Beeler, and O. Hilliges (2021) VariTex: variational neural face textures . In Proceedings of the IEEE/CVF International Conference on Computer Vision , Cited by: Â§I , Â§ II-B .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p2.1",
            "https://arxiv.org/html/2601.08179v1#S2.SS2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[3] C. Chen, Q. Fan, and R. Panda (2021) CrossViT: cross-attention multi-scale vision transformer for image classification . 2021 IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 347â€“356 . Cited by: Â§ III-B 1 .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S3.SS2.SSS1.p2.7"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[4] Y. Chen and S. Xiong (2023) Rethinking one-shot face reenactment: a spatialâ€“temporal reconstruction view . Knowledge Based Systems 277 ( C ). Cited by: Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[5] Y. Cui, M. Jia, T. Lin, Y. Song, and S. J. Belongie (2019) Class-balanced loss based on effective number of samples . 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 9260â€“9269 . Cited by: Â§ IV-C , Â§ IV-G 1 .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S4.SS3.p2.2",
            "https://arxiv.org/html/2601.08179v1#S4.SS7.SSS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[6] R. DanÄ›Äek, K. Chhatre, S. Tripathi, Y. Wen, M. Black, and T. Bolkart (2023) Emotional speech-driven animation with content-emotion disentanglement . In SIGGRAPH Asia 2023 Conference Papers , Cited by: Â§ II-A , Â§ II-A , Â§ III-B 1 .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S2.SS1.p1.1",
            "https://arxiv.org/html/2601.08179v1#S2.SS1.p2.1",
            "https://arxiv.org/html/2601.08179v1#S3.SS2.SSS1.p2.7"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[7] H. Ding, K. Sricharan, and R. Chellappa (2018) Exprgan: facial expression editing with controllable expression intensity . AAAI . Cited by: Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[8] M. C. Doukas, S. Zafeiriou, and V. Sharmanska (2021-10) HeadGAN: one-shot neural head synthesis and editing . In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 14398â€“14407 . Cited by: Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p2.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[9] Y. Feng, H. Feng, M. J. Black, and T. Bolkart (2021) Learning an animatable detailed 3D face model from in-the-wild images . In ACM Transactions on Graphics, (Proc. SIGGRAPH) , Vol. 40 . Cited by: Â§ III-B 1 , Â§ III-B 1 , Â§V .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S3.SS2.SSS1.p7.8",
            "https://arxiv.org/html/2601.08179v1#S3.SS2.SSS1.p8.12",
            "https://arxiv.org/html/2601.08179v1#S5.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[10] P. Grassal, M. Prinzler, T. Leistner, C. Rother, M. NieÃŸner, and J. Thies (2022) Neural head avatars from monocular rgb videos . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 18653â€“18664 . Cited by: Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p2.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[11] X. Gu, C. Wen, W. Ye, J. Song, and Y. Gao (2023) Seer: language instructed video prediction with latent diffusion models . ICLR 2024 . Cited by: Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p3.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[12] J. Guo, H. Manukyan, C. Yang, C. Wang, L. Khachatryan, S. Navasardyan, S. Song, H. Shi, and G. Huang (2023) FaceCLIP: facial image-to-video translation via a brief text description . IEEE Transactions on Circuits and Systems for Video Technology , pp. 1â€“1 . Cited by: Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[13] T. Hang, H. Yang, B. Liu, J. Fu, X. Geng, and B. Guo (2023) Language-guided face animation by recurrent stylegan-based generator . IEEE Transactions on Multimedia 25 ( ), pp. 1â€“12 . Cited by: Â§I , Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p1.1",
            "https://arxiv.org/html/2601.08179v1#S1.p3.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[14] K. He, X. Zhang, S. Ren, and J. Sun (2016) Deep Residual Learning for Image Recognition . In Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition , CVPR â€™16 , pp. 770â€“778 . External Links: Document , ISSN 1063-6919 Cited by: Â§ IV-C , Â§ IV-G 1 , TABLE X , TABLE X .",
          "urls": [
            "https://dx.doi.org/10.1109/CVPR.2016.90",
            "https://arxiv.org/html/2601.08179v1#S4.SS3.p2.2",
            "https://arxiv.org/html/2601.08179v1#S4.SS7.SSS1.p1.1",
            "https://arxiv.org/html/2601.08179v1#S4.T10.21.21.2.2",
            "https://arxiv.org/html/2601.08179v1#S4.T10.21.24.5.2"
          ],
          "dois": [
            "https://dx.doi.org/10.1109/CVPR.2016.90"
          ],
          "pdf_links": []
        },
        {
          "raw_text": "[15] Z. Huang, K. C.K. Chan, Y. Jiang, and Z. Liu (2023) Collaborative diffusion for multi-modal face generation and editing . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , Cited by: Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[16] S. Hwang, J. Hyung, D. Kim, M. Kim, and J. Choo (2023-10) FaceCLIPNeRF: text-driven 3d face manipulation using deformable neural radiance fields . In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 3469â€“3479 . Cited by: Â§I , Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p2.1",
            "https://arxiv.org/html/2601.08179v1#S1.p3.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[17] Y. Jiang, Z. Huang, X. Pan, C. C. Loy, and Z. Liu (2021) Talk-to-edit: fine-grained facial editing via dialog . In Proceedings of the IEEE/CVF International Conference on Computer Vision , Cited by: Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[18] T. Khakhulin, V. Sklyarova, V. Lempitsky, and E. Zakharov (2022) Realistic one-shot mesh-based head avatars . In European Conference of Computer vision (ECCV) , Cited by: Â§I , Â§ II-B , Â§ III-B 2 , Â§ III-B 2 , Figure 11 , Figure 11 , Â§ IV-B , Â§ IV-D 1 , Â§V .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p1.1",
            "https://arxiv.org/html/2601.08179v1#S2.SS2.p1.1",
            "https://arxiv.org/html/2601.08179v1#S3.SS2.SSS2.p2.3",
            "https://arxiv.org/html/2601.08179v1#S3.SS2.SSS2.p3.4",
            "https://arxiv.org/html/2601.08179v1#S4.F11",
            "https://arxiv.org/html/2601.08179v1#S4.F11.3.2",
            "https://arxiv.org/html/2601.08179v1#S4.SS2.p1.1",
            "https://arxiv.org/html/2601.08179v1#S4.SS4.SSS1.p4.1",
            "https://arxiv.org/html/2601.08179v1#S5.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[19] M. Kim, F. Liu, A. Jain, and X. Liu (2023-06) DCFace: synthetic face generation with dual condition diffusion model . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 12715â€“12725 . Cited by: Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[20] D. P. Kingma and J. Ba (2014) Adam: a method for stochastic optimization . ICLR abs/1412.6980 . Cited by: Â§ IV-B .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S4.SS2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[21] Y. Lee, T. Choi, H. Go, H. Lee, S. Cho, and J. Kim (2023) Exp-gan: 3d-aware facial image generation with expression control . In Computer Vision â€“ ACCV 2022: 16th Asian Conference on Computer Vision , pp. 151â€“167 . External Links: ISBN 978-3-031-26292-0 Cited by: Â§ II-B .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S2.SS2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[22] T. Li, T. Bolkart, M. J. Black, H. Li, and J. Romero (2017) Learning a model of facial shape and expression from 4d scans . ACM Transactions on Graphics (TOG) 36 , pp. 1 â€“ 17 . Cited by: Â§I , Â§ II-A , Â§ III-B 1 .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p3.1",
            "https://arxiv.org/html/2601.08179v1#S2.SS1.p1.1",
            "https://arxiv.org/html/2601.08179v1#S3.SS2.SSS1.p7.8"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[23] X. Li, D. Zhang, M. Li, and D. Lee (2023) Accurate head pose estimation using image rectification and a lightweight convolutional neural network . IEEE Transactions on Multimedia 25 ( ), pp. 2239â€“2251 . Cited by: Â§ IV-A 2 .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S4.SS1.SSS2.p1.2"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[24] K. Lin, A. Trevithick, K. Cheng, M. Sarkis, M. Ghafoorian, N. Bi, G. Reitmayr, and R. Ramamoorthi (2023) PVP: Personalized Video Prior for Editable Dynamic Portraits using StyleGAN . Computer Graphics Forum 42 ( 4 ), pp. 13 pages . Cited by: Â§ III-B 1 .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S3.SS2.SSS1.p2.7"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[25] P. Lucey, J. F. Cohn, T. Kanade, J. Saragih, Z. Ambadar, and I. Matthews (2010) The extended cohn-kanade dataset (ck+): a complete dataset for action unit and emotion-specified expression . In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops , Vol. , pp. 94â€“101 . Cited by: Â§ IV-A 1 .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S4.SS1.SSS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[26] H. Ma, T. Zhang, S. Sun, X. Yan, K. Han, and X. Xie (2024) CVTHead: one-shot controllable head avatar with vertex-feature transformer . IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) . Cited by: Â§I , Â§ II-B , Â§ III-B 2 , Figure 11 , Figure 11 , Â§ IV-B , Â§ IV-D 1 , Â§V .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p1.1",
            "https://arxiv.org/html/2601.08179v1#S2.SS2.p1.1",
            "https://arxiv.org/html/2601.08179v1#S3.SS2.SSS2.p3.4",
            "https://arxiv.org/html/2601.08179v1#S4.F11",
            "https://arxiv.org/html/2601.08179v1#S4.F11.3.2",
            "https://arxiv.org/html/2601.08179v1#S4.SS2.p1.1",
            "https://arxiv.org/html/2601.08179v1#S4.SS4.SSS1.p4.1",
            "https://arxiv.org/html/2601.08179v1#S5.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[27] N. Otberdout, C. Ferrari, M. Daoudi, S. Berretti, and A. Bimbo (2023) Generating complex 4d expression transitions by learning face landmark trajectories . IEEE Transactions on Affective Computing . Cited by: Â§I , Â§ II-A , Â§ II-A , Â§ IV-C .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p2.1",
            "https://arxiv.org/html/2601.08179v1#S2.SS1.p1.1",
            "https://arxiv.org/html/2601.08179v1#S2.SS1.p2.1",
            "https://arxiv.org/html/2601.08179v1#S4.SS3.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[28] N. Otberdout, C. Ferrari, M. Daoudi, S. Berritti, and A. Del Bimbo (2022-06) Sparse to dense dynamic 3d facial expression generation . In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , Cited by: Â§ II-A .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S2.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[29] F. Paraperas Papantoniou, P. P. Filntisis, P. Maragos, and A. Roussos (2022) Neural emotion director: speech-preserving semantic control of facial expressions in â€in-the-wildâ€ videos . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , Cited by: Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p2.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[30] O. Patashnik, Z. Wu, E. Shechtman, D. Cohen-Or, and D. Lischinski (2021-10) StyleCLIP: text-driven manipulation of stylegan imagery . In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pp. 2085â€“2094 . Cited by: Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p3.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[31] B. Peng, H. Fan, W. Wang, J. Dong, and S. Lyu (2022) A unified framework for high fidelity face swap and expression reenactment . IEEE Transactions on Circuits and Systems for Video Technology 32 ( 6 ), pp. 3673â€“3684 . Cited by: Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[32] S. Qian, T. Kirschstein, L. Schoneveld, D. Davoli, S. Giebenhain, and M. NieÃŸner (2024) GaussianAvatars: photorealistic head avatars with rigged 3d gaussians . In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , Cited by: Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p2.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[33] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. (2021) Learning transferable visual models from natural language supervision . In International conference on machine learning , pp. 8748â€“8763 . Cited by: Â§ III-B 1 .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S3.SS2.SSS1.p2.7"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[34] J. Russell and J. M. F. dez Dols (1997) Reading emotions from and into faces: resurrecting dimensional-contextual perspective . The psychology of facial expression , pp. 295â€“320 . Note: Cambridge University Press Cited by: Â§ IV-F .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S4.SS6.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[35] K. R. Scherer (2001) Appraisal considered as a process of multilevel sequential checking. . Oxford University Press , pp. 92 â€“ 120 . Cited by: Â§ IV-F .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S4.SS6.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[36] S. Su, J. Zhu, L. Gao, and J. Song (2024) Utilizing greedy nature for multimodal conditional image synthesis in transformers . IEEE Transactions on Multimedia 26 ( ), pp. 2354â€“2366 . Cited by: Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[37] T. Sutter, I. Daunhawer, and J. Vogt (2020) Multimodal generative learning utilizing jensen-shannon-divergence . Vol. 33 , pp. 6100â€“6110 . Cited by: Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p3.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[38] M. Suzuki and Y. Matsuo (2022) A survey of multimodal deep generative models . Advanced Robotics 36 , pp. 261 â€“ 278 . Cited by: Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p3.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[39] G. Tevet, B. Gordon, A. Hertz, A. H. Bermano, and D. Cohen-Or (2022) Motionclip: exposing human motion generation to clip space . In Computer Visionâ€“ECCV 2022: 17th European Conference , pp. 358â€“374 . Cited by: Â§ II-A , Â§ IV-B .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S2.SS1.p1.1",
            "https://arxiv.org/html/2601.08179v1#S4.SS2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[40] Y. Wu, S. Xu, J. Xiang, F. Wei, Q. Chen, J. Yang, and X. Tong (2023) AniPortraitGAN: animatable 3d portrait generation from 2d image collections . In SIGGRAPH Asia 2023 Conference Proceedings , Cited by: Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p2.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[41] W. Xia, Y. Yang, J. Xue, and B. Wu (2021) TediGAN: text-guided diverse face image generation and manipulation . In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , Cited by: Â§I .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[42] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang (2018) The unreasonable effectiveness of deep features as a perceptual metric . In CVPR , Cited by: Â§ IV-D 1 .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S4.SS4.SSS1.p4.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[43] Y. Zhang, Y. Li, lixiong Qin, X. Liu, and W. Deng (2023) Leave no stone unturned: mine extra knowledge for imbalanced facial expression recognition . In Thirty-seventh Conference on Neural Information Processing Systems , Cited by: Â§ IV-C , Â§ IV-G 1 , TABLE X , TABLE X .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S4.SS3.p2.2",
            "https://arxiv.org/html/2601.08179v1#S4.SS7.SSS1.p1.1",
            "https://arxiv.org/html/2601.08179v1#S4.T10.21.20.1.2",
            "https://arxiv.org/html/2601.08179v1#S4.T10.21.23.4.2"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[44] H. Zhu, W. Wu, W. Zhu, L. Jiang, S. Tang, L. Zhang, Z. Liu, and C. C. Loy (2022) CelebV-HQ: a large-scale video facial attributes dataset . In ECCV , Cited by: Â§ IV-A 2 .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S4.SS1.SSS2.p1.2"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[45] W. Zielonka, T. Bolkart, and J. Thies (2023-06) Instant volumetric head avatars . In IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR) , External Links: Document Cited by: Â§I .",
          "urls": [
            "https://dx.doi.org/",
            "https://arxiv.org/html/2601.08179v1#S1.p1.1"
          ],
          "dois": [
            "https://dx.doi.org/"
          ],
          "pdf_links": []
        },
        {
          "raw_text": "[46] K. Zou, S. Faisan, B. Yu, S. Valette, and H. Seo (2023) 4D facial expression diffusion model . External Links: 2303.16611 Cited by: Â§I , Â§ II-A , Â§ II-A , Â§ IV-C .",
          "urls": [
            "https://arxiv.org/html/2601.08179v1#S1.p2.1",
            "https://arxiv.org/html/2601.08179v1#S2.SS1.p1.1",
            "https://arxiv.org/html/2601.08179v1#S2.SS1.p2.1",
            "https://arxiv.org/html/2601.08179v1#S4.SS3.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        }
      ],
      "references_dois": [
        "https://dx.doi.org/10.1109/CVPR.2016.90",
        "https://dx.doi.org/"
      ],
      "fallback_urls": [],
      "errors": [],
      "missing_fields": [
        "versions",
        "last_updated_raw",
        "published_date",
        "license"
      ],
      "url_hint_if_missing": "Champs manquants: versions, last_updated_raw, published_date, license. Tu peux verifier ici: abs=https://arxiv.org/abs/2601.08179 | html=https://arxiv.org/html/2601.08179v1 | pdf=https://arxiv.org/pdf/2601.08179"
    }
  ],
  "bundle_html_file": "data_lake/raw/cache\\scrappingresults_arxiv_bundle_20260114_160426.html",
  "supported_fields": [
    "arxiv_id",
    "title",
    "authors",
    "abstract",
    "submitted_date",
    "abs_url",
    "pdf_url",
    "doi",
    "versions",
    "last_updated_raw",
    "html_url",
    "published_date",
    "license",
    "sections",
    "content_text",
    "references",
    "references_dois"
  ]
}