# ===============================  # #
# ğŸ”¢ RÃ‰FÃ‰RENTIEL (Ã‰tapes + Sources)  # #
# ===============================  # #
# âœ… But (1 phrase, simple) : on rend la sortie du tool prÃ©visible et on Ã©vite que tout casse si arXiv change un dÃ©tail.  # #
#
# ğŸ“Œ Ã‰tapes (codes Ã  rÃ©utiliser dans les hashtags, pour Ã©viter la redondance)
# [E1] NORMALISATION/DECOUPLAGE : transformer/standardiser les donnÃ©es (format stable) pour que FastAPI lise sans surprise.  # #
# [E2] ROBUSTESSE : continuer Ã  fonctionner malgrÃ© HTML qui change, pages bizarres, ou rÃ©sultats manquants (fallback + diag).  # #
# [E3] GESTION_ERREUR : capturer les erreurs (HTTP/timeouts/exceptions) et retourner ok=False + errors[] au lieu de crasher.  # #
# [E4] SCRAPING_ETHIQUE : limiter la frÃ©quence, mettre un User-Agent, ne pas spammer le site (politesse).  # #
# [E5] STRUCTURATION : prÃ©parer un contexte/sections propres (ex: method/references) sans envoyer du HTML brut au LLM.  # #
# [E6] TOOL : exÃ©cution du scraping en tant qu'outil externe (appelÃ© par l'orchestrateur).  # #
#
# ğŸ“š Sources prof (codes)
# [S1] Guide_Scraping_HTML_Python_IA_BOT (1).pdf â€” extrait : Â« Toujours produire une sortie structurÃ©e (JSON) Â».  # #
# [S2] Guide_Scraping_HTML_Python_IA_BOT (1).pdf â€” extrait : Â« Mettre en cache les rÃ©sultats Â».  # #
# [S3] Guide_Scraping_HTML_Python_IA_BOT (1).pdf â€” extrait : Â« GÃ©rer les erreurs (try/except, timeouts) Â».  # #
# [S4] Guide_Scraping_HTML_Python_IA_BOT (1).pdf â€” extrait : Â« Toujours dÃ©finir un User-Agent Â».  # #
# [S5] Kick-Off-IA-Bot-Agent-Conversationnel-Intelligent.pdf â€” extrait : Â« Scrappez Ã  faible frÃ©quence. Ã‰vitez absolument le spam de requÃªtes Â».  # #
# [S6] Kick-Off-IA-Bot-Agent-Conversationnel-Intelligent.pdf â€” extrait : Â« Gestion des erreurs : ... stratÃ©gies de fallback. Â».  # #
# [S7] Projet-IA-BOT-Concevoir-un-agent-pas-un-simple-chatbot (1).pdf â€” extrait : Â« Pas de gestion d'erreur ... timeouts. Un agent robuste gÃ¨re l'incertitude Â».  # #
# [S8] Projet-IA-BOT-Concevoir-un-agent-pas-un-simple-chatbot (1).pdf â€” extrait : Â« Chaque tool a un contrat clair : inputs structurÃ©s et outputs normalisÃ©s Â».  # #
#
# ğŸ§¾ Ã€ propos des â€œnumÃ©ros dâ€™erreurâ€ (ce que Ã§a veut dire)
# - 200 = OK (la page a Ã©tÃ© rÃ©cupÃ©rÃ©e)  # #
# - 429 = Too Many Requests (le site te â€œrate-limitâ€, donc on ralentit / on retry)  # #
# - 500/502/503/504 = erreurs serveur (souvent temporaires, on peut retry)  # #
# - codes internes ex: "http_429_search" = notre libellÃ© lisible : "type dâ€™erreur" + "oÃ¹" (search/abs/html).  # #

# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
# Scraper arXiv CS (ciblÃ© thÃ©matique + sortie structurÃ©e)  # #  | Ã‰tape: [E1] | Source: [S1]  # #
# Objectif :  # #  | Ã‰tape: [E1] | Source: [S0]  # #
# - Scraping ciblÃ© sur les thÃ¨mes demandÃ©s (pas "aspirateur")  # #  | Ã‰tape: [E1] | Source: [S0]  # #
# - Sortie JSON structurÃ©e (pas de HTML brut envoyÃ© au LLM)  # #  | Ã‰tape: [E1] | Source: [S1]  # #
# - Extraction minimale : title/authors/abstract/dates/urls/doi  # #  | Ã‰tape: [E1] | Source: [S8]  # #
# - Cache + politesse + robustesse  # #  | Ã‰tape: [E4] | Source: [S2]  # #
#   => on cherche via /search/cs puis on filtre via Subjects  # #  | Ã‰tape: [E1] | Source: [S0]  # #
# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # #

# ===============================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
# ğŸ“š Importations  # #  | Ã‰tape: [E1] | Source: [S0]  # #
# ===============================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
import os  # # Gestion chemins/dossiers # # Respect: cache local stable (sortie disque attendue) | Ã‰tape: [E2] | Source: [S2]  # #
import re  # # Regex parsing IDs + catÃ©gories # # Respect: extraction ciblÃ©e (pas "tout le texte") | Ã‰tape: [E1] | Source: [S8]  # #
import json  # # Export JSON # # Respect: sortie structurÃ©e JSON | Ã‰tape: [E1] | Source: [S1]  # #
import time  # # Politesse (sleep) # # Respect: Ã©viter spam requÃªtes | Ã‰tape: [E4] | Source: [S5]  # #
import random  # # Jitter # # Respect: frÃ©quence raisonnable | Ã‰tape: [E1] | Source: [S0]  # #
import datetime  # # Timestamp fichiers # # Respect: traÃ§abilitÃ© des fichiers | Ã‰tape: [E1] | Source: [S0]  # #
from typing import Dict, Any, List, Tuple, Optional  # # Typage # # Respect: tool prÃ©visible | Ã‰tape: [E1] | Source: [S0]  # #

import requests  # # HTTP GET # # Respect: scraping HTML public | Ã‰tape: [E2] | Source: [S3]  # #
from bs4 import BeautifulSoup, Tag  # # Parser HTML # # Respect: extraction ciblÃ©e d'Ã©lÃ©ments utiles | Ã‰tape: [E1] | Source: [S8]  # #


# ===============================  # #  | Ã‰tape: [E1] | Source: [S0] 
# ğŸ“Œ RÃ©solution robuste des chemins  
# ===============================  

def _find_project_root(start_dir: str) -> str:  # # DÃ©finit une fonction qui retrouve la racine du projet Ã  partir dâ€™un dossier de dÃ©part (Ã©vite d'Ã©crire les fichiers au mauvais endroit) | Ã‰tape: [E1] | Source: [S0]  # #
    cur = os.path.abspath(start_dir)  # # Convertit start_dir en chemin absolu : abspath transforme un chemin relatif en chemin complet utilisable partout | Ã‰tape: [E1] | Source: [S0]  # #
    while True:  # # Lance une boucle infinie pour remonter dossier par dossier jusquâ€™Ã  trouver un â€œmarqueurâ€ de racine | Ã‰tape: [E1] | Source: [S0]  # #
        if os.path.isdir(os.path.join(cur, "data_lake")):  # # VÃ©rifie si le dossier "data_lake" existe ici : join construit le chemin cur/data_lake, isdir confirme que câ€™est un dossier | Ã‰tape: [E2] | Source: [S2]  # #
            return cur  # # Stoppe la fonction et renvoie cur : return termine la fonction immÃ©diatement avec la racine trouvÃ©e | Ã‰tape: [E1] | Source: [S0]  # #
        if os.path.isfile(os.path.join(cur, "pyproject.toml")):  # # VÃ©rifie si "pyproject.toml" existe ici : isfile teste la prÃ©sence dâ€™un fichier marqueur de projet | Ã‰tape: [E1] | Source: [S0]  # #
            return cur  # # Renvoie cur comme racine dÃ¨s quâ€™un marqueur de projet est trouvÃ© (sortie immÃ©diate) | Ã‰tape: [E1] | Source: [S0]  # #
        if os.path.isfile(os.path.join(cur, "requirements.txt")):  # # VÃ©rifie si "requirements.txt" existe : isfile confirme quâ€™on est probablement Ã  la racine (dÃ©pendances Python) | Ã‰tape: [E1] | Source: [S0]  # #
            return cur  # # Renvoie cur comme racine si requirements.txt est trouvÃ© (sortie immÃ©diate) | Ã‰tape: [E1] | Source: [S0]  # #
        parent = os.path.dirname(cur)  # # Calcule le dossier parent : dirname enlÃ¨ve le dernier segment du chemin (remonte dâ€™un niveau) | Ã‰tape: [E1] | Source: [S0]  # #
        if parent == cur:  # # Teste si on est arrivÃ© tout en haut (plus possible de remonter) : parent==cur signifie â€œracine disque atteinteâ€ | Ã‰tape: [E1] | Source: [S0]  # #
            return os.path.abspath(start_dir)  # # Fallback : renvoie le chemin absolu du start_dir (abspath le rend stable mÃªme si relatif) | Ã‰tape: [E2] | Source: [S6]  # #
        cur = parent  # # Met Ã  jour cur avec le parent pour continuer la remontÃ©e dans la boucle | Ã‰tape: [E1] | Source: [S0]  # #



# ===============================  # #  | Ã‰tape: [E1] | Source: [S0] 
# ğŸ§­ Constantes arXiv 
# ===============================  

ARXIV_BASE = "https://arxiv.org"  # # DÃ©finit lâ€™URL de base dâ€™arXiv : on la rÃ©utilise pour construire toutes les autres URLs sans les rÃ©Ã©crire | Ã‰tape: [E2] | Source: [S3]  # #
ARXIV_SEARCH_CS = f"{ARXIV_BASE}/search/cs"  # # Construit lâ€™URL du endpoint de recherche Computer Science : f-string insÃ¨re ARXIV_BASE dans la chaÃ®ne | Ã‰tape: [E1] | Source: [S0]  # #

_THIS_FILE_DIR = os.path.dirname(os.path.abspath(__file__))  # # RÃ©cupÃ¨re le dossier rÃ©el du fichier Python : abspath calcule le chemin complet de __file__, dirname garde seulement le dossier | Ã‰tape: [E1] | Source: [S0]  # #
PROJECT_ROOT = _find_project_root(_THIS_FILE_DIR)  # # Appelle la fonction de remontÃ©e pour trouver la racine du projet (Ã©vite dâ€™Ã©crire dans le mauvais dossier si le CWD change) | Ã‰tape: [E1] | Source: [S0]  # #
DEFAULT_RAW_DIR = os.path.join(PROJECT_ROOT, "data_lake", "raw", "cache")  # # Construit le chemin du dossier de cache raw : join assemble proprement les segments (compatible Windows) | Ã‰tape: [E2] | Source: [S2]  # #

MAX_RESULTS_HARD_LIMIT = 100  # # Fixe un plafond dur du nombre de rÃ©sultats pour Ã©viter un scraping trop massif (sÃ©curitÃ©/performance) | Ã‰tape: [E1] | Source: [S0]  # #
PAGE_SIZE = 50  # # Fixe la taille dâ€™une page arXiv Ã  50 : sert Ã  paginer proprement sans dÃ©passer la limite attendue | Ã‰tape: [E1] | Source: [S0]  # #


# ===============================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
# ğŸ§¯ Robustesse HTTP 
# ===============================
HTTP_RETRY_STATUS = {429, 500, 502, 503, 504}  # # Codes Ã  retry # # Respect: agent robuste (ne pas casser au 1er incident) | Ã‰tape: [E2] | Source: [S3]  # #
HTTP_RETRY_MAX = 2  # # Nombre de retries/reessai # # Respect: frÃ©quence raisonnable (pas de spam) | Ã‰tape: [E2] | Source: [S3]  # #
HTTP_TIMEOUT_S = 30  # # Timeout # # Respect: robustesse (Ã©vite blocage) | Ã‰tape: [E2] | Source: [S3]  # #


# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # # DÃ©but de bloc â€œgrand titreâ€ : sert juste de repÃ¨re visuel dans le fichier
# ğŸ¯ ThÃ¨mes demandÃ©s -> sous-catÃ©gories arXiv autorisÃ©es  # #  | Ã‰tape: [E1] | Source: [S0]  # # Annonce que lâ€™on va dÃ©finir la table qui relie un thÃ¨me (ex: ai_ml) aux catÃ©gories arXiv autorisÃ©es
# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # # Fin de lâ€™en-tÃªte â€œgrand titreâ€ (lisibilitÃ©)

THEME_TO_ARXIV_SUBCATS: Dict[str, List[str]] = {  # # CrÃ©e un dictionnaire typÃ© (clÃ©=thÃ¨me, valeur=liste de catÃ©gories) : permet de filtrer les rÃ©sultats arXiv selon le thÃ¨me choisi | Ã‰tape: [E1] | Source: [S0]  # #
    "ai_ml": [  # # Liste des catÃ©gories autorisÃ©es pour le thÃ¨me IA/ML : on regroupe ici les sujets arXiv qui couvrent LLM, vision, multimodal, agents | Ã‰tape: [E1] | Source: [S0]  # #
        "cs.AI",    # # CatÃ©gorie arXiv â€œArtificial Intelligenceâ€ : correspond aux papiers IA/agents/raisonnement | Ã‰tape: [E1] | Source: [S0]  # #
        "cs.LG",    # # CatÃ©gorie â€œMachine Learning (CS)â€ : correspond aux papiers ML cÃ´tÃ© informatique | Ã‰tape: [E1] | Source: [S0]  # #
        "cs.CL",    # # CatÃ©gorie â€œComputation and Languageâ€ : correspond aux papiers NLP/LLM/traitement du langage | Ã‰tape: [E1] | Source: [S0]  # #
        "cs.CV",    # # CatÃ©gorie â€œComputer Visionâ€ : correspond aux papiers vision / image / multimodal | Ã‰tape: [E1] | Source: [S0]  # #
        "cs.MA",    # # CatÃ©gorie â€œMultiagent Systemsâ€ : correspond aux systÃ¨mes multi-agents (agents qui coopÃ¨rent) | Ã‰tape: [E1] | Source: [S0]  # #
        "cs.NE",    # # CatÃ©gorie â€œNeural and Evolutionary Computingâ€ : correspond aux approches rÃ©seaux neuronaux / deep learning (historique) | Ã‰tape: [E1] | Source: [S0]  # #
        "stat.ML",  # # CatÃ©gorie â€œMachine Learning (Stats)â€ : correspond aux cross-lists frÃ©quentes ML cÃ´tÃ© statistiques (Ã©vite de rater des papiers) | Ã‰tape: [E1] | Source: [S0]  # #
        "eess.IV",  # # CatÃ©gorie â€œImage and Video Processingâ€ : correspond Ã  vision/image parfois classÃ©e hors CS (Ã©vite de rater des papiers vision) | Ã‰tape: [E1] | Source: [S0]  # #
    ],
    "algo_ds": ["cs.DS", "cs.CC"],  # # DÃ©clare les catÃ©gories autorisÃ©es pour â€œAlgorithmique & Data Structuresâ€ : cs.DS=structures de donnÃ©es, cs.CC=complexitÃ© | Ã‰tape: [E1] | Source: [S0]  # #
    "net_sys": ["cs.NI", "cs.DC", "cs.OS"],  # # DÃ©clare les catÃ©gories autorisÃ©es pour â€œRÃ©seaux & SystÃ¨mesâ€ : cs.NI=rÃ©seau, cs.DC=distribuÃ©, cs.OS=systÃ¨mes dâ€™exploitation | Ã‰tape: [E1] | Source: [S0]  # #
    "cyber_crypto": ["cs.CR"],  # # DÃ©clare les catÃ©gories autorisÃ©es pour â€œCybersÃ©curitÃ© & Cryptoâ€ : cs.CR=cryptographie et sÃ©curitÃ© | Ã‰tape: [E1] | Source: [S0]  # #
    "pl_se": ["cs.PL", "cs.SE", "cs.LO"],  # # DÃ©clare les catÃ©gories autorisÃ©es pour â€œLangages & GÃ©nie logicielâ€ : cs.PL=langages, cs.SE=gÃ©nie logiciel, cs.LO=logique en CS | Ã‰tape: [E1] | Source: [S0]  # #
    "hci_data": ["cs.HC", "cs.IR", "cs.DB", "cs.MM"],  # # DÃ©clare les catÃ©gories autorisÃ©es pour â€œHCI & DonnÃ©esâ€ : cs.HC=interaction, cs.IR=recherche dâ€™info, cs.DB=bases de donnÃ©es, cs.MM=multimÃ©dia | Ã‰tape: [E1] | Source: [S0]  # #
}  # # Ferme le dictionnaire : cette table sera utilisÃ©e plus loin pour filtrer/valider les catÃ©gories extraites depuis la page arXiv | Ã‰tape: [E1] | Source: [S0]  # #


# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
# ğŸ§  Keywords fallback (si le site est modifiÃ© et que l'on perd les catÃ©gories (cs.AI etc.), on cherche via des mots-clÃ©s (AI/transformer/LLMâ€¦)  # #  
# ============================================================  
THEME_KEYWORDS: Dict[str, List[str]] = {  # # Support # # Respect: filtrage pertinence si catÃ©gories manquantes | Ã‰tape: [E1] | Source: [S0]  # #
    "ai_ml": ["machine learning", "deep learning", "llm", "agent", "transformer", "multimodal", "computer vision"],
    "algo_ds": ["algorithm", "data structure", "complexity", "graph", "optimization"],
    "net_sys": ["network", "distributed", "operating system", "cloud", "systems"],
    "cyber_crypto": ["security", "privacy", "cryptography", "attack", "defense", "malware"],
    "pl_se": ["programming language", "compiler", "software engineering", "static analysis", "type system"],
    "hci_data": ["human-computer interaction", "information retrieval", "database", "multimedia", "ranking", "search"],
}

# ===============================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
# ğŸ“¦ Champs renvoyÃ©s (minimal)  
# =============================== 
SUPPORTED_FIELDS = [  # # Liste Python = "contrat" des champs renvoyÃ©s (format standard et prÃ©visible) pour dÃ©coupler le scraping du reste | Ã‰tape: [E1] | Source: [S1]  # #
    "arxiv_id",  # # Champ JSON arxiv_id = identifiant unique arXiv du papier (FR: ID du papier) utilisÃ© pour relier /abs et /pdf | Ã‰tape: [E1] | Source: [S1]  # #
    "title",  # # Champ JSON title = titre du papier (FR: nom du papier) pour l'affichage et le contexte LLM | Ã‰tape: [E1] | Source: [S1]  # #
    "authors",  # # Champ JSON authors = liste des auteurs (FR: auteurs) pour attribuer le travail et contextualiser la source | Ã‰tape: [E1] | Source: [S1]  # #
    "abstract",  # # Champ JSON abstract = rÃ©sumÃ© du papier (FR: rÃ©sumÃ©) pour comprendre rapidement le contenu sans HTML brut | Ã‰tape: [E1] | Source: [S1]  # #
    "method",  # # Champ JSON method = section MÃ©thode (FR: "MÃ©thode") extraite de /html pour enrichir sans aspirer toute la page | Ã‰tape: [E1] | Source: [S8]  # #
    "references",  # # Champ JSON references = section RÃ©fÃ©rences (FR: "RÃ©fÃ©rences") pour garder les sources citÃ©es (utile QA) | Ã‰tape: [E1] | Source: [S8]  # #
    "submitted_date",  # # Champ JSON submitted_date = date de soumission (FR: date d'envoi Ã  arXiv) pour la traÃ§abilitÃ© temporelle | Ã‰tape: [E1] | Source: [S1]  # #
    "abs_url",  # # Champ JSON abs_url = lien arXiv /abs (FR: page dÃ©tails) pour relire/diagnostiquer la source | Ã‰tape: [E1] | Source: [S1]  # #
    "pdf_url",  # # Champ JSON pdf_url = lien arXiv /pdf (FR: tÃ©lÃ©chargement PDF) pour accÃ©der au document complet | Ã‰tape: [E1] | Source: [S1]  # #
    "doi",  # # Champ JSON doi = identifiant DOI (FR: identifiant Ã©diteur) si prÃ©sent, pour relier Ã  la publication officielle | Ã‰tape: [E1] | Source: [S1]  # #
    "versions",  # # Champ JSON versions = historique des versions v1,v2... (FR: versions) pour savoir ce qui a changÃ© | Ã‰tape: [E1] | Source: [S1]  # #
    "last_updated_raw",  # # Champ JSON last_updated_raw = derniÃ¨re mise Ã  jour brute (FR: derniÃ¨re maj) depuis l'historique arXiv | Ã‰tape: [E1] | Source: [S1]  # #
    "primary_category",  # # Champ JSON primary_category = catÃ©gorie principale (FR: thÃ¨me principal) ex: cs.AI pour filtrer thÃ©matiquement | Ã‰tape: [E1] | Source: [S1]  # #
    "all_categories",  # # Champ JSON all_categories = toutes les catÃ©gories (FR: tags/thÃ¨mes) pour gÃ©rer cross-list et filtrage robuste | Ã‰tape: [E1] | Source: [S1]  # #
    "missing_fields",  # # Champ JSON missing_fields = liste des champs non trouvÃ©s (FR: champs manquants) pour diagnostiquer sans planter | Ã‰tape: [E2] | Source: [S3]  # #
    "errors",  # # Champ JSON errors = erreurs liÃ©es Ã  cet item (FR: erreurs papier) pour remonter les soucis proprement | Ã‰tape: [E2] | Source: [S3]  # #
]



# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
# ğŸ§© Helpers base  # # Regroupe des petites fonctions utilitaires rÃ©utilisÃ©es partout (Ã©vite duplication et erreurs) | Ã‰tape: [E1] | Source: [S0]  # #
# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
def ensure_dir(path: str) -> None:  # # Fonction ensure_dir = garantit que le dossier existe avant dâ€™Ã©crire des fichiers (cache/exports) | Ã‰tape: [E2] | Source: [S2]  # #
    os.makedirs(path, exist_ok=True)  # # CrÃ©e le dossier path ; exist_ok=True = ne lÃ¨ve pas dâ€™erreur si dÃ©jÃ  prÃ©sent (Ã©vite crash) | Ã‰tape: [E2] | Source: [S2]  # #


def now_iso_for_filename() -> str:  # # Fonction now_iso_for_filename = gÃ©nÃ¨re un timestamp lisible pour nommer les fichiers sans collision | Ã‰tape: [E1] | Source: [S0]  # #
    return datetime.datetime.now().strftime("%Y%m%d_%H%M%S")  # # strftime = formate la date/heure en texte "YYYYMMDD_HHMMSS" pour tracer quand le fichier a Ã©tÃ© produit | Ã‰tape: [E1] | Source: [S0]  # #

def is_empty(value: Any) -> bool:  # # DÃ©tection "vide" # # Respect: qualitÃ© sortie JSON | Ã‰tape: [E1] | Source: [S1]  # #
    if value is None:  # # None # # Respect: qualitÃ© | Ã‰tape: [E1] | Source: [S0]  # #
        return True  # # Vide # # Respect: qualitÃ© | Ã‰tape: [E1] | Source: [S0]  # #
    if isinstance(value, str):  # # String # # Respect: qualitÃ© | Ã‰tape: [E1] | Source: [S0]  # #
        v = value.strip()  # # Trim # # Respect: nettoyage | Ã‰tape: [E1] | Source: [S0]  # #
        if v == "":  # # Vide # # Respect: qualitÃ© | Ã‰tape: [E1] | Source: [S0]  # #
            return True  # # Vide | Ã‰tape: [E1] | Source: [S0]  # #
        if v.lower() in {"n/a", "null", "none"}:  # # Marqueurs # # Respect: qualitÃ© | Ã‰tape: [E1] | Source: [S0]  # #
            return True  # # Vide | Ã‰tape: [E1] | Source: [S0]  # #
    if isinstance(value, list):  # # Liste # # Respect: qualitÃ© | Ã‰tape: [E1] | Source: [S0]  # #
        return len(value) == 0  # # Vide si liste vide # # Respect: qualitÃ© | Ã‰tape: [E1] | Source: [S0]  # #
    return False  # # Non vide # # Respect: qualitÃ© | Ã‰tape: [E1] | Source: [S0]  # #


def sleep_polite(min_s: float = 1.2, max_s: float = 2.0) -> None:  # # Politesse # # Respect: frÃ©quence raisonnable | Ã‰tape: [E4] | Source: [S5]  # #
    time.sleep(random.uniform(min_s, max_s))  # # Jitter # # Respect: anti-spam | Ã‰tape: [E4] | Source: [S5]  # #


def save_text_file(folder: str, filename: str, content: str) -> str:  # # Sauvegarde # # Respect: cache local visible | Ã‰tape: [E2] | Source: [S2]  # #
    ensure_dir(folder)  # # Assurer dossier # # Respect: cache disque | Ã‰tape: [E2] | Source: [S2]  # #
    path = os.path.join(folder, filename)  # # Chemin # # Respect: cohÃ©rence | Ã‰tape: [E1] | Source: [S0]  # #
    with open(path, "w", encoding="utf-8") as f:  # # UTF-8 # # Respect: robustesse encodage | Ã‰tape: [E1] | Source: [S0]  # #
        f.write(content)  # # Ã‰criture # # Respect: traÃ§abilitÃ©/debug | Ã‰tape: [E1] | Source: [S0]  # #
    return path  # # Retour chemin # # Respect: utilisateur peut retrouver le fichier | Ã‰tape: [E1] | Source: [S0]  # #


def normalize_url(href: str) -> str:  # # Normalise URL # # Respect: champs propres | Ã‰tape: [E1] | Source: [S0]  # #
    if not href:  # # Si vide # # Respect: robustesse | Ã‰tape: [E1] | Source: [S0]  # #
        return ""  # # Retour vide # # Respect: robustesse | Ã‰tape: [E1] | Source: [S0]  # #
    h = href.strip()  # # Trim # # Respect: sortie propre | Ã‰tape: [E1] | Source: [S0]  # #
    if h.startswith("//"):  # # SchÃ©ma manquant # # Respect: robustesse | Ã‰tape: [E1] | Source: [S0]  # #
        return "https:" + h  # # Force https # # Respect: sortie valide | Ã‰tape: [E2] | Source: [S3]  # #
    if h.startswith("/"):  # # Relatif # # Respect: robustesse | Ã‰tape: [E1] | Source: [S0]  # #
        return ARXIV_BASE + h  # # Absolu # # Respect: sortie valide | Ã‰tape: [E1] | Source: [S0]  # #
    return h  # # DÃ©jÃ  absolu # # Respect: sortie valide | Ã‰tape: [E1] | Source: [S0]  # #

def abs_url(arxiv_id: str) -> str:  # # Fonction abs_url = fabrique lâ€™URL â€œficheâ€ /abs Ã  partir de lâ€™identifiant arXiv (utile si le HTML ne donne pas le lien complet) | Ã‰tape: [E1] | Source: [S0]  # #
    return f"{ARXIV_BASE}/abs/{arxiv_id}"  # # f-string = insÃ¨re arxiv_id dans le modÃ¨le dâ€™URL pour obtenir ex: https://arxiv.org/abs/2601.08457 | Ã‰tape: [E1] | Source: [S0]  # #


def pdf_url(arxiv_id: str) -> str:  # # Fonction pdf_url = fabrique lâ€™URL de tÃ©lÃ©chargement /pdf Ã  partir de lâ€™identifiant arXiv (fallback si le lien PDF est absent) | Ã‰tape: [E1] | Source: [S0]  # #
    return f"{ARXIV_BASE}/pdf/{arxiv_id}"  # # f-string = construit ex: https://arxiv.org/pdf/2601.08457 pour tÃ©lÃ©charger/ouvrir le PDF | Ã‰tape: [E1] | Source: [S0]  # #


def compute_missing_fields(item: Dict[str, Any]) -> List[str]:  # # Fonction compute_missing_fields = liste les champs manquants dâ€™un item (contrÃ´le qualitÃ©) pour savoir ce que le parsing nâ€™a pas trouvÃ© | Ã‰tape: [E1] | Source: [S0]  # #
    missing: List[str] = []  # # On crÃ©e une liste vide â€œmissingâ€ qui va stocker les noms des champs absents (diagnostic) | Ã‰tape: [E1] | Source: [S0]  # #
    for f in SUPPORTED_FIELDS:  # # Boucle sur tous les champs attendus (contrat stable) pour vÃ©rifier chacun systÃ©matiquement | Ã‰tape: [E1] | Source: [S1]  # #
        if is_empty(item.get(f)):  # # is_empty() teste si la valeur est vide (None, "", liste videâ€¦) â†’ ici on lâ€™utilise pour dÃ©tecter un champ non rempli | Ã‰tape: [E2] | Source: [S6]  # #
            missing.append(f)  # # On ajoute le nom du champ manquant dans la liste pour pouvoir le renvoyer dans le JSON final | Ã‰tape: [E2] | Source: [S6]  # #
    return missing  # # On renvoie la liste des champs manquants â†’ Ã§a aide Ã  debug sans casser le reste du pipeline | Ã‰tape: [E1] | Source: [S1]  # #


def _detect_weird_page_signals(html: str) -> Dict[str, bool]:  # # Fonction _detect_weird_page_signals = repÃ¨re des â€œsignauxâ€ de page anormale (anti-bot, consentement, zÃ©ro rÃ©sultat) pour diagnostiquer pourquoi le parsing Ã©choue | Ã‰tape: [E2] | Source: [S6]  # #
    h = (html or "").lower()  # # lower() met tout en minuscules pour faire des tests â€œinâ€ insensibles Ã  la casse (plus robuste) | Ã‰tape: [E1] | Source: [S0]  # #
    return {  # # On renvoie un dict de drapeaux boolÃ©ens (diagnostic lisible dans le JSON) | Ã‰tape: [E2] | Source: [S6]  # #
        "contains_we_are_sorry": ("we are sorry" in h),  # # True si on voit un message de blocage du site (souvent anti-bot) | Ã‰tape: [E2] | Source: [S6]  # #
        "contains_robot": ("robot" in h),  # # True si la page mentionne â€œrobotâ€ (indice de dÃ©tection automatique) | Ã‰tape: [E2] | Source: [S6]  # #
        "contains_captcha": ("captcha" in h),  # # True si un CAPTCHA apparaÃ®t (le scraper ne peut pas rÃ©soudre Ã§a) | Ã‰tape: [E2] | Source: [S6]  # #
        "contains_consent": ("consent" in h or "cookie" in h),  # # True si une page cookies/consent bloque le contenu (HTML diffÃ©rent) | Ã‰tape: [E2] | Source: [S6]  # #
        "contains_no_results": ("no results found" in h),  # # True si la page annonce â€œaucun rÃ©sultatâ€ (pas un bug de parsing) | Ã‰tape: [E2] | Source: [S6]  # #
    }  # # Fin dict diagnostic (pas dâ€™effet sur scraping, juste une aide de debug) | Ã‰tape: [E2] | Source: [S6]  # #


def http_get_text(session: requests.Session, url: str, timeout_s: int = 30) -> Tuple[str, int]:  # # Fonction http_get_text = fait un GET HTTP et renvoie (HTML, code HTTP) sans faire crasher le tool si rÃ©seau/timeout | Ã‰tape: [E2] | Source: [S3]  # #
    headers = {  # # On prÃ©pare les en-tÃªtes HTTP envoyÃ©s Ã  arXiv (Ã§a aide Ã  Ãªtre acceptÃ© + parsing stable) | Ã‰tape: [E2] | Source: [S0]  # #
        "User-Agent": "Mozilla/5.0 DIXITBOT-arXivScraper/4.1",  # # User-Agent = â€œcarte dâ€™identitÃ©â€ HTTP ; ici on met un UA clair pour Ã©viter dâ€™Ãªtre vu comme un bot suspect | Ã‰tape: [E2] | Source: [S4]  # #
        "Accept-Language": "en-US,en;q=0.9",  # # Accept-Language = demande une page en anglais pour Ã©viter des variations de texte selon la langue | Ã‰tape: [E2] | Source: [S0]  # #
    }  # # Fin headers | Ã‰tape: [E2] | Source: [S0]  # #
    try:  # # try = on encapsule lâ€™appel rÃ©seau pour gÃ©rer proprement les erreurs au lieu de crasher tout le script | Ã‰tape: [E2] | Source: [S3]  # #
        resp = session.get(url, headers=headers, timeout=timeout_s)  # # session.get() exÃ©cute le GET ; timeout Ã©vite que Ã§a bloque indÃ©finiment si le site rÃ©pond mal | Ã‰tape: [E2] | Source: [S3]  # #
        return resp.text, resp.status_code  # # On renvoie le HTML + le status_code (200, 429, 500...) pour diagnostic + contrat stable | Ã‰tape: [E1] | Source: [S1]  # #
    except requests.RequestException as e:  # # requests.RequestException = toutes les erreurs rÃ©seau (timeout, DNS, connexion refusÃ©e, etc.) | Ã‰tape: [E2] | Source: [S0]  # #
        return f"REQUEST_EXCEPTION: {str(e)}", 0  # # On renvoie un â€œHTMLâ€ texte dâ€™erreur + code 0 (0 = erreur locale, pas une rÃ©ponse HTTP du serveur) | Ã‰tape: [E2] | Source: [S3]  # #

def build_search_url(query: str, start: int, size: int, sort: str) -> str:  # # Fonction build_search_url = construit lâ€™URL complÃ¨te de recherche arXiv â€œ/search/csâ€ avec query + pagination + tri (pour appeler arXiv de faÃ§on standard) | Ã‰tape: [E1] | Source: [S0]  # #
    q = requests.utils.quote((query or "").strip())  # # requests.utils.quote() encode la requÃªte (espacesâ†’%20, guillemetsâ†’%22â€¦) pour quâ€™elle soit valide dans une URL HTTP | Ã‰tape: [E1] | Source: [S0]  # #
    base = f"{ARXIV_SEARCH_CS}?query={q}&searchtype=all&abstracts=show&size={size}&start={start}"  # # f-string = assemble lâ€™URL â€œbaseâ€ avec paramÃ¨tres: query (mots-clÃ©s), size (nb rÃ©sultats/page), start (offset pagination) | Ã‰tape: [E1] | Source: [S0]  # #
    s = (sort or "relevance").strip().lower()  # # On normalise le champ sort (par dÃ©faut â€œrelevanceâ€), trim + lower pour comparer sans se tromper (robuste aux entrÃ©es utilisateur) | Ã‰tape: [E1] | Source: [S0]  # #
    if s in {"submitted_date", "submitted", "recent"}:  # # Si lâ€™utilisateur veut trier par date de soumission (ou un alias), on active le tri â€œrÃ©cents dâ€™abordâ€ | Ã‰tape: [E1] | Source: [S0]  # #
        return base + "&order=-announced_date_first"  # # On ajoute le paramÃ¨tre arXiv â€œorder=-announced_date_firstâ€ pour renvoyer les papiers les plus rÃ©cents en premier | Ã‰tape: [E1] | Source: [S0]  # #
    return base  # # Sinon, on retourne lâ€™URL de base (tri â€œrelevanceâ€ par dÃ©faut) pour garder un comportement stable et prÃ©visible | Ã‰tape: [E1] | Source: [S0]  # #


# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
# ğŸ§² Extraction catÃ©gories depuis "Subjects" + tags (robuste)  # #  | Ã‰tape: [E1] | Source: [S8]  # #
# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
_RE_ANY_CAT = re.compile(r"\(((?:cs|stat|eess)\.[A-Z]{2})\)")  # # Regex cat # # Respect: inclut cross-lists (stat.ML/eess.IV) | Ã‰tape: [E1] | Source: [S0]  # #
_RE_ARXIV_ID = re.compile(r"/abs/([^?#/]+)")  # # Regex ID # # Respect: extraction stable | Ã‰tape: [E1] | Source: [S8]  # #


def extract_categories_from_result(li: Tag) -> Tuple[str, List[str]]:  # # Lit catÃ©gories # # Respect: filtrage thÃ©matique aprÃ¨s search/cs | Ã‰tape: [E1] | Source: [S8]  # #
    cats: List[str] = []  # # Init # # Respect: structuration | Ã‰tape: [E1] | Source: [S0]  # #

    # (1) MÃ©thode robuste: tags visibles (quand arXiv rend des badges)
    for span in li.select("span.tag"):  # # Parcours tags # # Respect: extraction ciblÃ©e | Ã‰tape: [E1] | Source: [S8]  # #
        t = (span.get_text(" ", strip=True) or "").strip()  # # Texte tag # # Respect: nettoyage | Ã‰tape: [E1] | Source: [S0]  # #
        if re.fullmatch(r"(?:cs|stat|eess)\.[A-Z]{2}", t):  # # Si ressemble Ã  une cat # # Respect: filtrage fiable | Ã‰tape: [E1] | Source: [S0]  # #
            cats.append(t)  # # Ajoute # # Respect: structuration | Ã‰tape: [E1] | Source: [S0]  # #

    # (2) MÃ©thode fallback: regex sur la ligne "Subjects:  (cs.XX);  (stat.ML)"
    if not cats:  # # Si tags absents # # Respect: robustesse | Ã‰tape: [E1] | Source: [S0]  # #
        txt = li.get_text(" ", strip=True)  # # Texte bloc (minimum) # # Respect: extraction juste pour cat | Ã‰tape: [E1] | Source: [S8]  # #
        cats = _RE_ANY_CAT.findall(txt)  # # Extrait cats # # Respect: mapping demandÃ© | Ã‰tape: [E1] | Source: [S0]  # #

    # DÃ©doublonnage en gardant l'ordre
    cats = list(dict.fromkeys(cats))  # # DÃ©doublonne # # Respect: sortie propre | Ã‰tape: [E1] | Source: [S0]  # #
    primary = cats[0] if cats else ""  # # Premier # # Respect: structuration | Ã‰tape: [E1] | Source: [S0]  # #
    return primary, cats  # # Retour # # Respect: sortie structurÃ©e | Ã‰tape: [E1] | Source: [S1]  # #


# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
# ğŸ§¾ Parsing page search/cs -> items minimaux  # #  | Ã‰tape: [E1] | Source: [S1]  # #
# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
def parse_search_page(html: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:  # # Parse + diag # # Respect: robustesse | Ã‰tape: [E2] | Source: [S6]  # #
    soup = BeautifulSoup(html, "lxml")  # # Parse # # Respect: extraction ciblÃ©e | Ã‰tape: [E1] | Source: [S8]  # #
    page_title = soup.title.get_text(" ", strip=True) if soup.title else ""  # # Titre page # # Respect: diagnostic | Ã‰tape: [E2] | Source: [S6]  # #
    weird = _detect_weird_page_signals(html)  # # DÃ©tection anti-bot/no-results # # Respect: robustesse | Ã‰tape: [E2] | Source: [S6]  # #

    diag: Dict[str, Any] = {  # # Diagnostic # # Respect: traÃ§abilitÃ© | Ã‰tape: [E2] | Source: [S6]  # #
        "page_title": page_title,  # # Titre # # Respect: debug | Ã‰tape: [E1] | Source: [S0]  # #
        "has_abs_links": ("/abs/" in (html or "")),  # # Indicateur principal # # Respect: debug | Ã‰tape: [E1] | Source: [S0]  # #
        **weird,  # # Drapeaux # # Respect: debug | Ã‰tape: [E2] | Source: [S6]  # #
    }

    items: List[Dict[str, Any]] = []  # # RÃ©sultats # # Respect: sortie structurÃ©e | Ã‰tape: [E1] | Source: [S1]  # #

    # SÃ©lecteur principal (arXiv actuel)
    result_nodes = soup.select("ol.breathe-horizontal li.arxiv-result")  # # Noeuds rÃ©sultats # # Respect: extraction ciblÃ©e | Ã‰tape: [E1] | Source: [S8]  # #
    diag["selector_count_arxiv_result"] = len(result_nodes)  # # Compte # # Respect: debug | Ã‰tape: [E2] | Source: [S6]  # #
    # Fallback: si DOM change, on reconstruit via liens /abs/
    if not result_nodes and diag["has_abs_links"]:  # # Si le sÃ©lecteur principal ne trouve aucun bloc rÃ©sultat MAIS que le HTML contient des liens â€œ/abs/â€, on active un plan B pour ne pas renvoyer 0 items juste Ã  cause dâ€™un changement de DOM | Ã‰tape: [E2] | Source: [S6]  # #
        diag["fallback_mode"] = "abs_links"  # # On note dans le diagnostic quâ€™on est passÃ© en mode fallback â€œabs_linksâ€ (Ã§a explique pourquoi certains champs sont vides) | Ã‰tape: [E2] | Source: [S6]  # #
        abs_ids = _RE_ARXIV_ID.findall(html or "")  # # findall() extrait tous les identifiants arXiv prÃ©sents dans les URLs /abs/... du HTML, mÃªme si les classes HTML ont changÃ© | Ã‰tape: [E1] | Source: [S8]  # #
        abs_ids = list(dict.fromkeys(abs_ids))[:PAGE_SIZE]  # # dict.fromkeys() retire les doublons en gardant lâ€™ordre, puis on limite Ã  PAGE_SIZE (=50) pour contrÃ´ler le volume et respecter la pagination | Ã‰tape: [E1] | Source: [S0]  # #
        for arxiv_id in abs_ids:  # # On parcourt chaque identifiant trouvÃ© pour reconstruire des items minimaux (structure stable mÃªme sans parsing complet) | Ã‰tape: [E1] | Source: [S1]  # #
            items.append({  # # On ajoute un â€œitem minimalâ€ (dict) : on garde uniquement les champs indispensables et on laisse le reste vide plutÃ´t que dâ€™envoyer du HTML brut | Ã‰tape: [E1] | Source: [S1]  # #
                "arxiv_id": arxiv_id,  # # Identifiant arXiv (FR: lâ€™ID unique du papier) : sert Ã  construire les URLs et Ã  rÃ©fÃ©rencer le papier mÃªme si le reste nâ€™a pas Ã©tÃ© parsÃ© | Ã‰tape: [E1] | Source: [S0]  # #
                "title": "",  # # Titre vide car indisponible en fallback (on prÃ©fÃ¨re vide plutÃ´t que faux) | Ã‰tape: [E1] | Source: [S0]  # #
                "authors": [],  # # Auteurs vides en fallback (liste vide = format stable cÃ´tÃ© API) | Ã‰tape: [E1] | Source: [S0]  # #
                "abstract": "",  # # RÃ©sumÃ© (abstract) vide en fallback (on ne devine pas) | Ã‰tape: [E1] | Source: [S0]  # #
                "submitted_date": "",  # # Date de soumission vide en fallback (on la remplira plus tard via /abs si besoin) | Ã‰tape: [E1] | Source: [S0]  # #
                "abs_url": abs_url(arxiv_id),  # # abs_url() fabrique lâ€™URL /abs/{id} (FR: page fiche du papier) pour pouvoir enrichir ensuite sans dÃ©pendre du HTML de recherche | Ã‰tape: [E1] | Source: [S0]  # #
                "pdf_url": pdf_url(arxiv_id),  # # pdf_url() fabrique lâ€™URL /pdf/{id} (FR: lien direct PDF) pour garder une sortie utile mÃªme en mode fallback | Ã‰tape: [E1] | Source: [S0]  # #
                "primary_category": "",  # # CatÃ©gorie principale vide en fallback (on Ã©vite les faux labels) | Ã‰tape: [E1] | Source: [S0]  # #
                "all_categories": [],  # # Toutes catÃ©gories vides en fallback (liste stable cÃ´tÃ© API) | Ã‰tape: [E1] | Source: [S0]  # #
            })
        return items, diag  # # On retourne immÃ©diatement (items + diag) : câ€™est un retour contrÃ´lÃ©/traÃ§able plutÃ´t quâ€™un crash ou un â€œ0 rÃ©sultatâ€ incomprÃ©hensible | Ã‰tape: [E2] | Source: [S6]  # #

    # Mode normal
    for li in result_nodes:  # # On parcourt chaque bloc rÃ©sultat â€œli.arxiv-resultâ€ pour extraire les champs utiles de faÃ§on ciblÃ©e (pas â€œtout le HTMLâ€) | Ã‰tape: [E1] | Source: [S8]  # #
        title_el = li.select_one("p.title")  # # select_one() rÃ©cupÃ¨re la balise du titre (p.title) : extraction prÃ©cise dâ€™un champ essentiel | Ã‰tape: [E1] | Source: [S0]  # #
        authors_el = li.select_one("p.authors")  # # select_one() rÃ©cupÃ¨re la balise des auteurs (p.authors) : extraction prÃ©cise dâ€™un champ essentiel | Ã‰tape: [E1] | Source: [S0]  # #
        abstract_el = li.select_one("span.abstract-full")  # # select_one() rÃ©cupÃ¨re la balise du rÃ©sumÃ© complet (span.abstract-full) : extraction prÃ©cise sans prendre toute la page | Ã‰tape: [E1] | Source: [S0]  # #
        submitted_el = li.select_one("p.is-size-7")  # # select_one() rÃ©cupÃ¨re le bloc â€œSubmitted â€¦â€ (p.is-size-7) : utile pour la date | Ã‰tape: [E1] | Source: [S0]  # #

        abs_a = li.select_one('p.list-title a[href*="/abs/"]')  # # select_one() rÃ©cupÃ¨re le lien vers /abs/ (FR: page fiche du papier), car câ€™est la source la plus stable pour rÃ©cupÃ©rer lâ€™ID arXiv | Ã‰tape: [E1] | Source: [S0]  # #
        pdf_a = li.select_one('p.list-title a[href*="/pdf/"]')  # # select_one() rÃ©cupÃ¨re le lien vers /pdf/ (FR: tÃ©lÃ©chargement du PDF), car câ€™est un lien utile Ã  renvoyer Ã  lâ€™utilisateur | Ã‰tape: [E1] | Source: [S0]  # #
        abs_href = normalize_url(abs_a.get("href") if abs_a else "")  # # .get("href") lit lâ€™attribut href, puis normalize_url() convertit en URL absolue (FR: URL propre et utilisable) | Ã‰tape: [E1] | Source: [S0]  # #
        pdf_href = normalize_url(pdf_a.get("href") if pdf_a else "")  # # MÃªme logique pour le PDF : URL absolue pour Ã©viter les liens relatifs qui cassent | Ã‰tape: [E1] | Source: [S0]  # #

        arxiv_id = ""  # # On initialise lâ€™identifiant arXiv vide : on le remplira uniquement si on lâ€™extrait proprement (Ã©vite valeurs fausses) | Ã‰tape: [E1] | Source: [S0]  # #
        m = re.search(r"/abs/([^?#/]+)", abs_href) if abs_href else None  # # re.search() cherche â€œ/abs/{id}â€ dans lâ€™URL pour isoler lâ€™ID (ici on lâ€™utilise pour extraire un identifiant stable) | Ã‰tape: [E1] | Source: [S8]  # #
        if m:  # # Si la regex a trouvÃ© un ID, on sÃ©curise le remplissage (sinon on garde vide) | Ã‰tape: [E1] | Source: [S0]  # #
            arxiv_id = m.group(1).strip()  # # group(1) rÃ©cupÃ¨re la partie capturÃ©e (= lâ€™ID), strip() enlÃ¨ve les espaces parasites | Ã‰tape: [E1] | Source: [S0]  # #

        title_txt = title_el.get_text(" ", strip=True) if title_el else ""  # # get_text() rÃ©cupÃ¨re le texte du titre (FR: le nom du papier) ; strip=True nettoie pour Ã©viter bruit/espaces | Ã‰tape: [E1] | Source: [S8]  # #
        authors_txt = authors_el.get_text(" ", strip=True) if authors_el else ""  # # get_text() rÃ©cupÃ¨re la chaÃ®ne des auteurs (FR: noms), en mode â€œtexte propreâ€ | Ã‰tape: [E1] | Source: [S8]  # #
        authors = [a.strip() for a in authors_txt.replace("Authors:", "").split(",") if a.strip()]  # # On transforme la chaÃ®ne en liste (split â€œ,â€), strip() nettoie chaque nom : sortie structurÃ©e stable pour lâ€™API | Ã‰tape: [E1] | Source: [S1]  # #
        abstract = abstract_el.get_text(" ", strip=True) if abstract_el else ""  # # get_text() rÃ©cupÃ¨re le rÃ©sumÃ© (abstract) complet quand disponible, sans prendre toute la page | Ã‰tape: [E1] | Source: [S8]  # #
        abstract = abstract.replace("â–³ Less", "").strip()  # # On enlÃ¨ve le texte UI â€œâ–³ Lessâ€ et on strip() : rÃ©duit le bruit pour garder uniquement lâ€™info utile | Ã‰tape: [E1] | Source: [S0]  # #

        submitted_date = ""  # # On initialise la date de soumission vide (on la remplit seulement si on lâ€™extrait vraiment) | Ã‰tape: [E1] | Source: [S0]  # #
        if submitted_el:  # # Si le bloc date existe, on tente une extraction contrÃ´lÃ©e (sinon on laisse vide) | Ã‰tape: [E1] | Source: [S0]  # #
            txt = submitted_el.get_text(" ", strip=True)  # # get_text() rÃ©cupÃ¨re la phrase â€œSubmitted â€¦â€ en texte propre, pour que la regex fonctionne de faÃ§on prÃ©visible | Ã‰tape: [E1] | Source: [S8]  # #
            m3 = re.search(r"Submitted\s+(.+?)(?:;|$)", txt, flags=re.IGNORECASE)  # # re.search() extrait uniquement la portion date aprÃ¨s â€œSubmittedâ€ (on cible lâ€™info utile, pas toute la phrase) | Ã‰tape: [E1] | Source: [S8]  # #
            if m3:  # # Si la regex a trouvÃ© une date, on la rÃ©cupÃ¨re ; sinon on laisse vide (robuste) | Ã‰tape: [E1] | Source: [S0]  # #
                submitted_date = m3.group(1).strip()  # # group(1) = date capturÃ©e ; strip() pour nettoyage final | Ã‰tape: [E1] | Source: [S0]  # #

        primary_cat, all_cats = extract_categories_from_result(li)  # # Appel de extract_categories_from_result() : on rÃ©cupÃ¨re catÃ©gories pour filtrer par thÃ¨me (et rester â€œciblÃ©â€ sur les sujets demandÃ©s) | Ã‰tape: [E1] | Source: [S8]  # #

        if arxiv_id and is_empty(abs_href):  # # Si on a un ID mais pas dâ€™URL /abs valide (rare), on applique un fallback pour garantir un lien utilisable | Ã‰tape: [E2] | Source: [S6]  # #
            abs_href = abs_url(arxiv_id)  # # abs_url() reconstruit une URL /abs/{id} correcte : sortie utile mÃªme si le HTML manquait le lien | Ã‰tape: [E1] | Source: [S0]  # #
        if arxiv_id and is_empty(pdf_href):  # # Si on a un ID mais pas dâ€™URL PDF valide, on applique un fallback pour garantir le lien PDF | Ã‰tape: [E2] | Source: [S6]  # #
            pdf_href = pdf_url(arxiv_id)  # # pdf_url() reconstruit une URL /pdf/{id} correcte : sortie utile mÃªme si le HTML manquait le lien | Ã‰tape: [E1] | Source: [S0]  # #

        items.append({  # # On ajoute un dict â€œpapierâ€ Ã  la liste items : câ€™est le format standard que lâ€™API et le LLM consommeront (sans HTML brut) | Ã‰tape: [E1] | Source: [S1]  # #
            "arxiv_id": arxiv_id,  # # arxiv_id (FR: identifiant unique du papier) : clÃ© de rÃ©fÃ©rence pour retrouver la fiche et le PDF | Ã‰tape: [E1] | Source: [S0]  # #
            "title": title_txt,  # # title (FR: titre du papier) : sert Ã  afficher et Ã  filtrer la pertinence | Ã‰tape: [E1] | Source: [S0]  # #
            "authors": authors,  # # authors (FR: liste des auteurs) : format liste pour Ãªtre stable cÃ´tÃ© API | Ã‰tape: [E1] | Source: [S0]  # #
            "abstract": abstract,  # # abstract (FR: rÃ©sumÃ©) : texte principal utile, extrait proprement sans le HTML complet | Ã‰tape: [E1] | Source: [S0]  # #
            "method": "",  # # method (FR: â€œmÃ©thodeâ€) : placeholder rempli plus tard via /html, pour ajouter la section â€œMÃ©thodeâ€ sans aspirer toute la page | Ã‰tape: [E2] | Source: [S8]  # #
            "references": [],  # # references (FR: â€œrÃ©fÃ©rences/bibliographieâ€) : placeholder liste, rempli plus tard via /html, pour garder uniquement la bibliographie sans HTML brut | Ã‰tape: [E2] | Source: [S8]  # #
            "submitted_date": submitted_date,  # # submitted_date (FR: date de soumission) : utile pour trier/Ã©valuer la fraÃ®cheur des papiers | Ã‰tape: [E1] | Source: [S0]  # #
            "abs_url": abs_href,  # # abs_url (FR: lien de la fiche /abs) : page de dÃ©tails, base pour enrichissement DOI/versions | Ã‰tape: [E1] | Source: [S0]  # #
            "pdf_url": pdf_href,  # # pdf_url (FR: lien PDF /pdf) : tÃ©lÃ©chargement direct du papier | Ã‰tape: [E1] | Source: [S0]  # #
            "primary_category": primary_cat,  # # primary_category (FR: catÃ©gorie principale) : sert au filtrage thÃ©matique demandÃ© | Ã‰tape: [E1] | Source: [S0]  # #
            "all_categories": all_cats,  # # all_categories (FR: toutes les catÃ©gories) : sert Ã  dÃ©tecter cross-listing et Ã©viter de rater des papiers pertinents | Ã‰tape: [E1] | Source: [S0]  # #
        })

    return items, diag  # # On retourne (items, diag) : items = donnÃ©es structurÃ©es ; diag = explications debug (fallback/anti-bot/compteurs) pour robustesse et traÃ§abilitÃ© | Ã‰tape: [E2] | Source: [S6]  # #

# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
# ğŸ” Parsing /abs (DOI + versions + abstract fallback)           # #  | Ã‰tape: [E2] | Source: [S6]  # #
# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
def parse_abs_page(abs_html: str) -> Dict[str, Any]:  # # La fonction lit le HTML de la page /abs et en extrait UNIQUEMENT des champs stables (doi, abstract, versions) pour enrichir lâ€™item sans â€œaspirerâ€ tout le site | Ã‰tape: [E1] | Source: [S0]  # #
    soup = BeautifulSoup(abs_html, "lxml")  # # BeautifulSoup(...,"lxml") transforme le HTML brut en arbre navigable pour pouvoir cibler des blocs prÃ©cis (principe: extraction ciblÃ©e, pas du texte global) | Ã‰tape: [E1] | Source: [S8]  # #
    out: Dict[str, Any] = {"doi": "", "versions": [], "last_updated_raw": "", "abstract": ""}  # # On initialise un dict â€œcontrat stableâ€ avec valeurs vides (Ã©vite KeyError et garantit toujours les mÃªmes clÃ©s cÃ´tÃ© API) | Ã‰tape: [E1] | Source: [S1]  # #

    doi_a = soup.select_one('td.tablecell.doi a[href*="doi.org"]')  # # select_one() cherche le premier lien DOI (Ã©diteur) dans le tableau â€œdoiâ€ de arXiv : on vise un sÃ©lecteur prÃ©cis pour Ã©viter de prendre du bruit | Ã‰tape: [E1] | Source: [S0]  # #
    if doi_a:  # # Si le lien DOI existe, on remplit le champ ; sinon on laisse vide (robuste) | Ã‰tape: [E1] | Source: [S0]  # #
        out["doi"] = doi_a.get_text(" ", strip=True)  # # get_text(...,strip=True) rÃ©cupÃ¨re le texte lisible du lien DOI (FR: identifiant Ã©diteur) sans espaces parasites | Ã‰tape: [E1] | Source: [S8]  # #

    abs_el = soup.select_one("blockquote.abstract")  # # select_one() cible le bloc â€œAbstractâ€ de /abs (câ€™est la source la plus fiable si lâ€™abstract manquait sur la page de recherche) | Ã‰tape: [E1] | Source: [S0]  # #
    if abs_el:  # # Si le bloc Abstract existe, on extrait son texte ; sinon on garde vide (pas dâ€™invention) | Ã‰tape: [E1] | Source: [S0]  # #
        txt = abs_el.get_text(" ", strip=True)  # # get_text() extrait le contenu texte de lâ€™Abstract en supprimant les retours/espaces inutiles | Ã‰tape: [E1] | Source: [S8]  # #
        txt = re.sub(r"^\s*Abstract:\s*", "", txt, flags=re.IGNORECASE).strip()  # # re.sub() enlÃ¨ve le prÃ©fixe â€œAbstract:â€ (UI) pour ne garder que le contenu utile, puis strip() nettoie | Ã‰tape: [E1] | Source: [S0]  # #
        out["abstract"] = txt  # # On stocke lâ€™abstract nettoyÃ© dans le dict de sortie (contrat stable) | Ã‰tape: [E1] | Source: [S1]  # #

    versions: List[Dict[str, str]] = []  # # On prÃ©pare une liste structurÃ©e pour lâ€™historique des versions (v1, v2â€¦) au format dict, pour Ãªtre facile Ã  consommer par lâ€™API | Ã‰tape: [E1] | Source: [S0]  # #
    for li in soup.select("div.submission-history li"):  # # soup.select() rÃ©cupÃ¨re toutes les lignes de lâ€™historique de soumission (submission-history) pour extraire les versions proprement | Ã‰tape: [E1] | Source: [S8]  # #
        txt = li.get_text(" ", strip=True)  # # get_text() rÃ©cupÃ¨re le texte de chaque ligne (ex: â€œ[v2] Tue, â€¦â€) sous forme propre | Ã‰tape: [E1] | Source: [S8]  # #
        m = re.search(r"\[(v\d+)\]\s*(.*)$", txt)  # # re.search() repÃ¨re le numÃ©ro de version â€œ[vX]â€ et le reste de la ligne (date/infos) pour structurer sans dÃ©pendre de micro-HTML | Ã‰tape: [E1] | Source: [S8]  # #
        if m:  # # Si la ligne correspond au pattern (sÃ©curitÃ©), on ajoute une entrÃ©e version ; sinon on ignore (robuste) | Ã‰tape: [E1] | Source: [S0]  # #
            versions.append({"version": m.group(1), "raw": m.group(2).strip()})  # # On enregistre version=vX + raw=texte date ; group() rÃ©cupÃ¨re les groupes capturÃ©s ; strip() nettoie | Ã‰tape: [E1] | Source: [S1]  # #
    out["versions"] = versions  # # On attache la liste des versions au dict de sortie (contrat stable) | Ã‰tape: [E1] | Source: [S1]  # #
    out["last_updated_raw"] = versions[-1]["raw"] if versions else ""  # # On prend la derniÃ¨re entrÃ©e versions[-1] comme â€œderniÃ¨re mise Ã  jourâ€ ; si liste vide, on met "" (Ã©vite crash) | Ã‰tape: [E1] | Source: [S0]  # #

    return out  # # On renvoie le dict dâ€™enrichissement /abs (clÃ©->valeur), utilisÃ© ensuite pour complÃ©ter les items sans changer le main | Ã‰tape: [E1] | Source: [S1]  # #


# ============================================================  # #
# ğŸ§© Parsing /html arXiv : Method + References (ciblÃ©)           # # Ã‰tape: [E5] | Source: [S8]  # #
# ============================================================  # #

def extract_html_url_from_abs(abs_html: str, arxiv_id: str) -> str:  # # La fonction cherche lâ€™URL /html depuis la page /abs pour pouvoir rÃ©cupÃ©rer â€œMethodâ€ et â€œReferencesâ€ (si disponibles) sans supposer que tous les papiers ont du HTML | Ã‰tape: [E5] | Source: [S8]  # #
    soup = BeautifulSoup(abs_html, "lxml")  # # Parser /abs en arbre HTML pour pouvoir trouver un lien â€œ/html/...â€ via un sÃ©lecteur stable au lieu de regex fragile | Ã‰tape: [E5] | Source: [S0]  # #
    a = soup.select_one('a[href^="/html/"], a[href*="/html/"]')  # # select_one() prend le premier lien qui pointe vers /html (FR: version HTML du papier) sâ€™il existe, sinon None | Ã‰tape: [E5] | Source: [S0]  # #
    if not a:  # # Si aucun lien /html nâ€™est prÃ©sent (frÃ©quent), on retourne vide pour ne pas forcer une requÃªte inutile | Ã‰tape: [E5] | Source: [S0]  # #
        return ""  # # Retour vide = â€œpas de HTML disponibleâ€, le reste du pipeline garde method/references vides (contrat stable) | Ã‰tape: [E5] | Source: [S0]  # #
    href = (a.get("href") or "").strip()  # # .get("href") lit lâ€™attribut href du lien, or "" Ã©vite None, strip() nettoie : on sÃ©curise lâ€™entrÃ©e avant normalisation | Ã‰tape: [E1] | Source: [S1]  # #
    if not href:  # # Si href est vide aprÃ¨s nettoyage, on renvoie vide (robuste) | Ã‰tape: [E5] | Source: [S0]  # #
        return ""  # # Pas de lien utilisable | Ã‰tape: [E5] | Source: [S0]  # #
    return normalize_url(href)  # # normalize_url() transforme un lien relatif en URL absolue (FR: lien cliquable stable) | Ã‰tape: [E1] | Source: [S1]  # #


def parse_arxiv_html_method_and_references(html: str) -> tuple[str, list[str]]:  # # La fonction extrait DEUX blocs ciblÃ©s dans /html : â€œMethodâ€ (texte) + â€œReferencesâ€ (liste) pour rÃ©pondre au prof sans scraper tout le contenu | Ã‰tape: [E5] | Source: [S8]  # #
    soup = BeautifulSoup(html, "lxml")  # # Parser le HTML /html en arbre afin de sÃ©lectionner des sections prÃ©cises (robustesse si lâ€™ordre du texte change) | Ã‰tape: [E5] | Source: [S0]  # #

    method_text = ""  # # On initialise la variable method_text Ã  vide : si on ne trouve pas la section Method, on reste sur "" (contrat stable, pas dâ€™invention) | Ã‰tape: [E5] | Source: [S8]  # #
    references: list[str] = []  # # On initialise la liste references : si aucune bibliographie trouvÃ©e, on renvoie [] (format stable cÃ´tÃ© API) | Ã‰tape: [E5] | Source: [S8]  # #

    # âœ… RÃ©fÃ©rences : structure LaTeX HTML arXiv (biblist)
    for li in soup.select("ol.ltx_biblist li, div.ltx_bibliography li"):  # # soup.select() rÃ©cupÃ¨re les entrÃ©es de bibliographie arXiv/LaTeX (deux structures possibles) pour tolÃ©rer des variantes HTML | Ã‰tape: [E5] | Source: [S6]  # #
        t = li.get_text(" ", strip=True)  # # get_text() extrait le texte dâ€™une rÃ©fÃ©rence (auteurs, titre, venue) en Ã©vitant le HTML brut | Ã‰tape: [E5] | Source: [S0]  # #
        if t:  # # On vÃ©rifie que ce nâ€™est pas vide pour Ã©viter dâ€™ajouter des entrÃ©es nulles | Ã‰tape: [E5] | Source: [S0]  # #
            references.append(t)  # # On ajoute la rÃ©fÃ©rence dans la liste (format â€œliste de stringsâ€ simple et stable) | Ã‰tape: [E5] | Source: [S8]  # #

    # âœ… MÃ©thode : on cherche un titre qui ressemble Ã  â€œmethodâ€
    for sec in soup.select("section.ltx_section, div.ltx_section, section"):  # # On parcourt les sections possibles (LaTeX arXiv + HTML gÃ©nÃ©rique) pour trouver une section â€œMethodâ€ mÃªme si la structure varie | Ã‰tape: [E5] | Source: [S6]  # #
        title_el = sec.select_one(".ltx_title, h1, h2, h3, h4")  # # On cherche le titre de section (classe LaTeX ou titres HTML) pour savoir de quoi parle la section | Ã‰tape: [E5] | Source: [S0]  # #
        if not title_el:  # # Si la section nâ€™a pas de titre, on ne peut pas lâ€™identifier => on passe Ã  la suivante | Ã‰tape: [E5] | Source: [S6]  # #
            continue  # # Continue = on saute ce bloc et on garde le script robuste (pas dâ€™erreur) | Ã‰tape: [E5] | Source: [S0]  # #
        title_txt = title_el.get_text(" ", strip=True).lower()  # # On rÃ©cupÃ¨re le titre en texte, on le met en lower() pour faire un match insensible Ã  la casse (Method/METHOD/â€¦) | Ã‰tape: [E1] | Source: [S0]  # #
        if any(k in title_txt for k in ["method", "methods", "methodology", "approach"]):  # # On teste si le titre contient des mots-clÃ©s â€œmÃ©thodeâ€ pour capturer la section Method mÃªme si le libellÃ© varie | Ã‰tape: [E5] | Source: [S8]  # #
            tmp = sec.get_text(" ", strip=True)  # # On rÃ©cupÃ¨re le texte complet de la section (titre + contenu) sous forme propre, sans HTML brut | Ã‰tape: [E5] | Source: [S0]  # #
            tmp = re.sub(r"^\s*" + re.escape(title_el.get_text(" ", strip=True)) + r"\s*", "", tmp, flags=re.IGNORECASE)  # # re.sub() retire le titre au dÃ©but du texte pour ne garder que le contenu â€œmÃ©thodeâ€ (plus propre pour lâ€™agent) | Ã‰tape: [E5] | Source: [S0]  # #
            method_text = tmp.strip()  # # strip() final : on stocke le texte de mÃ©thode nettoyÃ© | Ã‰tape: [E5] | Source: [S8]  # #
            break  # # break stoppe la boucle dÃ¨s quâ€™on a trouvÃ© la premiÃ¨re section Method (Ã©vite de prendre plusieurs sections et de grossir inutilement) | Ã‰tape: [E5] | Source: [S0]  # #

    return method_text, references  # # On renvoie un tuple (method_text, references) : 2 blocs ciblÃ©s, simples, et prÃªts Ã  Ãªtre intÃ©grÃ©s dans lâ€™item JSON | Ã‰tape: [E1] | Source: [S1]  # #
# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
# ğŸ§  Filtrage thÃ©matique (par catÃ©gories + keywords)             # #  | Ã‰tape: [E1] | Source: [S0]  # #
# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
def _allowed_subcats_for_theme(theme: Optional[str]) -> List[str]:  # # Cette fonction dÃ©cide quelles sous-catÃ©gories arXiv sont autorisÃ©es selon le thÃ¨me demandÃ©, pour garder un pÃ©rimÃ¨tre clair et stable | Ã‰tape: [E1] | Source: [S0]  # #
    if theme and theme in THEME_TO_ARXIV_SUBCATS:  # # Ici on vÃ©rifie si lâ€™utilisateur a fourni un thÃ¨me valide, afin dâ€™appliquer le bon â€œfiltre thÃ©matiqueâ€ | Ã‰tape: [E1] | Source: [S0]  # #
        return THEME_TO_ARXIV_SUBCATS[theme]  # # Ici on renvoie directement la liste de catÃ©gories associÃ©e au thÃ¨me (ex: ai_ml â†’ cs.AI, cs.LGâ€¦) | Ã‰tape: [E1] | Source: [S0]  # #
    return sorted({c for lst in THEME_TO_ARXIV_SUBCATS.values() for c in lst})  # # Ici on renvoie lâ€™union de toutes les catÃ©gories autorisÃ©es (toujours limitÃ© aux thÃ¨mes dÃ©finis) quand aucun thÃ¨me nâ€™est fourni | Ã‰tape: [E1] | Source: [S0]  # #


def _keyword_filter(items: List[Dict[str, Any]], theme: Optional[str]) -> List[Dict[str, Any]]:  # # Cette fonction filtre les items via des mots-clÃ©s quand les catÃ©gories manquent ou sont instables, pour Ã©viter de perdre tous les rÃ©sultats si le parsing â€œSubjectsâ€ casse | Ã‰tape: [E2] | Source: [S6]  # #
    if not theme or theme not in THEME_KEYWORDS:  # # Ici on sort tout de suite si on nâ€™a pas de thÃ¨me exploitable (pas de filtrage keyword Ã  appliquer) | Ã‰tape: [E1] | Source: [S0]  # #
        return items  # # Ici on renvoie la liste inchangÃ©e pour ne pas supprimer des items sans raison (comportement stable) | Ã‰tape: [E1] | Source: [S1]  # #
    kws = [k.lower() for k in THEME_KEYWORDS[theme]]  # # Ici on met les keywords en minuscules pour comparer sans dÃ©pendre de la casse (robustesse) | Ã‰tape: [E1] | Source: [S0]  # #
    out: List[Dict[str, Any]] = []  # # Ici on prÃ©pare une nouvelle liste de sortie (format structurÃ©) pour stocker seulement les items qui matchent | Ã‰tape: [E1] | Source: [S1]  # #
    for it in items:  # # Ici on parcourt chaque item pour vÃ©rifier sâ€™il contient un des mots-clÃ©s du thÃ¨me | Ã‰tape: [E1] | Source: [S1]  # #
        blob = ((it.get("title") or "") + " " + (it.get("abstract") or "")).lower()  # # Ici on construit un â€œtexte de testâ€ (titre+abstract) en minuscules, car ce sont les champs les plus utiles pour un filtrage simple | Ã‰tape: [E1] | Source: [S0]  # #
        if any(k in blob for k in kws):  # # any() renvoie True si AU MOINS un keyword est prÃ©sent ; ici Ã§a sert Ã  garder lâ€™item si le sujet semble correspondre au thÃ¨me | Ã‰tape: [E1] | Source: [S0]  # #
            out.append(it)  # # Ici on ajoute lâ€™item Ã  la sortie car il est jugÃ© pertinent selon les mots-clÃ©s | Ã‰tape: [E1] | Source: [S1]  # #
    return out  # # Ici on renvoie la liste filtrÃ©e (explicite), utilisÃ©e comme fallback si les catÃ©gories sont inexploitables | Ã‰tape: [E1] | Source: [S0]  # #


def filter_items_by_subcats(items: List[Dict[str, Any]], allowed_subcats: List[str]) -> List[Dict[str, Any]]:  # # Cette fonction filtre les items par catÃ©gories arXiv (cs.AI, cs.CL, â€¦) pour respecter le pÃ©rimÃ¨tre du thÃ¨me demandÃ© | Ã‰tape: [E1] | Source: [S1]  # #
    allowed = set(allowed_subcats)  # # set() sert ici Ã  accÃ©lÃ©rer les tests â€œc in allowedâ€ (plus rapide quâ€™une liste) | Ã‰tape: [E1] | Source: [S0]  # #
    out: List[Dict[str, Any]] = []  # # Ici on initialise la liste de sortie qui contiendra uniquement les items autorisÃ©s | Ã‰tape: [E1] | Source: [S1]  # #
    for it in items:  # # Ici on parcourt chaque item collectÃ© depuis la page /search/cs | Ã‰tape: [E1] | Source: [S1]  # #
        cats = it.get("all_categories") or []  # # Ici on rÃ©cupÃ¨re les catÃ©gories de lâ€™item (ou [] si absent) pour dÃ©cider sâ€™il doit Ãªtre conservÃ© | Ã‰tape: [E1] | Source: [S0]  # #
        if not cats:  # # Ici, si les catÃ©gories nâ€™ont pas pu Ãªtre extraites (HTML changÃ©), on Ã©vite un faux nÃ©gatif en conservant lâ€™item | Ã‰tape: [E1] | Source: [S8]  # #
            out.append(it)  # # Ici on garde lâ€™item car on nâ€™a pas la preuve quâ€™il est hors pÃ©rimÃ¨tre (robustesse) | Ã‰tape: [E1] | Source: [S0]  # #
            continue  # # Ici on passe au suivant pour ne pas exÃ©cuter le test de matching sur une liste vide | Ã‰tape: [E1] | Source: [S0]  # #
        if any(c in allowed for c in cats):  # # any() teste si AU MOINS une catÃ©gorie de lâ€™item est autorisÃ©e (utile quand il y a plusieurs tags/cross-lists) | Ã‰tape: [E1] | Source: [S0]  # #
            out.append(it)  # # Ici on ajoute lâ€™item car il respecte le pÃ©rimÃ¨tre thÃ©matique | Ã‰tape: [E1] | Source: [S0]  # #
    return out  # # Ici on renvoie la liste filtrÃ©e finale par catÃ©gories | Ã‰tape: [E1] | Source: [S0]  # #


# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
# âœ… Fonction principale                                         # #  | Ã‰tape: [E1] | Source: [S0]  # #
# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
def scrape_arxiv_cs_scoped(
    user_query: str,  # # Texte de recherche utilisateur (FR: requÃªte) : câ€™est lâ€™entrÃ©e principale qui pilote lâ€™URL /search/cs | Ã‰tape: [E1] | Source: [S0]  # #
    theme: Optional[str] = None,  # # ThÃ¨me optionnel (FR: catÃ©gorie logique) : permet dâ€™activer un filtrage par sous-catÃ©gories arXiv | Ã‰tape: [E1] | Source: [S0]  # #
    max_results: int = 20,  # # Limite de rÃ©sultats : empÃªche un scraping massif et contrÃ´le le volume collectÃ© | Ã‰tape: [E1] | Source: [S0]  # #
    sort: str = "relevance",  # # Tri : â€œrelevanceâ€ ou â€œsubmitted_dateâ€, pour choisir lâ€™ordre des rÃ©sultats sans changer le parsing | Ã‰tape: [E1] | Source: [S0]  # #
    polite_min_s: float = 1.2,  # # Politesse (min) : dÃ©lai minimum entre requÃªtes pour Ã©viter dâ€™Ãªtre agressif cÃ´tÃ© serveur | Ã‰tape: [E4] | Source: [S5]  # #
    polite_max_s: float = 2.0,  # # Politesse (max) : dÃ©lai maximum (jitter) pour Ã©viter un rythme â€œrobotiqueâ€ | Ã‰tape: [E4] | Source: [S5]  # #
    data_lake_raw_dir: str = DEFAULT_RAW_DIR,  # # Dossier de sortie cache : garantit que JSON/HTML debug sont Ã©crits dans raw/cache | Ã‰tape: [E2] | Source: [S2]  # #
    enrich_abs: bool = True,  # # Enrichissement /abs : active la rÃ©cupÃ©ration DOI + versions + abstract fallback via la page /abs | Ã‰tape: [E1] | Source: [S0]  # #
    enable_keyword_filter: bool = True,  # # Filtrage keywords : fallback utile si les catÃ©gories â€œSubjectsâ€ ne sont pas fiables / manquantes | Ã‰tape: [E2] | Source: [S6]  # #
) -> Dict[str, Any]:  # # La fonction renvoie toujours un dict JSON stable (contrat) pour que FastAPI puisse lâ€™exposer sans surprise | Ã‰tape: [E1] | Source: [S1]  # #

    # ===============================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
    # ğŸ§± PrÃ©paration paramÃ¨tres                                     # #  | Ã‰tape: [E1] | Source: [S0]  # #
    # ===============================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
    max_results = int(max_results)  # # int() force le type entier (sÃ©curitÃ©) : Ã©vite de casser la pagination si on reÃ§oit â€œ5â€ en string | Ã‰tape: [E1] | Source: [S0]  # #
    if max_results < 1:  # # Ici on impose une borne basse pour ne jamais demander 0 rÃ©sultat (cas qui casse la logique de boucle) | Ã‰tape: [E1] | Source: [S0]  # #
        max_results = 1  # # Ici on corrige automatiquement en 1 (comportement stable) | Ã‰tape: [E1] | Source: [S0]  # #
    if max_results > MAX_RESULTS_HARD_LIMIT:  # # Ici on impose un cap dur pour empÃªcher un scraping massif par erreur | Ã‰tape: [E1] | Source: [S0]  # #
        max_results = MAX_RESULTS_HARD_LIMIT  # # Ici on applique le cap (anti-aspirateur) | Ã‰tape: [E1] | Source: [S0]  # #

    if not os.path.isabs(data_lake_raw_dir):  # # os.path.isabs() vÃ©rifie si le chemin est absolu ; ici Ã§a Ã©vite dâ€™Ã©crire â€œau hasardâ€ selon le CWD | Ã‰tape: [E2] | Source: [S2]  # #
        data_lake_raw_dir = os.path.abspath(os.path.join(PROJECT_ROOT, data_lake_raw_dir))  # # os.path.abspath() normalise vers un chemin absolu basÃ© sur PROJECT_ROOT (sortie prÃ©visible raw/cache) | Ã‰tape: [E2] | Source: [S2]  # #

    ensure_dir(data_lake_raw_dir)  # # ensure_dir() crÃ©e le dossier si nÃ©cessaire pour garantir que les fichiers JSON/HTML seront bien Ã©crits (pas dâ€™erreur â€œNo such fileâ€) | Ã‰tape: [E2] | Source: [S2]  # #
    ts = now_iso_for_filename()  # # now_iso_for_filename() fabrique un timestamp pour nommer les fichiers de maniÃ¨re unique et traÃ§able | Ã‰tape: [E1] | Source: [S0]  # #
    session = requests.Session()  # # requests.Session() garde une session HTTP rÃ©utilisable (cookies/connexions) : ici Ã§a rend les requÃªtes plus stables et plus efficaces | Ã‰tape: [E2] | Source: [S3]  # #

    errors_global: List[str] = []  # # Liste dâ€™erreurs globales (FR: erreurs du tool) : utilisÃ©e pour signaler 429/500/timeout sans dÃ©pendre des erreurs â€œpar itemâ€ | Ã‰tape: [E2] | Source: [S3]  # #

    # ===============================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
    # ğŸ¯ Allowed categories                                   # #  | Ã‰tape: [E1] | Source: [S0]  # #
    # ===============================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
    allowed_subcats = _allowed_subcats_for_theme(theme)  # # Cette ligne appelle la fonction qui choisit la liste de catÃ©gories arXiv autorisÃ©es selon le thÃ¨me, pour limiter le scraping au pÃ©rimÃ¨tre demandÃ© | Ã‰tape: [E1] | Source: [S0]  # #

    # ===============================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
    # ğŸ” Pagination search/cs                                  # #  | Ã‰tape: [E1] | Source: [S0]  # #
    # ===============================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
    collected: List[Dict[str, Any]] = []  # # Cette ligne crÃ©e la liste qui accumule les items bruts rÃ©cupÃ©rÃ©s sur les pages de rÃ©sultats arXiv (avant filtrage), afin de contrÃ´ler le nombre total collectÃ© | Ã‰tape: [E1] | Source: [S1]  # #
    bundle_parts: List[str] = []  # # Cette ligne prÃ©pare une liste de morceaux HTML â€œdebugâ€ pour reconstituer un bundle local (preuve + diagnostic) sans envoyer du HTML au LLM | Ã‰tape: [E2] | Source: [S2]  # #
    start = 0  # # Cette ligne initialise lâ€™offset de pagination (0, 50, 100, â€¦) pour parcourir les pages de rÃ©sultats de maniÃ¨re contrÃ´lÃ©e | Ã‰tape: [E1] | Source: [S0]  # #
    last_search_url = ""  # # Cette ligne initialise une variable de trace pour garder lâ€™URL de la derniÃ¨re requÃªte search (utile si Ã§a casse) | Ã‰tape: [E1] | Source: [S0]  # #
    last_search_http: Optional[int] = None  # # Cette ligne initialise le dernier code HTTP pour diagnostiquer rapidement un 500/429/timeout sans relancer | Ã‰tape: [E2] | Source: [S3]  # #
    diag_last: Dict[str, Any] = {}  # # Cette ligne initialise le dernier diagnostic de parsing (counts/selectors/flags) pour comprendre â€œpourquoi items=[]â€ | Ã‰tape: [E2] | Source: [S6]  # #
    anti_bot_or_weird_page = False  # # Cette ligne initialise un drapeau qui indique si on a dÃ©tectÃ© une page bizarre (anti-bot/consent) pour Ãªtre transparent sur la cause | Ã‰tape: [E2] | Source: [S6]  # #

    while len(collected) < max_results:  # # Cette ligne dÃ©marre une boucle qui continue tant quâ€™on nâ€™a pas collectÃ© assez dâ€™items, ce qui garantit quâ€™on respecte la limite demandÃ©e | Ã‰tape: [E1] | Source: [S0]  # #
        search_url = build_search_url(query=user_query, start=start, size=PAGE_SIZE, sort=sort)  # # Cette ligne appelle la fonction qui construit lâ€™URL /search/cs avec query+start+size+sort, ce qui rend la pagination propre et prÃ©visible | Ã‰tape: [E1] | Source: [S0]  # #
        last_search_url = search_url  # # Cette ligne stocke lâ€™URL courante dans une variable de trace, pour la retrouver dans le JSON si le parsing Ã©choue | Ã‰tape: [E1] | Source: [S0]  # #
        html, code = http_get_text(session=session, url=search_url, timeout_s=HTTP_TIMEOUT_S)  # # Cette ligne fait le GET HTTP via la fonction robuste (gÃ¨re timeout/erreurs rÃ©seau) et rÃ©cupÃ¨re (html, status_code) pour diagnostic | Ã‰tape: [E2] | Source: [S3]  # #
        last_search_http = code  # # Cette ligne mÃ©morise le code HTTP de la derniÃ¨re page search, pour expliquer un Ã©chec (ex: 500) sans re-parser | Ã‰tape: [E2] | Source: [S3]  # #

        weird = _detect_weird_page_signals(html)  # # Cette ligne appelle la fonction qui â€œscanneâ€ le HTML pour repÃ©rer consent/robot/captcha/no-results, afin dâ€™Ã©viter un faux parsing sur une page de blocage | Ã‰tape: [E2] | Source: [S6]  # #

        bundle_parts.append(f"<!-- SEARCH URL: {search_url} | HTTP {code} -->\n")  # # Cette ligne ajoute un en-tÃªte HTML dans le bundle debug pour tracer lâ€™URL et le code HTTP associÃ© Ã  ce bloc | Ã‰tape: [E2] | Source: [S3]  # #
        bundle_parts.append(f"<!-- WEIRD: {json.dumps(weird)} -->\n")  # # Cette ligne ajoute dans le bundle debug les drapeaux â€œweirdâ€ en JSON, pour comprendre si on a Ã©tÃ© bloquÃ© ou redirigÃ© | Ã‰tape: [E2] | Source: [S6]  # #
        bundle_parts.append((html or "")[:200000])  # # Cette ligne stocke seulement un extrait (200k chars) du HTML pour Ã©viter un fichier Ã©norme tout en gardant assez de matiÃ¨re pour diagnostiquer | Ã‰tape: [E2] | Source: [S2]  # #
        bundle_parts.append("\n<!-- END SEARCH -->\n")  # # Cette ligne marque la fin du bloc search dans le bundle debug, pour sÃ©parer clairement les pages (lisible en local) | Ã‰tape: [E1] | Source: [S0]  # #

        if code != 200:  # # Ici on dÃ©tecte un HTTP non-200 (ex: 500/429), car dans ce cas on ne peut pas faire confiance au contenu HTML pour parser correctement | Ã‰tape: [E2] | Source: [S3]  # #
            errors_global.append(f"SEARCH_HTTP_{code}")  # # Cette ligne ajoute une erreur globale normalisÃ©e (ex: SEARCH_HTTP_500) pour que lâ€™API puisse lâ€™exploiter facilement (contrat stable) | Ã‰tape: [E2] | Source: [S0]  # #
            break  # # Cette ligne stoppe la boucle pour Ã©viter dâ€™insister (risque de spam + pages inutiles) quand lâ€™HTTP est dÃ©jÃ  en erreur | Ã‰tape: [E1] | Source: [S0]  # #
        if weird.get("contains_we_are_sorry") or weird.get("contains_robot") or weird.get("contains_consent"):  # # Ici on teste les signaux anti-bot/consent, car ces pages ressemblent Ã  arXiv mais ne contiennent pas des rÃ©sultats fiables | Ã‰tape: [E2] | Source: [S6]  # #
            anti_bot_or_weird_page = True  # # Cette ligne met le flag Ã  True pour que le JSON final dise clairement â€œpage bizarre dÃ©tectÃ©eâ€ | Ã‰tape: [E2] | Source: [S6]  # #
            errors_global.append("ANTI_BOT_OR_WEIRD_PAGE")  # # Cette ligne ajoute une erreur globale explicite, afin que FastAPI / lâ€™agent puisse dÃ©cider dâ€™arrÃªter ou de prÃ©venir lâ€™utilisateur | Ã‰tape: [E2] | Source: [S1]  # #
            break  # # Cette ligne stoppe immÃ©diatement : on nâ€™insiste pas sur un blocage/consent, sinon on aggrave la situation cÃ´tÃ© serveur | Ã‰tape: [E2] | Source: [S0]  # #

        page_items, diag = parse_search_page(html)  # # Cette ligne appelle la fonction de parsing search/cs qui extrait les items + produit un diagnostic (compte de noeuds, flags, etc.) | Ã‰tape: [E2] | Source: [S6]  # #
        diag_last = diag  # # Cette ligne sauvegarde le dernier diagnostic dans une variable de trace pour le renvoyer dans le JSON final | Ã‰tape: [E2] | Source: [S6]  # #

        if diag.get("contains_no_results"):  # # Ici on dÃ©tecte explicitement â€œNo results foundâ€, car dans ce cas il est inutile de paginer davantage | Ã‰tape: [E2] | Source: [S6]  # #
            break  # # Cette ligne arrÃªte la boucle car il nâ€™y a rien Ã  collecter : câ€™est une fin normale (pas une erreur) | Ã‰tape: [E1] | Source: [S0]  # #
        if not page_items:  # # Ici on gÃ¨re le cas â€œHTML ok mais parsing videâ€ (DOM changÃ© ou selector cassÃ©), car il faut sortir plutÃ´t que boucler Ã  vide | Ã‰tape: [E2] | Source: [S0]  # #
            errors_global.append("NO_RESULTS_PARSED")  # # Cette ligne ajoute une erreur globale dÃ©diÃ©e au â€œparsing videâ€, utile pour distinguer ce cas dâ€™un â€œvrai 0 rÃ©sultatâ€ | Ã‰tape: [E2] | Source: [S6]  # #
            break  # # Cette ligne stoppe la boucle, sinon on paginerait en boucle sans rien ajouter (inutile) | Ã‰tape: [E1] | Source: [S0]  # #

        collected.extend(page_items)  # # Cette ligne ajoute tous les items parsÃ©s de la page Ã  la liste globale, pour accumuler progressivement jusquâ€™Ã  max_results | Ã‰tape: [E1] | Source: [S1]  # #

        # âœ… CORRECTION IMPORTANTE : si la page a < PAGE_SIZE rÃ©sultats, inutile d'aller Ã  start+50
        if len(page_items) < PAGE_SIZE:  # # Ici on dÃ©tecte une â€œderniÃ¨re page probableâ€ : si arXiv renvoie moins de 50 rÃ©sultats, la page suivante serait souvent vide ou peut dÃ©clencher des erreurs inutiles | Ã‰tape: [E1] | Source: [S1]  # #
            break  # # Cette ligne arrÃªte la boucle pour Ã©viter des requÃªtes inutiles, rÃ©duire les risques 500/429, et respecter une frÃ©quence raisonnable | Ã‰tape: [E1] | Source: [S0]  # #

        start += PAGE_SIZE  # # Cette ligne avance lâ€™offset de pagination (0â†’50â†’100â€¦) pour aller chercher la page suivante sans doublons | Ã‰tape: [E1] | Source: [S0]  # #
        sleep_polite(min_s=polite_min_s, max_s=polite_max_s)  # # Cette ligne applique une pause alÃ©atoire via la fonction de politesse, pour Ã©viter un rythme robotique et limiter le risque de blocage | Ã‰tape: [E4] | Source: [S5]  # #

    collected = collected[:max_results]  # # Cette ligne tronque la liste au nombre demandÃ©, au cas oÃ¹ la derniÃ¨re page a ajoutÃ© â€œtropâ€ dâ€™items (contrat: respecter max_results) | Ã‰tape: [E1] | Source: [S0]  # #

    # ===============================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
    # ğŸ§¹ Filtrage par catÃ©gories                               # #  | Ã‰tape: [E1] | Source: [S0]  # #
    # ===============================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
    filtered = filter_items_by_subcats(collected, allowed_subcats=allowed_subcats)  # # Cette ligne appelle la fonction de filtrage par catÃ©gories arXiv pour conserver uniquement les items qui matchent le pÃ©rimÃ¨tre du thÃ¨me | Ã‰tape: [E1] | Source: [S1]  # #
    if enable_keyword_filter:  # # Ici on vÃ©rifie si le fallback par mots-clÃ©s est activÃ©, pour lâ€™utiliser seulement si tu le souhaites (contrÃ´le) | Ã‰tape: [E1] | Source: [S0]  # #
        filtered = _keyword_filter(filtered, theme=theme)  # # Cette ligne applique le filtrage par mots-clÃ©s (fallback) afin de garder une pertinence minimale si les catÃ©gories sont manquantes/instables | Ã‰tape: [E2] | Source: [S6]  # #

    # ===============================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
    # ğŸ” Enrich /abs                                           # #  | Ã‰tape: [E1] | Source: [S0]  # #
    # ===============================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
    if enrich_abs:  # # Ici on vÃ©rifie si lâ€™enrichissement est activÃ© : si oui on va visiter /abs (et Ã©ventuellement /html) pour complÃ©ter quelques champs utiles | Ã‰tape: [E1] | Source: [S0]  # #
        for it in filtered:  # # Cette ligne parcourt chaque item filtrÃ© pour enrichir un par un, ce qui limite le volume et facilite le debug par item | Ã‰tape: [E1] | Source: [S0]  # #
            it["doi"] = ""  # # Cette ligne initialise le champ DOI Ã  vide pour garder un contrat stable mÃªme si le DOI nâ€™existe pas (pas de KeyError) | Ã‰tape: [E1] | Source: [S0]  # #
            it["versions"] = []  # # Cette ligne initialise lâ€™historique de versions Ã  [] pour Ãªtre stable mÃªme si on ne trouve pas de â€œsubmission-historyâ€ | Ã‰tape: [E1] | Source: [S0]  # #
            it["last_updated_raw"] = ""  # # Cette ligne initialise la derniÃ¨re date/ligne dâ€™update (raw) Ã  vide pour Ã©viter les champs manquants | Ã‰tape: [E1] | Source: [S0]  # #
            it["method"] = ""  # # Cette ligne initialise le champ method (FR: section â€œMÃ©thodeâ€) pour pouvoir le remplir depuis /html si dispo, sinon garder vide | Ã‰tape: [E5] | Source: [S8]  # #
            it["references"] = []  # # Cette ligne initialise le champ references (FR: bibliographie) en liste, car une rÃ©fÃ©rence = un Ã©lÃ©ment ; vide si non dispo | Ã‰tape: [E5] | Source: [S8]  # #
            it["errors"] = []  # # Cette ligne initialise la liste dâ€™erreurs par item (FR: erreurs papier) pour stocker abs_http_XXX/html_http_XXX sans casser le tool | Ã‰tape: [E3] | Source: [S3]  # #

            url_abs = it.get("abs_url") or ""  # # Cette ligne rÃ©cupÃ¨re lâ€™URL /abs depuis lâ€™item ; â€œor ''â€ Ã©vite None et garde un comportement stable | Ã‰tape: [E1] | Source: [S0]  # #
            if not url_abs:  # # Ici on vÃ©rifie que lâ€™URL /abs existe, sinon on ne peut pas enrichir (on saute proprement) | Ã‰tape: [E1] | Source: [S0]  # #
                continue  # # Cette ligne skip lâ€™item courant, car enrichir sans /abs est impossible ; on Ã©vite un crash et on continue les autres | Ã‰tape: [E1] | Source: [S0]  # #

            abs_html, abs_code = http_get_text(session=session, url=url_abs, timeout_s=HTTP_TIMEOUT_S)  # # Cette ligne fait un GET sur /abs via la fonction robuste et rÃ©cupÃ¨re (HTML, code) pour pouvoir parser OU enregistrer une erreur | Ã‰tape: [E2] | Source: [S3]  # #
            bundle_parts.append(f"<!-- ABS URL: {url_abs} | HTTP {abs_code} -->\n")  # # Cette ligne trace dans le bundle debug lâ€™URL /abs et le code HTTP, pour reproduire le problÃ¨me localement | Ã‰tape: [E2] | Source: [S3]  # #
            bundle_parts.append((abs_html or "")[:200000])  # # Cette ligne stocke un extrait du HTML /abs dans le bundle (limitÃ©) pour diagnostiquer sans gÃ©nÃ©rer un fichier trop gros | Ã‰tape: [E2] | Source: [S2]  # #
            bundle_parts.append("\n<!-- END ABS -->\n")  # # Cette ligne ferme le bloc /abs dans le bundle debug, pour sÃ©parer les pages proprement | Ã‰tape: [E1] | Source: [S0]  # #

            if abs_code == 200:  # # Ici on vÃ©rifie que /abs rÃ©pond OK avant de parser, car parser une page dâ€™erreur produirait des champs faux/vide | Ã‰tape: [E1] | Source: [S0]  # #
                abs_data = parse_abs_page(abs_html)  # # Cette ligne appelle la fonction qui extrait DOI + versions + abstract fallback depuis /abs (parsing ciblÃ©) | Ã‰tape: [E2] | Source: [S6]  # #
                it["doi"] = abs_data.get("doi", "")  # # Cette ligne copie le DOI extrait (ou vide) ; get() Ã©vite KeyError si le parsing nâ€™a rien trouvÃ© | Ã‰tape: [E1] | Source: [S0]  # #
                it["versions"] = abs_data.get("versions", [])  # # Cette ligne copie la liste des versions (ou []) ; utile pour tracer lâ€™historique v1/v2â€¦ | Ã‰tape: [E1] | Source: [S0]  # #
                it["last_updated_raw"] = abs_data.get("last_updated_raw", "")  # # Cette ligne copie la derniÃ¨re ligne dâ€™update, utile pour â€œderniÃ¨re majâ€ (ou vide si absent) | Ã‰tape: [E1] | Source: [S0]  # #

                html_url = extract_html_url_from_abs(abs_html=abs_html, arxiv_id=it.get("arxiv_id", ""))  # # Cette ligne appelle la fonction qui cherche dans /abs un lien vers /html, car Method/References sont plus faciles Ã  extraire depuis la page HTML | Ã‰tape: [E5] | Source: [S8]  # #
                if html_url:  # # Ici on vÃ©rifie quâ€™un lien /html a Ã©tÃ© trouvÃ©, sinon on ne tente pas lâ€™Ã©tape suivante | Ã‰tape: [E5] | Source: [S6]  # #
                    html_full, html_code = http_get_text(session=session, url=html_url, timeout_s=30)  # # Cette ligne fait le GET du /html (si dispo) pour rÃ©cupÃ©rer la structure LaTeX HTML (sections, biblist) | Ã‰tape: [E5] | Source: [S2]  # #
                    bundle_parts.append(f"<!-- HTML URL: {html_url} | HTTP {html_code} -->\n")  # # Cette ligne trace dans le bundle debug lâ€™URL /html et le code HTTP pour diagnostiquer un Ã©ventuel blocage | Ã‰tape: [E5] | Source: [S0]  # #
                    bundle_parts.append(html_full[:200000])  # # Cette ligne garde un extrait de la page /html dans le bundle debug, pour vÃ©rifier les sÃ©lecteurs â€œmethod/bibliographyâ€ | Ã‰tape: [E5] | Source: [S2]  # #
                    bundle_parts.append("\n<!-- END HTML -->\n")  # # Cette ligne ferme le bloc /html dans le bundle debug, pour sÃ©parer proprement les pages | Ã‰tape: [E5] | Source: [S2]  # #
                    if html_code == 200:  # # Ici on vÃ©rifie que /html est OK avant dâ€™extraire method/refs, sinon on log lâ€™erreur par item | Ã‰tape: [E5] | Source: [S6]  # #
                        method_txt, refs_list = parse_arxiv_html_method_and_references(html_full)  # # Cette ligne appelle la fonction qui extrait 2 blocs ciblÃ©s (Method + References) depuis /html | Ã‰tape: [E5] | Source: [S8]  # #
                        if method_txt:  # # Ici on teste si un texte â€œmethodâ€ a rÃ©ellement Ã©tÃ© trouvÃ©, pour ne pas Ã©craser avec du vide | Ã‰tape: [E5] | Source: [S8]  # #
                            it["method"] = method_txt  # # Cette ligne stocke le texte de la section â€œmethodâ€ dans lâ€™item, pour que lâ€™agent puisse rÃ©pondre avec plus de contenu utile | Ã‰tape: [E5] | Source: [S1]  # #
                        if refs_list:  # # Ici on teste si des rÃ©fÃ©rences ont Ã©tÃ© trouvÃ©es, car parfois la bib nâ€™existe pas en HTML arXiv | Ã‰tape: [E5] | Source: [S0]  # #
                            it["references"] = refs_list  # # Cette ligne stocke la liste de rÃ©fÃ©rences dans lâ€™item (format liste) pour faciliter le QA / citations | Ã‰tape: [E5] | Source: [S1]  # #
                    else:  # # Ici on traite le cas oÃ¹ /html rÃ©pond en erreur : on ne crash pas, on note juste lâ€™erreur dans it["errors"] | Ã‰tape: [E3] | Source: [S3]  # #
                        it["errors"].append(f"html_http_{html_code}")  # # Cette ligne ajoute une erreur â€œhtml_http_XXXâ€ au niveau item, pour diagnostiquer une panne /html sans arrÃªter tout le scraping | Ã‰tape: [E3] | Source: [S0]  # #

                if is_empty(it.get("abstract")) and not is_empty(abs_data.get("abstract")):  # # Ici on dÃ©clenche un fallback: si lâ€™abstract de search est vide, on rÃ©cupÃ¨re celui de /abs, pour complÃ©ter sans ajouter de bruit | Ã‰tape: [E2] | Source: [S6]  # #
                    it["abstract"] = abs_data.get("abstract", "")  # # Cette ligne injecte lâ€™abstract fallback depuis /abs dans lâ€™item, ce qui amÃ©liore la qualitÃ© des rÃ©ponses LLM | Ã‰tape: [E1] | Source: [S0]  # #
            else:  # # Ici on traite le cas /abs en erreur (non-200) : on ne crash pas, on stocke un code dâ€™erreur au niveau item | Ã‰tape: [E3] | Source: [S3]  # #
                it["errors"].append(f"abs_http_{abs_code}")  # # Cette ligne ajoute lâ€™erreur â€œabs_http_XXXâ€ Ã  lâ€™item, pour savoir exactement quel papier a Ã©chouÃ© Ã  lâ€™enrichissement | Ã‰tape: [E2] | Source: [S6]  # #

            sleep_polite(min_s=polite_min_s, max_s=polite_max_s)  # # Cette ligne attend un peu entre deux appels /abs (et /html) pour Ã©viter dâ€™enchaÃ®ner trop vite et rÃ©duire le risque de blocage | Ã‰tape: [E4] | Source: [S5]  # #

    # Missing fields
    for it in filtered:  # # Cette ligne parcourt tous les items filtrÃ©s pour calculer la liste des champs manquants, afin de diagnostiquer rapidement ce qui nâ€™a pas Ã©tÃ© extrait | Ã‰tape: [E2] | Source: [S6]  # #
        it["missing_fields"] = compute_missing_fields(it)  # # Cette ligne appelle la fonction qui compare SUPPORTED_FIELDS vs valeurs vides, et enregistre le rÃ©sultat dans lâ€™item (debug qualitÃ©) | Ã‰tape: [E1] | Source: [S1]  # #
    # ===============================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
    # ğŸ’¾ Sauvegardes cache raw                                # #  | Ã‰tape: [E2] | Source: [S2]  # #
    # ===============================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
    bundle_name = f"scrape_arxiv_cs_bundle_{ts}.html"  # # Cette ligne fabrique le nom du fichier â€œbundleâ€ HTML avec un timestamp, pour garder une preuve/trace de ce qui a Ã©tÃ© scrappÃ© Ã  un instant T (debug local) | Ã‰tape: [E2] | Source: [S2]  # #
    bundle_path = save_text_file(data_lake_raw_dir, bundle_name, "\n".join(bundle_parts))  # # Cette ligne appelle la fonction de sauvegarde qui Ã©crit sur disque le bundle HTML (en concatÃ©nant bundle_parts), pour pouvoir diagnostiquer un DOM cassÃ© ou une page anti-bot | Ã‰tape: [E2] | Source: [S2]  # #

    result: Dict[str, Any] = {  # # Cette ligne crÃ©e le dictionnaire final â€œresultâ€ (contrat de sortie stable) que FastAPI/ton agent consommera sans surprise | Ã‰tape: [E1] | Source: [S1]  # #
        "ok": (len(errors_global) == 0),  # # Cette ligne calcule le statut ok=True seulement si la liste errors_global est vide (si erreur globale => ok=False) pour signaler clairement un problÃ¨me â€œglobal toolâ€ | Ã‰tape: [E3] | Source: [S3]  # #
        "user_query": user_query,  # # Cette ligne renvoie la requÃªte utilisateur telle quâ€™utilisÃ©e, pour traÃ§abilitÃ© et reproduction du test | Ã‰tape: [E1] | Source: [S0]  # #
        "theme": theme,  # # Cette ligne renvoie le thÃ¨me demandÃ© (ou None), pour expliquer le filtrage appliquÃ© | Ã‰tape: [E1] | Source: [S0]  # #
        "allowed_subcats": allowed_subcats,  # # Cette ligne renvoie la liste de catÃ©gories autorisÃ©es, pour rendre le pÃ©rimÃ¨tre explicite cÃ´tÃ© API | Ã‰tape: [E1] | Source: [S0]  # #
        "sort": sort,  # # Cette ligne renvoie le mode de tri utilisÃ© (relevance ou date), pour traÃ§abilitÃ© | Ã‰tape: [E1] | Source: [S0]  # #
        "requested_max_results": max_results,  # # Cette ligne renvoie la limite demandÃ©e/normalisÃ©e, pour vÃ©rifier que lâ€™outil respecte le â€œcontrÃ´le de volumeâ€ | Ã‰tape: [E1] | Source: [S0]  # #
        "count_collected_cs": len(collected),  # # Cette ligne renvoie combien dâ€™items ont Ã©tÃ© collectÃ©s depuis search/cs (avant filtrage), utile pour debug pagination | Ã‰tape: [E2] | Source: [S6]  # #
        "count_after_theme_filter": len(filtered),  # # Cette ligne renvoie combien dâ€™items restent aprÃ¨s filtre thÃ¨me/catÃ©gories/keywords, utile pour comprendre un rÃ©sultat â€œtrop faibleâ€ | Ã‰tape: [E2] | Source: [S6]  # #
        "items": filtered,  # # Cette ligne renvoie la liste finale des items structurÃ©s (les donnÃ©es utiles), câ€™est le â€œpayload principalâ€ cÃ´tÃ© API | Ã‰tape: [E1] | Source: [S1]  # #
        "bundle_html_file": bundle_path,  # # Cette ligne renvoie le chemin du bundle HTML Ã©crit sur disque, pour inspection manuelle si le parsing casse (diagnostic) | Ã‰tape: [E2] | Source: [S2]  # #
        "supported_fields": SUPPORTED_FIELDS,  # # Cette ligne renvoie le schÃ©ma des champs supportÃ©s, pour que lâ€™API sache ce qui peut exister et ce qui peut manquer | Ã‰tape: [E1] | Source: [S1]  # #

        # Debug important
        "project_root": PROJECT_ROOT,  # # Cette ligne renvoie la racine projet dÃ©tectÃ©e, pour vÃ©rifier que lâ€™outil Ã©crit bien dans le bon projet (pas ailleurs) | Ã‰tape: [E1] | Source: [S0]  # #
        "raw_cache_dir": data_lake_raw_dir,  # # Cette ligne renvoie le dossier oÃ¹ les fichiers sont rÃ©ellement enregistrÃ©s, pour que tu retrouves facilement JSON/HTML | Ã‰tape: [E2] | Source: [S2]  # #
        "cwd_runtime": os.getcwd(),  # # Cette ligne renvoie le rÃ©pertoire courant dâ€™exÃ©cution (CWD), utile car uvicorn peut changer le CWD et casser des chemins relatifs | Ã‰tape: [E1] | Source: [S0]  # #
        "last_search_url": last_search_url,  # # Cette ligne renvoie la derniÃ¨re URL appelÃ©e sur search/cs, pour reproduire exactement le cas qui a plantÃ© | Ã‰tape: [E2] | Source: [S6]  # #
        "last_search_http": last_search_http,  # # Cette ligne renvoie le dernier code HTTP reÃ§u sur search/cs (ex: 200, 429, 500) ; IMPORTANT: si tu vois 0, Ã§a veut dire â€œerreur localeâ€ (timeout/exception rÃ©seau) et PAS une rÃ©ponse HTTP du site | Ã‰tape: [E3] | Source: [S3]  # #
        "parse_diag_last": diag_last,  # # Cette ligne renvoie le dernier diagnostic de parsing (compte de noeuds, flags anti-bot, etc.) pour comprendre pourquoi items=[] | Ã‰tape: [E2] | Source: [S6]  # #
        "anti_bot_or_weird_page": anti_bot_or_weird_page,  # # Cette ligne renvoie un boolÃ©en â€œon a dÃ©tectÃ© une page bizarreâ€, pour Ãªtre transparent sur une cause type consent/robot | Ã‰tape: [E2] | Source: [S6]  # #
    }  # # Cette ligne ferme le dict result, ce qui garantit que la sortie JSON est complÃ¨te et structurÃ©e | Ã‰tape: [E1] | Source: [S1]  # #

    json_name = f"scrape_arxiv_cs_{ts}.json"  # # Cette ligne construit le nom du fichier JSON (avec timestamp) pour versionner les rÃ©sultats et Ã©viter dâ€™Ã©craser un ancien test | Ã‰tape: [E2] | Source: [S2]  # #
    json_path = os.path.join(data_lake_raw_dir, json_name)  # # Cette ligne construit le chemin complet du JSON dans le cache raw, pour enregistrer localement au bon endroit | Ã‰tape: [E2] | Source: [S2]  # #
    with open(json_path, "w", encoding="utf-8") as f:  # # Cette ligne ouvre le fichier JSON en Ã©criture UTF-8, pour Ã©viter les soucis dâ€™accents et garantir une sauvegarde lisible | Ã‰tape: [E1] | Source: [S1]  # #
        json.dump(result, f, ensure_ascii=False, indent=2)  # # Cette ligne sÃ©rialise le dict result en JSON lisible (indent=2) sans Ã©chapper les caractÃ¨res non-ASCII, pour debug facile | Ã‰tape: [E1] | Source: [S1]  # #

    result["saved_to"] = json_path  # # Cette ligne ajoute dans la sortie le chemin du JSON sauvegardÃ© (super pratique pour lâ€™API et pour toi) | Ã‰tape: [E1] | Source: [S1]  # #
    return result  # # Cette ligne retourne le dict final (contrat stable) Ã  lâ€™appelant (main/FastAPI), sans side effect supplÃ©mentaire | Ã‰tape: [E1] | Source: [S0]  # #


# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
# âœ… Alias compatibilitÃ© avec ton main.py                      # #  | Ã‰tape: [E1] | Source: [S0]  # #
# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
def scrape_arxiv_cs(  # # Cette ligne dÃ©finit une fonction â€œaliasâ€ (mÃªme nom que lâ€™ancien scraper) pour ne pas casser ton main.py qui lâ€™appelle peut-Ãªtre encore | Ã‰tape: [E1] | Source: [S0]  # #
    query: str,  # # Cette ligne dÃ©finit le paramÃ¨tre query (texte utilisateur) : câ€™est lâ€™entrÃ©e principale de recherche | Ã‰tape: [E1] | Source: [S0]  # #
    max_results: int = 50,  # # Cette ligne fixe la limite par dÃ©faut Ã  50 (1 page), pour Ã©viter un scraping massif et rester dans la contrainte â€œPAGE_SIZE=50â€ | Ã‰tape: [E1] | Source: [S0]  # #
    sort: str = "relevance",  # # Cette ligne dÃ©finit le tri par dÃ©faut : pertinence, pour un comportement stable si lâ€™utilisateur nâ€™indique rien | Ã‰tape: [E1] | Source: [S0]  # #
    polite_min_s: float = 1.2,  # # Cette ligne dÃ©finit la pause minimale, pour ralentir entre requÃªtes et Ã©viter un rythme robotique | Ã‰tape: [E4] | Source: [S5]  # #
    polite_max_s: float = 2.0,  # # Cette ligne dÃ©finit la pause maximale, pour ajouter du jitter (variabilitÃ©) et rÃ©duire le risque de blocage | Ã‰tape: [E4] | Source: [S5]  # #
    data_lake_raw_dir: str = DEFAULT_RAW_DIR,  # # Cette ligne dÃ©finit le dossier de cache raw par dÃ©faut, pour enregistrer localement dans data_lake/raw/cache | Ã‰tape: [E2] | Source: [S2]  # #
    theme: Optional[str] = None,  # # Cette ligne dÃ©finit un thÃ¨me optionnel pour filtrer (ou None), ce qui garde lâ€™API flexible | Ã‰tape: [E1] | Source: [S0]  # #
) -> Dict[str, Any]:  # # Cette ligne annonce que la fonction retourne un dict JSON (contrat), pour que le reste du systÃ¨me puisse lâ€™utiliser sans surprise | Ã‰tape: [E1] | Source: [S1]  # #
    return scrape_arxiv_cs_scoped(  # # Cette ligne dÃ©lÃ¨gue Ã  la fonction principale â€œscopedâ€ (une seule implÃ©mentation) pour Ã©viter la duplication de logique | Ã‰tape: [E1] | Source: [S0]  # #
        user_query=query,  # # Cette ligne mappe query -> user_query (renommage), pour garder un contrat interne cohÃ©rent | Ã‰tape: [E1] | Source: [S0]  # #
        theme=theme,  # # Cette ligne transmet le thÃ¨me Ã  la fonction principale, pour activer le filtrage thÃ©matique si fourni | Ã‰tape: [E1] | Source: [S0]  # #
        max_results=max_results,  # # Cette ligne transmet max_results (limite), pour respecter le contrÃ´le de volume demandÃ© | Ã‰tape: [E1] | Source: [S0]  # #
        sort=sort,  # # Cette ligne transmet le tri choisi, pour que la search URL reflÃ¨te la prÃ©fÃ©rence (relevance vs date) | Ã‰tape: [E1] | Source: [S0]  # #
        polite_min_s=polite_min_s,  # # Cette ligne transmet la pause min, pour garder la politesse configurÃ©e par lâ€™appelant | Ã‰tape: [E4] | Source: [S5]  # #
        polite_max_s=polite_max_s,  # # Cette ligne transmet la pause max, pour garder la variabilitÃ© configurÃ©e par lâ€™appelant | Ã‰tape: [E4] | Source: [S5]  # #
        data_lake_raw_dir=data_lake_raw_dir,  # # Cette ligne transmet le dossier de cache, pour Ã©crire les fichiers au bon endroit (local) | Ã‰tape: [E2] | Source: [S2]  # #
        enrich_abs=True,  # # Cette ligne force enrich_abs=True : on enrichit /abs (doi/versions/abstract) car utile pour la qualitÃ© des rÃ©sultats | Ã‰tape: [E1] | Source: [S0]  # #
        enable_keyword_filter=True,  # # Cette ligne garde le keyword fallback, pour Ã©viter des faux nÃ©gatifs quand categories manquent ou sont instables | Ã‰tape: [E2] | Source: [S6]  # #
    )  # # Cette ligne ferme lâ€™appel forward, ce qui garantit que lâ€™alias retourne exactement le mÃªme contrat que la fonction principale | Ã‰tape: [E1] | Source: [S1]  # #


# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
# âœ… TEST LOCAL                                               # #  | Ã‰tape: [E1] | Source: [S0]  # #
# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
RUN_LOCAL_TEST = True  # # Cette ligne active/dÃ©sactive le test local : True = on peut lancer le fichier seul (sans FastAPI) pour valider rapidement le scraping | Ã‰tape: [E2] | Source: [S2]  # #

if __name__ == "__main__" and RUN_LOCAL_TEST:  # # Cette ligne exÃ©cute un scÃ©nario de test seulement si on lance le script directement (pas importÃ©), pour Ã©viter des effets de bord | Ã‰tape: [E2] | Source: [S3]  # #
    res = scrape_arxiv_cs_scoped(  # # Cette ligne lance la fonction principale en mode test (appel direct) pour vÃ©rifier la robustesse parsing + la sauvegarde cache | Ã‰tape: [E2] | Source: [S2]  # #
        user_query="multimodal transformer misogyny detection",  # # Cette ligne dÃ©finit une requÃªte dâ€™exemple (test reproductible) pour valider la chaÃ®ne search->abs->html | Ã‰tape: [E1] | Source: [S0]  # #
        theme="ai_ml",  # # Cette ligne fixe un thÃ¨me de test (ai_ml) pour vÃ©rifier le filtrage par catÃ©gories et/ou fallback keywords | Ã‰tape: [E1] | Source: [S0]  # #
        max_results=5,  # # Cette ligne limite le test Ã  5 rÃ©sultats, ce qui suffit pour valider sans â€œscraper tropâ€ | Ã‰tape: [E1] | Source: [S0]  # #
        sort="relevance",  # # Cette ligne fixe le tri pour le test, pour rendre les rÃ©sultats plus stables et comparables entre runs | Ã‰tape: [E1] | Source: [S0]  # #
        data_lake_raw_dir=DEFAULT_RAW_DIR,  # # Cette ligne indique oÃ¹ Ã©crire les fichiers du test, pour retrouver facilement JSON + bundle HTML | Ã‰tape: [E2] | Source: [S2]  # #
        enrich_abs=True,  # # Cette ligne active lâ€™enrichissement /abs et /html, pour tester aussi method/references et pas seulement le search | Ã‰tape: [E5] | Source: [S8]  # #
    )  # # Cette ligne ferme lâ€™appel test, ce qui garantit que res contient le dict â€œresultâ€ complet (contrat) | Ã‰tape: [E1] | Source: [S1]  # #

    print(json.dumps({  # # Cette ligne affiche un sous-ensemble des champs en JSON pretty, pour vÃ©rifier vite â€œÃ§a marcheâ€ sans ouvrir le gros fichier complet | Ã‰tape: [E2] | Source: [S2]  # #
        "count_collected_cs": res.get("count_collected_cs"),  # # Cette ligne affiche combien dâ€™items ont Ã©tÃ© collectÃ©s sur search/cs, utile pour valider pagination/selector | Ã‰tape: [E2] | Source: [S6]  # #
        "count_after_theme_filter": res.get("count_after_theme_filter"),  # # Cette ligne affiche combien dâ€™items restent aprÃ¨s filtrage, utile pour valider theme+keywords | Ã‰tape: [E2] | Source: [S6]  # #
        "saved_to": res.get("saved_to"),  # # Cette ligne affiche le chemin du JSON, pour que tu puisses lâ€™ouvrir directement sans chercher | Ã‰tape: [E2] | Source: [S2]  # #
        "bundle_html_file": res.get("bundle_html_file"),  # # Cette ligne affiche le chemin du bundle HTML, pour inspecter le HTML si un parsing est vide | Ã‰tape: [E2] | Source: [S2]  # #
        "anti_bot_or_weird_page": res.get("anti_bot_or_weird_page"),  # # Cette ligne affiche le flag anti-bot/weird, pour savoir si arXiv a renvoyÃ© une page de blocage/consent | Ã‰tape: [E2] | Source: [S6]  # #
        "last_search_http": res.get("last_search_http"),  # # Cette ligne affiche le dernier code HTTP search ; rappel: 0 = exception rÃ©seau locale (pas un HTTP du site) | Ã‰tape: [E3] | Source: [S3]  # #
        "parse_diag_last": res.get("parse_diag_last"),  # # Cette ligne affiche le dernier diagnostic de parsing (counts/flags) pour comprendre un rÃ©sultat vide | Ã‰tape: [E2] | Source: [S6]  # #
    }, ensure_ascii=False, indent=2))  # # Cette ligne force un JSON lisible (indent) et conserve les accents (ensure_ascii=False) pour un debug confortable | Ã‰tape: [E1] | Source: [S1]  # #
