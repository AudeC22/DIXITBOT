# ===============================  # #
# üî¢ R√âF√âRENTIEL (√âtapes + Sources)  # #
# ===============================  # #
# ‚úÖ But (1 phrase, simple) : on rend la sortie du tool pr√©visible et on √©vite que tout casse si arXiv change un d√©tail.  # #
#
# üìå √âtapes (codes √† r√©utiliser dans les hashtags, pour √©viter la redondance)
# [E1] NORMALISATION/DECOUPLAGE : transformer/standardiser les donn√©es (format stable) pour que FastAPI lise sans surprise.  # #
# [E2] ROBUSTESSE : continuer √† fonctionner malgr√© HTML qui change, pages bizarres, ou r√©sultats manquants (fallback + diag).  # #
# [E3] GESTION_ERREUR : capturer les erreurs (HTTP/timeouts/exceptions) et retourner ok=False + errors[] au lieu de crasher.  # #
# [E4] SCRAPING_ETHIQUE : limiter la fr√©quence, mettre un User-Agent, ne pas spammer le site (politesse).  # #
# [E5] STRUCTURATION : pr√©parer un contexte/sections propres (ex: method/references) sans envoyer du HTML brut au LLM.  # #
# [E6] TOOL : ex√©cution du scraping en tant qu'outil externe (appel√© par l'orchestrateur).  # #
#
# üìö Sources prof (codes)
# [S1] Guide_Scraping_HTML_Python_IA_BOT (1).pdf ‚Äî extrait : ¬´ Toujours produire une sortie structur√©e (JSON) ¬ª.  # #
# [S2] Guide_Scraping_HTML_Python_IA_BOT (1).pdf ‚Äî extrait : ¬´ Mettre en cache les r√©sultats ¬ª.  # #
# [S3] Guide_Scraping_HTML_Python_IA_BOT (1).pdf ‚Äî extrait : ¬´ G√©rer les erreurs (try/except, timeouts) ¬ª.  # #
# [S4] Guide_Scraping_HTML_Python_IA_BOT (1).pdf ‚Äî extrait : ¬´ Toujours d√©finir un User-Agent ¬ª.  # #
# [S5] Kick-Off-IA-Bot-Agent-Conversationnel-Intelligent.pdf ‚Äî extrait : ¬´ Scrappez √† faible fr√©quence. √âvitez absolument le spam de requ√™tes ¬ª.  # #
# [S6] Kick-Off-IA-Bot-Agent-Conversationnel-Intelligent.pdf ‚Äî extrait : ¬´ Gestion des erreurs : ... strat√©gies de fallback. ¬ª.  # #
# [S7] Projet-IA-BOT-Concevoir-un-agent-pas-un-simple-chatbot (1).pdf ‚Äî extrait : ¬´ Pas de gestion d'erreur ... timeouts. Un agent robuste g√®re l'incertitude ¬ª.  # #
# [S8] Projet-IA-BOT-Concevoir-un-agent-pas-un-simple-chatbot (1).pdf ‚Äî extrait : ¬´ Chaque tool a un contrat clair : inputs structur√©s et outputs normalis√©s ¬ª.  # #
#
# üßæ √Ä propos des ‚Äúnum√©ros d‚Äôerreur‚Äù (ce que √ßa veut dire)
# - 200 = OK (la page a √©t√© r√©cup√©r√©e)  # #
# - 429 = Too Many Requests (le site te ‚Äúrate-limit‚Äù, donc on ralentit / on retry)  # #
# - 500/502/503/504 = erreurs serveur (souvent temporaires, on peut retry)  # #
# - codes internes ex: "http_429_search" = notre libell√© lisible : "type d‚Äôerreur" + "o√π" (search/abs/html).  # #

# ============================================================  # #  | √âtape: [E1] | Source: [S0]  # #
# Scraper arXiv CS (cibl√© th√©matique + sortie structur√©e)  # #  | √âtape: [E1] | Source: [S1]  # #
# Objectif :  # #  | √âtape: [E1] | Source: [S0]  # #
# - Scraping cibl√© sur les th√®mes demand√©s (pas "aspirateur")  # #  | √âtape: [E1] | Source: [S0]  # #
# - Sortie JSON structur√©e (pas de HTML brut envoy√© au LLM)  # #  | √âtape: [E1] | Source: [S1]  # #
# - Extraction minimale : title/authors/abstract/dates/urls/doi  # #  | √âtape: [E1] | Source: [S8]  # #
# - Cache + politesse + robustesse  # #  | √âtape: [E4] | Source: [S2]  # #
#   => on cherche via /search/cs puis on filtre via Subjects  # #  | √âtape: [E1] | Source: [S0]  # #
# ============================================================  # #  | √âtape: [E1] | Source: [S0]  # #

# ===============================  # #  | √âtape: [E1] | Source: [S0]  # #
# üìö Importations  # #  | √âtape: [E1] | Source: [S0]  # #
# ===============================  # #  | √âtape: [E1] | Source: [S0]  # #
import os  # # Gestion chemins/dossiers # # Respect: cache local stable (sortie disque attendue) | √âtape: [E2] | Source: [S2]  # #
import re  # # Regex parsing IDs + cat√©gories # # Respect: extraction cibl√©e (pas "tout le texte") | √âtape: [E1] | Source: [S8]  # #
import json  # # Export JSON # # Respect: sortie structur√©e JSON | √âtape: [E1] | Source: [S1]  # #
import time  # # Politesse (sleep) # # Respect: √©viter spam requ√™tes | √âtape: [E4] | Source: [S5]  # #
import random  # # Jitter # # Respect: fr√©quence raisonnable | √âtape: [E1] | Source: [S0]  # #
import datetime  # # Timestamp fichiers # # Respect: tra√ßabilit√© des fichiers | √âtape: [E1] | Source: [S0]  # #
from typing import Dict, Any, List, Tuple, Optional  # # Typage # # Respect: tool pr√©visible | √âtape: [E1] | Source: [S0]  # #

import requests  # # HTTP GET # # Respect: scraping HTML public | √âtape: [E2] | Source: [S3]  # #
from bs4 import BeautifulSoup, Tag  # # Parser HTML # # Respect: extraction cibl√©e d'√©l√©ments utiles | √âtape: [E1] | Source: [S8]  # #


# ===============================  # #  | √âtape: [E1] | Source: [S0]  # #
# üìå R√©solution robuste des chemins
# ===============================  # #  | √âtape: [E1] | Source: [S0]  # #
def _find_project_root(start_dir: str) -> str:  # # üîé Trouver la racine projet # # Respect: √©crit toujours dans /data_lake du projet | √âtape: [E1] | Source: [S0]  # #
    cur = os.path.abspath(start_dir)  # # Normalise # # Respect: robustesse Windows/uvicorn | √âtape: [E1] | Source: [S0]  # #
    while True:  # # Boucle remont√©e # # Respect: √©viter chemins relatifs fragiles | √âtape: [E1] | Source: [S0]  # #
        if os.path.isdir(os.path.join(cur, "data_lake")):  # # Marqueur data_lake # # Respect: cache raw attendu par le projet | √âtape: [E2] | Source: [S2]  # #
            return cur  # # Racine OK # # Respect: √©criture stable | √âtape: [E1] | Source: [S0]  # #
        if os.path.isfile(os.path.join(cur, "pyproject.toml")):  # # Marqueur projet # # Respect: structure projet | √âtape: [E1] | Source: [S0]  # #
            return cur  # # Racine OK # # Respect: √©criture stable | √âtape: [E1] | Source: [S0]  # #
        if os.path.isfile(os.path.join(cur, "requirements.txt")):  # # Marqueur projet # # Respect: structure projet | √âtape: [E1] | Source: [S0]  # #
            return cur  # # Racine OK # # Respect: √©criture stable | √âtape: [E1] | Source: [S0]  # #
        parent = os.path.dirname(cur)  # # Parent # # Respect: progression contr√¥l√©e | √âtape: [E1] | Source: [S0]  # #
        if parent == cur:  # # Sommet atteint # # Respect: √©viter boucle infinie | √âtape: [E1] | Source: [S0]  # #
            return os.path.abspath(start_dir)  # # Fallback: dossier actuel # # Respect: coh√©rence minimale | √âtape: [E2] | Source: [S6]  # #
        cur = parent  # # Continue # # Respect: robustesse | √âtape: [E1] | Source: [S0]  # #


# ===============================  # #  | √âtape: [E1] | Source: [S0]  # #
# üß≠ Constantes arXiv  # #  | √âtape: [E1] | Source: [S0]  # #
# ===============================  # #  | √âtape: [E1] | Source: [S0]  # #
ARXIV_BASE = "https://arxiv.org"  # # Base URL # # Respect: source publique | √âtape: [E2] | Source: [S3]  # #
ARXIV_SEARCH_CS = f"{ARXIV_BASE}/search/cs"  # # ‚úÖ Endpoint CS HTML # # Respect: p√©rim√®tre CS directement | √âtape: [E1] | Source: [S0]  # #

_THIS_FILE_DIR = os.path.dirname(os.path.abspath(__file__))  # # Dossier du script # # Respect: d√©terminisme | √âtape: [E1] | Source: [S0]  # #
PROJECT_ROOT = _find_project_root(_THIS_FILE_DIR)  # # Racine projet # # Respect: √©crit au bon endroit | √âtape: [E1] | Source: [S0]  # #
DEFAULT_RAW_DIR = os.path.join(PROJECT_ROOT, "data_lake", "raw", "cache")  # # Cache raw # # Respect: stockage dans raw/cache | √âtape: [E2] | Source: [S2]  # #

MAX_RESULTS_HARD_LIMIT = 100  # # Cap anti-massif # # Respect: pas d'aspirateur | √âtape: [E1] | Source: [S0]  # #
PAGE_SIZE = 50  # # Taille page arXiv # # Respect: contr√¥le volume | √âtape: [E1] | Source: [S0]  # #

# ===============================  # #  | √âtape: [E1] | Source: [S0]  # #
# üßØ Robustesse HTTP  # #  | √âtape: [E2] | Source: [S3]  # #
# ===============================  # #  | √âtape: [E1] | Source: [S0]  # #
HTTP_RETRY_STATUS = {429, 500, 502, 503, 504}  # # Codes √† retry # # Respect: agent robuste (ne pas casser au 1er incident) | √âtape: [E2] | Source: [S3]  # #
HTTP_RETRY_MAX = 2  # # Nombre de retries # # Respect: fr√©quence raisonnable (pas de spam) | √âtape: [E2] | Source: [S3]  # #
HTTP_TIMEOUT_S = 30  # # Timeout # # Respect: robustesse (√©vite blocage) | √âtape: [E2] | Source: [S3]  # #


# ============================================================  # #  | √âtape: [E1] | Source: [S0]  # #
# üéØ Th√®mes demand√©s -> sous-cat√©gories arXiv autoris√©es  # #  | √âtape: [E1] | Source: [S0]  # #
# ============================================================  # #  | √âtape: [E1] | Source: [S0]  # #
THEME_TO_ARXIV_SUBCATS: Dict[str, List[str]] = {  # # Mapping th√®me->codes # # Respect: p√©rim√®tre strict + cross-lists fr√©quentes | √âtape: [E1] | Source: [S0]  # #
    "ai_ml": [  # # IA/ML/LLM/Agents/Vision/Multimodal # # Respect: couvre ML m√™me si class√© en stat.ML / eess.IV | √âtape: [E1] | Source: [S0]  # #
        "cs.AI",  # # Artificial Intelligence # # Respect: IA/Agents | √âtape: [E1] | Source: [S0]  # #
        "cs.LG",  # # Machine Learning (CS) # # Respect: ML | √âtape: [E1] | Source: [S0]  # #
        "cs.CL",  # # Computation and Language (NLP/LLM) # # Respect: LLM/NLP | √âtape: [E1] | Source: [S0]  # #
        "cs.CV",  # # Computer Vision and Pattern Recognition # # Respect: Vision/Multimodal | √âtape: [E1] | Source: [S0]  # #
        "cs.MA",  # # Multiagent Systems # # Respect: Agents | √âtape: [E1] | Source: [S0]  # #
        "cs.NE",  # # Neural and Evolutionary Computing # # Respect: Deep learning (historique) | √âtape: [E1] | Source: [S0]  # #
        "stat.ML",  # # Machine Learning (Stats) # # Respect: cross-list tr√®s fr√©quent (√©vite 0 r√©sultats) | √âtape: [E1] | Source: [S0]  # #
        "eess.IV",  # # Image and Video Processing # # Respect: Vision parfois hors CS | √âtape: [E1] | Source: [S0]  # #
    ],
    "algo_ds": ["cs.DS", "cs.CC"],  # # Algo/DS/Complexit√© # # Respect: p√©rim√®tre demand√© | √âtape: [E1] | Source: [S0]  # #
    "net_sys": ["cs.NI", "cs.DC", "cs.OS"],  # # R√©seau/Distrib/OS # # Respect: p√©rim√®tre demand√© | √âtape: [E1] | Source: [S0]  # #
    "cyber_crypto": ["cs.CR"],  # # Crypto/S√©cu # # Respect: p√©rim√®tre demand√© | √âtape: [E1] | Source: [S0]  # #
    "pl_se": ["cs.PL", "cs.SE", "cs.LO"],  # # Langages/SE/Logique # # Respect: p√©rim√®tre demand√© | √âtape: [E1] | Source: [S0]  # #
    "hci_data": ["cs.HC", "cs.IR", "cs.DB", "cs.MM"],  # # HCI/IR/DB/MM # # Respect: p√©rim√®tre demand√© | √âtape: [E1] | Source: [S0]  # #
}

# ============================================================  # #  | √âtape: [E1] | Source: [S0]  # #
# üß† Keywords fallback (si pas de th√®me explicite)  # #  | √âtape: [E2] | Source: [S6]  # #
# ============================================================  # #  | √âtape: [E1] | Source: [S0]  # #
THEME_KEYWORDS: Dict[str, List[str]] = {  # # Support # # Respect: filtrage pertinence si cat√©gories manquantes | √âtape: [E1] | Source: [S0]  # #
    "ai_ml": ["machine learning", "deep learning", "llm", "agent", "transformer", "multimodal", "computer vision"],
    "algo_ds": ["algorithm", "data structure", "complexity", "graph", "optimization"],
    "net_sys": ["network", "distributed", "operating system", "cloud", "systems"],
    "cyber_crypto": ["security", "privacy", "cryptography", "attack", "defense", "malware"],
    "pl_se": ["programming language", "compiler", "software engineering", "static analysis", "type system"],
    "hci_data": ["human-computer interaction", "information retrieval", "database", "multimedia", "ranking", "search"],
}

# ===============================  # #  | √âtape: [E1] | Source: [S0]  # #
# üì¶ Champs renvoy√©s (minimal)  # #  | √âtape: [E1] | Source: [S0]  # #
# ===============================  # #  | √âtape: [E1] | Source: [S0]  # #
SUPPORTED_FIELDS = [  # # Champs stables | √âtape: [E1] | Source: [S1]  # #
    "arxiv_id",  # # Identifiant arXiv = ID unique du papier | √âtape: [E1] | Source: [S1]  # #
    "title",  # # Titre = nom du papier | √âtape: [E1] | Source: [S0]  # #
    "authors",  # # Auteurs = liste des auteurs | √âtape: [E1] | Source: [S0]  # #
    "abstract",
    "method",  # # Methode (FR: section "M√©thode") # # √âtape: [E1] | Source: [S8]  # #
    "references",  # # References (FR: section "R√©f√©rences") # # √âtape: [E1]# #  # # R√©sum√© = abstract du papier | √âtape: [E1] | Source: [S8]  # #
    "submitted_date",  # # Date de soumission = quand le papier a √©t√© soumis | √âtape: [E1] | Source: [S1]  # #
    "abs_url",  # # Lien fiche = URL /abs (page d√©tails) | √âtape: [E1] | Source: [S1]  # #
    "pdf_url",  # # Lien PDF = URL /pdf (t√©l√©chargement) | √âtape: [E1] | Source: [S1]  # #
    "doi",  # # DOI = identifiant √©diteur (si pr√©sent) | √âtape: [E1] | Source: [S0]  # #
    "versions",  # # Versions = historique v1,v2, | √âtape: [E1] | Source: [S0]  # #
    "last_updated_raw",  # # Derni√®re maj = derni√®re ligne de l‚Äôhistorique | √âtape: [E1] | Source: [S1]  # #
    "primary_category",  # # Cat√©gorie principale = th√®me arXiv principal | √âtape: [E1] | Source: [S0]  # #
    "all_categories",  # # Toutes cat√©gories = tags arXiv du papier | √âtape: [E1] | Source: [S0]  # #
    "missing_fields",  # # Champs manquants = ce qui n‚Äôa pas √©t√© trouv√© | √âtape: [E2] | Source: [S3]  # #
    "errors",  # # Erreurs item = erreurs li√©es √† ce papier | √âtape: [E2] | Source: [S3]  # #
]


# ============================================================  # #  | √âtape: [E1] | Source: [S0]  # #
# üß© Helpers base  # #  | √âtape: [E1] | Source: [S0]  # #
# ============================================================  # #  | √âtape: [E1] | Source: [S0]  # #
def ensure_dir(path: str) -> None:  # # Cr√©er dossier # # Respect: cache disque demand√© | √âtape: [E2] | Source: [S2]  # #
    os.makedirs(path, exist_ok=True)  # # OK si existe # # Robustesse | √âtape: [E1] | Source: [S0]  # #


def now_iso_for_filename() -> str:  # # Timestamp filename # # Respect: tra√ßabilit√© | √âtape: [E1] | Source: [S0]  # #
    return datetime.datetime.now().strftime("%Y%m%d_%H%M%S")  # # Format stable # # Respect: noms de fichiers tra√ßables | √âtape: [E1] | Source: [S0]  # #


def is_empty(value: Any) -> bool:  # # D√©tection "vide" # # Respect: qualit√© sortie JSON | √âtape: [E1] | Source: [S1]  # #
    if value is None:  # # None # # Respect: qualit√© | √âtape: [E1] | Source: [S0]  # #
        return True  # # Vide # # Respect: qualit√© | √âtape: [E1] | Source: [S0]  # #
    if isinstance(value, str):  # # String # # Respect: qualit√© | √âtape: [E1] | Source: [S0]  # #
        v = value.strip()  # # Trim # # Respect: nettoyage | √âtape: [E1] | Source: [S0]  # #
        if v == "":  # # Vide # # Respect: qualit√© | √âtape: [E1] | Source: [S0]  # #
            return True  # # Vide | √âtape: [E1] | Source: [S0]  # #
        if v.lower() in {"n/a", "null", "none"}:  # # Marqueurs # # Respect: qualit√© | √âtape: [E1] | Source: [S0]  # #
            return True  # # Vide | √âtape: [E1] | Source: [S0]  # #
    if isinstance(value, list):  # # Liste # # Respect: qualit√© | √âtape: [E1] | Source: [S0]  # #
        return len(value) == 0  # # Vide si liste vide # # Respect: qualit√© | √âtape: [E1] | Source: [S0]  # #
    return False  # # Non vide # # Respect: qualit√© | √âtape: [E1] | Source: [S0]  # #


def sleep_polite(min_s: float = 1.2, max_s: float = 2.0) -> None:  # # Politesse # # Respect: fr√©quence raisonnable | √âtape: [E4] | Source: [S5]  # #
    time.sleep(random.uniform(min_s, max_s))  # # Jitter # # Respect: anti-spam | √âtape: [E4] | Source: [S5]  # #


def save_text_file(folder: str, filename: str, content: str) -> str:  # # Sauvegarde # # Respect: cache local visible | √âtape: [E2] | Source: [S2]  # #
    ensure_dir(folder)  # # Assurer dossier # # Respect: cache disque | √âtape: [E2] | Source: [S2]  # #
    path = os.path.join(folder, filename)  # # Chemin # # Respect: coh√©rence | √âtape: [E1] | Source: [S0]  # #
    with open(path, "w", encoding="utf-8") as f:  # # UTF-8 # # Respect: robustesse encodage | √âtape: [E1] | Source: [S0]  # #
        f.write(content)  # # √âcriture # # Respect: tra√ßabilit√©/debug | √âtape: [E1] | Source: [S0]  # #
    return path  # # Retour chemin # # Respect: utilisateur peut retrouver le fichier | √âtape: [E1] | Source: [S0]  # #


def normalize_url(href: str) -> str:  # # Normalise URL # # Respect: champs propres | √âtape: [E1] | Source: [S0]  # #
    if not href:  # # Si vide # # Respect: robustesse | √âtape: [E1] | Source: [S0]  # #
        return ""  # # Retour vide # # Respect: robustesse | √âtape: [E1] | Source: [S0]  # #
    h = href.strip()  # # Trim # # Respect: sortie propre | √âtape: [E1] | Source: [S0]  # #
    if h.startswith("//"):  # # Sch√©ma manquant # # Respect: robustesse | √âtape: [E1] | Source: [S0]  # #
        return "https:" + h  # # Force https # # Respect: sortie valide | √âtape: [E2] | Source: [S3]  # #
    if h.startswith("/"):  # # Relatif # # Respect: robustesse | √âtape: [E1] | Source: [S0]  # #
        return ARXIV_BASE + h  # # Absolu # # Respect: sortie valide | √âtape: [E1] | Source: [S0]  # #
    return h  # # D√©j√† absolu # # Respect: sortie valide | √âtape: [E1] | Source: [S0]  # #


def abs_url(arxiv_id: str) -> str:  # # /abs # # Respect: sortie utile | √âtape: [E1] | Source: [S0]  # #
    return f"{ARXIV_BASE}/abs/{arxiv_id}"  # # Construit URL # # Respect: champs minimaux utiles | √âtape: [E1] | Source: [S0]  # #


def pdf_url(arxiv_id: str) -> str:  # # /pdf # # Respect: sortie utile | √âtape: [E1] | Source: [S0]  # #
    return f"{ARXIV_BASE}/pdf/{arxiv_id}"  # # Construit URL # # Respect: champs minimaux utiles | √âtape: [E1] | Source: [S0]  # #


def compute_missing_fields(item: Dict[str, Any]) -> List[str]:  # # Missing fields # # Respect: debug qualit√© | √âtape: [E1] | Source: [S0]  # #
    missing: List[str] = []  # # Init # # Respect: structuration | √âtape: [E1] | Source: [S0]  # #
    for f in SUPPORTED_FIELDS:  # # Parcours # # Respect: champs stables | √âtape: [E1] | Source: [S1]  # #
        if is_empty(item.get(f)):  # # Si vide # # Respect: diagnostic | √âtape: [E2] | Source: [S6]  # #
            missing.append(f)  # # Ajoute # # Respect: diagnostic | √âtape: [E2] | Source: [S6]  # #
    return missing  # # Retour # # Respect: sortie structur√©e | √âtape: [E1] | Source: [S1]  # #


def _detect_weird_page_signals(html: str) -> Dict[str, bool]:  # # Analyse anti-bot/consent # # Respect: robustesse + transparence | √âtape: [E2] | Source: [S6]  # #
    h = (html or "").lower()  # # Lower # # Respect: d√©tection robuste | √âtape: [E1] | Source: [S0]  # #
    return {  # # Drapeaux # # Respect: diagnostic clair | √âtape: [E2] | Source: [S6]  # #
        "contains_we_are_sorry": ("we are sorry" in h),  # # Message blocage # # Respect: diagnostic | √âtape: [E2] | Source: [S6]  # #
        "contains_robot": ("robot" in h),  # # Mention robot # # Respect: diagnostic | √âtape: [E2] | Source: [S6]  # #
        "contains_captcha": ("captcha" in h),  # # CAPTCHA # # Respect: diagnostic | √âtape: [E2] | Source: [S6]  # #
        "contains_consent": ("consent" in h or "cookie" in h),  # # Consent/cookies # # Respect: diagnostic | √âtape: [E2] | Source: [S6]  # #
        "contains_no_results": ("no results found" in h),  # # Aucun r√©sultat # # Respect: diagnostic | √âtape: [E2] | Source: [S6]  # #
    }


def http_get_text(session: requests.Session, url: str, timeout_s: int = 30) -> Tuple[str, int]:  # # GET HTML robuste (attrape timeouts/erreurs r√©seau) | √âtape: [E2] | Source: [S3]  # #
    headers = {  # # Headers HTTP (UA + langue) | √âtape: [E2] | Source: [S0]  # #
        "User-Agent": "Mozilla/5.0 DIXITBOT-arXivScraper/4.1",  # # User-Agent clair (√©vite √™tre pris pour un bot anonyme) | √âtape: [E2] | Source: [S4]  # #
        "Accept-Language": "en-US,en;q=0.9",  # # Langue stable pour parser le HTML de fa√ßon pr√©visible | √âtape: [E2] | Source: [S0]  # #
    }
    try:  # # Try HTTP (ne pas faire crasher le tool) | √âtape: [E2] | Source: [S3]  # #
        resp = session.get(url, headers=headers, timeout=timeout_s)  # # GET avec timeout (√©vite rester bloqu√©) | √âtape: [E2] | Source: [S3]  # #
        return resp.text, resp.status_code  # # Retour (HTML, code HTTP) pour diagnostic + contrat stable | √âtape: [E1] | Source: [S1]  # #
    except requests.RequestException as e:  # # Erreur r√©seau/timeout | √âtape: [E2] | Source: [S0]  # #
        return f"REQUEST_EXCEPTION: {str(e)}", 0  # # Code 0 = erreur locale (pas HTTP) | √âtape: [E2] | Source: [S3]  # #


def build_search_url(query: str, start: int, size: int, sort: str) -> str:  # # URL CS HTML # # Respect: scope CS | √âtape: [E1] | Source: [S0]  # #
    q = requests.utils.quote((query or "").strip())  # # Encode query # # Respect: requ√™te propre | √âtape: [E1] | Source: [S0]  # #
    base = f"{ARXIV_SEARCH_CS}?query={q}&searchtype=all&abstracts=show&size={size}&start={start}"  # # URL stable # # Respect: HTML public | √âtape: [E1] | Source: [S0]  # #
    s = (sort or "relevance").strip().lower()  # # Normalise tri # # Respect: contr√¥le | √âtape: [E1] | Source: [S0]  # #
    if s in {"submitted_date", "submitted", "recent"}:  # # Tri r√©cents # # Respect: option projet | √âtape: [E1] | Source: [S0]  # #
        return base + "&order=-announced_date_first"  # # Ajoute param # # Respect: contr√¥le | √âtape: [E1] | Source: [S0]  # #
    return base  # # relevance par d√©faut # # Respect: comportement stable | √âtape: [E1] | Source: [S0]  # #


# ============================================================  # #  | √âtape: [E1] | Source: [S0]  # #
# üß≤ Extraction cat√©gories depuis "Subjects" + tags (robuste)  # #  | √âtape: [E1] | Source: [S8]  # #
# ============================================================  # #  | √âtape: [E1] | Source: [S0]  # #
_RE_ANY_CAT = re.compile(r"\(((?:cs|stat|eess)\.[A-Z]{2})\)")  # # Regex cat # # Respect: inclut cross-lists (stat.ML/eess.IV) | √âtape: [E1] | Source: [S0]  # #
_RE_ARXIV_ID = re.compile(r"/abs/([^?#/]+)")  # # Regex ID # # Respect: extraction stable | √âtape: [E1] | Source: [S8]  # #


def extract_categories_from_result(li: Tag) -> Tuple[str, List[str]]:  # # Lit cat√©gories # # Respect: filtrage th√©matique apr√®s search/cs | √âtape: [E1] | Source: [S8]  # #
    cats: List[str] = []  # # Init # # Respect: structuration | √âtape: [E1] | Source: [S0]  # #

    # (1) M√©thode robuste: tags visibles (quand arXiv rend des badges)
    for span in li.select("span.tag"):  # # Parcours tags # # Respect: extraction cibl√©e | √âtape: [E1] | Source: [S8]  # #
        t = (span.get_text(" ", strip=True) or "").strip()  # # Texte tag # # Respect: nettoyage | √âtape: [E1] | Source: [S0]  # #
        if re.fullmatch(r"(?:cs|stat|eess)\.[A-Z]{2}", t):  # # Si ressemble √† une cat # # Respect: filtrage fiable | √âtape: [E1] | Source: [S0]  # #
            cats.append(t)  # # Ajoute # # Respect: structuration | √âtape: [E1] | Source: [S0]  # #

    # (2) M√©thode fallback: regex sur la ligne "Subjects:  (cs.XX);  (stat.ML)"
    if not cats:  # # Si tags absents # # Respect: robustesse | √âtape: [E1] | Source: [S0]  # #
        txt = li.get_text(" ", strip=True)  # # Texte bloc (minimum) # # Respect: extraction juste pour cat | √âtape: [E1] | Source: [S8]  # #
        cats = _RE_ANY_CAT.findall(txt)  # # Extrait cats # # Respect: mapping demand√© | √âtape: [E1] | Source: [S0]  # #

    # D√©doublonnage en gardant l'ordre
    cats = list(dict.fromkeys(cats))  # # D√©doublonne # # Respect: sortie propre | √âtape: [E1] | Source: [S0]  # #
    primary = cats[0] if cats else ""  # # Premier # # Respect: structuration | √âtape: [E1] | Source: [S0]  # #
    return primary, cats  # # Retour # # Respect: sortie structur√©e | √âtape: [E1] | Source: [S1]  # #


# ============================================================  # #  | √âtape: [E1] | Source: [S0]  # #
# üßæ Parsing page search/cs -> items minimaux  # #  | √âtape: [E1] | Source: [S1]  # #
# ============================================================  # #  | √âtape: [E1] | Source: [S0]  # #
def parse_search_page(html: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:  # # Parse + diag # # Respect: robustesse | √âtape: [E2] | Source: [S6]  # #
    soup = BeautifulSoup(html, "lxml")  # # Parse # # Respect: extraction cibl√©e | √âtape: [E1] | Source: [S8]  # #
    page_title = soup.title.get_text(" ", strip=True) if soup.title else ""  # # Titre page # # Respect: diagnostic | √âtape: [E2] | Source: [S6]  # #
    weird = _detect_weird_page_signals(html)  # # D√©tection anti-bot/no-results # # Respect: robustesse | √âtape: [E2] | Source: [S6]  # #

    diag: Dict[str, Any] = {  # # Diagnostic # # Respect: tra√ßabilit√© | √âtape: [E2] | Source: [S6]  # #
        "page_title": page_title,  # # Titre # # Respect: debug | √âtape: [E1] | Source: [S0]  # #
        "has_abs_links": ("/abs/" in (html or "")),  # # Indicateur principal # # Respect: debug | √âtape: [E1] | Source: [S0]  # #
        **weird,  # # Drapeaux # # Respect: debug | √âtape: [E2] | Source: [S6]  # #
    }

    items: List[Dict[str, Any]] = []  # # R√©sultats # # Respect: sortie structur√©e | √âtape: [E1] | Source: [S1]  # #

    # S√©lecteur principal (arXiv actuel)
    result_nodes = soup.select("ol.breathe-horizontal li.arxiv-result")  # # Noeuds r√©sultats # # Respect: extraction cibl√©e | √âtape: [E1] | Source: [S8]  # #
    diag["selector_count_arxiv_result"] = len(result_nodes)  # # Compte # # Respect: debug | √âtape: [E2] | Source: [S6]  # #

    # Fallback: si DOM change, on reconstruit via liens /abs/
    if not result_nodes and diag["has_abs_links"]:  # # Pas de noeuds mais /abs/ pr√©sent # # Respect: robustesse HTML | √âtape: [E2] | Source: [S6]  # #
        diag["fallback_mode"] = "abs_links"  # # Indique fallback # # Respect: tra√ßabilit√© | √âtape: [E2] | Source: [S6]  # #
        abs_ids = _RE_ARXIV_ID.findall(html or "")  # # IDs # # Respect: extraction stable | √âtape: [E1] | Source: [S8]  # #
        abs_ids = list(dict.fromkeys(abs_ids))[:PAGE_SIZE]  # # D√©doublonne + limite # # Respect: contr√¥le volume | √âtape: [E1] | Source: [S0]  # #
        for arxiv_id in abs_ids:  # # Parcours # # Respect: sortie structur√©e | √âtape: [E1] | Source: [S1]  # #
            items.append({  # # Item minimal # # Respect: pas de HTML brut au LLM | √âtape: [E1] | Source: [S1]  # #
                "arxiv_id": arxiv_id,  # # ID # # Respect: identifiant | √âtape: [E1] | Source: [S0]  # #
                "title": "",  # # Vide # # Respect: minimal | √âtape: [E1] | Source: [S0]  # #
                "authors": [],  # # Vide # # Respect: minimal | √âtape: [E1] | Source: [S0]  # #
                "abstract": "",  # # Vide # # Respect: minimal | √âtape: [E1] | Source: [S0]  # #
                "submitted_date": "",  # # Vide # # Respect: minimal | √âtape: [E1] | Source: [S0]  # #
                "abs_url": abs_url(arxiv_id),  # # URL # # Respect: utile | √âtape: [E1] | Source: [S0]  # #
                "pdf_url": pdf_url(arxiv_id),  # # URL # # Respect: utile | √âtape: [E1] | Source: [S0]  # #
                "primary_category": "",  # # Vide # # Respect: filtrage possible plus tard | √âtape: [E1] | Source: [S0]  # #
                "all_categories": [],  # # Vide # # Respect: filtrage possible plus tard | √âtape: [E1] | Source: [S0]  # #
            })
        return items, diag  # # Retour # # Respect: robustesse | √âtape: [E2] | Source: [S6]  # #

    # Mode normal
    for li in result_nodes:  # # Parcours # # Respect: extraction minimale | √âtape: [E1] | Source: [S8]  # #
        title_el = li.select_one("p.title")  # # Titre # # Respect: champs essentiels | √âtape: [E1] | Source: [S0]  # #
        authors_el = li.select_one("p.authors")  # # Auteurs # # Respect: champs essentiels | √âtape: [E1] | Source: [S0]  # #
        abstract_el = li.select_one("span.abstract-full")  # # Abstract # # Respect: champs essentiels | √âtape: [E1] | Source: [S0]  # #
        submitted_el = li.select_one("p.is-size-7")  # # Date # # Respect: champs essentiels | √âtape: [E1] | Source: [S0]  # #

        abs_a = li.select_one('p.list-title a[href*="/abs/"]')  # # Lien abs # # Respect: stable | √âtape: [E1] | Source: [S0]  # #
        pdf_a = li.select_one('p.list-title a[href*="/pdf/"]')  # # Lien pdf # # Respect: utile | √âtape: [E1] | Source: [S0]  # #
        abs_href = normalize_url(abs_a.get("href") if abs_a else "")  # # Normalise # # Respect: sortie propre | √âtape: [E1] | Source: [S0]  # #
        pdf_href = normalize_url(pdf_a.get("href") if pdf_a else "")  # # Normalise # # Respect: sortie propre | √âtape: [E1] | Source: [S0]  # #

        arxiv_id = ""  # # Init # # Respect: structuration | √âtape: [E1] | Source: [S0]  # #
        m = re.search(r"/abs/([^?#/]+)", abs_href) if abs_href else None  # # Parse ID # # Respect: extraction pr√©cise | √âtape: [E1] | Source: [S8]  # #
        if m:  # # Si match # # Respect: robustesse | √âtape: [E1] | Source: [S0]  # #
            arxiv_id = m.group(1).strip()  # # ID # # Respect: structuration | √âtape: [E1] | Source: [S0]  # #

        title_txt = title_el.get_text(" ", strip=True) if title_el else ""  # # Titre # # Respect: extraction minimale | √âtape: [E1] | Source: [S8]  # #
        authors_txt = authors_el.get_text(" ", strip=True) if authors_el else ""  # # Auteurs # # Respect: extraction minimale | √âtape: [E1] | Source: [S8]  # #
        authors = [a.strip() for a in authors_txt.replace("Authors:", "").split(",") if a.strip()]  # # Liste # # Respect: sortie structur√©e | √âtape: [E1] | Source: [S1]  # #
        abstract = abstract_el.get_text(" ", strip=True) if abstract_el else ""  # # Abstract # # Respect: extraction minimale | √âtape: [E1] | Source: [S8]  # #
        abstract = abstract.replace("‚ñ≥ Less", "").strip()  # # Nettoyage # # Respect: r√©duire bruit | √âtape: [E1] | Source: [S0]  # #

        submitted_date = ""  # # Init # # Respect: structuration | √âtape: [E1] | Source: [S0]  # #
        if submitted_el:  # # Si pr√©sent # # Respect: robustesse | √âtape: [E1] | Source: [S0]  # #
            txt = submitted_el.get_text(" ", strip=True)  # # Texte # # Respect: extraction minimale | √âtape: [E1] | Source: [S8]  # #
            m3 = re.search(r"Submitted\s+(.+?)(?:;|$)", txt, flags=re.IGNORECASE)  # # Regex # # Respect: extraction cibl√©e | √âtape: [E1] | Source: [S8]  # #
            if m3:  # # Si match # # Respect: robustesse | √âtape: [E1] | Source: [S0]  # #
                submitted_date = m3.group(1).strip()  # # Date # # Respect: structuration | √âtape: [E1] | Source: [S0]  # #

        primary_cat, all_cats = extract_categories_from_result(li)  # # Cats # # Respect: filtrage th√©matique demand√© | √âtape: [E1] | Source: [S8]  # #

        if arxiv_id and is_empty(abs_href):  # # Fallback # # Respect: robustesse | √âtape: [E2] | Source: [S6]  # #
            abs_href = abs_url(arxiv_id)  # # Construit # # Respect: sortie utile | √âtape: [E1] | Source: [S0]  # #
        if arxiv_id and is_empty(pdf_href):  # # Fallback # # Respect: robustesse | √âtape: [E2] | Source: [S6]  # #
            pdf_href = pdf_url(arxiv_id)  # # Construit # # Respect: sortie utile | √âtape: [E1] | Source: [S0]  # #

        items.append({  # # Ajout item # # Respect: sortie structur√©e | √âtape: [E1] | Source: [S1]  # #
            "arxiv_id": arxiv_id,  # # ID # # Respect: minimal utile | √âtape: [E1] | Source: [S0]  # #
            "title": title_txt,  # # Title # # Respect: minimal utile | √âtape: [E1] | Source: [S0]  # #
            "authors": authors,  # # Authors # # Respect: minimal utile | √âtape: [E1] | Source: [S0]  # #
            "abstract": abstract,  # # Abstract
            "method": "",  # # Methode (FR: section "M√©thode") # # √âtape: [ROBUSTESSE_PARSING] | Source: [S8]  # #
            "references": [],  # # R√©f√©rences (FR: bibliographie) # # √âtape: [ROBUSTESSE_PARSING]# # # # Respect: minimal utile (pas HTML brut) | √âtape: [ROBUSTESSE_PARSING] | Source: [S8]  # #
            "submitted_date": submitted_date,  # # Date # # Respect: minimal utile | √âtape: [E1] | Source: [S0]  # #
            "abs_url": abs_href,  # # URL # # Respect: minimal utile | √âtape: [E1] | Source: [S0]  # #
            "pdf_url": pdf_href,  # # URL # # Respect: minimal utile | √âtape: [E1] | Source: [S0]  # #
            "primary_category": primary_cat,  # # Cat # # Respect: filtrage th√®mes | √âtape: [E1] | Source: [S0]  # #
            "all_categories": all_cats,  # # Cats # # Respect: filtrage th√®mes | √âtape: [E1] | Source: [S0]  # #
        })

    return items, diag  # # Retour # # Respect: robustesse + tra√ßabilit√© | √âtape: [E2] | Source: [S6]  # #


# ============================================================  # #  | √âtape: [E1] | Source: [S0]  # #
# üîé Parsing /abs (DOI + versions + abstract fallback)  # #  | √âtape: [E2] | Source: [S6]  # #
# ============================================================  # #  | √âtape: [E1] | Source: [S0]  # #
def parse_abs_page(abs_html: str) -> Dict[str, Any]:  # # Parse /abs # # Respect: enrichissement minimal seulement | √âtape: [E1] | Source: [S0]  # #
    soup = BeautifulSoup(abs_html, "lxml")  # # Parse # # Respect: extraction cibl√©e | √âtape: [E1] | Source: [S8]  # #
    out: Dict[str, Any] = {"doi": "", "versions": [], "last_updated_raw": "", "abstract": ""}  # # Init # # Respect: sortie structur√©e | √âtape: [E1] | Source: [S1]  # #

    doi_a = soup.select_one('td.tablecell.doi a[href*="doi.org"]')  # # DOI # # Respect: champ utile | √âtape: [E1] | Source: [S0]  # #
    if doi_a:  # # Si DOI # # Respect: robustesse | √âtape: [E1] | Source: [S0]  # #
        out["doi"] = doi_a.get_text(" ", strip=True)  # # Valeur # # Respect: extraction cibl√©e | √âtape: [E1] | Source: [S8]  # #

    abs_el = soup.select_one("blockquote.abstract")  # # Abstract # # Respect: essentiel | √âtape: [E1] | Source: [S0]  # #
    if abs_el:  # # Si pr√©sent # # Respect: robustesse | √âtape: [E1] | Source: [S0]  # #
        txt = abs_el.get_text(" ", strip=True)  # # Texte # # Respect: extraction cibl√©e | √âtape: [E1] | Source: [S8]  # #
        txt = re.sub(r"^\s*Abstract:\s*", "", txt, flags=re.IGNORECASE).strip()  # # Nettoyage # # Respect: r√©duire bruit | √âtape: [E1] | Source: [S0]  # #
        out["abstract"] = txt  # # Stocke # # Respect: sortie structur√©e | √âtape: [E1] | Source: [S1]  # #

    versions: List[Dict[str, str]] = []  # # Init # # Respect: structuration | √âtape: [E1] | Source: [S0]  # #
    for li in soup.select("div.submission-history li"):  # # Versions # # Respect: extraction utile | √âtape: [E1] | Source: [S8]  # #
        txt = li.get_text(" ", strip=True)  # # Texte # # Respect: extraction cibl√©e | √âtape: [E1] | Source: [S8]  # #
        m = re.search(r"\[(v\d+)\]\s*(.*)$", txt)  # # Parse # # Respect: extraction cibl√©e | √âtape: [E1] | Source: [S8]  # #
        if m:  # # Si match # # Respect: robustesse | √âtape: [E1] | Source: [S0]  # #
            versions.append({"version": m.group(1), "raw": m.group(2).strip()})  # # Ajoute # # Respect: sortie structur√©e | √âtape: [E1] | Source: [S1]  # #
    out["versions"] = versions  # # Stocke # # Respect: sortie structur√©e | √âtape: [E1] | Source: [S1]  # #
    out["last_updated_raw"] = versions[-1]["raw"] if versions else ""  # # Last # # Respect: champ utile | √âtape: [E1] | Source: [S0]  # #

    return out  # # Retour # # Respect: sortie structur√©e | √âtape: [E1] | Source: [S1]  # #


# ============================================================  # #  
# üß© Parsing /html arXiv : Method + References (cibl√©)           # # √âtape: [ROBUSTESSE_PARSING] | Source: [S8]  # #
# ============================================================  # #

def extract_html_url_from_abs(abs_html: str, arxiv_id: str) -> str:  # # Trouver le lien /html (si dispo) # # √âtape: [ROBUSTESSE_PARSING] | Source: [S8]  # #
    soup = BeautifulSoup(abs_html, "lxml")  # # Parser HTML # # √âtape: [ROBUSTESSE_PARSING] | Source: [S0]  # #
    a = soup.select_one('a[href^="/html/"], a[href*="/html/"]')  # # Cherche un lien HTML # # √âtape: [ROBUSTESSE_PARSING] | Source: [S0]  # #
    if not a:  # # Si aucun lien # # √âtape: [ROBUSTESSE_PARSING] | Source: [S0]  # #
        return ""  # # Pas de /html disponible # # √âtape: [ROBUSTESSE_PARSING] | Source: [S0]  # #
    href = (a.get("href") or "").strip()  # # R√©cup√®re href # # √âtape: [E1] | Source: [S1]  # #
    if not href:  # # Si vide # # √âtape: [ROBUSTESSE_PARSING] | Source: [S0]  # #
        return ""  # # Vide # # √âtape: [ROBUSTESSE_PARSING] | Source: [S0]  # #
    return normalize_url(href)  # # Normalise en URL absolue # # √âtape: [E1] | Source: [S1]  # #


def parse_arxiv_html_method_and_references(html: str) -> tuple[str, list[str]]:  # # Extraire 2 blocs (method+refs) # # √âtape: [E5] | Source: [S8]  # #
    soup = BeautifulSoup(html, "lxml")  # # Parser HTML # # √âtape: [ROBUSTESSE_PARSING] | Source: [S0]  # #

    method_text = ""  # # Texte Method # # √âtape: [E5] | Source: [S8]  # #
    references: list[str] = []  # # Liste refs # # √âtape: [E5] | Source: [S8]  # #

    # ‚úÖ R√©f√©rences : structure LaTeX HTML arXiv (biblist)         # # √âtape: [ROBUSTESSE_PARSING] | Source: [S6]  # #
    for li in soup.select("ol.ltx_biblist li, div.ltx_bibliography li"):  # # Liste bib # # √âtape: [ROBUSTESSE_PARSING] | Source: [S6]  # #
        t = li.get_text(" ", strip=True)  # # Texte ref # # √âtape: [E5] | Source: [S0]  # #
        if t:  # # Si non vide # # √âtape: [E5] | Source: [S0]  # #
            references.append(t)  # # Ajoute # # √âtape: [E5] | Source: [S8]  # #

    # ‚úÖ M√©thode : on cherche un titre qui ressemble √† ‚Äúmethod‚Äù     # # √âtape: [ROBUSTESSE_PARSING] | Source: [S8]  # #
    for sec in soup.select("section.ltx_section, div.ltx_section, section"):  # # Sections # # √âtape: [ROBUSTESSE_PARSING] | Source: [S6]  # #
        title_el = sec.select_one(".ltx_title, h1, h2, h3, h4")  # # Titre section # # √âtape: [ROBUSTESSE_PARSING] | Source: [S0]  # #
        if not title_el:  # # Pas de titre # # √âtape: [ROBUSTESSE_PARSING] | Source: [S6]  # #
            continue  # # Suivant
        title_txt = title_el.get_text(" ", strip=True).lower()  # # Normalise # # √âtape: [E1] | Source: [S0]  # #
        if any(k in title_txt for k in ["method", "methods", "methodology", "approach"]):  # # Match method # # √âtape: [ROBUSTESSE_PARSING] | Source: [S8]  # #
            # On r√©cup√®re le texte de la section sans le titre      # # √âtape: [E5] | Source: [S0]  # #
            tmp = sec.get_text(" ", strip=True)  # # Texte complet # # √âtape: [E5] | Source: [S0]  # #
            tmp = re.sub(r"^\s*" + re.escape(title_el.get_text(" ", strip=True)) + r"\s*", "", tmp, flags=re.IGNORECASE)  # # Retire titre # # √âtape: [E5] | Source: [S0]  # #
            method_text = tmp.strip()  # # Stocke # # √âtape: [E5] | Source: [S8]  # #
            break  # # Stop au premier match # # √âtape: [ROBUSTESSE_PARSING] | Source: [S0]  # #

    return method_text, references  # # Retour 2 blocs # # √âtape: [E1] | Source: [S1]  # #


# ============================================================  # #  | √âtape: [E1] | Source: [S0]  # #
# üß† Filtrage th√©matique (par cat√©gories + keywords)  # #  | √âtape: [E1] | Source: [S0]  # #
# ============================================================  # #  | √âtape: [E1] | Source: [S0]  # #
def _allowed_subcats_for_theme(theme: Optional[str]) -> List[str]:  # # Allowed cats # # Respect: scope strict | √âtape: [E1] | Source: [S0]  # #
    if theme and theme in THEME_TO_ARXIV_SUBCATS:  # # Si th√®me # # Respect: scope demand√© | √âtape: [E1] | Source: [S0]  # #
        return THEME_TO_ARXIV_SUBCATS[theme]  # # Retour liste # # Respect: p√©rim√®tre strict | √âtape: [E1] | Source: [S0]  # #
    return sorted({c for lst in THEME_TO_ARXIV_SUBCATS.values() for c in lst})  # # Union # # Respect: limit√© aux 6 th√®mes | √âtape: [E1] | Source: [S0]  # #


def _keyword_filter(items: List[Dict[str, Any]], theme: Optional[str]) -> List[Dict[str, Any]]:  # # Keyword fallback # # Respect: pertinence | √âtape: [E2] | Source: [S6]  # #
    if not theme or theme not in THEME_KEYWORDS:  # # Si pas de th√®me # # Respect: logique simple | √âtape: [E1] | Source: [S0]  # #
        return items  # # Pas de filtre # # Respect: ne pas inventer | √âtape: [E1] | Source: [S1]  # #
    kws = [k.lower() for k in THEME_KEYWORDS[theme]]  # # Lower # # Respect: robustesse | √âtape: [E1] | Source: [S0]  # #
    out: List[Dict[str, Any]] = []  # # Init # # Respect: sortie structur√©e | √âtape: [E1] | Source: [S1]  # #
    for it in items:  # # Parcours # # Respect: traitement contr√¥l√© | √âtape: [E1] | Source: [S1]  # #
        blob = ((it.get("title") or "") + " " + (it.get("abstract") or "")).lower()  # # Texte # # Respect: filtrage minimal | √âtape: [E1] | Source: [S0]  # #
        if any(k in blob for k in kws):  # # Match # # Respect: pertinence | √âtape: [E1] | Source: [S0]  # #
            out.append(it)  # # Ajoute # # Respect: sortie structur√©e | √âtape: [E1] | Source: [S1]  # #
    return out  # # Retour # # Respect: filtrage explicite | √âtape: [E1] | Source: [S0]  # #


def filter_items_by_subcats(items: List[Dict[str, Any]], allowed_subcats: List[str]) -> List[Dict[str, Any]]:  # # Filtre cat # # Respect: p√©rim√®tre | √âtape: [E1] | Source: [S1]  # #
    allowed = set(allowed_subcats)  # # Set # # Respect: performance | √âtape: [E1] | Source: [S0]  # #
    out: List[Dict[str, Any]] = []  # # Init # # Respect: sortie structur√©e | √âtape: [E1] | Source: [S1]  # #
    for it in items:  # # Parcours # # Respect: traitement contr√¥l√© | √âtape: [E1] | Source: [S1]  # #
        cats = it.get("all_categories") or []  # # Cats # # Respect: filtrage th√©matique | √âtape: [E1] | Source: [S0]  # #
        if not cats:  # # Si extraction rat√©e # # Respect: robustesse (√©viter faux n√©gatif) | √âtape: [E1] | Source: [S8]  # #
            out.append(it)  # # Conserver # # Respect: ne pas jeter sans preuve | √âtape: [E1] | Source: [S0]  # #
            continue  # # Next # # Respect: robustesse | √âtape: [E1] | Source: [S0]  # #
        if any(c in allowed for c in cats):  # # Match # # Respect: scope demand√© | √âtape: [E1] | Source: [S0]  # #
            out.append(it)  # # Garder # # Respect: p√©rim√®tre strict | √âtape: [E1] | Source: [S0]  # #
    return out  # # Retour # # Respect: filtrage explicite | √âtape: [E1] | Source: [S0]  # #


# ============================================================  # #  | √âtape: [E1] | Source: [S0]  # #
# ‚úÖ Fonction principale  # #  | √âtape: [E1] | Source: [S0]  # #
# ============================================================  # #  | √âtape: [E1] | Source: [S0]  # #
def scrape_arxiv_cs_scoped(
    user_query: str,  # # Query user # # Respect: besoin informationnel | √âtape: [E1] | Source: [S0]  # #
    theme: Optional[str] = None,  # # Th√®me # # Respect: scope demand√© | √âtape: [E1] | Source: [S0]  # #
    max_results: int = 20,  # # Limite # # Respect: pas massif | √âtape: [E1] | Source: [S0]  # #
    sort: str = "relevance",  # # Tri # # Respect: contr√¥le | √âtape: [E1] | Source: [S0]  # #
    polite_min_s: float = 1.2,  # # Politesse # # Respect: faible fr√©quence | √âtape: [E4] | Source: [S5]  # #
    polite_max_s: float = 2.0,  # # Politesse # # Respect: faible fr√©quence | √âtape: [E4] | Source: [S5]  # #
    data_lake_raw_dir: str = DEFAULT_RAW_DIR,  # # Cache # # Respect: √©crire dans raw/cache | √âtape: [E2] | Source: [S2]  # #
    enrich_abs: bool = True,  # # Enrich /abs # # Respect: utile (doi/versions) | √âtape: [E1] | Source: [S0]  # #
    enable_keyword_filter: bool = True,  # # Keyword fallback # # Respect: pertinence | √âtape: [E2] | Source: [S6]  # #
) -> Dict[str, Any]:  # # Retour structur√© # # Respect: sortie JSON | √âtape: [E1] | Source: [S1]  # #

    # ===============================  # #  | √âtape: [E1] | Source: [S0]  # #
    # üß± Pr√©paration param√®tres  # #  | √âtape: [E1] | Source: [S0]  # #
    # ===============================  # #  | √âtape: [E1] | Source: [S0]  # #
    max_results = int(max_results)  # # Cast # # Respect: robustesse | √âtape: [E1] | Source: [S0]  # #
    if max_results < 1:  # # Borne basse # # Respect: robustesse | √âtape: [E1] | Source: [S0]  # #
        max_results = 1  # # Fix # # Respect: robustesse | √âtape: [E1] | Source: [S0]  # #
    if max_results > MAX_RESULTS_HARD_LIMIT:  # # Cap # # Respect: pas massif | √âtape: [E1] | Source: [S0]  # #
        max_results = MAX_RESULTS_HARD_LIMIT  # # Fix # # Respect: pas aspirateur | √âtape: [E1] | Source: [S0]  # #

    if not os.path.isabs(data_lake_raw_dir):  # # Si relatif # # Respect: √©viter √©crire "ailleurs" | √âtape: [E2] | Source: [S2]  # #
        data_lake_raw_dir = os.path.abspath(os.path.join(PROJECT_ROOT, data_lake_raw_dir))  # # Base projet # # Respect: cache local attendu | √âtape: [E2] | Source: [S2]  # #

    ensure_dir(data_lake_raw_dir)  # # Dossier cache # # Respect: cache local visible | √âtape: [E2] | Source: [S2]  # #
    ts = now_iso_for_filename()  # # Timestamp # # Respect: tra√ßabilit√© | √âtape: [E1] | Source: [S0]  # #
    session = requests.Session()  # # Cr√©e session HTTP | √âtape: [E2] | Source: [S3]  # #

    errors_global: List[str] = []  # # Erreurs globales tool (contrat stable) | √âtape: [E1]# #  # # Session HTTP # # Respect: performance + robustesse | √âtape: [E1] | Source: [S1]  # #

    # ===============================  # #  | √âtape: [E1] | Source: [S0]  # #
    # üéØ Allowed categories  # #  | √âtape: [E1] | Source: [S0]  # #
    # ===============================  # #  | √âtape: [E1] | Source: [S0]  # #
    allowed_subcats = _allowed_subcats_for_theme(theme)  # # Liste cats # # Respect: p√©rim√®tre th√®mes | √âtape: [E1] | Source: [S0]  # #

    # ===============================  # #  | √âtape: [E1] | Source: [S0]  # #
    # üîé Pagination search/cs  # #  | √âtape: [E1] | Source: [S0]  # #
    # ===============================  # #  | √âtape: [E1] | Source: [S0]  # #
    collected: List[Dict[str, Any]] = []  # # Items bruts CS # # Respect: collecte contr√¥l√©e | √âtape: [E1] | Source: [S1]  # #
    bundle_parts: List[str] = []  # # HTML debug # # Respect: cache local (pas envoy√© au LLM) | √âtape: [E2] | Source: [S2]  # #
    start = 0  # # Pagination # # Respect: contr√¥le volume | √âtape: [E1] | Source: [S0]  # #
    last_search_url = ""  # # Debug # # Respect: tra√ßabilit√© | √âtape: [E1] | Source: [S0]  # #
    last_search_http: Optional[int] = None  # # Debug # # Respect: tra√ßabilit√© | √âtape: [E2] | Source: [S3]  # #
    diag_last: Dict[str, Any] = {}  # # Debug # # Respect: tra√ßabilit√© | √âtape: [E2] | Source: [S6]  # #
    anti_bot_or_weird_page = False  # # Flag # # Respect: transparence | √âtape: [E2] | Source: [S6]  # #

    while len(collected) < max_results:  # # Loop # # Respect: contr√¥le volume | √âtape: [E1] | Source: [S0]  # #
        search_url = build_search_url(query=user_query, start=start, size=PAGE_SIZE, sort=sort)  # # URL # # Respect: query simple | √âtape: [E1] | Source: [S0]  # #
        last_search_url = search_url  # # Trace # # Respect: debug | √âtape: [E1] | Source: [S0]  # #
        html, code = http_get_text(session=session, url=search_url, timeout_s=HTTP_TIMEOUT_S)  # # GET # # Respect: timeout+retry | √âtape: [E2] | Source: [S3]  # #
        last_search_http = code  # # Trace # # Respect: debug | √âtape: [E2] | Source: [S3]  # #

        weird = _detect_weird_page_signals(html)  # # Signaux # # Respect: diagnostiquer consent/robot/no-results | √âtape: [E2] | Source: [S6]  # #

        bundle_parts.append(f"<!-- SEARCH URL: {search_url} | HTTP {code} -->\n")  # # En-t√™te debug # # Respect: tra√ßabilit√© | √âtape: [E2] | Source: [S3]  # #
        bundle_parts.append(f"<!-- WEIRD: {json.dumps(weird)} -->\n")  # # Signaux debug # # Respect: tra√ßabilit√© | √âtape: [E2] | Source: [S6]  # #
        bundle_parts.append((html or "")[:200000])  # # Coupe 200k # # Respect: pas massif, cache debug local | √âtape: [E2] | Source: [S2]  # #
        bundle_parts.append("\n<!-- END SEARCH -->\n")  # # Fin bloc # # Respect: tra√ßabilit√© | √âtape: [E1] | Source: [S0]  # #

        if code != 200:  # # HTTP non-200 => erreur tool | √âtape: [E2] | Source: [S3]  # #
            errors_global.append(f"SEARCH_HTTP_{code}")  # # Log erreur globale | √âtape: [E2] | Source: [S0]  # #
            break  # # Stop # # Respect: ne pas boucler | √âtape: [E1] | Source: [S0]  # #
        if weird.get("contains_we_are_sorry") or weird.get("contains_robot") or weird.get("contains_consent"):  # # D√©tecte page anti-bot/consent (√©vite faux parsing) | √âtape: [E2] | Source: [S3]  # #
            anti_bot_or_weird_page = True  # # Marque page bizarre (pour diagnostic) | √âtape: [E2] | Source: [S6]  # #
            errors_global.append("ANTI_BOT_OR_WEIRD_PAGE")  # # Erreur globale tool (contrat stable) | √âtape: [E1] | Source: [S1]  # #
            break  # # Stop (on n'insiste pas) | √âtape: [E2] | Source: [S0]  # #

        page_items, diag = parse_search_page(html)  # # Parse # # Respect: extraction cibl√©e | √âtape: [E2] | Source: [S6]  # #
        diag_last = diag  # # Trace # # Respect: debug | √âtape: [E2] | Source: [S6]  # #

        if diag.get("contains_no_results"):  # # Aucun r√©sultat # # Respect: robustesse | √âtape: [E2] | Source: [S6]  # #
            break  # # Stop # # Respect: contr√¥le | √âtape: [E1] | Source: [S0]  # #
        if not page_items:  # # Aucun r√©sultat pars√© => diag + erreur soft | √âtape: [E2] | Source: [S0]  # #
            errors_global.append("NO_RESULTS_PARSED")  # # Indique parsing vide | √âtape: [E2] | Source: [S6]  # #
            break  # # Stop # # Respect: contr√¥le | √âtape: [E1] | Source: [S0]  # #

        collected.extend(page_items)  # # Ajoute # # Respect: collecte contr√¥l√©e | √âtape: [E1] | Source: [S1]  # #

        # ‚úÖ CORRECTION IMPORTANTE : si la page a < PAGE_SIZE r√©sultats, inutile d'aller √† start+50
        if len(page_items) < PAGE_SIZE:  # # Derni√®re page probable # # Respect: √©viter requ√™tes inutiles (et erreurs 500) | √âtape: [E1] | Source: [S1]  # #
            break  # # Stop # # Respect: fr√©quence raisonnable | √âtape: [E1] | Source: [S0]  # #

        start += PAGE_SIZE  # # Next page # # Respect: pagination contr√¥l√©e | √âtape: [E1] | Source: [S0]  # #
        sleep_polite(min_s=polite_min_s, max_s=polite_max_s)  # # Politesse # # Respect: √©viter spam | √âtape: [E4] | Source: [S5]  # #

    collected = collected[:max_results]  # # Tronque # # Respect: limite demand√©e | √âtape: [E1] | Source: [S0]  # #

    # ===============================  # #  | √âtape: [E1] | Source: [S0]  # #
    # üßπ Filtrage par cat√©gories  # #  | √âtape: [E1] | Source: [S0]  # #
    # ===============================  # #  | √âtape: [E1] | Source: [S0]  # #
    filtered = filter_items_by_subcats(collected, allowed_subcats=allowed_subcats)  # # Filtre cats # # Respect: scope demand√© | √âtape: [E1] | Source: [S1]  # #
    if enable_keyword_filter:  # # Si activ√© # # Respect: pertinence | √âtape: [E1] | Source: [S0]  # #
        filtered = _keyword_filter(filtered, theme=theme)  # # Filtre mots-cl√©s # # Respect: fallback si cats manquent | √âtape: [E2] | Source: [S6]  # #

    # ===============================  # #  | √âtape: [E1] | Source: [S0]  # #
    # üîé Enrich /abs  # #  | √âtape: [E1] | Source: [S0]  # #
    # ===============================  # #  | √âtape: [E1] | Source: [S0]  # #
    if enrich_abs:  # # Si enrich # # Respect: enrichissement minimal | √âtape: [E1] | Source: [S0]  # #
        for it in filtered:  # # Parcours # # Respect: traitement contr√¥l√© | √âtape: [E1] | Source: [S0]  # #
            it["doi"] = ""  # # Init # # Respect: champs stables | √âtape: [E1] | Source: [S0]  # #
            it["versions"] = []  # # Init # # Respect: champs stables | √âtape: [E1] | Source: [S0]  # #
            it["last_updated_raw"] = ""
            it["method"] = ""  # # M√©thode # # √âtape: [ROBUSTESSE_PARSING] | Source: [S8]  # #
            it["references"] = []  # # R√©f√©rences # # √âtape: [ROBUSTESSE_PARSING]# #  # # Init # # Respect: champs stables | √âtape: [ROBUSTESSE_PARSING] | Source: [S8]  # #
            it["errors"] = []  # # Init # # Respect: sortie structur√©e | √âtape: [E1] | Source: [S1]  # #

            url_abs = it.get("abs_url") or ""  # # URL # # Respect: utile | √âtape: [E1] | Source: [S0]  # #
            if not url_abs:  # # Si absent # # Respect: robustesse | √âtape: [E1] | Source: [S0]  # #
                continue  # # Skip # # Respect: robustesse | √âtape: [E1] | Source: [S0]  # #

            abs_html, abs_code = http_get_text(session=session, url=url_abs, timeout_s=HTTP_TIMEOUT_S)  # # GET /abs # # Respect: timeout+retry | √âtape: [E2] | Source: [S3]  # #
            bundle_parts.append(f"<!-- ABS URL: {url_abs} | HTTP {abs_code} -->\n")  # # Debug # # Respect: tra√ßabilit√© | √âtape: [E2] | Source: [S3]  # #
            bundle_parts.append((abs_html or "")[:200000])  # # Coupe # # Respect: pas massif | √âtape: [E1] | Source: [S0]  # #
            bundle_parts.append("\n<!-- END ABS -->\n")  # # Fin # # Respect: tra√ßabilit√© | √âtape: [E1] | Source: [S0]  # #

            if abs_code == 200:  # # OK # # Respect: robustesse | √âtape: [E1] | Source: [S0]  # #
                abs_data = parse_abs_page(abs_html)  # # Parse # # Respect: extraction cibl√©e | √âtape: [E1] | Source: [S8]  # #
                it["doi"] = abs_data.get("doi", "")  # # DOI # # Respect: champ utile | √âtape: [E1] | Source: [S0]  # #
                it["versions"] = abs_data.get("versions", [])  # # Versions # # Respect: champ utile | √âtape: [E1] | Source: [S0]  # #
                it["last_updated_raw"] = abs_data.get("last_updated_raw", "")  # # Last
                html_url = extract_html_url_from_abs(abs_html=abs_html, arxiv_id=it.get("arxiv_id", ""))  # # Cherche lien /html # # √âtape: [ROBUSTESSE_PARSING] | Source: [S8]  # #
                if html_url:  # # Si /html existe # # √âtape: [ROBUSTESSE_PARSING] | Source: [S6]  # #
                    html_full, html_code = http_get_text(session=session, url=html_url, timeout_s=30)  # # GET /html # # √âtape: [ROBUSTESSE_PARSING] | Source: [S2]  # #
                    bundle_parts.append(f"<!-- HTML URL: {html_url} | HTTP {html_code} -->\n")  # # Trace # # √âtape: [ROBUSTESSE_PARSING] | Source: [S0]  # #
                    bundle_parts.append(html_full[:200000])  # # Cache debug # # √âtape: [ROBUSTESSE_PARSING] | Source: [S2]  # #
                    bundle_parts.append("\n<!-- END HTML -->\n")  # # Fin # # √âtape: [ROBUSTESSE_PARSING] | Source: [S2]  # #
                    if html_code == 200:  # # OK # # √âtape: [ROBUSTESSE_PARSING] | Source: [S6]  # #
                        method_txt, refs_list = parse_arxiv_html_method_and_references(html_full)  # # Parse sections # # √âtape: [ROBUSTESSE_PARSING] | Source: [S8]  # #
                        if method_txt:  # # Si trouv√© # # √âtape: [ROBUSTESSE_PARSING] | Source: [S8]  # #
                            it["method"] = method_txt  # # Stocke # # √âtape: [E1] | Source: [S1]  # #
                        if refs_list:  # # Si trouv√© # # √âtape: [ROBUSTESSE_PARSING] | Source: [S0]  # #
                            it["references"] = refs_list  # # Stocke # # √âtape: [E1] | Source: [S1]  # #
                    else:  # # KO # # √âtape: [ROBUSTESSE_PARSING] | Source: [S0]  # #
                        it["errors"].append(f"html_http_{html_code}")  # # Trace # # √âtape: [ROBUSTESSE_PARSING] | Source: [S0]  # #
 # # Respect: champ utile | √âtape: [E1] | Source: [S0]  # #
                if is_empty(it.get("abstract")) and not is_empty(abs_data.get("abstract")):  # # Fallback abstract # # Respect: compl√©ter sans bruit | √âtape: [E2] | Source: [S6]  # #
                    it["abstract"] = abs_data.get("abstract", "")  # # Inject # # Respect: qualit√© | √âtape: [E1] | Source: [S0]  # #
            else:  # # KO # # Respect: robustesse | √âtape: [E1] | Source: [S0]  # #
                it["errors"].append(f"abs_http_{abs_code}")  # # Trace # # Respect: diagnostic | √âtape: [E2] | Source: [S6]  # #

            sleep_polite(min_s=polite_min_s, max_s=polite_max_s)  # # Politesse # # Respect: √©viter spam | √âtape: [E4] | Source: [S5]  # #

    # Missing fields
    for it in filtered:  # # Parcours # # Respect: diagnostic qualit√© | √âtape: [E2] | Source: [S6]  # #
        it["missing_fields"] = compute_missing_fields(it)  # # Ajoute # # Respect: sortie structur√©e + debug | √âtape: [E1] | Source: [S1]  # #

    # ===============================  # #  | √âtape: [E1] | Source: [S0]  # #
    # üíæ Sauvegardes cache raw  # #  | √âtape: [E2] | Source: [S2]  # #
    # ===============================  # #  | √âtape: [E1] | Source: [S0]  # #
    bundle_name = f"scrape_arxiv_cs_bundle_{ts}.html"  # # Nom # # Respect: cache debug | √âtape: [E2] | Source: [S2]  # #
    bundle_path = save_text_file(data_lake_raw_dir, bundle_name, "\n".join(bundle_parts))  # # Save # # Respect: cache local visible | √âtape: [E2] | Source: [S2]  # #

    result: Dict[str, Any] = {  # # R√©sultat # # Respect: sortie JSON structur√©e | √âtape: [E1] | Source: [S1]  # #
        "ok": (len(errors_global) == 0),  # # OK seulement si aucune erreur globale | √âtape: [E1]# #  # # Statut # # Respect: API stable | √âtape: [E1] | Source: [S0]  # #
        "user_query": user_query,  # # Query # # Respect: tra√ßabilit√© | √âtape: [E1] | Source: [S0]  # #
        "theme": theme,  # # Th√®me # # Respect: tra√ßabilit√© | √âtape: [E1] | Source: [S0]  # #
        "allowed_subcats": allowed_subcats,  # # P√©rim√®tre # # Respect: scope explicite | √âtape: [E1] | Source: [S0]  # #
        "sort": sort,  # # Tri # # Respect: contr√¥le | √âtape: [E1] | Source: [S0]  # #
        "requested_max_results": max_results,  # # Limite # # Respect: pas massif | √âtape: [E1] | Source: [S0]  # #
        "count_collected_cs": len(collected),  # # Collecte # # Respect: debug | √âtape: [E1] | Source: [S0]  # #
        "count_after_theme_filter": len(filtered),  # # Apr√®s filtre # # Respect: debug | √âtape: [E1] | Source: [S0]  # #
        "items": filtered,  # # Items # # Respect: sortie structur√©e | √âtape: [E1] | Source: [S1]  # #
        "bundle_html_file": bundle_path,  # # HTML debug # # Respect: cache local (pas LLM) | √âtape: [E2] | Source: [S2]  # #
        "supported_fields": SUPPORTED_FIELDS,  # # Sch√©ma # # Respect: contrat clair | √âtape: [E1] | Source: [S1]  # #
        # Debug important
        "project_root": PROJECT_ROOT,  # # O√π est le projet # # Respect: tra√ßabilit√© | √âtape: [E1] | Source: [S0]  # #
        "raw_cache_dir": data_lake_raw_dir,  # # O√π √©crit-on # # Respect: visibilit√© | √âtape: [E2] | Source: [S2]  # #
        "cwd_runtime": os.getcwd(),  # # CWD # # Respect: debug uvicorn | √âtape: [E1] | Source: [S0]  # #
        "last_search_url": last_search_url,  # # Derni√®re URL # # Respect: debug | √âtape: [E1] | Source: [S0]  # #
        "last_search_http": last_search_http,  # # Dernier code HTTP search (0 = erreur r√©seau) | √âtape: [E2]# #  # # Dernier HTTP # # Respect: debug | √âtape: [E2] | Source: [S3]  # #
        "parse_diag_last": diag_last,  # # Dernier diag # # Respect: debug | √âtape: [E2] | Source: [S6]  # #
        "anti_bot_or_weird_page": anti_bot_or_weird_page,  # # Flag # # Respect: transparence | √âtape: [E2] | Source: [S6]  # #
    }  # # Fin result # # Respect: JSON propre | √âtape: [E1] | Source: [S1]  # #

    json_name = f"scrape_arxiv_cs_{ts}.json"  # # Nom json # # Respect: cache r√©sultat | √âtape: [E2] | Source: [S2]  # #
    json_path = os.path.join(data_lake_raw_dir, json_name)  # # Path # # Respect: cache local | √âtape: [E2] | Source: [S2]  # #
    with open(json_path, "w", encoding="utf-8") as f:  # # Open # # Respect: robustesse encodage | √âtape: [E1] | Source: [S1]  # #
        json.dump(result, f, ensure_ascii=False, indent=2)  # # Dump # # Respect: sortie structur√©e | √âtape: [E1] | Source: [S1]  # #

    result["saved_to"] = json_path  # # Chemin # # Respect: retrouver facilement le fichier | √âtape: [E1] | Source: [S1]  # #
    return result  # # Retour # # Respect: contrat clair | √âtape: [E1] | Source: [S0]  # #


# ============================================================  # #  | √âtape: [E1] | Source: [S0]  # #
# ‚úÖ Alias compatibilit√© avec ton main.py  # #  | √âtape: [E1] | Source: [S0]  # #
# ============================================================  # #  | √âtape: [E1] | Source: [S0]  # #
def scrape_arxiv_cs(  # # Alias # # Respect: ne pas casser ton main.py existant | √âtape: [E1] | Source: [S0]  # #
    query: str,  # # Query # # Respect: input simple | √âtape: [E1] | Source: [S0]  # #
    max_results: int = 50,  # # Limite # # Respect: contr√¥le volume | √âtape: [E1] | Source: [S0]  # #
    sort: str = "relevance",  # # Tri # # Respect: contr√¥le | √âtape: [E1] | Source: [S0]  # #
    polite_min_s: float = 1.2,  # # Politesse # # Respect: fr√©quence raisonnable | √âtape: [E4] | Source: [S5]  # #
    polite_max_s: float = 2.0,  # # Politesse # # Respect: fr√©quence raisonnable | √âtape: [E4] | Source: [S5]  # #
    data_lake_raw_dir: str = DEFAULT_RAW_DIR,  # # Cache # # Respect: √©crit dans raw/cache | √âtape: [E2] | Source: [S2]  # #
    theme: Optional[str] = None,  # # Th√®me # # Respect: scope | √âtape: [E1] | Source: [S0]  # #
) -> Dict[str, Any]:  # # Retour structur√© # # Respect: JSON | √âtape: [E1] | Source: [S1]  # #
    return scrape_arxiv_cs_scoped(  # # Forward # # Respect: point d'entr√©e unique | √âtape: [E1] | Source: [S0]  # #
        user_query=query,  # # Map # # Respect: coh√©rence | √âtape: [E1] | Source: [S0]  # #
        theme=theme,  # # Map # # Respect: coh√©rence | √âtape: [E1] | Source: [S0]  # #
        max_results=max_results,  # # Map # # Respect: coh√©rence | √âtape: [E1] | Source: [S0]  # #
        sort=sort,  # # Map # # Respect: coh√©rence | √âtape: [E1] | Source: [S0]  # #
        polite_min_s=polite_min_s,  # # Map # # Respect: coh√©rence | √âtape: [E4] | Source: [S5]  # #
        polite_max_s=polite_max_s,  # # Map # # Respect: coh√©rence | √âtape: [E4] | Source: [S5]  # #
        data_lake_raw_dir=data_lake_raw_dir,  # # Map # # Respect: coh√©rence | √âtape: [E2] | Source: [S2]  # #
        enrich_abs=True,  # # On enrichit # # Respect: utile (doi/versions/abstract) | √âtape: [E1] | Source: [S0]  # #
        enable_keyword_filter=True,  # # On garde fallback # # Respect: √©vite faux n√©gatifs | √âtape: [E2] | Source: [S6]  # #
    )


# ============================================================  # #  | √âtape: [E1] | Source: [S0]  # #
# ‚úÖ TEST LOCAL  # #  | √âtape: [E1] | Source: [S0]  # #
# ============================================================  # #  | √âtape: [E1] | Source: [S0]  # #
RUN_LOCAL_TEST = True  # # True = test ON # # Respect: debug local sans FastAPI | √âtape: [E1] | Source: [S0]  # #

if __name__ == "__main__" and RUN_LOCAL_TEST:  # # Entry # # Respect: ex√©cution locale ma√Ætris√©e | √âtape: [E2] | Source: [S3]  # #
    res = scrape_arxiv_cs_scoped(  # # Run # # Respect: test contr√¥l√© | √âtape: [E1] | Source: [S0]  # #
        user_query="multimodal transformer misogyny detection",  # # Exemple # # Respect: besoin informationnel | √âtape: [E1] | Source: [S0]  # #
        theme="ai_ml",  # # Th√®me # # Respect: p√©rim√®tre demand√© | √âtape: [E1] | Source: [S0]  # #
        max_results=5,  # # Limite # # Respect: pas massif | √âtape: [E1] | Source: [S0]  # #
        sort="relevance",  # # Tri # # Respect: contr√¥le | √âtape: [E1] | Source: [S0]  # #
        data_lake_raw_dir=DEFAULT_RAW_DIR,  # # Cache # # Respect: √©crit au bon endroit | √âtape: [E2] | Source: [S2]  # #
        enrich_abs=True,  # # Enrich # # Respect: utile | √âtape: [E1] | Source: [S0]  # #
    )  # # Fin # # Respect: test | √âtape: [E1] | Source: [S0]  # #
    print(json.dumps({  # # Print # # Respect: debug lisible | √âtape: [E1] | Source: [S1]  # #
        "count_collected_cs": res.get("count_collected_cs"),  # # Info # # Respect: debug | √âtape: [E1] | Source: [S0]  # #
        "count_after_theme_filter": res.get("count_after_theme_filter"),  # # Info # # Respect: debug | √âtape: [E1] | Source: [S0]  # #
        "saved_to": res.get("saved_to"),  # # Info # # Respect: retrouver JSON | √âtape: [E1] | Source: [S1]  # #
        "bundle_html_file": res.get("bundle_html_file"),  # # Info # # Respect: retrouver HTML | √âtape: [E1] | Source: [S0]  # #
        "anti_bot_or_weird_page": res.get("anti_bot_or_weird_page"),  # # Info # # Respect: transparence | √âtape: [E2] | Source: [S6]  # #
        "last_search_http": res.get("last_search_http"),  # # Info # # Respect: debug | √âtape: [E2] | Source: [S3]  # #
        "parse_diag_last": res.get("parse_diag_last"),  # # Info # # Respect: debug | √âtape: [E2] | Source: [S6]  # #
    }, ensure_ascii=False, indent=2))  # # Pretty # # Respect: lecture facile | √âtape: [E1] | Source: [S0]  # #