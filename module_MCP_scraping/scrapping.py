# ===============================  # #
# ğŸ”¢ RÃ‰FÃ‰RENTIEL (Ã‰tapes + Sources)  # #
# ===============================  # #
# âœ… But (1 phrase, simple) : on rend la sortie du tool prÃ©visible et on Ã©vite que tout casse si arXiv change un dÃ©tail.  # #
#
# ğŸ“Œ Ã‰tapes (codes Ã  rÃ©utiliser dans les hashtags, pour Ã©viter la redondance)
# [E1] NORMALISATION/DECOUPLAGE : transformer/standardiser les donnÃ©es (format stable) pour que FastAPI lise sans surprise.  # #
# [E2] ROBUSTESSE : continuer Ã  fonctionner malgrÃ© HTML qui change, pages bizarres, ou rÃ©sultats manquants (fallback + diag).  # #
# [E3] GESTION_ERREUR : capturer les erreurs (HTTP/timeouts/exceptions) et retourner ok=False + errors[] au lieu de crasher.  # #
# [E4] SCRAPING_ETHIQUE : limiter la frÃ©quence, mettre un User-Agent, ne pas spammer le site (politesse).  # #
# [E5] STRUCTURATION : prÃ©parer un contexte/sections propres (ex: method/references) sans envoyer du HTML brut au LLM.  # #
# [E6] TOOL : exÃ©cution du scraping en tant qu'outil externe (appelÃ© par l'orchestrateur).  # #
#
# ğŸ“š Sources prof (codes)
# [S1] Guide_Scraping_HTML_Python_IA_BOT (1).pdf â€” extrait : Â« Toujours produire une sortie structurÃ©e (JSON) Â».  # #
# [S2] Guide_Scraping_HTML_Python_IA_BOT (1).pdf â€” extrait : Â« Mettre en cache les rÃ©sultats Â».  # #
# [S3] Guide_Scraping_HTML_Python_IA_BOT (1).pdf â€” extrait : Â« GÃ©rer les erreurs (try/except, timeouts) Â».  # #
# [S4] Guide_Scraping_HTML_Python_IA_BOT (1).pdf â€” extrait : Â« Toujours dÃ©finir un User-Agent Â».  # #
# [S5] Kick-Off-IA-Bot-Agent-Conversationnel-Intelligent.pdf â€” extrait : Â« Scrappez Ã  faible frÃ©quence. Ã‰vitez absolument le spam de requÃªtes Â».  # #
# [S6] Kick-Off-IA-Bot-Agent-Conversationnel-Intelligent.pdf â€” extrait : Â« Gestion des erreurs : ... stratÃ©gies de fallback. Â».  # #
# [S7] Projet-IA-BOT-Concevoir-un-agent-pas-un-simple-chatbot (1).pdf â€” extrait : Â« Pas de gestion d'erreur ... timeouts. Un agent robuste gÃ¨re l'incertitude Â».  # #
# [S8] Projet-IA-BOT-Concevoir-un-agent-pas-un-simple-chatbot (1).pdf â€” extrait : Â« Chaque tool a un contrat clair : inputs structurÃ©s et outputs normalisÃ©s Â».  # #
#
# ğŸ§¾ Ã€ propos des â€œnumÃ©ros dâ€™erreurâ€ (ce que Ã§a veut dire)
# - 200 = OK (la page a Ã©tÃ© rÃ©cupÃ©rÃ©e)  # #
# - 429 = Too Many Requests (le site te â€œrate-limitâ€, donc on ralentit / on retry)  # #
# - 500/502/503/504 = erreurs serveur (souvent temporaires, on peut retry)  # #
# - codes internes ex: "http_429_search" = notre libellÃ© lisible : "type dâ€™erreur" + "oÃ¹" (search/abs/html).  # #

# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
# Scraper arXiv CS (ciblÃ© thÃ©matique + sortie structurÃ©e)  # #  | Ã‰tape: [E1] | Source: [S1]  # #
# Objectif :  # #  | Ã‰tape: [E1] | Source: [S0]  # #
# - Scraping ciblÃ© sur les thÃ¨mes demandÃ©s (pas "aspirateur")  # #  | Ã‰tape: [E1] | Source: [S0]  # #
# - Sortie JSON structurÃ©e (pas de HTML brut envoyÃ© au LLM)  # #  | Ã‰tape: [E1] | Source: [S1]  # #
# - Extraction minimale : title/authors/abstract/dates/urls/doi  # #  | Ã‰tape: [E1] | Source: [S8]  # #
# - Cache + politesse + robustesse  # #  | Ã‰tape: [E4] | Source: [S2]  # #
#   => on cherche via /search/cs puis on filtre via Subjects  # #  | Ã‰tape: [E1] | Source: [S0]  # #
# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # #

# ===============================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
# ğŸ“š Importations  # #  | Ã‰tape: [E1] | Source: [S0]  # #
# ===============================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
import os  # # Gestion chemins/dossiers # # Respect: cache local stable (sortie disque attendue) | Ã‰tape: [E2] | Source: [S2]  # #
import re  # # Regex parsing IDs + catÃ©gories # # Respect: extraction ciblÃ©e (pas "tout le texte") | Ã‰tape: [E1] | Source: [S8]  # #
import json  # # Export JSON # # Respect: sortie structurÃ©e JSON | Ã‰tape: [E1] | Source: [S1]  # #
import time  # # Politesse (sleep) # # Respect: Ã©viter spam requÃªtes | Ã‰tape: [E4] | Source: [S5]  # #
import random  # # Jitter # # Respect: frÃ©quence raisonnable | Ã‰tape: [E1] | Source: [S0]  # #
import datetime  # # Timestamp fichiers # # Respect: traÃ§abilitÃ© des fichiers | Ã‰tape: [E1] | Source: [S0]  # #
from typing import Dict, Any, List, Tuple, Optional  # # Typage # # Respect: tool prÃ©visible | Ã‰tape: [E1] | Source: [S0]  # #

import requests  # # HTTP GET # # Respect: scraping HTML public | Ã‰tape: [E2] | Source: [S3]  # #
from bs4 import BeautifulSoup, Tag  # # Parser HTML # # Respect: extraction ciblÃ©e d'Ã©lÃ©ments utiles | Ã‰tape: [E1] | Source: [S8]  # #


# ===============================  # #  | Ã‰tape: [E1] | Source: [S0] 
# ğŸ“Œ RÃ©solution robuste des chemins  
# ===============================  

def _find_project_root(start_dir: str) -> str:  # # DÃ©finit une fonction qui retrouve la racine du projet Ã  partir dâ€™un dossier de dÃ©part (Ã©vite d'Ã©crire les fichiers au mauvais endroit) | Ã‰tape: [E1] | Source: [S0]  # #
    cur = os.path.abspath(start_dir)  # # Convertit start_dir en chemin absolu : abspath transforme un chemin relatif en chemin complet utilisable partout | Ã‰tape: [E1] | Source: [S0]  # #
    while True:  # # Lance une boucle infinie pour remonter dossier par dossier jusquâ€™Ã  trouver un â€œmarqueurâ€ de racine | Ã‰tape: [E1] | Source: [S0]  # #
        if os.path.isdir(os.path.join(cur, "data_lake")):  # # VÃ©rifie si le dossier "data_lake" existe ici : join construit le chemin cur/data_lake, isdir confirme que câ€™est un dossier | Ã‰tape: [E2] | Source: [S2]  # #
            return cur  # # Stoppe la fonction et renvoie cur : return termine la fonction immÃ©diatement avec la racine trouvÃ©e | Ã‰tape: [E1] | Source: [S0]  # #
        if os.path.isfile(os.path.join(cur, "pyproject.toml")):  # # VÃ©rifie si "pyproject.toml" existe ici : isfile teste la prÃ©sence dâ€™un fichier marqueur de projet | Ã‰tape: [E1] | Source: [S0]  # #
            return cur  # # Renvoie cur comme racine dÃ¨s quâ€™un marqueur de projet est trouvÃ© (sortie immÃ©diate) | Ã‰tape: [E1] | Source: [S0]  # #
        if os.path.isfile(os.path.join(cur, "requirements.txt")):  # # VÃ©rifie si "requirements.txt" existe : isfile confirme quâ€™on est probablement Ã  la racine (dÃ©pendances Python) | Ã‰tape: [E1] | Source: [S0]  # #
            return cur  # # Renvoie cur comme racine si requirements.txt est trouvÃ© (sortie immÃ©diate) | Ã‰tape: [E1] | Source: [S0]  # #
        parent = os.path.dirname(cur)  # # Calcule le dossier parent : dirname enlÃ¨ve le dernier segment du chemin (remonte dâ€™un niveau) | Ã‰tape: [E1] | Source: [S0]  # #
        if parent == cur:  # # Teste si on est arrivÃ© tout en haut (plus possible de remonter) : parent==cur signifie â€œracine disque atteinteâ€ | Ã‰tape: [E1] | Source: [S0]  # #
            return os.path.abspath(start_dir)  # # Fallback : renvoie le chemin absolu du start_dir (abspath le rend stable mÃªme si relatif) | Ã‰tape: [E2] | Source: [S6]  # #
        cur = parent  # # Met Ã  jour cur avec le parent pour continuer la remontÃ©e dans la boucle | Ã‰tape: [E1] | Source: [S0]  # #



# ===============================  # #  | Ã‰tape: [E1] | Source: [S0] 
# ğŸ§­ Constantes arXiv 
# ===============================  

ARXIV_BASE = "https://arxiv.org"  # # DÃ©finit lâ€™URL de base dâ€™arXiv : on la rÃ©utilise pour construire toutes les autres URLs sans les rÃ©Ã©crire | Ã‰tape: [E2] | Source: [S3]  # #
ARXIV_SEARCH_CS = f"{ARXIV_BASE}/search/cs"  # # Construit lâ€™URL du endpoint de recherche Computer Science : f-string insÃ¨re ARXIV_BASE dans la chaÃ®ne | Ã‰tape: [E1] | Source: [S0]  # #

_THIS_FILE_DIR = os.path.dirname(os.path.abspath(__file__))  # # RÃ©cupÃ¨re le dossier rÃ©el du fichier Python : abspath calcule le chemin complet de __file__, dirname garde seulement le dossier | Ã‰tape: [E1] | Source: [S0]  # #
PROJECT_ROOT = _find_project_root(_THIS_FILE_DIR)  # # Appelle la fonction de remontÃ©e pour trouver la racine du projet (Ã©vite dâ€™Ã©crire dans le mauvais dossier si le CWD change) | Ã‰tape: [E1] | Source: [S0]  # #
DEFAULT_RAW_DIR = os.path.join(PROJECT_ROOT, "data_lake", "raw", "cache")  # # Construit le chemin du dossier de cache raw : join assemble proprement les segments (compatible Windows) | Ã‰tape: [E2] | Source: [S2]  # #

MAX_RESULTS_HARD_LIMIT = 100  # # Fixe un plafond dur du nombre de rÃ©sultats pour Ã©viter un scraping trop massif (sÃ©curitÃ©/performance) | Ã‰tape: [E1] | Source: [S0]  # #
PAGE_SIZE = 50  # # Fixe la taille dâ€™une page arXiv Ã  50 : sert Ã  paginer proprement sans dÃ©passer la limite attendue | Ã‰tape: [E1] | Source: [S0]  # #


# ===============================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
# ğŸ§¯ Robustesse HTTP 
# ===============================
HTTP_RETRY_STATUS = {429, 500, 502, 503, 504}  # # Codes Ã  retry # # Respect: agent robuste (ne pas casser au 1er incident) | Ã‰tape: [E2] | Source: [S3]  # #
HTTP_RETRY_MAX = 2  # # Nombre de retries/reessai # # Respect: frÃ©quence raisonnable (pas de spam) | Ã‰tape: [E2] | Source: [S3]  # #
HTTP_TIMEOUT_S = 30  # # Timeout # # Respect: robustesse (Ã©vite blocage) | Ã‰tape: [E2] | Source: [S3]  # #


# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # # DÃ©but de bloc â€œgrand titreâ€ : sert juste de repÃ¨re visuel dans le fichier
# ğŸ¯ ThÃ¨mes demandÃ©s -> sous-catÃ©gories arXiv autorisÃ©es  # #  | Ã‰tape: [E1] | Source: [S0]  # # Annonce que lâ€™on va dÃ©finir la table qui relie un thÃ¨me (ex: ai_ml) aux catÃ©gories arXiv autorisÃ©es
# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # # Fin de lâ€™en-tÃªte â€œgrand titreâ€ (lisibilitÃ©)

THEME_TO_ARXIV_SUBCATS: Dict[str, List[str]] = {  # # CrÃ©e un dictionnaire typÃ© (clÃ©=thÃ¨me, valeur=liste de catÃ©gories) : permet de filtrer les rÃ©sultats arXiv selon le thÃ¨me choisi | Ã‰tape: [E1] | Source: [S0]  # #
    "ai_ml": [  # # Liste des catÃ©gories autorisÃ©es pour le thÃ¨me IA/ML : on regroupe ici les sujets arXiv qui couvrent LLM, vision, multimodal, agents | Ã‰tape: [E1] | Source: [S0]  # #
        "cs.AI",    # # CatÃ©gorie arXiv â€œArtificial Intelligenceâ€ : correspond aux papiers IA/agents/raisonnement | Ã‰tape: [E1] | Source: [S0]  # #
        "cs.LG",    # # CatÃ©gorie â€œMachine Learning (CS)â€ : correspond aux papiers ML cÃ´tÃ© informatique | Ã‰tape: [E1] | Source: [S0]  # #
        "cs.CL",    # # CatÃ©gorie â€œComputation and Languageâ€ : correspond aux papiers NLP/LLM/traitement du langage | Ã‰tape: [E1] | Source: [S0]  # #
        "cs.CV",    # # CatÃ©gorie â€œComputer Visionâ€ : correspond aux papiers vision / image / multimodal | Ã‰tape: [E1] | Source: [S0]  # #
        "cs.MA",    # # CatÃ©gorie â€œMultiagent Systemsâ€ : correspond aux systÃ¨mes multi-agents (agents qui coopÃ¨rent) | Ã‰tape: [E1] | Source: [S0]  # #
        "cs.NE",    # # CatÃ©gorie â€œNeural and Evolutionary Computingâ€ : correspond aux approches rÃ©seaux neuronaux / deep learning (historique) | Ã‰tape: [E1] | Source: [S0]  # #
        "stat.ML",  # # CatÃ©gorie â€œMachine Learning (Stats)â€ : correspond aux cross-lists frÃ©quentes ML cÃ´tÃ© statistiques (Ã©vite de rater des papiers) | Ã‰tape: [E1] | Source: [S0]  # #
        "eess.IV",  # # CatÃ©gorie â€œImage and Video Processingâ€ : correspond Ã  vision/image parfois classÃ©e hors CS (Ã©vite de rater des papiers vision) | Ã‰tape: [E1] | Source: [S0]  # #
    ],
    "algo_ds": ["cs.DS", "cs.CC"],  # # DÃ©clare les catÃ©gories autorisÃ©es pour â€œAlgorithmique & Data Structuresâ€ : cs.DS=structures de donnÃ©es, cs.CC=complexitÃ© | Ã‰tape: [E1] | Source: [S0]  # #
    "net_sys": ["cs.NI", "cs.DC", "cs.OS"],  # # DÃ©clare les catÃ©gories autorisÃ©es pour â€œRÃ©seaux & SystÃ¨mesâ€ : cs.NI=rÃ©seau, cs.DC=distribuÃ©, cs.OS=systÃ¨mes dâ€™exploitation | Ã‰tape: [E1] | Source: [S0]  # #
    "cyber_crypto": ["cs.CR"],  # # DÃ©clare les catÃ©gories autorisÃ©es pour â€œCybersÃ©curitÃ© & Cryptoâ€ : cs.CR=cryptographie et sÃ©curitÃ© | Ã‰tape: [E1] | Source: [S0]  # #
    "pl_se": ["cs.PL", "cs.SE", "cs.LO"],  # # DÃ©clare les catÃ©gories autorisÃ©es pour â€œLangages & GÃ©nie logicielâ€ : cs.PL=langages, cs.SE=gÃ©nie logiciel, cs.LO=logique en CS | Ã‰tape: [E1] | Source: [S0]  # #
    "hci_data": ["cs.HC", "cs.IR", "cs.DB", "cs.MM"],  # # DÃ©clare les catÃ©gories autorisÃ©es pour â€œHCI & DonnÃ©esâ€ : cs.HC=interaction, cs.IR=recherche dâ€™info, cs.DB=bases de donnÃ©es, cs.MM=multimÃ©dia | Ã‰tape: [E1] | Source: [S0]  # #
}  # # Ferme le dictionnaire : cette table sera utilisÃ©e plus loin pour filtrer/valider les catÃ©gories extraites depuis la page arXiv | Ã‰tape: [E1] | Source: [S0]  # #


# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
# ğŸ§  Keywords fallback (si le site est modifiÃ© et que l'on perd les catÃ©gories (cs.AI etc.), on cherche via des mots-clÃ©s (AI/transformer/LLMâ€¦)  # #  
# ============================================================  
THEME_KEYWORDS: Dict[str, List[str]] = {  # # Support # # Respect: filtrage pertinence si catÃ©gories manquantes | Ã‰tape: [E1] | Source: [S0]  # #
    "ai_ml": ["machine learning", "deep learning", "llm", "agent", "transformer", "multimodal", "computer vision"],
    "algo_ds": ["algorithm", "data structure", "complexity", "graph", "optimization"],
    "net_sys": ["network", "distributed", "operating system", "cloud", "systems"],
    "cyber_crypto": ["security", "privacy", "cryptography", "attack", "defense", "malware"],
    "pl_se": ["programming language", "compiler", "software engineering", "static analysis", "type system"],
    "hci_data": ["human-computer interaction", "information retrieval", "database", "multimedia", "ranking", "search"],
}

# ===============================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
# ğŸ“¦ Champs renvoyÃ©s (minimal)  
# =============================== 
SUPPORTED_FIELDS = [  # # Liste Python = "contrat" des champs renvoyÃ©s (format standard et prÃ©visible) pour dÃ©coupler le scraping du reste | Ã‰tape: [E1] | Source: [S1]  # #
    "arxiv_id",  # # Champ JSON arxiv_id = identifiant unique arXiv du papier (FR: ID du papier) utilisÃ© pour relier /abs et /pdf | Ã‰tape: [E1] | Source: [S1]  # #
    "title",  # # Champ JSON title = titre du papier (FR: nom du papier) pour l'affichage et le contexte LLM | Ã‰tape: [E1] | Source: [S1]  # #
    "authors",  # # Champ JSON authors = liste des auteurs (FR: auteurs) pour attribuer le travail et contextualiser la source | Ã‰tape: [E1] | Source: [S1]  # #
    "abstract",  # # Champ JSON abstract = rÃ©sumÃ© du papier (FR: rÃ©sumÃ©) pour comprendre rapidement le contenu sans HTML brut | Ã‰tape: [E1] | Source: [S1]  # #
    "method",  # # Champ JSON method = section MÃ©thode (FR: "MÃ©thode") extraite de /html pour enrichir sans aspirer toute la page | Ã‰tape: [E1] | Source: [S8]  # #
    "references",  # # Champ JSON references = section RÃ©fÃ©rences (FR: "RÃ©fÃ©rences") pour garder les sources citÃ©es (utile QA) | Ã‰tape: [E1] | Source: [S8]  # #
    "submitted_date",  # # Champ JSON submitted_date = date de soumission (FR: date d'envoi Ã  arXiv) pour la traÃ§abilitÃ© temporelle | Ã‰tape: [E1] | Source: [S1]  # #
    "abs_url",  # # Champ JSON abs_url = lien arXiv /abs (FR: page dÃ©tails) pour relire/diagnostiquer la source | Ã‰tape: [E1] | Source: [S1]  # #
    "pdf_url",  # # Champ JSON pdf_url = lien arXiv /pdf (FR: tÃ©lÃ©chargement PDF) pour accÃ©der au document complet | Ã‰tape: [E1] | Source: [S1]  # #
    "doi",  # # Champ JSON doi = identifiant DOI (FR: identifiant Ã©diteur) si prÃ©sent, pour relier Ã  la publication officielle | Ã‰tape: [E1] | Source: [S1]  # #
    "versions",  # # Champ JSON versions = historique des versions v1,v2... (FR: versions) pour savoir ce qui a changÃ© | Ã‰tape: [E1] | Source: [S1]  # #
    "last_updated_raw",  # # Champ JSON last_updated_raw = derniÃ¨re mise Ã  jour brute (FR: derniÃ¨re maj) depuis l'historique arXiv | Ã‰tape: [E1] | Source: [S1]  # #
    "primary_category",  # # Champ JSON primary_category = catÃ©gorie principale (FR: thÃ¨me principal) ex: cs.AI pour filtrer thÃ©matiquement | Ã‰tape: [E1] | Source: [S1]  # #
    "all_categories",  # # Champ JSON all_categories = toutes les catÃ©gories (FR: tags/thÃ¨mes) pour gÃ©rer cross-list et filtrage robuste | Ã‰tape: [E1] | Source: [S1]  # #
    "missing_fields",  # # Champ JSON missing_fields = liste des champs non trouvÃ©s (FR: champs manquants) pour diagnostiquer sans planter | Ã‰tape: [E2] | Source: [S3]  # #
    "errors",  # # Champ JSON errors = erreurs liÃ©es Ã  cet item (FR: erreurs papier) pour remonter les soucis proprement | Ã‰tape: [E2] | Source: [S3]  # #
]



# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
# ğŸ§© Helpers base  # # Regroupe des petites fonctions utilitaires rÃ©utilisÃ©es partout (Ã©vite duplication et erreurs) | Ã‰tape: [E1] | Source: [S0]  # #
# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
def ensure_dir(path: str) -> None:  # # Fonction ensure_dir = garantit que le dossier existe avant dâ€™Ã©crire des fichiers (cache/exports) | Ã‰tape: [E2] | Source: [S2]  # #
    os.makedirs(path, exist_ok=True)  # # CrÃ©e le dossier path ; exist_ok=True = ne lÃ¨ve pas dâ€™erreur si dÃ©jÃ  prÃ©sent (Ã©vite crash) | Ã‰tape: [E2] | Source: [S2]  # #


def now_iso_for_filename() -> str:  # # Fonction now_iso_for_filename = gÃ©nÃ¨re un timestamp lisible pour nommer les fichiers sans collision | Ã‰tape: [E1] | Source: [S0]  # #
    return datetime.datetime.now().strftime("%Y%m%d_%H%M%S")  # # strftime = formate la date/heure en texte "YYYYMMDD_HHMMSS" pour tracer quand le fichier a Ã©tÃ© produit | Ã‰tape: [E1] | Source: [S0]  # #

def is_empty(value: Any) -> bool:  # # DÃ©tection "vide" # # Respect: qualitÃ© sortie JSON | Ã‰tape: [E1] | Source: [S1]  # #
    if value is None:  # # None # # Respect: qualitÃ© | Ã‰tape: [E1] | Source: [S0]  # #
        return True  # # Vide # # Respect: qualitÃ© | Ã‰tape: [E1] | Source: [S0]  # #
    if isinstance(value, str):  # # String # # Respect: qualitÃ© | Ã‰tape: [E1] | Source: [S0]  # #
        v = value.strip()  # # Trim # # Respect: nettoyage | Ã‰tape: [E1] | Source: [S0]  # #
        if v == "":  # # Vide # # Respect: qualitÃ© | Ã‰tape: [E1] | Source: [S0]  # #
            return True  # # Vide | Ã‰tape: [E1] | Source: [S0]  # #
        if v.lower() in {"n/a", "null", "none"}:  # # Marqueurs # # Respect: qualitÃ© | Ã‰tape: [E1] | Source: [S0]  # #
            return True  # # Vide | Ã‰tape: [E1] | Source: [S0]  # #
    if isinstance(value, list):  # # Liste # # Respect: qualitÃ© | Ã‰tape: [E1] | Source: [S0]  # #
        return len(value) == 0  # # Vide si liste vide # # Respect: qualitÃ© | Ã‰tape: [E1] | Source: [S0]  # #
    return False  # # Non vide # # Respect: qualitÃ© | Ã‰tape: [E1] | Source: [S0]  # #


def sleep_polite(min_s: float = 1.2, max_s: float = 2.0) -> None:  # # Politesse # # Respect: frÃ©quence raisonnable | Ã‰tape: [E4] | Source: [S5]  # #
    time.sleep(random.uniform(min_s, max_s))  # # Jitter # # Respect: anti-spam | Ã‰tape: [E4] | Source: [S5]  # #


def save_text_file(folder: str, filename: str, content: str) -> str:  # # Sauvegarde # # Respect: cache local visible | Ã‰tape: [E2] | Source: [S2]  # #
    ensure_dir(folder)  # # Assurer dossier # # Respect: cache disque | Ã‰tape: [E2] | Source: [S2]  # #
    path = os.path.join(folder, filename)  # # Chemin # # Respect: cohÃ©rence | Ã‰tape: [E1] | Source: [S0]  # #
    with open(path, "w", encoding="utf-8") as f:  # # UTF-8 # # Respect: robustesse encodage | Ã‰tape: [E1] | Source: [S0]  # #
        f.write(content)  # # Ã‰criture # # Respect: traÃ§abilitÃ©/debug | Ã‰tape: [E1] | Source: [S0]  # #
    return path  # # Retour chemin # # Respect: utilisateur peut retrouver le fichier | Ã‰tape: [E1] | Source: [S0]  # #


def normalize_url(href: str) -> str:  # # Normalise URL # # Respect: champs propres | Ã‰tape: [E1] | Source: [S0]  # #
    if not href:  # # Si vide # # Respect: robustesse | Ã‰tape: [E1] | Source: [S0]  # #
        return ""  # # Retour vide # # Respect: robustesse | Ã‰tape: [E1] | Source: [S0]  # #
    h = href.strip()  # # Trim # # Respect: sortie propre | Ã‰tape: [E1] | Source: [S0]  # #
    if h.startswith("//"):  # # SchÃ©ma manquant # # Respect: robustesse | Ã‰tape: [E1] | Source: [S0]  # #
        return "https:" + h  # # Force https # # Respect: sortie valide | Ã‰tape: [E2] | Source: [S3]  # #
    if h.startswith("/"):  # # Relatif # # Respect: robustesse | Ã‰tape: [E1] | Source: [S0]  # #
        return ARXIV_BASE + h  # # Absolu # # Respect: sortie valide | Ã‰tape: [E1] | Source: [S0]  # #
    return h  # # DÃ©jÃ  absolu # # Respect: sortie valide | Ã‰tape: [E1] | Source: [S0]  # #

def abs_url(arxiv_id: str) -> str:  # # Fonction abs_url = fabrique lâ€™URL â€œficheâ€ /abs Ã  partir de lâ€™identifiant arXiv (utile si le HTML ne donne pas le lien complet) | Ã‰tape: [E1] | Source: [S0]  # #
    return f"{ARXIV_BASE}/abs/{arxiv_id}"  # # f-string = insÃ¨re arxiv_id dans le modÃ¨le dâ€™URL pour obtenir ex: https://arxiv.org/abs/2601.08457 | Ã‰tape: [E1] | Source: [S0]  # #


def pdf_url(arxiv_id: str) -> str:  # # Fonction pdf_url = fabrique lâ€™URL de tÃ©lÃ©chargement /pdf Ã  partir de lâ€™identifiant arXiv (fallback si le lien PDF est absent) | Ã‰tape: [E1] | Source: [S0]  # #
    return f"{ARXIV_BASE}/pdf/{arxiv_id}"  # # f-string = construit ex: https://arxiv.org/pdf/2601.08457 pour tÃ©lÃ©charger/ouvrir le PDF | Ã‰tape: [E1] | Source: [S0]  # #


def compute_missing_fields(item: Dict[str, Any]) -> List[str]:  # # Fonction compute_missing_fields = liste les champs manquants dâ€™un item (contrÃ´le qualitÃ©) pour savoir ce que le parsing nâ€™a pas trouvÃ© | Ã‰tape: [E1] | Source: [S0]  # #
    missing: List[str] = []  # # On crÃ©e une liste vide â€œmissingâ€ qui va stocker les noms des champs absents (diagnostic) | Ã‰tape: [E1] | Source: [S0]  # #
    for f in SUPPORTED_FIELDS:  # # Boucle sur tous les champs attendus (contrat stable) pour vÃ©rifier chacun systÃ©matiquement | Ã‰tape: [E1] | Source: [S1]  # #
        if is_empty(item.get(f)):  # # is_empty() teste si la valeur est vide (None, "", liste videâ€¦) â†’ ici on lâ€™utilise pour dÃ©tecter un champ non rempli | Ã‰tape: [E2] | Source: [S6]  # #
            missing.append(f)  # # On ajoute le nom du champ manquant dans la liste pour pouvoir le renvoyer dans le JSON final | Ã‰tape: [E2] | Source: [S6]  # #
    return missing  # # On renvoie la liste des champs manquants â†’ Ã§a aide Ã  debug sans casser le reste du pipeline | Ã‰tape: [E1] | Source: [S1]  # #


def _detect_weird_page_signals(html: str) -> Dict[str, bool]:  # # Fonction _detect_weird_page_signals = repÃ¨re des â€œsignauxâ€ de page anormale (anti-bot, consentement, zÃ©ro rÃ©sultat) pour diagnostiquer pourquoi le parsing Ã©choue | Ã‰tape: [E2] | Source: [S6]  # #
    h = (html or "").lower()  # # lower() met tout en minuscules pour faire des tests â€œinâ€ insensibles Ã  la casse (plus robuste) | Ã‰tape: [E1] | Source: [S0]  # #
    return {  # # On renvoie un dict de drapeaux boolÃ©ens (diagnostic lisible dans le JSON) | Ã‰tape: [E2] | Source: [S6]  # #
        "contains_we_are_sorry": ("we are sorry" in h),  # # True si on voit un message de blocage du site (souvent anti-bot) | Ã‰tape: [E2] | Source: [S6]  # #
        "contains_robot": ("robot" in h),  # # True si la page mentionne â€œrobotâ€ (indice de dÃ©tection automatique) | Ã‰tape: [E2] | Source: [S6]  # #
        "contains_captcha": ("captcha" in h),  # # True si un CAPTCHA apparaÃ®t (le scraper ne peut pas rÃ©soudre Ã§a) | Ã‰tape: [E2] | Source: [S6]  # #
        "contains_consent": ("consent" in h or "cookie" in h),  # # True si une page cookies/consent bloque le contenu (HTML diffÃ©rent) | Ã‰tape: [E2] | Source: [S6]  # #
        "contains_no_results": ("no results found" in h),  # # True si la page annonce â€œaucun rÃ©sultatâ€ (pas un bug de parsing) | Ã‰tape: [E2] | Source: [S6]  # #
    }  # # Fin dict diagnostic (pas dâ€™effet sur scraping, juste une aide de debug) | Ã‰tape: [E2] | Source: [S6]  # #


def http_get_text(session: requests.Session, url: str, timeout_s: int = 30) -> Tuple[str, int]:  # # Fonction http_get_text = fait un GET HTTP et renvoie (HTML, code HTTP) sans faire crasher le tool si rÃ©seau/timeout | Ã‰tape: [E2] | Source: [S3]  # #
    headers = {  # # On prÃ©pare les en-tÃªtes HTTP envoyÃ©s Ã  arXiv (Ã§a aide Ã  Ãªtre acceptÃ© + parsing stable) | Ã‰tape: [E2] | Source: [S0]  # #
        "User-Agent": "Mozilla/5.0 DIXITBOT-arXivScraper/4.1",  # # User-Agent = â€œcarte dâ€™identitÃ©â€ HTTP ; ici on met un UA clair pour Ã©viter dâ€™Ãªtre vu comme un bot suspect | Ã‰tape: [E2] | Source: [S4]  # #
        "Accept-Language": "en-US,en;q=0.9",  # # Accept-Language = demande une page en anglais pour Ã©viter des variations de texte selon la langue | Ã‰tape: [E2] | Source: [S0]  # #
    }  # # Fin headers | Ã‰tape: [E2] | Source: [S0]  # #
    try:  # # try = on encapsule lâ€™appel rÃ©seau pour gÃ©rer proprement les erreurs au lieu de crasher tout le script | Ã‰tape: [E2] | Source: [S3]  # #
        resp = session.get(url, headers=headers, timeout=timeout_s)  # # session.get() exÃ©cute le GET ; timeout Ã©vite que Ã§a bloque indÃ©finiment si le site rÃ©pond mal | Ã‰tape: [E2] | Source: [S3]  # #
        return resp.text, resp.status_code  # # On renvoie le HTML + le status_code (200, 429, 500...) pour diagnostic + contrat stable | Ã‰tape: [E1] | Source: [S1]  # #
    except requests.RequestException as e:  # # requests.RequestException = toutes les erreurs rÃ©seau (timeout, DNS, connexion refusÃ©e, etc.) | Ã‰tape: [E2] | Source: [S0]  # #
        return f"REQUEST_EXCEPTION: {str(e)}", 0  # # On renvoie un â€œHTMLâ€ texte dâ€™erreur + code 0 (0 = erreur locale, pas une rÃ©ponse HTTP du serveur) | Ã‰tape: [E2] | Source: [S3]  # #

def build_search_url(query: str, start: int, size: int, sort: str) -> str:  # # Fonction build_search_url = construit lâ€™URL complÃ¨te de recherche arXiv â€œ/search/csâ€ avec query + pagination + tri (pour appeler arXiv de faÃ§on standard) | Ã‰tape: [E1] | Source: [S0]  # #
    q = requests.utils.quote((query or "").strip())  # # requests.utils.quote() encode la requÃªte (espacesâ†’%20, guillemetsâ†’%22â€¦) pour quâ€™elle soit valide dans une URL HTTP | Ã‰tape: [E1] | Source: [S0]  # #
    base = f"{ARXIV_SEARCH_CS}?query={q}&searchtype=all&abstracts=show&size={size}&start={start}"  # # f-string = assemble lâ€™URL â€œbaseâ€ avec paramÃ¨tres: query (mots-clÃ©s), size (nb rÃ©sultats/page), start (offset pagination) | Ã‰tape: [E1] | Source: [S0]  # #
    s = (sort or "relevance").strip().lower()  # # On normalise le champ sort (par dÃ©faut â€œrelevanceâ€), trim + lower pour comparer sans se tromper (robuste aux entrÃ©es utilisateur) | Ã‰tape: [E1] | Source: [S0]  # #
    if s in {"submitted_date", "submitted", "recent"}:  # # Si lâ€™utilisateur veut trier par date de soumission (ou un alias), on active le tri â€œrÃ©cents dâ€™abordâ€ | Ã‰tape: [E1] | Source: [S0]  # #
        return base + "&order=-announced_date_first"  # # On ajoute le paramÃ¨tre arXiv â€œorder=-announced_date_firstâ€ pour renvoyer les papiers les plus rÃ©cents en premier | Ã‰tape: [E1] | Source: [S0]  # #
    return base  # # Sinon, on retourne lâ€™URL de base (tri â€œrelevanceâ€ par dÃ©faut) pour garder un comportement stable et prÃ©visible | Ã‰tape: [E1] | Source: [S0]  # #


# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
# ğŸ§² Extraction catÃ©gories depuis "Subjects" + tags (robuste)  # #  | Ã‰tape: [E1] | Source: [S8]  # #
# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
_RE_ANY_CAT = re.compile(r"\(((?:cs|stat|eess)\.[A-Z]{2})\)")  # # Regex cat # # Respect: inclut cross-lists (stat.ML/eess.IV) | Ã‰tape: [E1] | Source: [S0]  # #
_RE_ARXIV_ID = re.compile(r"/abs/([^?#/]+)")  # # Regex ID # # Respect: extraction stable | Ã‰tape: [E1] | Source: [S8]  # #


def extract_categories_from_result(li: Tag) -> Tuple[str, List[str]]:  # # Lit catÃ©gories # # Respect: filtrage thÃ©matique aprÃ¨s search/cs | Ã‰tape: [E1] | Source: [S8]  # #
    cats: List[str] = []  # # Init # # Respect: structuration | Ã‰tape: [E1] | Source: [S0]  # #

    # (1) MÃ©thode robuste: tags visibles (quand arXiv rend des badges)
    for span in li.select("span.tag"):  # # Parcours tags # # Respect: extraction ciblÃ©e | Ã‰tape: [E1] | Source: [S8]  # #
        t = (span.get_text(" ", strip=True) or "").strip()  # # Texte tag # # Respect: nettoyage | Ã‰tape: [E1] | Source: [S0]  # #
        if re.fullmatch(r"(?:cs|stat|eess)\.[A-Z]{2}", t):  # # Si ressemble Ã  une cat # # Respect: filtrage fiable | Ã‰tape: [E1] | Source: [S0]  # #
            cats.append(t)  # # Ajoute # # Respect: structuration | Ã‰tape: [E1] | Source: [S0]  # #

    # (2) MÃ©thode fallback: regex sur la ligne "Subjects:  (cs.XX);  (stat.ML)"
    if not cats:  # # Si tags absents # # Respect: robustesse | Ã‰tape: [E1] | Source: [S0]  # #
        txt = li.get_text(" ", strip=True)  # # Texte bloc (minimum) # # Respect: extraction juste pour cat | Ã‰tape: [E1] | Source: [S8]  # #
        cats = _RE_ANY_CAT.findall(txt)  # # Extrait cats # # Respect: mapping demandÃ© | Ã‰tape: [E1] | Source: [S0]  # #

    # DÃ©doublonnage en gardant l'ordre
    cats = list(dict.fromkeys(cats))  # # DÃ©doublonne # # Respect: sortie propre | Ã‰tape: [E1] | Source: [S0]  # #
    primary = cats[0] if cats else ""  # # Premier # # Respect: structuration | Ã‰tape: [E1] | Source: [S0]  # #
    return primary, cats  # # Retour # # Respect: sortie structurÃ©e | Ã‰tape: [E1] | Source: [S1]  # #


# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
# ğŸ§¾ Parsing page search/cs -> items minimaux  # #  | Ã‰tape: [E1] | Source: [S1]  # #
# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
def parse_search_page(html: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:  # # Parse + diag # # Respect: robustesse | Ã‰tape: [E2] | Source: [S6]  # #
    soup = BeautifulSoup(html, "lxml")  # # Parse # # Respect: extraction ciblÃ©e | Ã‰tape: [E1] | Source: [S8]  # #
    page_title = soup.title.get_text(" ", strip=True) if soup.title else ""  # # Titre page # # Respect: diagnostic | Ã‰tape: [E2] | Source: [S6]  # #
    weird = _detect_weird_page_signals(html)  # # DÃ©tection anti-bot/no-results # # Respect: robustesse | Ã‰tape: [E2] | Source: [S6]  # #

    diag: Dict[str, Any] = {  # # Diagnostic # # Respect: traÃ§abilitÃ© | Ã‰tape: [E2] | Source: [S6]  # #
        "page_title": page_title,  # # Titre # # Respect: debug | Ã‰tape: [E1] | Source: [S0]  # #
        "has_abs_links": ("/abs/" in (html or "")),  # # Indicateur principal # # Respect: debug | Ã‰tape: [E1] | Source: [S0]  # #
        **weird,  # # Drapeaux # # Respect: debug | Ã‰tape: [E2] | Source: [S6]  # #
    }

    items: List[Dict[str, Any]] = []  # # RÃ©sultats # # Respect: sortie structurÃ©e | Ã‰tape: [E1] | Source: [S1]  # #

    # SÃ©lecteur principal (arXiv actuel)
    result_nodes = soup.select("ol.breathe-horizontal li.arxiv-result")  # # Noeuds rÃ©sultats # # Respect: extraction ciblÃ©e | Ã‰tape: [E1] | Source: [S8]  # #
    diag["selector_count_arxiv_result"] = len(result_nodes)  # # Compte # # Respect: debug | Ã‰tape: [E2] | Source: [S6]  # #
    # Fallback: si DOM change, on reconstruit via liens /abs/
    if not result_nodes and diag["has_abs_links"]:  # # Si le sÃ©lecteur principal ne trouve aucun bloc rÃ©sultat MAIS que le HTML contient des liens â€œ/abs/â€, on active un plan B pour ne pas renvoyer 0 items juste Ã  cause dâ€™un changement de DOM | Ã‰tape: [E2] | Source: [S6]  # #
        diag["fallback_mode"] = "abs_links"  # # On note dans le diagnostic quâ€™on est passÃ© en mode fallback â€œabs_linksâ€ (Ã§a explique pourquoi certains champs sont vides) | Ã‰tape: [E2] | Source: [S6]  # #
        abs_ids = _RE_ARXIV_ID.findall(html or "")  # # findall() extrait tous les identifiants arXiv prÃ©sents dans les URLs /abs/... du HTML, mÃªme si les classes HTML ont changÃ© | Ã‰tape: [E1] | Source: [S8]  # #
        abs_ids = list(dict.fromkeys(abs_ids))[:PAGE_SIZE]  # # dict.fromkeys() retire les doublons en gardant lâ€™ordre, puis on limite Ã  PAGE_SIZE (=50) pour contrÃ´ler le volume et respecter la pagination | Ã‰tape: [E1] | Source: [S0]  # #
        for arxiv_id in abs_ids:  # # On parcourt chaque identifiant trouvÃ© pour reconstruire des items minimaux (structure stable mÃªme sans parsing complet) | Ã‰tape: [E1] | Source: [S1]  # #
            items.append({  # # On ajoute un â€œitem minimalâ€ (dict) : on garde uniquement les champs indispensables et on laisse le reste vide plutÃ´t que dâ€™envoyer du HTML brut | Ã‰tape: [E1] | Source: [S1]  # #
                "arxiv_id": arxiv_id,  # # Identifiant arXiv (FR: lâ€™ID unique du papier) : sert Ã  construire les URLs et Ã  rÃ©fÃ©rencer le papier mÃªme si le reste nâ€™a pas Ã©tÃ© parsÃ© | Ã‰tape: [E1] | Source: [S0]  # #
                "title": "",  # # Titre vide car indisponible en fallback (on prÃ©fÃ¨re vide plutÃ´t que faux) | Ã‰tape: [E1] | Source: [S0]  # #
                "authors": [],  # # Auteurs vides en fallback (liste vide = format stable cÃ´tÃ© API) | Ã‰tape: [E1] | Source: [S0]  # #
                "abstract": "",  # # RÃ©sumÃ© (abstract) vide en fallback (on ne devine pas) | Ã‰tape: [E1] | Source: [S0]  # #
                "submitted_date": "",  # # Date de soumission vide en fallback (on la remplira plus tard via /abs si besoin) | Ã‰tape: [E1] | Source: [S0]  # #
                "abs_url": abs_url(arxiv_id),  # # abs_url() fabrique lâ€™URL /abs/{id} (FR: page fiche du papier) pour pouvoir enrichir ensuite sans dÃ©pendre du HTML de recherche | Ã‰tape: [E1] | Source: [S0]  # #
                "pdf_url": pdf_url(arxiv_id),  # # pdf_url() fabrique lâ€™URL /pdf/{id} (FR: lien direct PDF) pour garder une sortie utile mÃªme en mode fallback | Ã‰tape: [E1] | Source: [S0]  # #
                "primary_category": "",  # # CatÃ©gorie principale vide en fallback (on Ã©vite les faux labels) | Ã‰tape: [E1] | Source: [S0]  # #
                "all_categories": [],  # # Toutes catÃ©gories vides en fallback (liste stable cÃ´tÃ© API) | Ã‰tape: [E1] | Source: [S0]  # #
            })
        return items, diag  # # On retourne immÃ©diatement (items + diag) : câ€™est un retour contrÃ´lÃ©/traÃ§able plutÃ´t quâ€™un crash ou un â€œ0 rÃ©sultatâ€ incomprÃ©hensible | Ã‰tape: [E2] | Source: [S6]  # #

    # Mode normal
    for li in result_nodes:  # # On parcourt chaque bloc rÃ©sultat â€œli.arxiv-resultâ€ pour extraire les champs utiles de faÃ§on ciblÃ©e (pas â€œtout le HTMLâ€) | Ã‰tape: [E1] | Source: [S8]  # #
        title_el = li.select_one("p.title")  # # select_one() rÃ©cupÃ¨re la balise du titre (p.title) : extraction prÃ©cise dâ€™un champ essentiel | Ã‰tape: [E1] | Source: [S0]  # #
        authors_el = li.select_one("p.authors")  # # select_one() rÃ©cupÃ¨re la balise des auteurs (p.authors) : extraction prÃ©cise dâ€™un champ essentiel | Ã‰tape: [E1] | Source: [S0]  # #
        abstract_el = li.select_one("span.abstract-full")  # # select_one() rÃ©cupÃ¨re la balise du rÃ©sumÃ© complet (span.abstract-full) : extraction prÃ©cise sans prendre toute la page | Ã‰tape: [E1] | Source: [S0]  # #
        submitted_el = li.select_one("p.is-size-7")  # # select_one() rÃ©cupÃ¨re le bloc â€œSubmitted â€¦â€ (p.is-size-7) : utile pour la date | Ã‰tape: [E1] | Source: [S0]  # #

        abs_a = li.select_one('p.list-title a[href*="/abs/"]')  # # select_one() rÃ©cupÃ¨re le lien vers /abs/ (FR: page fiche du papier), car câ€™est la source la plus stable pour rÃ©cupÃ©rer lâ€™ID arXiv | Ã‰tape: [E1] | Source: [S0]  # #
        pdf_a = li.select_one('p.list-title a[href*="/pdf/"]')  # # select_one() rÃ©cupÃ¨re le lien vers /pdf/ (FR: tÃ©lÃ©chargement du PDF), car câ€™est un lien utile Ã  renvoyer Ã  lâ€™utilisateur | Ã‰tape: [E1] | Source: [S0]  # #
        abs_href = normalize_url(abs_a.get("href") if abs_a else "")  # # .get("href") lit lâ€™attribut href, puis normalize_url() convertit en URL absolue (FR: URL propre et utilisable) | Ã‰tape: [E1] | Source: [S0]  # #
        pdf_href = normalize_url(pdf_a.get("href") if pdf_a else "")  # # MÃªme logique pour le PDF : URL absolue pour Ã©viter les liens relatifs qui cassent | Ã‰tape: [E1] | Source: [S0]  # #

        arxiv_id = ""  # # On initialise lâ€™identifiant arXiv vide : on le remplira uniquement si on lâ€™extrait proprement (Ã©vite valeurs fausses) | Ã‰tape: [E1] | Source: [S0]  # #
        m = re.search(r"/abs/([^?#/]+)", abs_href) if abs_href else None  # # re.search() cherche â€œ/abs/{id}â€ dans lâ€™URL pour isoler lâ€™ID (ici on lâ€™utilise pour extraire un identifiant stable) | Ã‰tape: [E1] | Source: [S8]  # #
        if m:  # # Si la regex a trouvÃ© un ID, on sÃ©curise le remplissage (sinon on garde vide) | Ã‰tape: [E1] | Source: [S0]  # #
            arxiv_id = m.group(1).strip()  # # group(1) rÃ©cupÃ¨re la partie capturÃ©e (= lâ€™ID), strip() enlÃ¨ve les espaces parasites | Ã‰tape: [E1] | Source: [S0]  # #

        title_txt = title_el.get_text(" ", strip=True) if title_el else ""  # # get_text() rÃ©cupÃ¨re le texte du titre (FR: le nom du papier) ; strip=True nettoie pour Ã©viter bruit/espaces | Ã‰tape: [E1] | Source: [S8]  # #
        authors_txt = authors_el.get_text(" ", strip=True) if authors_el else ""  # # get_text() rÃ©cupÃ¨re la chaÃ®ne des auteurs (FR: noms), en mode â€œtexte propreâ€ | Ã‰tape: [E1] | Source: [S8]  # #
        authors = [a.strip() for a in authors_txt.replace("Authors:", "").split(",") if a.strip()]  # # On transforme la chaÃ®ne en liste (split â€œ,â€), strip() nettoie chaque nom : sortie structurÃ©e stable pour lâ€™API | Ã‰tape: [E1] | Source: [S1]  # #
        abstract = abstract_el.get_text(" ", strip=True) if abstract_el else ""  # # get_text() rÃ©cupÃ¨re le rÃ©sumÃ© (abstract) complet quand disponible, sans prendre toute la page | Ã‰tape: [E1] | Source: [S8]  # #
        abstract = abstract.replace("â–³ Less", "").strip()  # # On enlÃ¨ve le texte UI â€œâ–³ Lessâ€ et on strip() : rÃ©duit le bruit pour garder uniquement lâ€™info utile | Ã‰tape: [E1] | Source: [S0]  # #

        submitted_date = ""  # # On initialise la date de soumission vide (on la remplit seulement si on lâ€™extrait vraiment) | Ã‰tape: [E1] | Source: [S0]  # #
        if submitted_el:  # # Si le bloc date existe, on tente une extraction contrÃ´lÃ©e (sinon on laisse vide) | Ã‰tape: [E1] | Source: [S0]  # #
            txt = submitted_el.get_text(" ", strip=True)  # # get_text() rÃ©cupÃ¨re la phrase â€œSubmitted â€¦â€ en texte propre, pour que la regex fonctionne de faÃ§on prÃ©visible | Ã‰tape: [E1] | Source: [S8]  # #
            m3 = re.search(r"Submitted\s+(.+?)(?:;|$)", txt, flags=re.IGNORECASE)  # # re.search() extrait uniquement la portion date aprÃ¨s â€œSubmittedâ€ (on cible lâ€™info utile, pas toute la phrase) | Ã‰tape: [E1] | Source: [S8]  # #
            if m3:  # # Si la regex a trouvÃ© une date, on la rÃ©cupÃ¨re ; sinon on laisse vide (robuste) | Ã‰tape: [E1] | Source: [S0]  # #
                submitted_date = m3.group(1).strip()  # # group(1) = date capturÃ©e ; strip() pour nettoyage final | Ã‰tape: [E1] | Source: [S0]  # #

        primary_cat, all_cats = extract_categories_from_result(li)  # # Appel de extract_categories_from_result() : on rÃ©cupÃ¨re catÃ©gories pour filtrer par thÃ¨me (et rester â€œciblÃ©â€ sur les sujets demandÃ©s) | Ã‰tape: [E1] | Source: [S8]  # #

        if arxiv_id and is_empty(abs_href):  # # Si on a un ID mais pas dâ€™URL /abs valide (rare), on applique un fallback pour garantir un lien utilisable | Ã‰tape: [E2] | Source: [S6]  # #
            abs_href = abs_url(arxiv_id)  # # abs_url() reconstruit une URL /abs/{id} correcte : sortie utile mÃªme si le HTML manquait le lien | Ã‰tape: [E1] | Source: [S0]  # #
        if arxiv_id and is_empty(pdf_href):  # # Si on a un ID mais pas dâ€™URL PDF valide, on applique un fallback pour garantir le lien PDF | Ã‰tape: [E2] | Source: [S6]  # #
            pdf_href = pdf_url(arxiv_id)  # # pdf_url() reconstruit une URL /pdf/{id} correcte : sortie utile mÃªme si le HTML manquait le lien | Ã‰tape: [E1] | Source: [S0]  # #

        items.append({  # # On ajoute un dict â€œpapierâ€ Ã  la liste items : câ€™est le format standard que lâ€™API et le LLM consommeront (sans HTML brut) | Ã‰tape: [E1] | Source: [S1]  # #
            "arxiv_id": arxiv_id,  # # arxiv_id (FR: identifiant unique du papier) : clÃ© de rÃ©fÃ©rence pour retrouver la fiche et le PDF | Ã‰tape: [E1] | Source: [S0]  # #
            "title": title_txt,  # # title (FR: titre du papier) : sert Ã  afficher et Ã  filtrer la pertinence | Ã‰tape: [E1] | Source: [S0]  # #
            "authors": authors,  # # authors (FR: liste des auteurs) : format liste pour Ãªtre stable cÃ´tÃ© API | Ã‰tape: [E1] | Source: [S0]  # #
            "abstract": abstract,  # # abstract (FR: rÃ©sumÃ©) : texte principal utile, extrait proprement sans le HTML complet | Ã‰tape: [E1] | Source: [S0]  # #
            "method": "",  # # method (FR: â€œmÃ©thodeâ€) : placeholder rempli plus tard via /html, pour ajouter la section â€œMÃ©thodeâ€ sans aspirer toute la page | Ã‰tape: [E2] | Source: [S8]  # #
            "references": [],  # # references (FR: â€œrÃ©fÃ©rences/bibliographieâ€) : placeholder liste, rempli plus tard via /html, pour garder uniquement la bibliographie sans HTML brut | Ã‰tape: [E2] | Source: [S8]  # #
            "submitted_date": submitted_date,  # # submitted_date (FR: date de soumission) : utile pour trier/Ã©valuer la fraÃ®cheur des papiers | Ã‰tape: [E1] | Source: [S0]  # #
            "abs_url": abs_href,  # # abs_url (FR: lien de la fiche /abs) : page de dÃ©tails, base pour enrichissement DOI/versions | Ã‰tape: [E1] | Source: [S0]  # #
            "pdf_url": pdf_href,  # # pdf_url (FR: lien PDF /pdf) : tÃ©lÃ©chargement direct du papier | Ã‰tape: [E1] | Source: [S0]  # #
            "primary_category": primary_cat,  # # primary_category (FR: catÃ©gorie principale) : sert au filtrage thÃ©matique demandÃ© | Ã‰tape: [E1] | Source: [S0]  # #
            "all_categories": all_cats,  # # all_categories (FR: toutes les catÃ©gories) : sert Ã  dÃ©tecter cross-listing et Ã©viter de rater des papiers pertinents | Ã‰tape: [E1] | Source: [S0]  # #
        })

    return items, diag  # # On retourne (items, diag) : items = donnÃ©es structurÃ©es ; diag = explications debug (fallback/anti-bot/compteurs) pour robustesse et traÃ§abilitÃ© | Ã‰tape: [E2] | Source: [S6]  # #

# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
# ğŸ” Parsing /abs (DOI + versions + abstract fallback)           # #  | Ã‰tape: [E2] | Source: [S6]  # #
# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
def parse_abs_page(abs_html: str) -> Dict[str, Any]:  # # La fonction lit le HTML de la page /abs et en extrait UNIQUEMENT des champs stables (doi, abstract, versions) pour enrichir lâ€™item sans â€œaspirerâ€ tout le site | Ã‰tape: [E1] | Source: [S0]  # #
    soup = BeautifulSoup(abs_html, "lxml")  # # BeautifulSoup(...,"lxml") transforme le HTML brut en arbre navigable pour pouvoir cibler des blocs prÃ©cis (principe: extraction ciblÃ©e, pas du texte global) | Ã‰tape: [E1] | Source: [S8]  # #
    out: Dict[str, Any] = {"doi": "", "versions": [], "last_updated_raw": "", "abstract": ""}  # # On initialise un dict â€œcontrat stableâ€ avec valeurs vides (Ã©vite KeyError et garantit toujours les mÃªmes clÃ©s cÃ´tÃ© API) | Ã‰tape: [E1] | Source: [S1]  # #

    doi_a = soup.select_one('td.tablecell.doi a[href*="doi.org"]')  # # select_one() cherche le premier lien DOI (Ã©diteur) dans le tableau â€œdoiâ€ de arXiv : on vise un sÃ©lecteur prÃ©cis pour Ã©viter de prendre du bruit | Ã‰tape: [E1] | Source: [S0]  # #
    if doi_a:  # # Si le lien DOI existe, on remplit le champ ; sinon on laisse vide (robuste) | Ã‰tape: [E1] | Source: [S0]  # #
        out["doi"] = doi_a.get_text(" ", strip=True)  # # get_text(...,strip=True) rÃ©cupÃ¨re le texte lisible du lien DOI (FR: identifiant Ã©diteur) sans espaces parasites | Ã‰tape: [E1] | Source: [S8]  # #

    abs_el = soup.select_one("blockquote.abstract")  # # select_one() cible le bloc â€œAbstractâ€ de /abs (câ€™est la source la plus fiable si lâ€™abstract manquait sur la page de recherche) | Ã‰tape: [E1] | Source: [S0]  # #
    if abs_el:  # # Si le bloc Abstract existe, on extrait son texte ; sinon on garde vide (pas dâ€™invention) | Ã‰tape: [E1] | Source: [S0]  # #
        txt = abs_el.get_text(" ", strip=True)  # # get_text() extrait le contenu texte de lâ€™Abstract en supprimant les retours/espaces inutiles | Ã‰tape: [E1] | Source: [S8]  # #
        txt = re.sub(r"^\s*Abstract:\s*", "", txt, flags=re.IGNORECASE).strip()  # # re.sub() enlÃ¨ve le prÃ©fixe â€œAbstract:â€ (UI) pour ne garder que le contenu utile, puis strip() nettoie | Ã‰tape: [E1] | Source: [S0]  # #
        out["abstract"] = txt  # # On stocke lâ€™abstract nettoyÃ© dans le dict de sortie (contrat stable) | Ã‰tape: [E1] | Source: [S1]  # #

    versions: List[Dict[str, str]] = []  # # On prÃ©pare une liste structurÃ©e pour lâ€™historique des versions (v1, v2â€¦) au format dict, pour Ãªtre facile Ã  consommer par lâ€™API | Ã‰tape: [E1] | Source: [S0]  # #
    for li in soup.select("div.submission-history li"):  # # soup.select() rÃ©cupÃ¨re toutes les lignes de lâ€™historique de soumission (submission-history) pour extraire les versions proprement | Ã‰tape: [E1] | Source: [S8]  # #
        txt = li.get_text(" ", strip=True)  # # get_text() rÃ©cupÃ¨re le texte de chaque ligne (ex: â€œ[v2] Tue, â€¦â€) sous forme propre | Ã‰tape: [E1] | Source: [S8]  # #
        m = re.search(r"\[(v\d+)\]\s*(.*)$", txt)  # # re.search() repÃ¨re le numÃ©ro de version â€œ[vX]â€ et le reste de la ligne (date/infos) pour structurer sans dÃ©pendre de micro-HTML | Ã‰tape: [E1] | Source: [S8]  # #
        if m:  # # Si la ligne correspond au pattern (sÃ©curitÃ©), on ajoute une entrÃ©e version ; sinon on ignore (robuste) | Ã‰tape: [E1] | Source: [S0]  # #
            versions.append({"version": m.group(1), "raw": m.group(2).strip()})  # # On enregistre version=vX + raw=texte date ; group() rÃ©cupÃ¨re les groupes capturÃ©s ; strip() nettoie | Ã‰tape: [E1] | Source: [S1]  # #
    out["versions"] = versions  # # On attache la liste des versions au dict de sortie (contrat stable) | Ã‰tape: [E1] | Source: [S1]  # #
    out["last_updated_raw"] = versions[-1]["raw"] if versions else ""  # # On prend la derniÃ¨re entrÃ©e versions[-1] comme â€œderniÃ¨re mise Ã  jourâ€ ; si liste vide, on met "" (Ã©vite crash) | Ã‰tape: [E1] | Source: [S0]  # #

    return out  # # On renvoie le dict dâ€™enrichissement /abs (clÃ©->valeur), utilisÃ© ensuite pour complÃ©ter les items sans changer le main | Ã‰tape: [E1] | Source: [S1]  # #


# ============================================================  # #
# ğŸ§© Parsing /html arXiv : Method + References (ciblÃ©)           # # Ã‰tape: [E5] | Source: [S8]  # #
# ============================================================  # #

def extract_html_url_from_abs(abs_html: str, arxiv_id: str) -> str:  # # La fonction cherche lâ€™URL /html depuis la page /abs pour pouvoir rÃ©cupÃ©rer â€œMethodâ€ et â€œReferencesâ€ (si disponibles) sans supposer que tous les papiers ont du HTML | Ã‰tape: [E5] | Source: [S8]  # #
    soup = BeautifulSoup(abs_html, "lxml")  # # Parser /abs en arbre HTML pour pouvoir trouver un lien â€œ/html/...â€ via un sÃ©lecteur stable au lieu de regex fragile | Ã‰tape: [E5] | Source: [S0]  # #
    a = soup.select_one('a[href^="/html/"], a[href*="/html/"]')  # # select_one() prend le premier lien qui pointe vers /html (FR: version HTML du papier) sâ€™il existe, sinon None | Ã‰tape: [E5] | Source: [S0]  # #
    if not a:  # # Si aucun lien /html nâ€™est prÃ©sent (frÃ©quent), on retourne vide pour ne pas forcer une requÃªte inutile | Ã‰tape: [E5] | Source: [S0]  # #
        return ""  # # Retour vide = â€œpas de HTML disponibleâ€, le reste du pipeline garde method/references vides (contrat stable) | Ã‰tape: [E5] | Source: [S0]  # #
    href = (a.get("href") or "").strip()  # # .get("href") lit lâ€™attribut href du lien, or "" Ã©vite None, strip() nettoie : on sÃ©curise lâ€™entrÃ©e avant normalisation | Ã‰tape: [E1] | Source: [S1]  # #
    if not href:  # # Si href est vide aprÃ¨s nettoyage, on renvoie vide (robuste) | Ã‰tape: [E5] | Source: [S0]  # #
        return ""  # # Pas de lien utilisable | Ã‰tape: [E5] | Source: [S0]  # #
    return normalize_url(href)  # # normalize_url() transforme un lien relatif en URL absolue (FR: lien cliquable stable) | Ã‰tape: [E1] | Source: [S1]  # #


def parse_arxiv_html_method_and_references(html: str) -> tuple[str, list[str]]:  # # La fonction extrait DEUX blocs ciblÃ©s dans /html : â€œMethodâ€ (texte) + â€œReferencesâ€ (liste) pour rÃ©pondre au prof sans scraper tout le contenu | Ã‰tape: [E5] | Source: [S8]  # #
    soup = BeautifulSoup(html, "lxml")  # # Parser le HTML /html en arbre afin de sÃ©lectionner des sections prÃ©cises (robustesse si lâ€™ordre du texte change) | Ã‰tape: [E5] | Source: [S0]  # #

    method_text = ""  # # On initialise la variable method_text Ã  vide : si on ne trouve pas la section Method, on reste sur "" (contrat stable, pas dâ€™invention) | Ã‰tape: [E5] | Source: [S8]  # #
    references: list[str] = []  # # On initialise la liste references : si aucune bibliographie trouvÃ©e, on renvoie [] (format stable cÃ´tÃ© API) | Ã‰tape: [E5] | Source: [S8]  # #

    # âœ… RÃ©fÃ©rences : structure LaTeX HTML arXiv (biblist)
    for li in soup.select("ol.ltx_biblist li, div.ltx_bibliography li"):  # # soup.select() rÃ©cupÃ¨re les entrÃ©es de bibliographie arXiv/LaTeX (deux structures possibles) pour tolÃ©rer des variantes HTML | Ã‰tape: [E5] | Source: [S6]  # #
        t = li.get_text(" ", strip=True)  # # get_text() extrait le texte dâ€™une rÃ©fÃ©rence (auteurs, titre, venue) en Ã©vitant le HTML brut | Ã‰tape: [E5] | Source: [S0]  # #
        if t:  # # On vÃ©rifie que ce nâ€™est pas vide pour Ã©viter dâ€™ajouter des entrÃ©es nulles | Ã‰tape: [E5] | Source: [S0]  # #
            references.append(t)  # # On ajoute la rÃ©fÃ©rence dans la liste (format â€œliste de stringsâ€ simple et stable) | Ã‰tape: [E5] | Source: [S8]  # #

    # âœ… MÃ©thode : on cherche un titre qui ressemble Ã  â€œmethodâ€
    for sec in soup.select("section.ltx_section, div.ltx_section, section"):  # # On parcourt les sections possibles (LaTeX arXiv + HTML gÃ©nÃ©rique) pour trouver une section â€œMethodâ€ mÃªme si la structure varie | Ã‰tape: [E5] | Source: [S6]  # #
        title_el = sec.select_one(".ltx_title, h1, h2, h3, h4")  # # On cherche le titre de section (classe LaTeX ou titres HTML) pour savoir de quoi parle la section | Ã‰tape: [E5] | Source: [S0]  # #
        if not title_el:  # # Si la section nâ€™a pas de titre, on ne peut pas lâ€™identifier => on passe Ã  la suivante | Ã‰tape: [E5] | Source: [S6]  # #
            continue  # # Continue = on saute ce bloc et on garde le script robuste (pas dâ€™erreur) | Ã‰tape: [E5] | Source: [S0]  # #
        title_txt = title_el.get_text(" ", strip=True).lower()  # # On rÃ©cupÃ¨re le titre en texte, on le met en lower() pour faire un match insensible Ã  la casse (Method/METHOD/â€¦) | Ã‰tape: [E1] | Source: [S0]  # #
        if any(k in title_txt for k in ["method", "methods", "methodology", "approach"]):  # # On teste si le titre contient des mots-clÃ©s â€œmÃ©thodeâ€ pour capturer la section Method mÃªme si le libellÃ© varie | Ã‰tape: [E5] | Source: [S8]  # #
            tmp = sec.get_text(" ", strip=True)  # # On rÃ©cupÃ¨re le texte complet de la section (titre + contenu) sous forme propre, sans HTML brut | Ã‰tape: [E5] | Source: [S0]  # #
            tmp = re.sub(r"^\s*" + re.escape(title_el.get_text(" ", strip=True)) + r"\s*", "", tmp, flags=re.IGNORECASE)  # # re.sub() retire le titre au dÃ©but du texte pour ne garder que le contenu â€œmÃ©thodeâ€ (plus propre pour lâ€™agent) | Ã‰tape: [E5] | Source: [S0]  # #
            method_text = tmp.strip()  # # strip() final : on stocke le texte de mÃ©thode nettoyÃ© | Ã‰tape: [E5] | Source: [S8]  # #
            break  # # break stoppe la boucle dÃ¨s quâ€™on a trouvÃ© la premiÃ¨re section Method (Ã©vite de prendre plusieurs sections et de grossir inutilement) | Ã‰tape: [E5] | Source: [S0]  # #

    return method_text, references  # # On renvoie un tuple (method_text, references) : 2 blocs ciblÃ©s, simples, et prÃªts Ã  Ãªtre intÃ©grÃ©s dans lâ€™item JSON | Ã‰tape: [E1] | Source: [S1]  # #
# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
# ğŸ§  Filtrage thÃ©matique (par catÃ©gories + keywords)             # #  | Ã‰tape: [E1] | Source: [S0]  # #
# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
def _allowed_subcats_for_theme(theme: Optional[str]) -> List[str]:  # # Cette fonction dÃ©cide quelles sous-catÃ©gories arXiv sont autorisÃ©es selon le thÃ¨me demandÃ©, pour garder un pÃ©rimÃ¨tre clair et stable | Ã‰tape: [E1] | Source: [S0]  # #
    if theme and theme in THEME_TO_ARXIV_SUBCATS:  # # Ici on vÃ©rifie si lâ€™utilisateur a fourni un thÃ¨me valide, afin dâ€™appliquer le bon â€œfiltre thÃ©matiqueâ€ | Ã‰tape: [E1] | Source: [S0]  # #
        return THEME_TO_ARXIV_SUBCATS[theme]  # # Ici on renvoie directement la liste de catÃ©gories associÃ©e au thÃ¨me (ex: ai_ml â†’ cs.AI, cs.LGâ€¦) | Ã‰tape: [E1] | Source: [S0]  # #
    return sorted({c for lst in THEME_TO_ARXIV_SUBCATS.values() for c in lst})  # # Ici on renvoie lâ€™union de toutes les catÃ©gories autorisÃ©es (toujours limitÃ© aux thÃ¨mes dÃ©finis) quand aucun thÃ¨me nâ€™est fourni | Ã‰tape: [E1] | Source: [S0]  # #


def _keyword_filter(items: List[Dict[str, Any]], theme: Optional[str]) -> List[Dict[str, Any]]:  # # Cette fonction filtre les items via des mots-clÃ©s quand les catÃ©gories manquent ou sont instables, pour Ã©viter de perdre tous les rÃ©sultats si le parsing â€œSubjectsâ€ casse | Ã‰tape: [E2] | Source: [S6]  # #
    if not theme or theme not in THEME_KEYWORDS:  # # Ici on sort tout de suite si on nâ€™a pas de thÃ¨me exploitable (pas de filtrage keyword Ã  appliquer) | Ã‰tape: [E1] | Source: [S0]  # #
        return items  # # Ici on renvoie la liste inchangÃ©e pour ne pas supprimer des items sans raison (comportement stable) | Ã‰tape: [E1] | Source: [S1]  # #
    kws = [k.lower() for k in THEME_KEYWORDS[theme]]  # # Ici on met les keywords en minuscules pour comparer sans dÃ©pendre de la casse (robustesse) | Ã‰tape: [E1] | Source: [S0]  # #
    out: List[Dict[str, Any]] = []  # # Ici on prÃ©pare une nouvelle liste de sortie (format structurÃ©) pour stocker seulement les items qui matchent | Ã‰tape: [E1] | Source: [S1]  # #
    for it in items:  # # Ici on parcourt chaque item pour vÃ©rifier sâ€™il contient un des mots-clÃ©s du thÃ¨me | Ã‰tape: [E1] | Source: [S1]  # #
        blob = ((it.get("title") or "") + " " + (it.get("abstract") or "")).lower()  # # Ici on construit un â€œtexte de testâ€ (titre+abstract) en minuscules, car ce sont les champs les plus utiles pour un filtrage simple | Ã‰tape: [E1] | Source: [S0]  # #
        if any(k in blob for k in kws):  # # any() renvoie True si AU MOINS un keyword est prÃ©sent ; ici Ã§a sert Ã  garder lâ€™item si le sujet semble correspondre au thÃ¨me | Ã‰tape: [E1] | Source: [S0]  # #
            out.append(it)  # # Ici on ajoute lâ€™item Ã  la sortie car il est jugÃ© pertinent selon les mots-clÃ©s | Ã‰tape: [E1] | Source: [S1]  # #
    return out  # # Ici on renvoie la liste filtrÃ©e (explicite), utilisÃ©e comme fallback si les catÃ©gories sont inexploitables | Ã‰tape: [E1] | Source: [S0]  # #


def filter_items_by_subcats(items: List[Dict[str, Any]], allowed_subcats: List[str]) -> List[Dict[str, Any]]:  # # Cette fonction filtre les items par catÃ©gories arXiv (cs.AI, cs.CL, â€¦) pour respecter le pÃ©rimÃ¨tre du thÃ¨me demandÃ© | Ã‰tape: [E1] | Source: [S1]  # #
    allowed = set(allowed_subcats)  # # set() sert ici Ã  accÃ©lÃ©rer les tests â€œc in allowedâ€ (plus rapide quâ€™une liste) | Ã‰tape: [E1] | Source: [S0]  # #
    out: List[Dict[str, Any]] = []  # # Ici on initialise la liste de sortie qui contiendra uniquement les items autorisÃ©s | Ã‰tape: [E1] | Source: [S1]  # #
    for it in items:  # # Ici on parcourt chaque item collectÃ© depuis la page /search/cs | Ã‰tape: [E1] | Source: [S1]  # #
        cats = it.get("all_categories") or []  # # Ici on rÃ©cupÃ¨re les catÃ©gories de lâ€™item (ou [] si absent) pour dÃ©cider sâ€™il doit Ãªtre conservÃ© | Ã‰tape: [E1] | Source: [S0]  # #
        if not cats:  # # Ici, si les catÃ©gories nâ€™ont pas pu Ãªtre extraites (HTML changÃ©), on Ã©vite un faux nÃ©gatif en conservant lâ€™item | Ã‰tape: [E1] | Source: [S8]  # #
            out.append(it)  # # Ici on garde lâ€™item car on nâ€™a pas la preuve quâ€™il est hors pÃ©rimÃ¨tre (robustesse) | Ã‰tape: [E1] | Source: [S0]  # #
            continue  # # Ici on passe au suivant pour ne pas exÃ©cuter le test de matching sur une liste vide | Ã‰tape: [E1] | Source: [S0]  # #
        if any(c in allowed for c in cats):  # # any() teste si AU MOINS une catÃ©gorie de lâ€™item est autorisÃ©e (utile quand il y a plusieurs tags/cross-lists) | Ã‰tape: [E1] | Source: [S0]  # #
            out.append(it)  # # Ici on ajoute lâ€™item car il respecte le pÃ©rimÃ¨tre thÃ©matique | Ã‰tape: [E1] | Source: [S0]  # #
    return out  # # Ici on renvoie la liste filtrÃ©e finale par catÃ©gories | Ã‰tape: [E1] | Source: [S0]  # #


# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
# âœ… Fonction principale                                         # #  | Ã‰tape: [E1] | Source: [S0]  # #
# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
def scrape_arxiv_cs_scoped(
    user_query: str,  # # Texte de recherche utilisateur (FR: requÃªte) : câ€™est lâ€™entrÃ©e principale qui pilote lâ€™URL /search/cs | Ã‰tape: [E1] | Source: [S0]  # #
    theme: Optional[str] = None,  # # ThÃ¨me optionnel (FR: catÃ©gorie logique) : permet dâ€™activer un filtrage par sous-catÃ©gories arXiv | Ã‰tape: [E1] | Source: [S0]  # #
    max_results: int = 20,  # # Limite de rÃ©sultats : empÃªche un scraping massif et contrÃ´le le volume collectÃ© | Ã‰tape: [E1] | Source: [S0]  # #
    sort: str = "relevance",  # # Tri : â€œrelevanceâ€ ou â€œsubmitted_dateâ€, pour choisir lâ€™ordre des rÃ©sultats sans changer le parsing | Ã‰tape: [E1] | Source: [S0]  # #
    polite_min_s: float = 1.2,  # # Politesse (min) : dÃ©lai minimum entre requÃªtes pour Ã©viter dâ€™Ãªtre agressif cÃ´tÃ© serveur | Ã‰tape: [E4] | Source: [S5]  # #
    polite_max_s: float = 2.0,  # # Politesse (max) : dÃ©lai maximum (jitter) pour Ã©viter un rythme â€œrobotiqueâ€ | Ã‰tape: [E4] | Source: [S5]  # #
    data_lake_raw_dir: str = DEFAULT_RAW_DIR,  # # Dossier de sortie cache : garantit que JSON/HTML debug sont Ã©crits dans raw/cache | Ã‰tape: [E2] | Source: [S2]  # #
    enrich_abs: bool = True,  # # Enrichissement /abs : active la rÃ©cupÃ©ration DOI + versions + abstract fallback via la page /abs | Ã‰tape: [E1] | Source: [S0]  # #
    enable_keyword_filter: bool = True,  # # Filtrage keywords : fallback utile si les catÃ©gories â€œSubjectsâ€ ne sont pas fiables / manquantes | Ã‰tape: [E2] | Source: [S6]  # #
) -> Dict[str, Any]:  # # La fonction renvoie toujours un dict JSON stable (contrat) pour que FastAPI puisse lâ€™exposer sans surprise | Ã‰tape: [E1] | Source: [S1]  # #

    # ===============================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
    # ğŸ§± PrÃ©paration paramÃ¨tres                                     # #  | Ã‰tape: [E1] | Source: [S0]  # #
    # ===============================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
    max_results = int(max_results)  # # int() force le type entier (sÃ©curitÃ©) : Ã©vite de casser la pagination si on reÃ§oit â€œ5â€ en string | Ã‰tape: [E1] | Source: [S0]  # #
    if max_results < 1:  # # Ici on impose une borne basse pour ne jamais demander 0 rÃ©sultat (cas qui casse la logique de boucle) | Ã‰tape: [E1] | Source: [S0]  # #
        max_results = 1  # # Ici on corrige automatiquement en 1 (comportement stable) | Ã‰tape: [E1] | Source: [S0]  # #
    if max_results > MAX_RESULTS_HARD_LIMIT:  # # Ici on impose un cap dur pour empÃªcher un scraping massif par erreur | Ã‰tape: [E1] | Source: [S0]  # #
        max_results = MAX_RESULTS_HARD_LIMIT  # # Ici on applique le cap (anti-aspirateur) | Ã‰tape: [E1] | Source: [S0]  # #

    if not os.path.isabs(data_lake_raw_dir):  # # os.path.isabs() vÃ©rifie si le chemin est absolu ; ici Ã§a Ã©vite dâ€™Ã©crire â€œau hasardâ€ selon le CWD | Ã‰tape: [E2] | Source: [S2]  # #
        data_lake_raw_dir = os.path.abspath(os.path.join(PROJECT_ROOT, data_lake_raw_dir))  # # os.path.abspath() normalise vers un chemin absolu basÃ© sur PROJECT_ROOT (sortie prÃ©visible raw/cache) | Ã‰tape: [E2] | Source: [S2]  # #

    ensure_dir(data_lake_raw_dir)  # # ensure_dir() crÃ©e le dossier si nÃ©cessaire pour garantir que les fichiers JSON/HTML seront bien Ã©crits (pas dâ€™erreur â€œNo such fileâ€) | Ã‰tape: [E2] | Source: [S2]  # #
    ts = now_iso_for_filename()  # # now_iso_for_filename() fabrique un timestamp pour nommer les fichiers de maniÃ¨re unique et traÃ§able | Ã‰tape: [E1] | Source: [S0]  # #
    session = requests.Session()  # # requests.Session() garde une session HTTP rÃ©utilisable (cookies/connexions) : ici Ã§a rend les requÃªtes plus stables et plus efficaces | Ã‰tape: [E2] | Source: [S3]  # #

    errors_global: List[str] = []  # # Liste dâ€™erreurs globales (FR: erreurs du tool) : utilisÃ©e pour signaler 429/500/timeout sans dÃ©pendre des erreurs â€œpar itemâ€ | Ã‰tape: [E2] | Source: [S3]  # #

#---------------------------------------------------------------------------------
    # ===============================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
    # ğŸ¯ Allowed categories  # #  | Ã‰tape: [E1] | Source: [S0]  # #
    # ===============================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
    allowed_subcats = _allowed_subcats_for_theme(theme)  # # Liste cats # # Respect: pÃ©rimÃ¨tre thÃ¨mes | Ã‰tape: [E1] | Source: [S0]  # #

    # ===============================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
    # ğŸ” Pagination search/cs  # #  | Ã‰tape: [E1] | Source: [S0]  # #
    # ===============================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
    collected: List[Dict[str, Any]] = []  # # Items bruts CS # # Respect: collecte contrÃ´lÃ©e | Ã‰tape: [E1] | Source: [S1]  # #
    bundle_parts: List[str] = []  # # HTML debug # # Respect: cache local (pas envoyÃ© au LLM) | Ã‰tape: [E2] | Source: [S2]  # #
    start = 0  # # Pagination # # Respect: contrÃ´le volume | Ã‰tape: [E1] | Source: [S0]  # #
    last_search_url = ""  # # Debug # # Respect: traÃ§abilitÃ© | Ã‰tape: [E1] | Source: [S0]  # #
    last_search_http: Optional[int] = None  # # Debug # # Respect: traÃ§abilitÃ© | Ã‰tape: [E2] | Source: [S3]  # #
    diag_last: Dict[str, Any] = {}  # # Debug # # Respect: traÃ§abilitÃ© | Ã‰tape: [E2] | Source: [S6]  # #
    anti_bot_or_weird_page = False  # # Flag # # Respect: transparence | Ã‰tape: [E2] | Source: [S6]  # #

    while len(collected) < max_results:  # # Loop # # Respect: contrÃ´le volume | Ã‰tape: [E1] | Source: [S0]  # #
        search_url = build_search_url(query=user_query, start=start, size=PAGE_SIZE, sort=sort)  # # URL # # Respect: query simple | Ã‰tape: [E1] | Source: [S0]  # #
        last_search_url = search_url  # # Trace # # Respect: debug | Ã‰tape: [E1] | Source: [S0]  # #
        html, code = http_get_text(session=session, url=search_url, timeout_s=HTTP_TIMEOUT_S)  # # GET # # Respect: timeout+retry | Ã‰tape: [E2] | Source: [S3]  # #
        last_search_http = code  # # Trace # # Respect: debug | Ã‰tape: [E2] | Source: [S3]  # #

        weird = _detect_weird_page_signals(html)  # # Signaux # # Respect: diagnostiquer consent/robot/no-results | Ã‰tape: [E2] | Source: [S6]  # #

        bundle_parts.append(f"<!-- SEARCH URL: {search_url} | HTTP {code} -->\n")  # # En-tÃªte debug # # Respect: traÃ§abilitÃ© | Ã‰tape: [E2] | Source: [S3]  # #
        bundle_parts.append(f"<!-- WEIRD: {json.dumps(weird)} -->\n")  # # Signaux debug # # Respect: traÃ§abilitÃ© | Ã‰tape: [E2] | Source: [S6]  # #
        bundle_parts.append((html or "")[:200000])  # # Coupe 200k # # Respect: pas massif, cache debug local | Ã‰tape: [E2] | Source: [S2]  # #
        bundle_parts.append("\n<!-- END SEARCH -->\n")  # # Fin bloc # # Respect: traÃ§abilitÃ© | Ã‰tape: [E1] | Source: [S0]  # #

        if code != 200:  # # HTTP non-200 => erreur tool | Ã‰tape: [E2] | Source: [S3]  # #
            errors_global.append(f"SEARCH_HTTP_{code}")  # # Log erreur globale | Ã‰tape: [E2] | Source: [S0]  # #
            break  # # Stop # # Respect: ne pas boucler | Ã‰tape: [E1] | Source: [S0]  # #
        if weird.get("contains_we_are_sorry") or weird.get("contains_robot") or weird.get("contains_consent"):  # # DÃ©tecte page anti-bot/consent (Ã©vite faux parsing) | Ã‰tape: [E2] | Source: [S3]  # #
            anti_bot_or_weird_page = True  # # Marque page bizarre (pour diagnostic) | Ã‰tape: [E2] | Source: [S6]  # #
            errors_global.append("ANTI_BOT_OR_WEIRD_PAGE")  # # Erreur globale tool (contrat stable) | Ã‰tape: [E1] | Source: [S1]  # #
            break  # # Stop (on n'insiste pas) | Ã‰tape: [E2] | Source: [S0]  # #

        page_items, diag = parse_search_page(html)  # # Parse # # Respect: extraction ciblÃ©e | Ã‰tape: [E2] | Source: [S6]  # #
        diag_last = diag  # # Trace # # Respect: debug | Ã‰tape: [E2] | Source: [S6]  # #

        if diag.get("contains_no_results"):  # # Aucun rÃ©sultat # # Respect: robustesse | Ã‰tape: [E2] | Source: [S6]  # #
            break  # # Stop # # Respect: contrÃ´le | Ã‰tape: [E1] | Source: [S0]  # #
        if not page_items:  # # Aucun rÃ©sultat parsÃ© => diag + erreur soft | Ã‰tape: [E2] | Source: [S0]  # #
            errors_global.append("NO_RESULTS_PARSED")  # # Indique parsing vide | Ã‰tape: [E2] | Source: [S6]  # #
            break  # # Stop # # Respect: contrÃ´le | Ã‰tape: [E1] | Source: [S0]  # #

        collected.extend(page_items)  # # Ajoute # # Respect: collecte contrÃ´lÃ©e | Ã‰tape: [E1] | Source: [S1]  # #

        # âœ… CORRECTION IMPORTANTE : si la page a < PAGE_SIZE rÃ©sultats, inutile d'aller Ã  start+50
        if len(page_items) < PAGE_SIZE:  # # DerniÃ¨re page probable # # Respect: Ã©viter requÃªtes inutiles (et erreurs 500) | Ã‰tape: [E1] | Source: [S1]  # #
            break  # # Stop # # Respect: frÃ©quence raisonnable | Ã‰tape: [E1] | Source: [S0]  # #

        start += PAGE_SIZE  # # Next page # # Respect: pagination contrÃ´lÃ©e | Ã‰tape: [E1] | Source: [S0]  # #
        sleep_polite(min_s=polite_min_s, max_s=polite_max_s)  # # Politesse # # Respect: Ã©viter spam | Ã‰tape: [E4] | Source: [S5]  # #

    collected = collected[:max_results]  # # Tronque # # Respect: limite demandÃ©e | Ã‰tape: [E1] | Source: [S0]  # #

    # ===============================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
    # ğŸ§¹ Filtrage par catÃ©gories  # #  | Ã‰tape: [E1] | Source: [S0]  # #
    # ===============================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
    filtered = filter_items_by_subcats(collected, allowed_subcats=allowed_subcats)  # # Filtre cats # # Respect: scope demandÃ© | Ã‰tape: [E1] | Source: [S1]  # #
    if enable_keyword_filter:  # # Si activÃ© # # Respect: pertinence | Ã‰tape: [E1] | Source: [S0]  # #
        filtered = _keyword_filter(filtered, theme=theme)  # # Filtre mots-clÃ©s # # Respect: fallback si cats manquent | Ã‰tape: [E2] | Source: [S6]  # #

    # ===============================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
    # ğŸ” Enrich /abs  # #  | Ã‰tape: [E1] | Source: [S0]  # #
    # ===============================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
    if enrich_abs:  # # Si enrich # # Respect: enrichissement minimal | Ã‰tape: [E1] | Source: [S0]  # #
        for it in filtered:  # # Parcours # # Respect: traitement contrÃ´lÃ© | Ã‰tape: [E1] | Source: [S0]  # #
            it["doi"] = ""  # # Init # # Respect: champs stables | Ã‰tape: [E1] | Source: [S0]  # #
            it["versions"] = []  # # Init # # Respect: champs stables | Ã‰tape: [E1] | Source: [S0]  # #
            it["last_updated_raw"] = ""
            it["method"] = ""  # # MÃ©thode # # Ã‰tape: [ROBUSTESSE_PARSING] | Source: [S8]  # #
            it["references"] = []  # # RÃ©fÃ©rences # # Ã‰tape: [ROBUSTESSE_PARSING]# #  # # Init # # Respect: champs stables | Ã‰tape: [ROBUSTESSE_PARSING] | Source: [S8]  # #
            it["errors"] = []  # # Init # # Respect: sortie structurÃ©e | Ã‰tape: [E1] | Source: [S1]  # #

            url_abs = it.get("abs_url") or ""  # # URL # # Respect: utile | Ã‰tape: [E1] | Source: [S0]  # #
            if not url_abs:  # # Si absent # # Respect: robustesse | Ã‰tape: [E1] | Source: [S0]  # #
                continue  # # Skip # # Respect: robustesse | Ã‰tape: [E1] | Source: [S0]  # #

            abs_html, abs_code = http_get_text(session=session, url=url_abs, timeout_s=HTTP_TIMEOUT_S)  # # GET /abs # # Respect: timeout+retry | Ã‰tape: [E2] | Source: [S3]  # #
            bundle_parts.append(f"<!-- ABS URL: {url_abs} | HTTP {abs_code} -->\n")  # # Debug # # Respect: traÃ§abilitÃ© | Ã‰tape: [E2] | Source: [S3]  # #
            bundle_parts.append((abs_html or "")[:200000])  # # Coupe # # Respect: pas massif | Ã‰tape: [E1] | Source: [S0]  # #
            bundle_parts.append("\n<!-- END ABS -->\n")  # # Fin # # Respect: traÃ§abilitÃ© | Ã‰tape: [E1] | Source: [S0]  # #

            if abs_code == 200:  # # OK # # Respect: robustesse | Ã‰tape: [E1] | Source: [S0]  # #
                abs_data = parse_abs_page(abs_html)  # # Parse # # Respect: extraction ciblÃ©e | Ã‰tape: [E1] | Source: [S8]  # #
                it["doi"] = abs_data.get("doi", "")  # # DOI # # Respect: champ utile | Ã‰tape: [E1] | Source: [S0]  # #
                it["versions"] = abs_data.get("versions", [])  # # Versions # # Respect: champ utile | Ã‰tape: [E1] | Source: [S0]  # #
                it["last_updated_raw"] = abs_data.get("last_updated_raw", "")  # # Last
                html_url = extract_html_url_from_abs(abs_html=abs_html, arxiv_id=it.get("arxiv_id", ""))  # # Cherche lien /html # # Ã‰tape: [ROBUSTESSE_PARSING] | Source: [S8]  # #
                if html_url:  # # Si /html existe # # Ã‰tape: [ROBUSTESSE_PARSING] | Source: [S6]  # #
                    html_full, html_code = http_get_text(session=session, url=html_url, timeout_s=30)  # # GET /html # # Ã‰tape: [ROBUSTESSE_PARSING] | Source: [S2]  # #
                    bundle_parts.append(f"<!-- HTML URL: {html_url} | HTTP {html_code} -->\n")  # # Trace # # Ã‰tape: [ROBUSTESSE_PARSING] | Source: [S0]  # #
                    bundle_parts.append(html_full[:200000])  # # Cache debug # # Ã‰tape: [ROBUSTESSE_PARSING] | Source: [S2]  # #
                    bundle_parts.append("\n<!-- END HTML -->\n")  # # Fin # # Ã‰tape: [ROBUSTESSE_PARSING] | Source: [S2]  # #
                    if html_code == 200:  # # OK # # Ã‰tape: [ROBUSTESSE_PARSING] | Source: [S6]  # #
                        method_txt, refs_list = parse_arxiv_html_method_and_references(html_full)  # # Parse sections # # Ã‰tape: [ROBUSTESSE_PARSING] | Source: [S8]  # #
                        if method_txt:  # # Si trouvÃ© # # Ã‰tape: [ROBUSTESSE_PARSING] | Source: [S8]  # #
                            it["method"] = method_txt  # # Stocke # # Ã‰tape: [E1] | Source: [S1]  # #
                        if refs_list:  # # Si trouvÃ© # # Ã‰tape: [ROBUSTESSE_PARSING] | Source: [S0]  # #
                            it["references"] = refs_list  # # Stocke # # Ã‰tape: [E1] | Source: [S1]  # #
                    else:  # # KO # # Ã‰tape: [ROBUSTESSE_PARSING] | Source: [S0]  # #
                        it["errors"].append(f"html_http_{html_code}")  # # Trace # # Ã‰tape: [ROBUSTESSE_PARSING] | Source: [S0]  # #
 # # Respect: champ utile | Ã‰tape: [E1] | Source: [S0]  # #
                if is_empty(it.get("abstract")) and not is_empty(abs_data.get("abstract")):  # # Fallback abstract # # Respect: complÃ©ter sans bruit | Ã‰tape: [E2] | Source: [S6]  # #
                    it["abstract"] = abs_data.get("abstract", "")  # # Inject # # Respect: qualitÃ© | Ã‰tape: [E1] | Source: [S0]  # #
            else:  # # KO # # Respect: robustesse | Ã‰tape: [E1] | Source: [S0]  # #
                it["errors"].append(f"abs_http_{abs_code}")  # # Trace # # Respect: diagnostic | Ã‰tape: [E2] | Source: [S6]  # #

            sleep_polite(min_s=polite_min_s, max_s=polite_max_s)  # # Politesse # # Respect: Ã©viter spam | Ã‰tape: [E4] | Source: [S5]  # #

    # Missing fields
    for it in filtered:  # # Parcours # # Respect: diagnostic qualitÃ© | Ã‰tape: [E2] | Source: [S6]  # #
        it["missing_fields"] = compute_missing_fields(it)  # # Ajoute # # Respect: sortie structurÃ©e + debug | Ã‰tape: [E1] | Source: [S1]  # #

    # ===============================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
    # ğŸ’¾ Sauvegardes cache raw  # #  | Ã‰tape: [E2] | Source: [S2]  # #
    # ===============================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
    bundle_name = f"scrape_arxiv_cs_bundle_{ts}.html"  # # Nom # # Respect: cache debug | Ã‰tape: [E2] | Source: [S2]  # #
    bundle_path = save_text_file(data_lake_raw_dir, bundle_name, "\n".join(bundle_parts))  # # Save # # Respect: cache local visible | Ã‰tape: [E2] | Source: [S2]  # #

    result: Dict[str, Any] = {  # # RÃ©sultat # # Respect: sortie JSON structurÃ©e | Ã‰tape: [E1] | Source: [S1]  # #
        "ok": (len(errors_global) == 0),  # # OK seulement si aucune erreur globale | Ã‰tape: [E1]# #  # # Statut # # Respect: API stable | Ã‰tape: [E1] | Source: [S0]  # #
        "user_query": user_query,  # # Query # # Respect: traÃ§abilitÃ© | Ã‰tape: [E1] | Source: [S0]  # #
        "theme": theme,  # # ThÃ¨me # # Respect: traÃ§abilitÃ© | Ã‰tape: [E1] | Source: [S0]  # #
        "allowed_subcats": allowed_subcats,  # # PÃ©rimÃ¨tre # # Respect: scope explicite | Ã‰tape: [E1] | Source: [S0]  # #
        "sort": sort,  # # Tri # # Respect: contrÃ´le | Ã‰tape: [E1] | Source: [S0]  # #
        "requested_max_results": max_results,  # # Limite # # Respect: pas massif | Ã‰tape: [E1] | Source: [S0]  # #
        "count_collected_cs": len(collected),  # # Collecte # # Respect: debug | Ã‰tape: [E1] | Source: [S0]  # #
        "count_after_theme_filter": len(filtered),  # # AprÃ¨s filtre # # Respect: debug | Ã‰tape: [E1] | Source: [S0]  # #
        "items": filtered,  # # Items # # Respect: sortie structurÃ©e | Ã‰tape: [E1] | Source: [S1]  # #
        "bundle_html_file": bundle_path,  # # HTML debug # # Respect: cache local (pas LLM) | Ã‰tape: [E2] | Source: [S2]  # #
        "supported_fields": SUPPORTED_FIELDS,  # # SchÃ©ma # # Respect: contrat clair | Ã‰tape: [E1] | Source: [S1]  # #
        # Debug important
        "project_root": PROJECT_ROOT,  # # OÃ¹ est le projet # # Respect: traÃ§abilitÃ© | Ã‰tape: [E1] | Source: [S0]  # #
        "raw_cache_dir": data_lake_raw_dir,  # # OÃ¹ Ã©crit-on # # Respect: visibilitÃ© | Ã‰tape: [E2] | Source: [S2]  # #
        "cwd_runtime": os.getcwd(),  # # CWD # # Respect: debug uvicorn | Ã‰tape: [E1] | Source: [S0]  # #
        "last_search_url": last_search_url,  # # DerniÃ¨re URL # # Respect: debug | Ã‰tape: [E1] | Source: [S0]  # #
        "last_search_http": last_search_http,  # # Dernier code HTTP search (0 = erreur rÃ©seau) | Ã‰tape: [E2]# #  # # Dernier HTTP # # Respect: debug | Ã‰tape: [E2] | Source: [S3]  # #
        "parse_diag_last": diag_last,  # # Dernier diag # # Respect: debug | Ã‰tape: [E2] | Source: [S6]  # #
        "anti_bot_or_weird_page": anti_bot_or_weird_page,  # # Flag # # Respect: transparence | Ã‰tape: [E2] | Source: [S6]  # #
    }  # # Fin result # # Respect: JSON propre | Ã‰tape: [E1] | Source: [S1]  # #

    json_name = f"scrape_arxiv_cs_{ts}.json"  # # Nom json # # Respect: cache rÃ©sultat | Ã‰tape: [E2] | Source: [S2]  # #
    json_path = os.path.join(data_lake_raw_dir, json_name)  # # Path # # Respect: cache local | Ã‰tape: [E2] | Source: [S2]  # #
    with open(json_path, "w", encoding="utf-8") as f:  # # Open # # Respect: robustesse encodage | Ã‰tape: [E1] | Source: [S1]  # #
        json.dump(result, f, ensure_ascii=False, indent=2)  # # Dump # # Respect: sortie structurÃ©e | Ã‰tape: [E1] | Source: [S1]  # #

    result["saved_to"] = json_path  # # Chemin # # Respect: retrouver facilement le fichier | Ã‰tape: [E1] | Source: [S1]  # #
    return result  # # Retour # # Respect: contrat clair | Ã‰tape: [E1] | Source: [S0]  # #


# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
# âœ… Alias compatibilitÃ© avec ton main.py  # #  | Ã‰tape: [E1] | Source: [S0]  # #
# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
def scrape_arxiv_cs(  # # Alias # # Respect: ne pas casser ton main.py existant | Ã‰tape: [E1] | Source: [S0]  # #
    query: str,  # # Query # # Respect: input simple | Ã‰tape: [E1] | Source: [S0]  # #
    max_results: int = 50,  # # Limite # # Respect: contrÃ´le volume | Ã‰tape: [E1] | Source: [S0]  # #
    sort: str = "relevance",  # # Tri # # Respect: contrÃ´le | Ã‰tape: [E1] | Source: [S0]  # #
    polite_min_s: float = 1.2,  # # Politesse # # Respect: frÃ©quence raisonnable | Ã‰tape: [E4] | Source: [S5]  # #
    polite_max_s: float = 2.0,  # # Politesse # # Respect: frÃ©quence raisonnable | Ã‰tape: [E4] | Source: [S5]  # #
    data_lake_raw_dir: str = DEFAULT_RAW_DIR,  # # Cache # # Respect: Ã©crit dans raw/cache | Ã‰tape: [E2] | Source: [S2]  # #
    theme: Optional[str] = None,  # # ThÃ¨me # # Respect: scope | Ã‰tape: [E1] | Source: [S0]  # #
) -> Dict[str, Any]:  # # Retour structurÃ© # # Respect: JSON | Ã‰tape: [E1] | Source: [S1]  # #
    return scrape_arxiv_cs_scoped(  # # Forward # # Respect: point d'entrÃ©e unique | Ã‰tape: [E1] | Source: [S0]  # #
        user_query=query,  # # Map # # Respect: cohÃ©rence | Ã‰tape: [E1] | Source: [S0]  # #
        theme=theme,  # # Map # # Respect: cohÃ©rence | Ã‰tape: [E1] | Source: [S0]  # #
        max_results=max_results,  # # Map # # Respect: cohÃ©rence | Ã‰tape: [E1] | Source: [S0]  # #
        sort=sort,  # # Map # # Respect: cohÃ©rence | Ã‰tape: [E1] | Source: [S0]  # #
        polite_min_s=polite_min_s,  # # Map # # Respect: cohÃ©rence | Ã‰tape: [E4] | Source: [S5]  # #
        polite_max_s=polite_max_s,  # # Map # # Respect: cohÃ©rence | Ã‰tape: [E4] | Source: [S5]  # #
        data_lake_raw_dir=data_lake_raw_dir,  # # Map # # Respect: cohÃ©rence | Ã‰tape: [E2] | Source: [S2]  # #
        enrich_abs=True,  # # On enrichit # # Respect: utile (doi/versions/abstract) | Ã‰tape: [E1] | Source: [S0]  # #
        enable_keyword_filter=True,  # # On garde fallback # # Respect: Ã©vite faux nÃ©gatifs | Ã‰tape: [E2] | Source: [S6]  # #
    )


# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
# âœ… TEST LOCAL  # #  | Ã‰tape: [E1] | Source: [S0]  # #
# ============================================================  # #  | Ã‰tape: [E1] | Source: [S0]  # #
RUN_LOCAL_TEST = True  # # True = test ON # # Respect: debug local sans FastAPI | Ã‰tape: [E1] | Source: [S0]  # #

if __name__ == "__main__" and RUN_LOCAL_TEST:  # # Entry # # Respect: exÃ©cution locale maÃ®trisÃ©e | Ã‰tape: [E2] | Source: [S3]  # #
    res = scrape_arxiv_cs_scoped(  # # Run # # Respect: test contrÃ´lÃ© | Ã‰tape: [E1] | Source: [S0]  # #
        user_query="multimodal transformer misogyny detection",  # # Exemple # # Respect: besoin informationnel | Ã‰tape: [E1] | Source: [S0]  # #
        theme="ai_ml",  # # ThÃ¨me # # Respect: pÃ©rimÃ¨tre demandÃ© | Ã‰tape: [E1] | Source: [S0]  # #
        max_results=5,  # # Limite # # Respect: pas massif | Ã‰tape: [E1] | Source: [S0]  # #
        sort="relevance",  # # Tri # # Respect: contrÃ´le | Ã‰tape: [E1] | Source: [S0]  # #
        data_lake_raw_dir=DEFAULT_RAW_DIR,  # # Cache # # Respect: Ã©crit au bon endroit | Ã‰tape: [E2] | Source: [S2]  # #
        enrich_abs=True,  # # Enrich # # Respect: utile | Ã‰tape: [E1] | Source: [S0]  # #
    )  # # Fin # # Respect: test | Ã‰tape: [E1] | Source: [S0]  # #
    print(json.dumps({  # # Print # # Respect: debug lisible | Ã‰tape: [E1] | Source: [S1]  # #
        "count_collected_cs": res.get("count_collected_cs"),  # # Info # # Respect: debug | Ã‰tape: [E1] | Source: [S0]  # #
        "count_after_theme_filter": res.get("count_after_theme_filter"),  # # Info # # Respect: debug | Ã‰tape: [E1] | Source: [S0]  # #
        "saved_to": res.get("saved_to"),  # # Info # # Respect: retrouver JSON | Ã‰tape: [E1] | Source: [S1]  # #
        "bundle_html_file": res.get("bundle_html_file"),  # # Info # # Respect: retrouver HTML | Ã‰tape: [E1] | Source: [S0]  # #
        "anti_bot_or_weird_page": res.get("anti_bot_or_weird_page"),  # # Info # # Respect: transparence | Ã‰tape: [E2] | Source: [S6]  # #
        "last_search_http": res.get("last_search_http"),  # # Info # # Respect: debug | Ã‰tape: [E2] | Source: [S3]  # #
        "parse_diag_last": res.get("parse_diag_last"),  # # Info # # Respect: debug | Ã‰tape: [E2] | Source: [S6]  # #
    }, ensure_ascii=False, indent=2))  # # Pretty # # Respect: lecture facile | Ã‰tape: [E1] | Source: [S0]  # #