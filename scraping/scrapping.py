# ============================================================
# ğŸ•·ï¸ SCRAPER arXiv â€” Structure pÃ©dagogique (GET -> SELECT -> STORE)
# ============================================================

import os  # # ğŸ“ GÃ©rer dossiers/chemins
import re  # # ğŸ” Extraire des infos avec regex
import json  # # ğŸ§¾ Sauvegarder en JSON
import time  # # â±ï¸ Pause polie entre requÃªtes
import random  # # ğŸ² Jitter pour rythme naturel
import requests  # # ğŸŒ Faire des GET HTTP
from bs4 import BeautifulSoup  # # ğŸ§  Parser HTML et sÃ©lectionner des balises

ARXIV_BASE = "https://arxiv.org"  # # ğŸŒ Domaine arXiv
ARXIV_SEARCH_CS = "https://arxiv.org/search/cs"  # # ğŸ” URL recherche Computer Science

# ============================================================
# ğŸ§± A) STOCKAGE DES PAGES (HTML BRUT) â€” "on garde tous les GET"
# ============================================================

def store_raw_html(raw_dir: str, filename: str, html_text: str) -> str:  # # ğŸ’¾ Stocker un HTML brut sur disque
    os.makedirs(raw_dir, exist_ok=True)  # # ğŸ“ CrÃ©er le dossier si besoin
    path = os.path.join(raw_dir, filename)  # # ğŸ§© Construire le chemin complet
    with open(path, "w", encoding="utf-8") as f:  # # âœï¸ Ouvrir un fichier texte
        f.write(html_text)  # # ğŸ§¾ Ã‰crire le HTML brut
    return path  # # ğŸ“Œ Retourner le chemin du fichier sauvegardÃ©

# ============================================================
# ğŸŒ B) GET â€” tÃ©lÃ©charger le HTML dâ€™une page
# ============================================================

def http_get(url: str, session: requests.Session, timeout_s: int = 30) -> str:  # # ğŸŒ GET = tÃ©lÃ©charger la page en HTML
    headers = {"User-Agent": "Mozilla/5.0 arXivScraper/1.0"}  # # ğŸªª User-Agent simple pour Ã©viter refus basiques
    response = session.get(url, headers=headers, timeout=timeout_s)  # # ğŸš€ Faire la requÃªte HTTP GET
    response.raise_for_status()  # # âŒ Lever erreur si status 4xx/5xx
    return response.text  # # ğŸ“„ Retourner le contenu HTML brut

def sleep_polite(min_s: float = 1.5, max_s: float = 2.0) -> None:  # # ğŸ˜‡ Pause polie
    time.sleep(random.uniform(min_s, max_s))  # # â³ Attendre un peu (anti-spam)

# ============================================================
# ğŸ” C) SELECT â€” extraire titres/auteurs/abstracts depuis la page search
# ============================================================

def parse_search_page(html: str) -> list:  # # ğŸ§© Transformer un HTML search -> liste dâ€™articles
    soup = BeautifulSoup(html, "lxml")  # # ğŸ² PARSE : convertir HTML en objet navigable
    papers = []  # # ğŸ“¦ STOCKAGE : liste de dictionnaires (futur JSON)

    for item in soup.select("ol.breathe-horizontal li.arxiv-result"):  # # ğŸ“š SELECT : chaque rÃ©sultat dans la liste
        title_el = item.select_one("p.title")  # # ğŸ·ï¸ SELECT : balise titre
        authors_el = item.select_one("p.authors")  # # ğŸ‘¥ SELECT : balise auteurs
        abstract_el = item.select_one("span.abstract-full")  # # ğŸ§¾ SELECT : balise abstract complet
        abs_link_el = item.select_one('p.list-title a[href^="/abs/"]')  # # ğŸ”— SELECT : lien /abs/
        pdf_link_el = item.select_one('a[href^="/pdf/"]')  # # ğŸ“„ SELECT : lien /pdf/

        title = title_el.get_text(" ", strip=True) if title_el else ""  # # ğŸ·ï¸ EXTRACTION : texte du titre
        authors_txt = authors_el.get_text(" ", strip=True) if authors_el else ""  # # ğŸ‘¥ EXTRACTION : texte auteurs
        authors = [a.strip() for a in authors_txt.replace("Authors:", "").split(",") if a.strip()]  # # ğŸ§  Nettoyage auteurs

        abstract = abstract_el.get_text(" ", strip=True) if abstract_el else ""  # # ğŸ§¾ EXTRACTION : texte abstract
        abstract = abstract.replace("â–³ Less", "").strip()  # # ğŸ§¹ Nettoyage petit artefact

        abs_url = ARXIV_BASE + abs_link_el["href"] if abs_link_el and abs_link_el.get("href") else ""  # # ğŸ”— EXTRACTION : URL abstract
        pdf_url = ARXIV_BASE + pdf_link_el["href"] if pdf_link_el and pdf_link_el.get("href") else ""  # # ğŸ“„ EXTRACTION : URL PDF

        arxiv_id = ""  # # ğŸ†” PrÃ©parer lâ€™ID
        m = re.search(r"/abs/([^/]+)$", abs_url)  # # ğŸ” EXTRACTION : arXiv ID depuis /abs/
        if m:  # # âœ… Si match
            arxiv_id = m.group(1)  # # ğŸ†” Stocker lâ€™ID

        papers.append({  # # ğŸ“¦ STOCKAGE : dict = 1 article structurÃ©
            "arxiv_id": arxiv_id,  # # ğŸ†”
            "title": title,  # # ğŸ·ï¸
            "authors": authors,  # # ğŸ‘¥
            "abstract": abstract,  # # ğŸ§¾ âœ… Oui : on rÃ©cupÃ¨re lâ€™abstract ici
            "abs_url": abs_url,  # # ğŸ”—
            "pdf_url": pdf_url,  # # ğŸ“„ (si demande PDF -> on renvoie le lien)
        })  # # âœ… Fin dict

    return papers  # # ğŸ“¤ Retourne la liste de rÃ©sultats

# ============================================================
# ğŸ§  D) PIPELINE â€” enchaÃ®ner GET -> SELECT -> STORE -> JSON
# ============================================================

def scrape_arxiv_cs(query: str, max_results: int = 10, raw_dir: str = "data_lake/raw") -> dict:  # # ğŸš€ Fonction principale
    session = requests.Session()  # # ğŸ”Œ CrÃ©er une session HTTP (plus efficace)
    max_results = min(int(max_results), 100)  # # ğŸš§ Limite max 100 demandÃ©e

    # ğŸ”— Construire lâ€™URL search (page 1)
    url = f"{ARXIV_SEARCH_CS}?query={requests.utils.quote(query)}&searchtype=all&abstracts=show&size=50&start=0"  # # ğŸ” URL search

    html_search = http_get(url, session=session)  # # ğŸŒ GET : tÃ©lÃ©charger HTML de la page search
    store_raw_html(raw_dir, "arxiv_search_page_0.html", html_search)  # # ğŸ’¾ STOCKAGE : garder le HTML brut

    papers = parse_search_page(html_search)  # # ğŸ” SELECT : extraire les champs depuis le HTML
    papers = papers[:max_results]  # # âœ‚ï¸ Appliquer la limite (MVP)

    result = {  # # ğŸ§¾ JSON final (API-friendly)
        "ok": True,  # # âœ…
        "query": query,  # # ğŸ”
        "max_results": max_results,  # # ğŸ¯
        "count": len(papers),  # # ğŸ”¢
        "hit_limit_100": (max_results == 100),  # # ğŸš§ Indicateur limite
        "message_if_limit": "Limite 100 atteinte (max_results)." if max_results == 100 else "",  # # ğŸ§¾ Message
        "items": papers,  # # ğŸ“š RÃ©sultats
    }  # # âœ… Fin JSON

    out_json = os.path.join(raw_dir, "arxiv_raw.json")  # # ğŸ“ Chemin JSON raw
    with open(out_json, "w", encoding="utf-8") as f:  # # âœï¸ Ouvrir fichier JSON
        json.dump(result, f, ensure_ascii=False, indent=2)  # # ğŸ§¾ Sauvegarder

    return result  # # ğŸ“¤ Retourner au backend

# ============================================================
# ğŸ§ª TEST LOCAL (facultatif) â€” exÃ©cuter le scraper seul
# ============================================================

RUN_LOCAL_TEST = True  # # âœ… Mets True pour tester | Mets False pour dÃ©sactiver (ou commente la ligne avec #)

if __name__ == "__main__" and RUN_LOCAL_TEST:  # # â–¶ï¸ Lance le test seulement si le fichier est exÃ©cutÃ© directement

    print("ğŸš€ Lancement du scraping arXiv (test local)...")  # # ğŸ–¨ï¸ Message dÃ©but

    results = scrape_arxiv_cs(  # # ğŸ•·ï¸ Appel de TA fonction principale (nom correct)
        query="multimodal transformer",  # # ğŸ” Exemple requÃªte
        max_results=5,  # # ğŸ¯ Petit test
        sort="relevance",  # # ğŸ§­ Tri
        subcategory="cs.AI",  # # ğŸ§© Sous-catÃ©gorie (optionnel)
        polite_min_s=1.5,  # # ğŸ˜‡ Politesse
        polite_max_s=2.0,  # # ğŸ˜‡ Politesse
        data_lake_raw_dir="data_lake/raw",  # # ğŸ’¾ Dossier raw
    )  # # âœ… Fin appel

    print(f"âœ… OK: {results.get('count')} articles rÃ©cupÃ©rÃ©s")  # # ğŸ–¨ï¸ Nombre rÃ©cupÃ©rÃ©
    print(f"ğŸ’¾ JSON sauvegardÃ©: {results.get('saved_to')}")  # # ğŸ“Œ OÃ¹ est le fichier raw

    # ğŸ‘€ AperÃ§u dâ€™un article (le premier)
    items = results.get("items", [])  # # ğŸ“¦ RÃ©cupÃ©rer la liste
    if items:  # # âœ… Si non vide
        print("ğŸ§¾ AperÃ§u 1er article:")  # # ğŸ–¨ï¸
        print(json.dumps(items[0], indent=2, ensure_ascii=False))  # # ğŸ§¾ Afficher 1er item en JSON
