# ============================================================  # # ğŸ“Œ DÃ©but du script
# ğŸ•·ï¸ arXiv Scraper (search/cs) -> 1 HTML (bundle) + 1 JSON (raw)  # # ğŸ¯ Objectif du script
# ============================================================  # # ğŸ“Œ SÃ©parateur visuel

import os  # # ğŸ“ Gestion des chemins/dossiers
import re  # # ğŸ” Regex (id, versions, etc.)
import json  # # ğŸ§¾ Export JSON
import time  # # â±ï¸ Politesse (sleep)
import random  # # ğŸ² Jitter (Ã©viter rythme robot)
import datetime  # # ğŸ•’ Timestamp fichiers
from typing import Dict, Any, List, Optional, Tuple  # # ğŸ§© Typage pour clartÃ©

import requests  # # ğŸŒ HTTP (GET)
from bs4 import BeautifulSoup  # # ğŸ² Parsing HTML
from pypdf import PdfReader  # # ğŸ“„ Extraction PDF (1er essai)
from pdfminer.high_level import extract_text  # # ğŸ“„ Extraction PDF (fallback)
from io import BytesIO  # # ğŸ§  Parser PDF en mÃ©moire (pas de fichier PDF)

ARXIV_BASE = "https://arxiv.org"  # # ğŸŒ Domaine arXiv
ARXIV_SEARCH_CS = "https://arxiv.org/search/cs"  # # ğŸ” Recherche CS (HTML)
DEFAULT_RAW_DIR = os.path.join("data_lake", "raw")  # # ğŸ“¦ Stockage raw (HTML+JSON)
MAX_RESULTS_HARD_LIMIT = 100  # # ğŸš§ Max global demandÃ©
PAGE_SIZE = 50  # # ğŸ“„ Pagination arXiv (50)

# ============================================================  # # ğŸ“Œ SÃ©parateur
# âœ… A) Helpers (dossiers, timestamps, â€œvideâ€, politesse, GET)
# ============================================================  # # ğŸ“Œ SÃ©parateur

def ensure_dir(path: str) -> None:  # # ğŸ“ CrÃ©er dossier si besoin
    os.makedirs(path, exist_ok=True)  # # âœ…

def now_iso_for_filename() -> str:  # # ğŸ•’ Timestamp pour nom fichier
    return datetime.datetime.now().strftime("%Y%m%d_%H%M%S")  # # ğŸ§¾ Exemple: 20260113_154500

def is_empty(value: Any) -> bool:  # # ğŸ§ª DÃ©finition du â€œvideâ€ demandÃ©e
    if value is None:  # # âœ… None
        return True  # # âœ… vide
    if isinstance(value, str):  # # ğŸ§¾ Si string
        v = value.strip()  # # ğŸ§¹ Nettoyage
        if v == "":  # # âœ… vide si ""
            return True  # # âœ…
        if v.lower() in {"n/a", "null", "none"}:  # # âœ… vide si "N/A", "null", "None" (string)
            return True  # # âœ…
    if isinstance(value, list):  # # ğŸ“¦ Si liste
        return len(value) == 0  # # âœ… vide si liste vide
    return False  # # âŒ sinon non vide

def sleep_polite(min_s: float = 1.5, max_s: float = 2.0) -> None:  # # ğŸ˜‡ Pause polie
    time.sleep(random.uniform(min_s, max_s))  # # â³

def http_get_text(session: requests.Session, url: str, timeout_s: int = 30) -> Tuple[str, int]:  # # ğŸŒ GET HTML (texte)
    headers = {"User-Agent": "Mozilla/5.0 DIXITBOT-arXivScraper/1.0"}  # # ğŸªª UA simple
    resp = session.get(url, headers=headers, timeout=timeout_s)  # # ğŸš€ GET
    return resp.text, resp.status_code  # # ğŸ“„ HTML + code HTTP

def http_get_bytes(session: requests.Session, url: str, timeout_s: int = 60) -> Tuple[bytes, int]:  # # ğŸŒ GET binaire (PDF)
    headers = {"User-Agent": "Mozilla/5.0 DIXITBOT-arXivScraper/1.0"}  # # ğŸªª UA simple
    resp = session.get(url, headers=headers, timeout=timeout_s)  # # ğŸš€ GET
    return resp.content, resp.status_code  # # ğŸ“¦ bytes + code HTTP

def save_text_file(folder: str, filename: str, content: str) -> str:  # # ğŸ’¾ Sauver un fichier texte (bundle HTML / JSON)
    ensure_dir(folder)  # # ğŸ“
    path = os.path.join(folder, filename)  # # ğŸ§©
    with open(path, "w", encoding="utf-8") as f:  # # âœï¸
        f.write(content)  # # ğŸ§¾
    return path  # # ğŸ“Œ

# ============================================================  # # ğŸ“Œ SÃ©parateur
# ğŸ” B) URL builder (tri compatible arXiv)
# ============================================================  # # ğŸ“Œ SÃ©parateur

def build_search_url(query: str, start: int, size: int, sort: str) -> str:  # # ğŸ”— Construire URL search/cs
    q = requests.utils.quote(query)  # # ğŸ” Encoder requÃªte
    base = f"{ARXIV_SEARCH_CS}?query={q}&searchtype=all&abstracts=show&size={size}&start={start}"  # # ğŸ”— Base URL
    s = (sort or "relevance").strip().lower()  # # ğŸ§  Normaliser sort
    if s in {"submitted_date", "submitted", "recent"}:  # # ğŸ—“ï¸ â€œles plus rÃ©centsâ€
        return base + "&order=-announced_date_first"  # # âœ… Valeur qui fonctionne (HTML search)
    if s in {"last_updated_date", "updated", "last_updated"}:  # # ğŸ” â€œderniÃ¨res mises Ã  jourâ€
        return base + "&order=-last_updated_date"  # # âœ… Valeur utilisÃ©e par arXiv
    # âœ… Par dÃ©faut â€œrelevanceâ€ => on NE met pas order (Ã©vite 400 sur -relevance)
    return base  # # âœ…

# ============================================================  # # ğŸ“Œ SÃ©parateur
# ğŸ“„ C) Parsing page de recherche (on rÃ©cupÃ¨re dÃ©jÃ : titre, auteurs, abstract, pdf, abs, submitted)
# ============================================================  # # ğŸ“Œ SÃ©parateur

def parse_search_page(html: str) -> List[Dict[str, Any]]:  # # ğŸ§© HTML -> items
    soup = BeautifulSoup(html, "lxml")  # # ğŸ² Parser HTML
    items: List[Dict[str, Any]] = []  # # ğŸ“¦ RÃ©sultats

    for li in soup.select("ol.breathe-horizontal li.arxiv-result"):  # # ğŸ“š Chaque rÃ©sultat
        title_el = li.select_one("p.title")  # # ğŸ·ï¸ Titre
        authors_el = li.select_one("p.authors")  # # ğŸ‘¥ Auteurs
        abstract_el = li.select_one("span.abstract-full")  # # ğŸ§¾ Abstract (full)
        abs_link_el = li.select_one('p.list-title a[href^="/abs/"]')  # # ğŸ”— /abs/
        pdf_link_el = li.select_one('p.list-title a[href^="/pdf/"]')  # # ğŸ“„ /pdf/
        submitted_el = li.select_one("p.is-size-7")  # # ğŸ—“ï¸ Bloc Submitted (search page)

        title = title_el.get_text(" ", strip=True) if title_el else ""  # # ğŸ·ï¸
        authors_txt = authors_el.get_text(" ", strip=True) if authors_el else ""  # # ğŸ‘¥
        authors = [a.strip() for a in authors_txt.replace("Authors:", "").split(",") if a.strip()]  # # ğŸ‘¥
        abstract = abstract_el.get_text(" ", strip=True) if abstract_el else ""  # # ğŸ§¾
        abstract = abstract.replace("â–³ Less", "").strip()  # # ğŸ§¹

        abs_url = (ARXIV_BASE + abs_link_el.get("href", "")) if abs_link_el else ""  # # ğŸ”—
        pdf_url = (ARXIV_BASE + pdf_link_el.get("href", "")) if pdf_link_el else ""  # # ğŸ“„

        arxiv_id = ""  # # ğŸ†”
        m = re.search(r"/abs/([^/]+)$", abs_url) if abs_url else None  # # ğŸ”
        if m:  # # âœ…
            arxiv_id = m.group(1)  # # ğŸ†”

        submitted_date = ""  # # ğŸ—“ï¸
        if submitted_el:  # # âœ…
            txt = submitted_el.get_text(" ", strip=True)  # # ğŸ§¾
            m2 = re.search(r"Submitted\s+(\d+\s+\w+,\s+\d{4})", txt)  # # ğŸ”
            if m2:  # # âœ…
                submitted_date = m2.group(1)  # # ğŸ—“ï¸

        items.append({  # # ğŸ“¦ Item minimal
            "arxiv_id": arxiv_id,  # # ğŸ†”
            "title": title,  # # ğŸ·ï¸
            "authors": authors,  # # ğŸ‘¥
            "abstract": abstract,  # # ğŸ§¾
            "submitted_date": submitted_date,  # # ğŸ—“ï¸
            "abs_url": abs_url,  # # ğŸ”—
            "pdf_url": pdf_url,  # # ğŸ“„
        })  # # âœ…

    return items  # # ğŸ“¤

# ============================================================  # # ğŸ“Œ SÃ©parateur
# ğŸ“Œ D) Parsing page /abs (enrichissement â€œBâ€)
# ============================================================  # # ğŸ“Œ SÃ©parateur

def parse_abs_page(html: str, arxiv_id: str) -> Dict[str, Any]:  # # ğŸ§© /abs HTML -> dict
    soup = BeautifulSoup(html, "lxml")  # # ğŸ²
    out: Dict[str, Any] = {}  # # ğŸ“¦

    doi_el = soup.select_one('td.tablecell.arxivid a[href^="https://doi.org/"]')  # # ğŸ”— DOI (parfois)
    if doi_el:  # # âœ…
        out["doi"] = doi_el.get_text(" ", strip=True)  # # ğŸ§¾

    license_el = soup.select_one('div.submission-history + div.metatable td.tablecell a[href*="license"]')  # # ğŸ“œ Licence (selon structure)
    if license_el:  # # âœ…
        out["license"] = license_el.get_text(" ", strip=True)  # # ğŸ“œ

    # ğŸ·ï¸ CatÃ©gories / subjects
    subjects_el = soup.select_one("td.tablecell.subjects")  # # ğŸ§  Subjects
    if subjects_el:  # # âœ…
        out["subjects"] = subjects_el.get_text(" ", strip=True)  # # ğŸ§ 

    # ğŸ’¬ Comments / Journal ref
    comments_el = soup.select_one("td.tablecell.comments")  # # ğŸ’¬
    if comments_el:  # # âœ…
        out["comments"] = comments_el.get_text(" ", strip=True)  # # ğŸ’¬
    jref_el = soup.select_one("td.tablecell.jref")  # # ğŸ“š
    if jref_el:  # # âœ…
        out["journal_ref"] = jref_el.get_text(" ", strip=True)  # # ğŸ“š

    # ğŸ” Submission history (versions + dates)
    versions: List[Dict[str, str]] = []  # # ğŸ“¦
    for li in soup.select("div.submission-history li"):  # # ğŸ” Chaque version
        txt = li.get_text(" ", strip=True)  # # ğŸ§¾
        m = re.search(r"\[(v\d+)\]\s*(.*)$", txt)  # # ğŸ”
        if m:  # # âœ…
            versions.append({"version": m.group(1), "raw": m.group(2)})  # # ğŸ“¦
    if versions:  # # âœ…
        out["versions"] = versions  # # ğŸ”
        out["last_updated_date"] = versions[-1].get("raw", "")  # # ğŸ—“ï¸ Approx (texte brut)

    # ğŸ“„ PDF URL stable (au cas oÃ¹ search page lâ€™a ratÃ©)
    out["pdf_url"] = f"{ARXIV_BASE}/pdf/{arxiv_id}" if arxiv_id else ""  # # ğŸ“„

    return out  # # ğŸ“¤

# ============================================================  # # ğŸ“Œ SÃ©parateur
# ğŸ§¾ E) /html/<id> (tentative â€œCâ€) + extraction contenu
# ============================================================  # # ğŸ“Œ SÃ©parateur

def try_fetch_html_content(session: requests.Session, arxiv_id: str) -> Tuple[str, bool, str]:  # # ğŸŒ /html -> texte
    if not arxiv_id:  # # ğŸš«
        return "", True, ""  # # âœ… html_unavailable
    url = f"{ARXIV_BASE}/html/{arxiv_id}"  # # ğŸ”—
    html, code = http_get_text(session=session, url=url)  # # ğŸŒ GET
    if code != 200:  # # âŒ
        return "", True, url  # # âœ…
    soup = BeautifulSoup(html, "lxml")  # # ğŸ²
    main = soup.select_one("main")  # # ğŸ¯ Contenu principal
    text = main.get_text("\n", strip=True) if main else soup.get_text("\n", strip=True)  # # ğŸ§¾
    return text, False, url  # # âœ…

# ============================================================  # # ğŸ“Œ SÃ©parateur
# ğŸ“„ F) Fallback PDF (tentative â€œDâ€) â€” parsing en mÃ©moire (pas de stockage)
# ============================================================  # # ğŸ“Œ SÃ©parateur

def extract_pdf_text_in_memory(session: requests.Session, pdf_url: str) -> Tuple[str, str]:  # # ğŸ“„ PDF -> texte
    if not pdf_url:  # # ğŸš«
        return "", ""  # # âœ…
    pdf_bytes, code = http_get_bytes(session=session, url=pdf_url)  # # ğŸŒ GET PDF bytes
    if code != 200 or not pdf_bytes:  # # âŒ
        return "", pdf_url  # # âœ…
    text = ""  # # ğŸ§¾
    try:  # # ğŸ§ª 1) pypdf
        reader = PdfReader(BytesIO(pdf_bytes))  # # ğŸ“„ Ouvrir en mÃ©moire
        pages_text: List[str] = []  # # ğŸ“¦
        for p in reader.pages:  # # ğŸ” Pages
            t = p.extract_text() or ""  # # ğŸ§¾
            if t.strip():  # # âœ…
                pages_text.append(t)  # # â•
        text = "\n".join(pages_text).strip()  # # ğŸ§¾
    except Exception:  # # âŒ
        text = ""  # # ğŸ§¾

    if len(text) < 500:  # # ğŸ›Ÿ Si pypdf trop pauvre -> pdfminer.six
        try:  # # ğŸ§ª 2) pdfminer
            text = extract_text(BytesIO(pdf_bytes)).strip()  # # ğŸ§¾
        except Exception:  # # âŒ
            text = text  # # ğŸ§¾ (on garde ce quâ€™on a)

    return text, pdf_url  # # ğŸ“¤

# ============================================================  # # ğŸ“Œ SÃ©parateur
# ğŸš€ G) Fonction principale (1 HTML bundle + 1 JSON)
# ============================================================  # # ğŸ“Œ SÃ©parateur

def scrape_arxiv_cs(  # # ğŸš€ Fonction principale (appel backend / test local)
    query: str,  # # ğŸ” Mots-clÃ©s
    max_results: int = 20,  # # ğŸ¯ Nombre dâ€™articles
    sort: str = "relevance",  # # ğŸ” relevance / submitted_date / last_updated_date
    polite_min_s: float = 1.5,  # # ğŸ˜‡
    polite_max_s: float = 2.0,  # # ğŸ˜‡
    data_lake_raw_dir: str = DEFAULT_RAW_DIR,  # # ğŸ’¾
) -> Dict[str, Any]:  # # ğŸ§¾ JSON retour

    # ğŸ”¢ A) normalisation max_results
    max_results = int(max_results)  # # ğŸ”¢
    if max_results < 1:  # # ğŸš«
        max_results = 1  # # âœ…
    if max_results > MAX_RESULTS_HARD_LIMIT:  # # ğŸš§
        max_results = MAX_RESULTS_HARD_LIMIT  # # âœ…

    ts = now_iso_for_filename()  # # ğŸ•’
    ensure_dir(data_lake_raw_dir)  # # ğŸ“

    bundle_parts: List[str] = []  # # ğŸ§¾ Un SEUL fichier HTML final (bundle)
    session = requests.Session()  # # ğŸ”Œ Session HTTP

    # ğŸŒ B) 1) GET pages search (jusquâ€™Ã  max_results)
    collected: List[Dict[str, Any]] = []  # # ğŸ“¦
    start = 0  # # ğŸ“„
    while len(collected) < max_results:  # # ğŸ”
        search_url = build_search_url(query=query, start=start, size=PAGE_SIZE, sort=sort)  # # ğŸ”—
        search_html, code = http_get_text(session=session, url=search_url)  # # ğŸŒ GET
        bundle_parts.append(f"<!-- ===== SEARCH URL: {search_url} | HTTP {code} ===== -->\n")  # # ğŸ§¾
        bundle_parts.append(search_html)  # # ğŸ§¾ Ajouter HTML brut
        bundle_parts.append("\n<!-- ===== END SEARCH ===== -->\n")  # # ğŸ§¾

        if code != 200:  # # âŒ
            break  # # ğŸ›‘

        page_items = parse_search_page(search_html)  # # ğŸ” SELECT
        if not page_items:  # # ğŸ›‘
            break  # # âœ…
        collected.extend(page_items)  # # â•
        start += PAGE_SIZE  # # â¡ï¸
        sleep_polite(min_s=polite_min_s, max_s=polite_max_s)  # # ğŸ˜‡

        if start > 1000:  # # ğŸ›¡ï¸
            break  # # âœ…

    collected = collected[:max_results]  # # âœ‚ï¸

    # ğŸŒ C) 2) Enrichissement /abs + /html + fallback PDF (sans sauvegarder PDF)
    required_fields = [  # # âœ… â€œrien dâ€™optionnelâ€ => on tracke tout
        "arxiv_id", "title", "authors", "abstract", "submitted_date", "abs_url", "pdf_url",
        "doi", "license", "subjects", "comments", "journal_ref", "versions", "last_updated_date",
        "content_text", "refs_text"
    ]  # # ğŸ“‹

    for item in collected:  # # ğŸ” Articles
        arxiv_id = item.get("arxiv_id", "")  # # ğŸ†”
        abs_url = item.get("abs_url", "")  # # ğŸ”—
        pdf_url = item.get("pdf_url", "")  # # ğŸ“„

        item["doi"] = ""  # # ğŸ§¾
        item["license"] = ""  # # ğŸ“œ
        item["subjects"] = ""  # # ğŸ§ 
        item["comments"] = ""  # # ğŸ’¬
        item["journal_ref"] = ""  # # ğŸ“š
        item["versions"] = []  # # ğŸ”
        item["last_updated_date"] = ""  # # ğŸ—“ï¸
        item["content_text"] = ""  # # ğŸ§¾
        item["refs_text"] = ""  # # ğŸ”— (souvent dur Ã  extraire, mais champ prÃ©sent)
        item["html_unavailable"] = False  # # ğŸš«
        item["fallback_urls"] = []  # # ğŸ”— Pages utiles si Ã©chec

        # âœ… B) /abs
        if abs_url:  # # âœ…
            abs_html, abs_code = http_get_text(session=session, url=abs_url)  # # ğŸŒ GET
            bundle_parts.append(f"<!-- ===== ABS URL: {abs_url} | HTTP {abs_code} ===== -->\n")  # # ğŸ§¾
            bundle_parts.append(abs_html)  # # ğŸ§¾
            bundle_parts.append("\n<!-- ===== END ABS ===== -->\n")  # # ğŸ§¾
            if abs_code == 200:  # # âœ…
                enriched = parse_abs_page(abs_html, arxiv_id)  # # ğŸ§©
                for k, v in enriched.items():  # # ğŸ”
                    item[k] = v  # # âœ…
            else:  # # âŒ
                item["fallback_urls"].append(abs_url)  # # ğŸ”—
        else:  # # âŒ
            item["fallback_urls"].append(f"{ARXIV_BASE}/abs/{arxiv_id}")  # # ğŸ”—

        sleep_polite(min_s=polite_min_s, max_s=polite_max_s)  # # ğŸ˜‡

        # âœ… C) /html
        html_text, html_unavailable, html_url = try_fetch_html_content(session=session, arxiv_id=arxiv_id)  # # ğŸŒ
        item["html_unavailable"] = html_unavailable  # # ğŸš«
        if not html_unavailable and html_text:  # # âœ…
            item["content_text"] = html_text  # # ğŸ§¾
        if html_url:  # # âœ…
            item["fallback_urls"].append(html_url)  # # ğŸ”—

        sleep_polite(min_s=polite_min_s, max_s=polite_max_s)  # # ğŸ˜‡

        # âœ… D) Missing fields + fallback PDF (uniquement pour complÃ©ter)
        missing_fields: List[str] = []  # # ğŸ“¦
        for f in required_fields:  # # ğŸ”
            if is_empty(item.get(f)):  # # ğŸ§ª
                missing_fields.append(f)  # # â•
        item["missing_fields"] = missing_fields  # # ğŸš©

        if missing_fields:  # # âœ…
            pdf_text, pdf_used_url = extract_pdf_text_in_memory(session=session, pdf_url=pdf_url)  # # ğŸ“„
            if pdf_used_url:  # # âœ…
                item["fallback_urls"].append(pdf_used_url)  # # ğŸ”—
            # ğŸ§¾ On ne remplit que les champs vides
            if is_empty(item.get("content_text")) and pdf_text:  # # âœ…
                item["content_text"] = pdf_text  # # ğŸ§¾
            # ğŸ” Recalcul missing_fields aprÃ¨s fallback
            missing2: List[str] = []  # # ğŸ“¦
            for f in required_fields:  # # ğŸ”
                if is_empty(item.get(f)):  # # ğŸ§ª
                    missing2.append(f)  # # â•
            item["missing_fields"] = missing2  # # ğŸš©

        sleep_polite(min_s=polite_min_s, max_s=polite_max_s)  # # ğŸ˜‡

    # ğŸ’¾ H) Sauvegardes finales : 1 HTML bundle + 1 JSON
    bundle_html = "\n".join(bundle_parts)  # # ğŸ§¾ Concat
    html_name = f"arxiv_bundle_{ts}.html"  # # ğŸ§¾
    html_path = save_text_file(data_lake_raw_dir, html_name, bundle_html)  # # ğŸ’¾

    hit_limit_100 = (max_results == MAX_RESULTS_HARD_LIMIT)  # # ğŸš§
    message_if_limit = "Limite 100 atteinte (max_results)." if hit_limit_100 else ""  # # ğŸ§¾

    result: Dict[str, Any] = {  # # ğŸ§¾ JSON final
        "ok": True,  # # âœ…
        "query": query,  # # ğŸ”
        "sort": sort,  # # ğŸ”
        "count": len(collected),  # # ğŸ”¢
        "max_results": max_results,  # # ğŸ¯
        "hit_limit_100": hit_limit_100,  # # ğŸš§
        "message_if_limit": message_if_limit,  # # ğŸ§¾
        "items": collected,  # # ğŸ“š
        "bundle_html_file": html_path,  # # ğŸ’¾ 1 seul HTML
    }  # # âœ…

    json_name = f"arxiv_raw_{ts}.json"  # # ğŸ§¾
    json_path = os.path.join(data_lake_raw_dir, json_name)  # # ğŸ“
    with open(json_path, "w", encoding="utf-8") as f:  # # âœï¸
        json.dump(result, f, ensure_ascii=False, indent=2)  # # ğŸ§¾

    result["saved_to"] = json_path  # # ğŸ“Œ
    return result  # # ğŸ“¤

# ============================================================  # # ğŸ“Œ SÃ©parateur
# ğŸ§ª TEST LOCAL (1 ligne ON/OFF)
# ============================================================  # # ğŸ“Œ SÃ©parateur

RUN_LOCAL_TEST = True  # # âœ… True = test ON | False = test OFF (ou mets # devant la ligne)

if __name__ == "__main__" and RUN_LOCAL_TEST:  # # â–¶ï¸ ExÃ©cuter seulement en local
    print("ğŸš€ Lancement du scraping arXiv (test local)...")  # # ğŸ–¨ï¸
    results = scrape_arxiv_cs(query="multimodal transformer", max_results=5, sort="submitted_date")  # # ğŸ•·ï¸
    print(f"âœ… OK: {results.get('count')} articles rÃ©cupÃ©rÃ©s")  # # ğŸ–¨ï¸
    print(f"ğŸ’¾ JSON sauvegardÃ©: {results.get('saved_to')}")  # # ğŸ–¨ï¸
    print(f"ğŸ’¾ HTML bundle sauvegardÃ©: {results.get('bundle_html_file')}")  # # ğŸ–¨ï¸
