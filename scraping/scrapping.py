# ============================================================  # # ğŸ“Œ DÃ©but du script
# ğŸ•·ï¸ arXiv Scraper (CS search -> /abs -> /html) -> 1 HTML bundle + 1 JSON  # # ğŸ¯ Objectif
# âœ… Extraction: search results + /abs (doi, versions, html link) + /html (date, licence, sections, refs)  # # âœ…
# âœ… Sortie: JSON (items enrichis) + 1 fichier HTML "bundle" (debug) dans data_lake/raw  # # âœ…
# ============================================================  # # ğŸ“Œ SÃ©parateur visuel

# ===============================  # # ğŸ§© Importations
import os  # # ğŸ“ Gestion des chemins/dossiers
import re  # # ğŸ” Regex (ID, dates, versions)
import json  # # ğŸ§¾ Export JSON
import time  # # â±ï¸ Politesse (sleep)
import random  # # ğŸ² Jitter pour Ã©viter un rythme trop "robot"
import datetime  # # ğŸ•’ Timestamp fichiers
from typing import Dict, Any, List, Tuple, Optional  # # ğŸ§© Typage pour clartÃ©

import requests  # # ğŸŒ HTTP GET (tÃ©lÃ©charger pages)
from bs4 import BeautifulSoup, Tag  # # ğŸ² Parser HTML + manipuler balises

# ===============================  # # ğŸŒ Constantes arXiv
ARXIV_BASE = "https://arxiv.org"  # # ğŸŒ Domaine arXiv
ARXIV_SEARCH_CS = f"{ARXIV_BASE}/search/cs"  # # ğŸ” Endpoint recherche Computer Science
DEFAULT_RAW_DIR = os.path.join("data_lake", "raw")  # # ğŸ“¦ Stockage raw (bundle + json)
MAX_RESULTS_HARD_LIMIT = 100  # # ğŸš§ Limite globale demandÃ©e
PAGE_SIZE = 50  # # ğŸ“„ Taille page arXiv (pagination)

# ===============================  # # âœ… Champs supportÃ©s (ce quâ€™on renvoie dans JSON)
SUPPORTED_FIELDS = [  # # âœ… Liste de champs (pour missing_fields)
    "arxiv_id",  # # ğŸ†” Identifiant (ex: 2601.07830v1)
    "title",  # # ğŸ·ï¸ Titre
    "authors",  # # ğŸ‘¥ Auteurs
    "abstract",  # # ğŸ§¾ Abstract (depuis search et/ou /abs)
    "submitted_date",  # # ğŸ—“ï¸ "Submitted ..." (depuis search)
    "abs_url",  # # ğŸ”— URL /abs
    "pdf_url",  # # ğŸ“„ URL /pdf (arXiv)
    "doi",  # # ğŸ”— DOI (souvent sur /abs, parfois dans references)
    "versions",  # # ğŸ” Liste versions (v1, v2...) depuis /abs
    "last_updated_raw",  # # ğŸ—“ï¸ DerniÃ¨re version raw (depuis /abs)
    "html_url",  # # ğŸŒ URL HTML experimental (depuis /abs OU construit)
    "published_date",  # # ğŸ—“ï¸ Date watermark sur /html (ex: 28 Nov 2025)
    "license",  # # ğŸªª Licence affichÃ©e sur /html (ex: arXiv.org perpetual non-exclusive license)
    "sections",  # # ğŸ§± Titres + contenus (comme ton Excel)
    "content_text",  # # ğŸ§¾ Texte global concatÃ©nÃ© (fallback)
    "references",  # # ğŸ“š RÃ©fÃ©rences (raw + liens)
    "references_dois",  # # ğŸ”— Liste DOI trouvÃ©s dans les rÃ©fÃ©rences
]

# ============================================================  # # ğŸ“Œ SÃ©parateur
# âœ… A) Helpers (dossiers, timestamps, â€œvideâ€, politesse, GET)
# ============================================================  # # ğŸ“Œ SÃ©parateur

def ensure_dir(path: str) -> None:  # # ğŸ“ CrÃ©er dossier si besoin
    os.makedirs(path, exist_ok=True)  # # âœ… CrÃ©e (sans erreur si existe)

def now_iso_for_filename() -> str:  # # ğŸ•’ Timestamp pour noms de fichiers
    return datetime.datetime.now().strftime("%Y%m%d_%H%M%S")  # # ğŸ§¾ Exemple: 20260114_101500

def is_empty(value: Any) -> bool:  # # ğŸ§ª DÃ©finition du â€œvideâ€ (selon tes rÃ¨gles)
    if value is None:  # # âœ… None
        return True  # # âœ…
    if isinstance(value, str):  # # ğŸ§¾ Si string
        v = value.strip()  # # ğŸ§¹ Trim
        if v == "":  # # âœ… vide si ""
            return True  # # âœ…
        if v.lower() in {"n/a", "null", "none"}:  # # âœ… vide si "N/A", "null", "None" (string)
            return True  # # âœ…
    if isinstance(value, list):  # # ğŸ“¦ Si liste
        return len(value) == 0  # # âœ… vide si liste vide
    return False  # # âŒ sinon non vide

def sleep_polite(min_s: float = 1.5, max_s: float = 2.0) -> None:  # # ğŸ˜‡ Pause polie
    time.sleep(random.uniform(min_s, max_s))  # # â³ Attendre 1.5 Ã  2.0 secondes

def http_get_text(session: requests.Session, url: str, timeout_s: int = 30) -> Tuple[str, int]:  # # ğŸŒ GET HTML -> (texte, status)
    headers = {  # # ğŸªª User-Agent (Ã©vite certains blocages)
        "User-Agent": "Mozilla/5.0 DIXITBOT-arXivScraper/2.0",  # # ğŸªª Identifiant simple
        "Accept-Language": "en-US,en;q=0.9",  # # ğŸŒ Langue (stabilitÃ© parsing)
    }  # # âœ… Fin headers
    resp = session.get(url, headers=headers, timeout=timeout_s)  # # ğŸš€ GET
    return resp.text, resp.status_code  # # ğŸ“„ Retourner HTML + code

def save_text_file(folder: str, filename: str, content: str) -> str:  # # ğŸ’¾ Sauver texte dans un fichier
    ensure_dir(folder)  # # ğŸ“ Assurer dossier
    path = os.path.join(folder, filename)  # # ğŸ§© Construire chemin
    with open(path, "w", encoding="utf-8") as f:  # # âœï¸ Ouvrir en Ã©criture UTF-8
        f.write(content)  # # ğŸ§¾ Ã‰crire contenu
    return path  # # ğŸ“Œ Retourner chemin

def normalize_url(href: str) -> str:  # # ğŸ”— Normaliser un href relatif/absolu
    if not href:  # # ğŸš« Si vide
        return ""  # # âœ…
    h = href.strip()  # # ğŸ§¹ Nettoyage
    if h.startswith("//"):  # # ğŸŒ URL sans schÃ©ma
        return "https:" + h  # # âœ… Ajouter https:
    if h.startswith("/"):  # # âœ… URL relative
        return ARXIV_BASE + h  # # ğŸ”— PrÃ©fixer domaine
    return h  # # âœ… DÃ©jÃ  absolu

def abs_url(arxiv_id: str) -> str:  # # ğŸ”— Construire URL /abs
    return f"{ARXIV_BASE}/abs/{arxiv_id}"  # # âœ…

def pdf_url(arxiv_id: str) -> str:  # # ğŸ“„ Construire URL /pdf (arXiv)
    return f"{ARXIV_BASE}/pdf/{arxiv_id}"  # # âœ…

def html_url(arxiv_id: str) -> str:  # # ğŸŒ Construire URL /html
    return f"{ARXIV_BASE}/html/{arxiv_id}"  # # âœ…

def compute_missing_fields(item: Dict[str, Any]) -> List[str]:  # # ğŸš© Calculer champs vides
    missing: List[str] = []  # # ğŸ“¦ Liste champs manquants
    for f in SUPPORTED_FIELDS:  # # ğŸ” Pour chaque champ attendu
        if is_empty(item.get(f)):  # # ğŸ§ª Si vide
            missing.append(f)  # # â• Ajouter
    return missing  # # ğŸ“¤ Retourner liste

# ============================================================  # # ğŸ“Œ SÃ©parateur
# ğŸ” B) URL builder (tri compatible arXiv)
# ============================================================  # # ğŸ“Œ SÃ©parateur

def build_search_url(query: str, start: int, size: int, sort: str) -> str:  # # ğŸ”— Construire URL search/cs
    q = requests.utils.quote(query)  # # ğŸ” Encoder requÃªte (espaces etc.)
    base = f"{ARXIV_SEARCH_CS}?query={q}&searchtype=all&abstracts=show&size={size}&start={start}"  # # ğŸ”— Base URL
    s = (sort or "relevance").strip().lower()  # # ğŸ§  Normaliser sort
    if s in {"submitted_date", "submitted", "recent"}:  # # ğŸ—“ï¸ Tri = rÃ©cents (soumission)
        return base + "&order=-announced_date_first"  # # âœ… ParamÃ¨tre arXiv OK
    # âš ï¸ "relevance" est le dÃ©faut du site : PAS besoin de &order=-relevance (400)  # # ğŸš«
    return base  # # âœ… Relevance default

# ============================================================  # # ğŸ“Œ SÃ©parateur
# ğŸ§© C) Parsing SEARCH page (liste rÃ©sultats) â€” robuste
# ============================================================  # # ğŸ“Œ SÃ©parateur

def find_abs_and_pdf_hrefs(li: Tag) -> Tuple[str, str]:  # # ğŸ” Trouver href /abs et /pdf dans un item search
    abs_href = ""  # # ğŸ”— Href /abs
    pdf_href = ""  # # ğŸ“„ Href /pdf

    for a in li.select("a[href]"):  # # ğŸ” Parcourir tous les liens du bloc
        href = (a.get("href") or "").strip()  # # ğŸ§¾ Lire href
        if not href:  # # ğŸš« Vide
            continue  # # âœ… Next
        if (not abs_href) and re.search(r"/abs/[^?#/]+", href):  # # ğŸ”— Lien abstract
            abs_href = href  # # âœ…
        if (not pdf_href) and re.search(r"/pdf/[^?#/]+", href):  # # ğŸ“„ Lien pdf
            pdf_href = href  # # âœ…
        if abs_href and pdf_href:  # # âœ… DÃ¨s quâ€™on a les deux
            break  # # âœ… Stop

    return abs_href, pdf_href  # # ğŸ“¤

def extract_arxiv_id_from_any(href: str) -> str:  # # ğŸ†” Extraire ID depuis /abs ou /pdf
    if not href:  # # ğŸš«
        return ""  # # âœ…
    m = re.search(r"/abs/([^?#/]+)", href)  # # ğŸ”
    if m:  # # âœ…
        return m.group(1).strip()  # # ğŸ†”
    m2 = re.search(r"/pdf/([^?#/]+)", href)  # # ğŸ”
    if m2:  # # âœ…
        return m2.group(1).strip()  # # ğŸ†”
    return ""  # # âŒ

def parse_search_page(html: str) -> List[Dict[str, Any]]:  # # ğŸ§© HTML search -> items (base)
    soup = BeautifulSoup(html, "lxml")  # # ğŸ² Parser HTML (lxml)
    items: List[Dict[str, Any]] = []  # # ğŸ“¦ Liste rÃ©sultats

    for li in soup.select("ol.breathe-horizontal li.arxiv-result"):  # # ğŸ“š Chaque rÃ©sultat
        title_el = li.select_one("p.title")  # # ğŸ·ï¸ Titre
        authors_el = li.select_one("p.authors")  # # ğŸ‘¥ Auteurs
        abstract_el = li.select_one("span.abstract-full")  # # ğŸ§¾ Abstract
        submitted_el = li.select_one("p.is-size-7")  # # ğŸ—“ï¸ Bloc date soumis (souvent)

        abs_href, pdf_href = find_abs_and_pdf_hrefs(li)  # # ğŸ” Liens
        arxiv_id = extract_arxiv_id_from_any(abs_href or pdf_href)  # # ğŸ†” ID depuis lien

        title = title_el.get_text(" ", strip=True) if title_el else ""  # # ğŸ·ï¸ Texte titre
        authors_txt = authors_el.get_text(" ", strip=True) if authors_el else ""  # # ğŸ‘¥ Texte auteurs brut
        authors = [a.strip() for a in authors_txt.replace("Authors:", "").split(",") if a.strip()]  # # ğŸ‘¥ Liste auteurs
        abstract = abstract_el.get_text(" ", strip=True) if abstract_el else ""  # # ğŸ§¾ Texte abstract
        abstract = abstract.replace("â–³ Less", "").strip()  # # ğŸ§¹ Nettoyage

        submitted_date = ""  # # ğŸ—“ï¸ Date "Submitted ..."
        if submitted_el:  # # âœ… Si prÃ©sent
            txt = submitted_el.get_text(" ", strip=True)  # # ğŸ§¾ Texte
            m3 = re.search(r"Submitted\s+(.+?)(?:;|$)", txt, flags=re.IGNORECASE)  # # ğŸ” "Submitted X"
            if m3:  # # âœ…
                submitted_date = m3.group(1).strip()  # # ğŸ—“ï¸

        abs_full = normalize_url(abs_href)  # # ğŸ”— URL abs complÃ¨te (si trouvÃ©e)
        pdf_full = normalize_url(pdf_href)  # # ğŸ“„ URL pdf complÃ¨te (si trouvÃ©e)

        if arxiv_id and is_empty(abs_full):  # # âœ… Garantir abs_url si on a l'ID
            abs_full = abs_url(arxiv_id)  # # ğŸ”—
        if arxiv_id and is_empty(pdf_full):  # # âœ… Garantir pdf_url si on a l'ID
            pdf_full = pdf_url(arxiv_id)  # # ğŸ“„

        items.append({  # # ğŸ“¦ Ajouter item
            "arxiv_id": arxiv_id,  # # ğŸ†”
            "title": title,  # # ğŸ·ï¸
            "authors": authors,  # # ğŸ‘¥
            "abstract": abstract,  # # ğŸ§¾
            "submitted_date": submitted_date,  # # ğŸ—“ï¸
            "abs_url": abs_full,  # # ğŸ”—
            "pdf_url": pdf_full,  # # ğŸ“„
        })  # # âœ… Fin item

    return items  # # ğŸ“¤ Retour

# ============================================================  # # ğŸ“Œ SÃ©parateur
# ğŸ“Œ D) Parsing /abs (versions + doi + lien HTML experimental + abstract fallback)
# ============================================================  # # ğŸ“Œ SÃ©parateur

def parse_abs_page(abs_html: str) -> Dict[str, Any]:  # # ğŸ§© /abs -> dict enrichissement
    soup = BeautifulSoup(abs_html, "lxml")  # # ğŸ² Parser HTML
    out: Dict[str, Any] = {  # # ğŸ“¦ Structure sortie
        "doi": "",  # # ğŸ”— DOI
        "versions": [],  # # ğŸ” Versions
        "last_updated_raw": "",  # # ğŸ—“ï¸ DerniÃ¨re version raw
        "html_experimental_url": "",  # # ğŸŒ Lien /html
        "abstract": "",  # # ğŸ§¾ Abstract (fallback depuis /abs si besoin)
    }  # # âœ…

    doi_a = soup.select_one('td.tablecell.doi a[href*="doi.org"]')  # # ğŸ” DOI table
    if doi_a:  # # âœ…
        out["doi"] = doi_a.get_text(" ", strip=True)  # # ğŸ§¾ Texte DOI

    html_a = soup.select_one('div.full-text a[href*="/html/"]')  # # ğŸ” HTML experimental
    if html_a:  # # âœ…
        out["html_experimental_url"] = normalize_url(html_a.get("href") or "")  # # ğŸŒ URL normalisÃ©e

    abs_el = soup.select_one("blockquote.abstract")  # # ğŸ” Abstract sur /abs
    if abs_el:  # # âœ…
        txt = abs_el.get_text(" ", strip=True)  # # ğŸ§¾ Texte brut
        txt = re.sub(r"^\s*Abstract:\s*", "", txt, flags=re.IGNORECASE).strip()  # # ğŸ§¹ Enlever "Abstract:"
        out["abstract"] = txt  # # ğŸ§¾

    versions: List[Dict[str, str]] = []  # # ğŸ“¦ Liste versions
    for li in soup.select("div.submission-history li"):  # # ğŸ” Parcourir historique
        txt = li.get_text(" ", strip=True)  # # ğŸ§¾ Texte
        m = re.search(r"\[(v\d+)\]\s*(.*)$", txt)  # # ğŸ” [v1] ...
        if m:  # # âœ…
            versions.append({"version": m.group(1), "raw": m.group(2).strip()})  # # ğŸ“¦
    out["versions"] = versions  # # ğŸ”
    out["last_updated_raw"] = versions[-1]["raw"] if versions else ""  # # ğŸ—“ï¸

    return out  # # ğŸ“¤

# ============================================================  # # ğŸ“Œ SÃ©parateur
# ğŸŒ E) Parsing /html (date watermark + licence + sections + rÃ©fÃ©rences)
# ============================================================  # # ğŸ“Œ SÃ©parateur

def clean_text(s: str) -> str:  # # ğŸ§¼ Nettoyage texte simple
    if not s:  # # ğŸš«
        return ""  # # âœ…
    s = re.sub(r"\s+", " ", s)  # # ğŸ§¹ Espaces multiples -> 1
    return s.strip()  # # âœ…

def is_heading(el: Tag) -> bool:  # # ğŸ·ï¸ DÃ©tecter un titre de section
    if not isinstance(el, Tag):  # # ğŸ›¡ï¸
        return False  # # âœ…
    if el.name in {"h1", "h2", "h3", "h4", "h5", "h6"}:  # # âœ… Titres HTML
        return True  # # âœ…
    role = (el.get("role") or "").strip().lower()  # # âœ… ARIA
    if role == "heading":  # # âœ…
        return True  # # âœ…
    classes = " ".join(el.get("class", [])).lower()  # # âœ… Classes
    if any(k in classes for k in ["ltx_title", "title", "heading", "section-title"]):  # # âœ… Heuristique LaTeXML
        return bool(clean_text(el.get_text(" ", strip=True)))  # # âœ… Texte non vide
    return False  # # âœ…

def collect_section_content(heading_el: Tag, max_chars: int = 8000) -> str:  # # ğŸ“¦ Contenu aprÃ¨s un titre
    contents: List[str] = []  # # ğŸ“¦ Blocs texte
    total = 0  # # ğŸ”¢ Compteur
    for sib in heading_el.next_siblings:  # # ğŸ” Parcourir frÃ¨res suivants
        if isinstance(sib, Tag):  # # âœ… Balise
            if is_heading(sib):  # # ğŸ›‘ Stop au prochain titre
                break  # # âœ…
            if sib.name in {"p", "div", "ul", "ol", "table", "figure", "section"}:  # # âœ… Blocs pertinents
                txt = clean_text(sib.get_text(" ", strip=True))  # # ğŸ§¾ Texte bloc
                if txt:  # # âœ… Non vide
                    contents.append(txt)  # # â• Ajouter
                    total += len(txt)  # # ğŸ”¢ Compter
        if total >= max_chars:  # # ğŸ›‘ Limite taille
            break  # # âœ…
    return clean_text(" ".join(contents))  # # ğŸ§¾ Retour texte section

def extract_sections_from_html(soup: BeautifulSoup) -> List[Dict[str, Any]]:  # # ğŸ§± Extraire sections titre+contenu
    # âœ… On cible l'article LaTeXML si possible (plus propre)  # # ğŸ¯
    root = soup.select_one("article.ltx_document") or soup.select_one("main") or soup.body or soup  # # ğŸ¯ Root
    headings: List[Tag] = []  # # ğŸ“¦ Liste titres
    for el in root.find_all(True):  # # ğŸ” Parcourir toutes balises
        if is_heading(el):  # # âœ… Filtre titres
            title_text = clean_text(el.get_text(" ", strip=True))  # # ğŸ§¾
            if title_text:  # # âœ…
                headings.append(el)  # # â•
    sections: List[Dict[str, Any]] = []  # # ğŸ“¦ RÃ©sultat
    for i, h in enumerate(headings, start=1):  # # ğŸ” Titres numÃ©rotÃ©s
        title_text = clean_text(h.get_text(" ", strip=True))  # # ğŸ·ï¸ Titre
        level = h.name if h.name in {"h1", "h2", "h3", "h4", "h5", "h6"} else "custom"  # # ğŸ§­ Niveau
        section_text = collect_section_content(h)  # # ğŸ“¦ Contenu associÃ©
        if section_text:  # # âœ… Garder uniquement si contenu
            sections.append({  # # ğŸ“¦ Ajouter section
                "section_index": i,  # # ğŸ”¢ Index
                "heading_level": level,  # # ğŸ§­ Niveau
                "heading": title_text,  # # ğŸ·ï¸
                "text": section_text,  # # ğŸ§¾
            })  # # âœ…
    return sections  # # ğŸ“¤

def extract_references_from_html(soup: BeautifulSoup) -> Tuple[List[Dict[str, Any]], List[str]]:  # # ğŸ“š RÃ©fÃ©rences + DOI
    refs: List[Dict[str, Any]] = []  # # ğŸ“¦ RÃ©fÃ©rences
    dois_flat: List[str] = []  # # ğŸ”— DOI uniques

    # âœ… Ton indication : class="ltx_biblist" id="bib.L1"  # # ğŸ¯
    bib = soup.select_one(".ltx_biblist") or soup.select_one(".ltx_bibliography")  # # ğŸ” Conteneur
    if not bib:  # # ğŸš« Pas de bibliographie
        return refs, dois_flat  # # âœ…

    for bi in bib.select(".ltx_bibitem, li, div"):  # # ğŸ” Items bib
        txt = clean_text(bi.get_text(" ", strip=True))  # # ğŸ§¾ Texte ref
        if not txt:  # # ğŸš«
            continue  # # âœ…
        links = [clean_text(a.get("href", "")) for a in bi.select("a[href]")]  # # ğŸ”— Tous les liens
        links = [l for l in links if l]  # # ğŸ§¹ Filtrer
        dois = [l for l in links if "doi.org/" in l]  # # ğŸ”— DOI links
        for d in dois:  # # ğŸ”
            if d not in dois_flat:  # # âœ… Uniques
                dois_flat.append(d)  # # â•
        pdf_links = [l for l in links if ("/doi/pdf" in l) or l.lower().endswith(".pdf")]  # # ğŸ“„ PDFs
        refs.append({  # # ğŸ“¦ Ajouter
            "raw_text": txt,  # # ğŸ§¾
            "urls": links,  # # ğŸ”—
            "dois": dois,  # # ğŸ”—
            "pdf_links": pdf_links,  # # ğŸ“„
        })  # # âœ…

    return refs, dois_flat  # # ğŸ“¤

def parse_html_page(html_text: str) -> Dict[str, Any]:  # # ğŸ§© /html -> dict
    soup = BeautifulSoup(html_text, "lxml")  # # ğŸ² Parser
    out: Dict[str, Any] = {  # # ğŸ“¦ Structure
        "published_date": "",  # # ğŸ—“ï¸
        "license": "",  # # ğŸªª
        "sections": [],  # # ğŸ§±
        "content_text": "",  # # ğŸ§¾
        "references": [],  # # ğŸ“š
        "references_dois": [],  # # ğŸ”—
    }  # # âœ…

    wm = soup.select_one("#watermark-tr")  # # ğŸ” Watermark (date publication)
    if wm:  # # âœ…
        wm_text = clean_text(wm.get_text(" ", strip=True))  # # ğŸ§¾
        m = re.search(r"\]\s*([0-9]{1,2}\s+\w+\s+[0-9]{4})", wm_text)  # # ğŸ” AprÃ¨s ] = date
        if m:  # # âœ…
            out["published_date"] = m.group(1).strip()  # # ğŸ—“ï¸

    lic = soup.select_one("a#license-tr")  # # ğŸ” Licence (ton exemple)
    if lic:  # # âœ…
        lic_text = clean_text(lic.get_text(" ", strip=True))  # # ğŸ§¾
        lic_text = re.sub(r"^\s*License:\s*", "", lic_text, flags=re.IGNORECASE).strip()  # # ğŸ§¹ Enlever "License:"
        out["license"] = lic_text  # # ğŸªª

    sections = extract_sections_from_html(soup)  # # ğŸ§± Sections titre+contenu
    out["sections"] = sections  # # âœ…

    # âœ… content_text = concat simple (utile si tu veux aussi un gros texte)  # # ğŸ§¾
    if sections:  # # âœ…
        out["content_text"] = "\n\n".join([f"{s['heading']}\n{s['text']}" for s in sections])  # # ğŸ§¾
    else:  # # ğŸ›Ÿ Fallback texte global
        doc = soup.select_one("article.ltx_document") or soup.select_one("main") or soup.body  # # ğŸ§¾ Root
        out["content_text"] = doc.get_text("\n", strip=True) if doc else ""  # # ğŸ§¾

    refs, dois_flat = extract_references_from_html(soup)  # # ğŸ“š
    out["references"] = refs  # # ğŸ“š
    out["references_dois"] = dois_flat  # ğŸ”—

    return out  # # ğŸ“¤

# ============================================================  # # ğŸ“Œ SÃ©parateur
# ğŸš€ F) Fonction principale (1 HTML bundle + 1 JSON)
# ============================================================  # # ğŸ“Œ SÃ©parateur

def scrape_arxiv_cs(  # # ğŸš€ Fonction principale
    query: str,  # # ğŸ” RequÃªte utilisateur
    max_results: int = 20,  # # ğŸ¯ Nombre dâ€™articles
    sort: str = "relevance",  # # ğŸ”ƒ relevance | submitted_date
    polite_min_s: float = 1.5,  # # ğŸ˜‡ Politesse min
    polite_max_s: float = 2.0,  # # ğŸ˜‡ Politesse max
    data_lake_raw_dir: str = DEFAULT_RAW_DIR,  # # ğŸ’¾ Dossier de sortie
) -> Dict[str, Any]:  # # ğŸ§¾ Retour JSON (dict)

    max_results = int(max_results)  # # ğŸ”¢ Normaliser type
    if max_results < 1:  # # ğŸš«
        max_results = 1  # # âœ…
    if max_results > MAX_RESULTS_HARD_LIMIT:  # # ğŸš§
        max_results = MAX_RESULTS_HARD_LIMIT  # # âœ…

    ts = now_iso_for_filename()  # # ğŸ•’ Timestamp
    ensure_dir(data_lake_raw_dir)  # # ğŸ“ Dossier raw
    session = requests.Session()  # # ğŸ”Œ Session HTTP rÃ©utilisable
    bundle_parts: List[str] = []  # # ğŸ§¾ HTML bundle (debug)

    collected: List[Dict[str, Any]] = []  # # ğŸ“¦ Items (multi-pages)
    start = 0  # # ğŸ“„ Offset pagination

    # =====================  # # ğŸ“„ 1) Pagination search
    while len(collected) < max_results:  # # ğŸ” Tant quâ€™on nâ€™a pas assez
        search_url = build_search_url(query=query, start=start, size=PAGE_SIZE, sort=sort)  # # ğŸ”— URL search
        search_html, code = http_get_text(session=session, url=search_url)  # # ğŸŒ GET search
        bundle_parts.append(f"<!-- ===== SEARCH URL: {search_url} | HTTP {code} ===== -->\n")  # # ğŸ§¾
        bundle_parts.append(search_html)  # # ğŸ§¾
        bundle_parts.append("\n<!-- ===== END SEARCH ===== -->\n")  # # ğŸ§¾
        if code != 200:  # # âŒ Search KO
            break  # # ğŸ›‘
        page_items = parse_search_page(search_html)  # # ğŸ” Parse search
        if not page_items:  # # ğŸ›‘ Plus de rÃ©sultats
            break  # # âœ…
        collected.extend(page_items)  # # â• Ajouter page
        start += PAGE_SIZE  # # â¡ï¸ Page suivante
        sleep_polite(min_s=polite_min_s, max_s=polite_max_s)  # # ğŸ˜‡ Pause

    collected = collected[:max_results]  # # âœ‚ï¸ Couper au bon nombre

    # =====================  # # ğŸ§© 2) Enrichissement /abs + /html
    for item in collected:  # # ğŸ” Pour chaque article
        arxiv_id = item.get("arxiv_id", "")  # # ğŸ†”
        item["doi"] = ""  # # ğŸ”— Init
        item["versions"] = []  # # ğŸ” Init
        item["last_updated_raw"] = ""  # # ğŸ—“ï¸ Init
        item["html_url"] = ""  # # ğŸŒ Init
        item["published_date"] = ""  # # ğŸ—“ï¸ Init
        item["license"] = ""  # # ğŸªª Init
        item["sections"] = []  # # ğŸ§± Init
        item["content_text"] = ""  # # ğŸ§¾ Init
        item["references"] = []  # # ğŸ“š Init
        item["references_dois"] = []  # # ğŸ”— Init
        item["fallback_urls"] = []  # # ğŸ”— Init
        item["errors"] = []  # # ğŸ§¾ Init

        # âœ… Garantir abs/pdf si on a l'ID  # # ğŸ”—
        if arxiv_id:  # # âœ…
            item["abs_url"] = item.get("abs_url") or abs_url(arxiv_id)  # # ğŸ”—
            item["pdf_url"] = item.get("pdf_url") or pdf_url(arxiv_id)  # # ğŸ“„

        # ----------  # # ğŸ“Œ /abs
        if item.get("abs_url"):  # # âœ… Si URL /abs dispo
            abs_html, abs_code = http_get_text(session=session, url=item["abs_url"])  # # ğŸŒ GET /abs
            bundle_parts.append(f"<!-- ===== ABS URL: {item['abs_url']} | HTTP {abs_code} ===== -->\n")  # # ğŸ§¾
            bundle_parts.append(abs_html)  # # ğŸ§¾
            bundle_parts.append("\n<!-- ===== END ABS ===== -->\n")  # # ğŸ§¾
            if abs_code == 200:  # # âœ… OK
                abs_data = parse_abs_page(abs_html)  # # ğŸ” Parse /abs
                item["doi"] = abs_data.get("doi", "")  # # ğŸ”— DOI
                item["versions"] = abs_data.get("versions", [])  # # ğŸ” Versions
                item["last_updated_raw"] = abs_data.get("last_updated_raw", "")  # # ğŸ—“ï¸ Last update
                item["html_url"] = abs_data.get("html_experimental_url", "")  # # ğŸŒ HTML experimental
                if is_empty(item.get("abstract")) and not is_empty(abs_data.get("abstract")):  # # âœ… Fallback abstract
                    item["abstract"] = abs_data.get("abstract", "")  # # ğŸ§¾
            else:  # # âŒ /abs KO
                item["errors"].append(f"abs_http_{abs_code}")  # # ğŸ§¾ Log
                item["fallback_urls"].append(item["abs_url"])  # # ğŸ”— Hint
        else:  # # âŒ Pas dâ€™abs_url
            item["errors"].append("missing_abs_url")  # # ğŸ§¾

        sleep_polite(min_s=polite_min_s, max_s=polite_max_s)  # # ğŸ˜‡ Pause

        # ----------  # # ğŸ“Œ /html
        if is_empty(item.get("html_url")) and arxiv_id:  # # âœ… Si /abs nâ€™a pas donnÃ© html_url
            item["html_url"] = html_url(arxiv_id)  # # ğŸŒ Construire /html/<id>
        if item.get("html_url"):  # # âœ… Si URL /html dispo
            h_html, h_code = http_get_text(session=session, url=item["html_url"])  # # ğŸŒ GET /html
            bundle_parts.append(f"<!-- ===== HTML URL: {item['html_url']} | HTTP {h_code} ===== -->\n")  # # ğŸ§¾
            bundle_parts.append(h_html)  # # ğŸ§¾
            bundle_parts.append("\n<!-- ===== END HTML ===== -->\n")  # # ğŸ§¾
            if h_code == 200:  # # âœ… OK
                html_data = parse_html_page(h_html)  # # ğŸ” Parse /html
                item["published_date"] = html_data.get("published_date", "")  # # ğŸ—“ï¸
                item["license"] = html_data.get("license", "")  # # ğŸªª
                item["sections"] = html_data.get("sections", [])  # # ğŸ§±
                item["content_text"] = html_data.get("content_text", "")  # # ğŸ§¾
                item["references"] = html_data.get("references", [])  # # ğŸ“š
                item["references_dois"] = html_data.get("references_dois", [])  # # ğŸ”—
                if is_empty(item.get("doi")) and html_data.get("references_dois"):  # # âœ… DOI fallback depuis refs
                    # âœ… On tente de prendre le 1er DOI trouvÃ© (si le /abs nâ€™en avait pas)  # # ğŸ”—
                    first_doi_link = html_data["references_dois"][0]  # # ğŸ”—
                    item["doi"] = first_doi_link  # # ğŸ”—
            else:  # # âŒ /html KO
                item["errors"].append(f"html_http_{h_code}")  # # ğŸ§¾
                item["fallback_urls"].append(item["html_url"])  # # ğŸ”—
        else:  # # âŒ
            item["errors"].append("missing_html_url")  # # ğŸ§¾

        sleep_polite(min_s=polite_min_s, max_s=polite_max_s)  # # ğŸ˜‡ Pause

        # ----------  # # ğŸš© Missing fields + hints
        item["missing_fields"] = compute_missing_fields(item)  # # ğŸš©
        if item["missing_fields"]:  # # âœ…
            item["url_hint_if_missing"] = (  # # ğŸ§¾ Construire message
                f"Champs manquants: {', '.join(item['missing_fields'])}. "  # # ğŸ§¾
                f"Tu peux vÃ©rifier ici: abs={item.get('abs_url','')} | html={item.get('html_url','')} | pdf={item.get('pdf_url','')}"  # # ğŸ§¾
            )  # # âœ…
        else:  # # âœ…
            item["url_hint_if_missing"] = ""  # # âœ…

    # =====================  # # ğŸ’¾ 3) Sauvegarde bundle + JSON
    bundle_html = "\n".join(bundle_parts)  # # ğŸ§¾ Concat bundle
    bundle_name = f"arxiv_bundle_{ts}.html"  # # ğŸ§¾ Nom bundle
    bundle_path = save_text_file(data_lake_raw_dir, bundle_name, bundle_html)  # # ğŸ’¾ Save bundle

    result: Dict[str, Any] = {  # # ğŸ§¾ JSON final
        "ok": True,  # # âœ…
        "query": query,  # # ğŸ”
        "sort": sort,  # # ğŸ”ƒ
        "count": len(collected),  # # ğŸ”¢
        "max_results": max_results,  # # ğŸ¯
        "hit_limit_100": (max_results == MAX_RESULTS_HARD_LIMIT),  # # ğŸš§
        "message_if_limit": "Limite 100 atteinte (max_results)." if (max_results == MAX_RESULTS_HARD_LIMIT) else "",  # # ğŸ§¾
        "items": collected,  # # ğŸ“š
        "bundle_html_file": bundle_path,  # # ğŸ’¾
        "supported_fields": SUPPORTED_FIELDS,  # # âœ…
    }  # # âœ…

    json_name = f"arxiv_raw_{ts}.json"  # # ğŸ§¾ Nom JSON
    json_path = os.path.join(data_lake_raw_dir, json_name)  # # ğŸ“ Chemin JSON
    with open(json_path, "w", encoding="utf-8") as f:  # # âœï¸
        json.dump(result, f, ensure_ascii=False, indent=2)  # # ğŸ§¾ Ã‰crire JSON

    result["saved_to"] = json_path  # # ğŸ“Œ Ajouter chemin JSON
    return result  # # ğŸ“¤ Retourner rÃ©sultat

# ============================================================  # # ğŸ“Œ SÃ©parateur
# ğŸ§ª TEST LOCAL (1 ligne ON/OFF)
# ============================================================  # # ğŸ“Œ SÃ©parateur

RUN_LOCAL_TEST = false  # # âœ… True = test ON | False = test OFF

if __name__ == "__main__" and RUN_LOCAL_TEST:  # # â–¶ï¸ ExÃ©cution directe
    print("ğŸš€ Lancement du scraping arXiv (test local)...")  # # ğŸ–¨ï¸ Log
    results = scrape_arxiv_cs(query="multimodal transformer", max_results=3, sort="relevance")  # # ğŸ•·ï¸ Run
    print(f"âœ… OK: {results.get('count')} articles rÃ©cupÃ©rÃ©s")  # # ğŸ–¨ï¸
    print(f"ğŸ’¾ JSON sauvegardÃ©: {results.get('saved_to')}")  # # ğŸ–¨ï¸
    print(f"ğŸ’¾ HTML bundle sauvegardÃ©: {results.get('bundle_html_file')}")  # # ğŸ–¨ï¸
    if results.get("items"):  # # âœ…
        print("ğŸ§¾ AperÃ§u item 1 (clÃ©s principales):")  # # ğŸ–¨ï¸
        first = results["items"][0]  # # ğŸ“¦
        print(json.dumps({k: first.get(k) for k in ["arxiv_id","title","published_date","license"]}, ensure_ascii=False, indent=2))  # # ğŸ§¾
