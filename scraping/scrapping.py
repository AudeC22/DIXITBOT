# ============================================================  # # ğŸ“Œ DÃ©but du script
# ğŸ•·ï¸ arXiv Scraper (CS search -> /abs -> /html) -> 1 HTML bundle + 1 JSON  # # ğŸ¯ Objectif
# âœ… Version "robuste + simple" : on ne garde QUE ce qu'on sait rÃ©cupÃ©rer de faÃ§on fiable  # # âœ…
# ============================================================  # # ğŸ“Œ SÃ©parateur visuel

import os  # # ğŸ“ Gestion des chemins/dossiers
import re  # # ğŸ” Regex (ID, watermark)
import json  # # ğŸ§¾ Export JSON
import time  # # â±ï¸ Politesse (sleep)
import random  # # ğŸ² Jitter (Ã©viter rythme robot)
import datetime  # # ğŸ•’ Timestamp fichiers
from typing import Dict, Any, List, Tuple  # # ğŸ§© Typage

import requests  # # ğŸŒ HTTP (GET)
from bs4 import BeautifulSoup  # # ğŸ² Parsing HTML (select)

# ============================================================  # # ğŸ“Œ SÃ©parateur
# ğŸŒ Constantes  # # ğŸ§  ParamÃ¨tres globaux
# ============================================================  # # ğŸ“Œ SÃ©parateur

ARXIV_BASE = "https://arxiv.org"  # # ğŸŒ Domaine arXiv
ARXIV_SEARCH_CS = "https://arxiv.org/search/cs"  # # ğŸ” Recherche Computer Science
DEFAULT_RAW_DIR = os.path.join("data_lake", "raw")  # # ğŸ“¦ Stockage raw (HTML bundle + JSON)
MAX_RESULTS_HARD_LIMIT = 100  # # ğŸš§ Max global demandÃ©
PAGE_SIZE = 50  # # ğŸ“„ Pagination arXiv (50)

SUPPORTED_FIELDS = [  # # âœ… Champs effectivement supportÃ©s dans CETTE version
    "arxiv_id",  # # ğŸ†”
    "title",  # # ğŸ·ï¸
    "authors",  # # ğŸ‘¥
    "abstract",  # # ğŸ§¾
    "submitted_date",  # # ğŸ—“ï¸ (depuis search)
    "abs_url",  # # ğŸ”—
    "pdf_url",  # # ğŸ“„ (PDF arXiv)
    "html_url",  # # ğŸŒ (HTML arXiv /html/<id>vN)
    "published_date",  # # ğŸ—“ï¸ (watermark /html)
    "doi",  # # ğŸ”— (trouvÃ© dans /html via bibliographie)
    "license",  # # âš–ï¸ (license-tr /html)
    "references",  # # ğŸ“š (liste structurÃ©e)
    "references_dois",  # # ğŸ”— (liste de DOI trouvÃ©s)
    "missing_fields",  # # ğŸš©
    "errors",  # # ğŸ§¾
    "url_hint_if_missing",  # # ğŸ§­
]  # # âœ…

# ============================================================  # # ğŸ“Œ SÃ©parateur
# âœ… A) Helpers (dossiers, timestamps, â€œvideâ€, politesse, GET)  # # ğŸ§°
# ============================================================  # # ğŸ“Œ SÃ©parateur

def ensure_dir(path: str) -> None:  # # ğŸ“ CrÃ©er dossier si besoin
    os.makedirs(path, exist_ok=True)  # # âœ…

def now_iso_for_filename() -> str:  # # ğŸ•’ Timestamp pour nom fichier
    return datetime.datetime.now().strftime("%Y%m%d_%H%M%S")  # # ğŸ§¾ Exemple: 20260114_101500

def is_empty(value: Any) -> bool:  # # ğŸ§ª DÃ©finition du â€œvideâ€ (tes rÃ¨gles)
    if value is None:  # # âœ… None
        return True  # # âœ…
    if isinstance(value, str):  # # ğŸ§¾ String
        v = value.strip()  # # ğŸ§¹
        if v == "":  # # âœ… vide si ""
            return True  # # âœ…
        if v.lower() in {"n/a", "null", "none"}:  # # âœ… vide si "N/A", "null", "None" (string)
            return True  # # âœ…
    if isinstance(value, list):  # # ğŸ“¦ Liste
        return len(value) == 0  # # âœ… vide si liste vide
    return False  # # âŒ sinon non vide

def sleep_polite(min_s: float = 1.5, max_s: float = 2.0) -> None:  # # ğŸ˜‡ Pause polie
    time.sleep(random.uniform(min_s, max_s))  # # â³

def http_get_text(session: requests.Session, url: str, timeout_s: int = 30) -> Tuple[str, int]:  # # ğŸŒ GET HTML (texte)
    headers = {  # # ğŸªª UA "gentil" + clair
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) DIXITBOT-arXivScraper/Final",  # # ğŸªª
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",  # # âœ…
        "Accept-Language": "en-US,en;q=0.9,fr;q=0.8",  # # âœ…
    }  # # âœ…
    resp = session.get(url, headers=headers, timeout=timeout_s)  # # ğŸš€ GET
    return resp.text, resp.status_code  # # ğŸ“„ HTML + code HTTP

def save_text_file(folder: str, filename: str, content: str) -> str:  # # ğŸ’¾ Sauver un fichier texte
    ensure_dir(folder)  # # ğŸ“
    path = os.path.join(folder, filename)  # # ğŸ§©
    with open(path, "w", encoding="utf-8") as f:  # # âœï¸
        f.write(content)  # # ğŸ§¾
    return path  # # ğŸ“Œ

def normalize_url(href: str) -> str:  # # ğŸ”— Normaliser href -> URL complÃ¨te
    if not href:  # # ğŸš«
        return ""  # # âœ…
    h = href.strip()  # # ğŸ§¹
    if h.startswith("//"):  # # ğŸŒ URL sans schÃ©ma
        return "https:" + h  # # âœ…
    if h.startswith("/"):  # # âœ… relatif
        return ARXIV_BASE + h  # # ğŸ”—
    return h  # # âœ… dÃ©jÃ  absolu

def abs_url(arxiv_id: str) -> str:  # # ğŸ”— URL /abs
    return f"{ARXIV_BASE}/abs/{arxiv_id}"  # # âœ…

def pdf_url(arxiv_id: str) -> str:  # # ğŸ“„ URL /pdf
    return f"{ARXIV_BASE}/pdf/{arxiv_id}"  # # âœ…

def html_url(arxiv_id_with_version: str) -> str:  # # ğŸŒ URL /html (arxiv_id peut dÃ©jÃ  inclure vN)
    return f"{ARXIV_BASE}/html/{arxiv_id_with_version}"  # # âœ…

# ============================================================  # # ğŸ“Œ SÃ©parateur
# ğŸ” B) URL builder (tri)  # # ğŸ§­
# ============================================================  # # ğŸ“Œ SÃ©parateur

def build_search_url(query: str, start: int, size: int, sort: str) -> str:  # # ğŸ”— Construire URL search/cs
    q = requests.utils.quote(query)  # # ğŸ” Encoder requÃªte
    base = f"{ARXIV_SEARCH_CS}?query={q}&searchtype=all&abstracts=show&size={size}&start={start}"  # # ğŸ”— Base
    s = (sort or "relevance").strip().lower()  # # ğŸ§  Normaliser
    if s in {"submitted_date", "submitted", "recent"}:  # # ğŸ—“ï¸ Plus rÃ©cents (soumission)
        return base + "&order=-announced_date_first"  # # âœ… (param arXiv connu)
    if s in {"last_updated_date", "updated", "last_updated"}:  # # ğŸ”„ DerniÃ¨res MAJ
        return base + "&order=-last_updated_date_first"  # # âœ… tentative (fallback si 400)
    # âœ… relevance : SURTOUT ne pas mettre &order=-relevance (Ã§a provoque un HTTP 400)  # # âœ…
    return base  # # âœ… relevance

# ============================================================  # # ğŸ“Œ SÃ©parateur
# ğŸ§© C) Parsing SEARCH page (liste) â€” trÃ¨s robuste  # # âœ…
# ============================================================  # # ğŸ“Œ SÃ©parateur

def extract_arxiv_id_from_href(href: str) -> str:  # # ğŸ†” Extraire lâ€™ID depuis un href /abs ou /pdf
    if not href:  # # ğŸš«
        return ""  # # âœ…
    m = re.search(r"/abs/([^?#/]+)", href)  # # ğŸ” /abs/<id>
    if m:  # # âœ…
        return m.group(1).strip()  # # ğŸ†”
    m2 = re.search(r"/pdf/([^?#/]+)", href)  # # ğŸ” /pdf/<id>
    if m2:  # # âœ…
        return m2.group(1).strip()  # # ğŸ†”
    return ""  # # âŒ

def find_first_href_matching(li: BeautifulSoup, pattern: str) -> str:  # # ğŸ” Trouver le 1er href matchant un regex
    for a in li.select("a[href]"):  # # ğŸ” Tous les liens du rÃ©sultat
        href = (a.get("href") or "").strip()  # # ğŸ§¾
        if href and re.search(pattern, href):  # # âœ… match
            return href  # # âœ…
    return ""  # # âŒ

def parse_search_page(search_html: str) -> List[Dict[str, Any]]:  # # ğŸ§© HTML -> items basiques
    soup = BeautifulSoup(search_html, "lxml")  # # ğŸ² Parse
    items: List[Dict[str, Any]] = []  # # ğŸ“¦

    for li in soup.select("ol.breathe-horizontal li.arxiv-result"):  # # ğŸ“š Un rÃ©sultat arXiv
        title_el = li.select_one("p.title")  # # ğŸ·ï¸ Titre
        authors_el = li.select_one("p.authors")  # # ğŸ‘¥ Auteurs
        abstract_el = li.select_one("span.abstract-full")  # # ğŸ§¾ Abstract full
        meta_el = li.select_one("p.is-size-7")  # # ğŸ—“ï¸ Ligne 'Submitted ...'

        title = title_el.get_text(" ", strip=True) if title_el else ""  # # ğŸ·ï¸
        authors_txt = authors_el.get_text(" ", strip=True) if authors_el else ""  # # ğŸ‘¥
        authors = [a.strip() for a in authors_txt.replace("Authors:", "").split(",") if a.strip()]  # # ğŸ‘¥
        abstract = abstract_el.get_text(" ", strip=True) if abstract_el else ""  # # ğŸ§¾
        abstract = abstract.replace("â–³ Less", "").strip()  # # ğŸ§¹

        submitted_date = ""  # # ğŸ—“ï¸
        if meta_el:  # # âœ…
            meta_txt = meta_el.get_text(" ", strip=True)  # # ğŸ§¾
            m = re.search(r"Submitted\s+(.+?)(?:;|$)", meta_txt, flags=re.IGNORECASE)  # # ğŸ”
            if m:  # # âœ…
                submitted_date = m.group(1).strip()  # # ğŸ—“ï¸

        abs_href = find_first_href_matching(li, r"/abs/[^?#/]+")  # # ğŸ”— /abs
        pdf_href = find_first_href_matching(li, r"/pdf/[^?#/]+")  # # ğŸ“„ /pdf
        arxiv_id = extract_arxiv_id_from_href(abs_href or pdf_href)  # # ğŸ†”

        abs_full = normalize_url(abs_href)  # # ğŸ”—
        pdf_full = normalize_url(pdf_href)  # # ğŸ“„

        if arxiv_id and is_empty(abs_full):  # # âœ…
            abs_full = abs_url(arxiv_id)  # # ğŸ”—
        if arxiv_id and is_empty(pdf_full):  # # âœ…
            pdf_full = pdf_url(arxiv_id)  # # ğŸ“„

        items.append({  # # ğŸ“¦ Item minimal issu de search
            "arxiv_id": arxiv_id,  # # ğŸ†”
            "title": title,  # # ğŸ·ï¸
            "authors": authors,  # # ğŸ‘¥
            "abstract": abstract,  # # ğŸ§¾
            "submitted_date": submitted_date,  # # ğŸ—“ï¸
            "abs_url": abs_full,  # # ğŸ”—
            "pdf_url": pdf_full,  # # ğŸ“„
        })  # # âœ…

    return items  # # ğŸ“¤

# ============================================================  # # ğŸ“Œ SÃ©parateur
# ğŸŒ D) Parsing /html (watermark date + DOI + refs + licence)  # # âœ…
# ============================================================  # # ğŸ“Œ SÃ©parateur

def parse_html_page(html_text: str, current_html_url: str) -> Dict[str, Any]:  # # ğŸ§© /html -> dict enrichi
    soup = BeautifulSoup(html_text, "lxml")  # # ğŸ²
    out: Dict[str, Any] = {  # # ğŸ“¦
        "published_date": "",  # # ğŸ—“ï¸
        "doi": "",  # # ğŸ”—
        "license": "",  # # âš–ï¸
        "references": [],  # # ğŸ“š
        "references_dois": [],  # # ğŸ”—
        "html_base_url": current_html_url,  # # ğŸŒ base sans #S1
    }  # # âœ…

    wm = soup.select_one("#watermark-tr")  # # ğŸ” <div id="watermark-tr"> ... 28 Nov 2025</div>
    if wm:  # # âœ…
        wm_text = wm.get_text(" ", strip=True)  # # ğŸ§¾
        m = re.search(r"\]\s*([0-9]{1,2}\s+\w+\s+[0-9]{4})", wm_text)  # # ğŸ” aprÃ¨s ]
        if m:  # # âœ…
            out["published_date"] = m.group(1).strip()  # # ğŸ—“ï¸

    lic = soup.select_one("#license-tr")  # # ğŸ” <a id="license-tr"...>License: ...</a>
    if lic:  # # âœ…
        out["license"] = lic.get_text(" ", strip=True)  # # âš–ï¸

    bib = soup.select_one('.ltx_biblist[id^="bib."]')  # # ğŸ” class="ltx_biblist" id="bib.L1"
    if bib:  # # âœ…
        refs: List[Dict[str, Any]] = []  # # ğŸ“¦
        dois_flat: List[str] = []  # # ğŸ”—
        for ref in bib.select(".ltx_bibitem"):  # # ğŸ” Chaque rÃ©fÃ©rence
            raw_text = ref.get_text(" ", strip=True)  # # ğŸ§¾
            if not raw_text:  # # ğŸš«
                continue  # # âœ…
            hrefs = [normalize_url((a.get("href") or "").strip()) for a in ref.select("a[href]")]  # # ğŸ”—
            hrefs = [h for h in hrefs if h]  # # ğŸ§¹
            doi_hrefs = [h for h in hrefs if "doi.org/" in h]  # # ğŸ”—
            for d in doi_hrefs:  # # ğŸ”
                if d not in dois_flat:  # # âœ…
                    dois_flat.append(d)  # # â•
            refs.append({  # # ğŸ“¦ RÃ©fÃ©rence structurÃ©e
                "raw_text": raw_text,  # # ğŸ§¾
                "urls": hrefs,  # # ğŸ”—
                "dois": doi_hrefs,  # # ğŸ”—
            })  # # âœ…
        out["references"] = refs  # # ğŸ“š
        out["references_dois"] = dois_flat  # # ğŸ”—

    if is_empty(out["doi"]) and out["references_dois"]:  # # âœ…
        out["doi"] = out["references_dois"][0]  # # ğŸ”—

    toc_a = soup.select_one('li.ltx_tocentry a.ltx_ref[href*="/html/"][href*="#"]')  # # ğŸ” href=".../html/...#S1"
    if toc_a:  # # âœ…
        href = (toc_a.get("href") or "").strip()  # # ğŸ§¾
        if href:  # # âœ…
            base = href.split("#", 1)[0]  # # âœ‚ï¸
            out["html_base_url"] = normalize_url(base)  # # ğŸŒ

    return out  # # ğŸ“¤

# ============================================================  # # ğŸ“Œ SÃ©parateur
# ğŸ§® E) Champs manquants + hints  # # ğŸš©
# ============================================================  # # ğŸ“Œ SÃ©parateur

def compute_missing_fields(item: Dict[str, Any]) -> List[str]:  # # ğŸš© Champs vides
    missing: List[str] = []  # # ğŸ“¦
    for f in SUPPORTED_FIELDS:  # # ğŸ”
        if f in {"missing_fields", "errors", "url_hint_if_missing"}:  # # ğŸ§  champs calculÃ©s
            continue  # # âœ…
        if is_empty(item.get(f)):  # # ğŸ§ª
            missing.append(f)  # # â•
    return missing  # # ğŸ“¤

# ============================================================  # # ğŸ“Œ SÃ©parateur
# ğŸš€ F) Fonction principale (1 HTML bundle + 1 JSON)  # # âœ…
# ============================================================  # # ğŸ“Œ SÃ©parateur

def scrape_arxiv_cs(  # # ğŸš€ Fonction principale
    query: str,  # # ğŸ”
    max_results: int = 20,  # # ğŸ¯
    sort: str = "relevance",  # # ğŸ”€ relevance | submitted_date | last_updated_date
    polite_min_s: float = 1.5,  # # ğŸ˜‡
    polite_max_s: float = 2.0,  # # ğŸ˜‡
    data_lake_raw_dir: str = DEFAULT_RAW_DIR,  # # ğŸ’¾
) -> Dict[str, Any]:  # # ğŸ“¤ JSON final

    max_results = int(max_results)  # # ğŸ”¢
    if max_results < 1:  # # ğŸš«
        max_results = 1  # # âœ…
    if max_results > MAX_RESULTS_HARD_LIMIT:  # # ğŸš§
        max_results = MAX_RESULTS_HARD_LIMIT  # # âœ…

    ts = now_iso_for_filename()  # # ğŸ•’
    ensure_dir(data_lake_raw_dir)  # # ğŸ“
    session = requests.Session()  # # ğŸ”Œ
    bundle_parts: List[str] = []  # # ğŸ§¾ HTML bundle (UN SEUL fichier)
    collected: List[Dict[str, Any]] = []  # # ğŸ“¦ RÃ©sultats (liste)
    start = 0  # # ğŸ“„ Pagination offset
    last_search_url_used = ""  # # ğŸ§¾ debug

    while len(collected) < max_results:  # # ğŸ”
        search_url = build_search_url(query=query, start=start, size=PAGE_SIZE, sort=sort)  # # ğŸ”—
        last_search_url_used = search_url  # # ğŸ§¾
        search_html, code = http_get_text(session=session, url=search_url)  # # ğŸŒ

        if code == 400 and (sort or "").strip().lower() in {"last_updated_date", "updated", "last_updated"}:  # # ğŸ§¯
            search_url_retry = build_search_url(query=query, start=start, size=PAGE_SIZE, sort="submitted_date")  # # ğŸ›Ÿ
            search_html, code = http_get_text(session=session, url=search_url_retry)  # # ğŸŒ
            bundle_parts.append(f"<!-- NOTE: order=-last_updated_date_first a renvoyÃ© 400, fallback submitted_date -->\n")  # # ğŸ§¾
            last_search_url_used = search_url_retry  # # ğŸ§¾

        bundle_parts.append(f"<!-- ===== SEARCH URL: {last_search_url_used} | HTTP {code} ===== -->\n")  # # ğŸ§¾
        bundle_parts.append(search_html)  # # ğŸ§¾
        bundle_parts.append("\n<!-- ===== END SEARCH ===== -->\n")  # # ğŸ§¾

        if code != 200:  # # âŒ
            break  # # ğŸ›‘

        page_items = parse_search_page(search_html)  # # ğŸ” extraction search
        if not page_items:  # # ğŸ›‘
            break  # # âœ…

        collected.extend(page_items)  # # â•
        start += PAGE_SIZE  # # â¡ï¸ page suivante
        sleep_polite(min_s=polite_min_s, max_s=polite_max_s)  # # ğŸ˜‡

    collected = collected[:max_results]  # # âœ‚ï¸

    for item in collected:  # # ğŸ”
        item["html_url"] = ""  # # ğŸŒ init
        item["published_date"] = ""  # # ğŸ—“ï¸ init
        item["doi"] = ""  # # ğŸ”— init
        item["license"] = ""  # # âš–ï¸ init
        item["references"] = []  # # ğŸ“š init
        item["references_dois"] = []  # # ğŸ”— init
        item["errors"] = []  # # ğŸ§¾ init

        arxiv_id = (item.get("arxiv_id") or "").strip()  # # ğŸ†”
        if is_empty(arxiv_id):  # # ğŸš«
            item["errors"].append("missing_arxiv_id_from_search")  # # ğŸ§¾
            item["missing_fields"] = compute_missing_fields(item)  # # ğŸš©
            item["url_hint_if_missing"] = "ID arXiv introuvable depuis la page de recherche : ouvre le HTML bundle et cherche un lien /abs/."  # # ğŸ§­
            continue  # # âœ…

        if is_empty(item.get("abs_url")):  # # ğŸ”—
            item["abs_url"] = abs_url(arxiv_id)  # # ğŸ”—
        if is_empty(item.get("pdf_url")):  # # ğŸ“„
            item["pdf_url"] = pdf_url(arxiv_id)  # # ğŸ“„

        item["html_url"] = html_url(arxiv_id)  # # ğŸŒ

        html_text, html_code = http_get_text(session=session, url=item["html_url"])  # # ğŸŒ
        bundle_parts.append(f"<!-- ===== HTML URL: {item['html_url']} | HTTP {html_code} ===== -->\n")  # # ğŸ§¾
        bundle_parts.append(html_text)  # # ğŸ§¾
        bundle_parts.append("\n<!-- ===== END HTML ===== -->\n")  # # ğŸ§¾

        if html_code != 200:  # # âŒ
            item["errors"].append(f"html_http_{html_code}")  # # ğŸ§¾
            item["missing_fields"] = compute_missing_fields(item)  # # ğŸš©
            item["url_hint_if_missing"] = f"Page HTML indisponible ({html_code}). VÃ©rifie l'abs: {item.get('abs_url','')}"  # # ğŸ§­
            sleep_polite(min_s=polite_min_s, max_s=polite_max_s)  # # ğŸ˜‡
            continue  # # âœ…

        html_data = parse_html_page(html_text=html_text, current_html_url=item["html_url"])  # # ğŸ”
        item["published_date"] = html_data.get("published_date", "")  # # ğŸ—“ï¸
        item["doi"] = html_data.get("doi", "")  # # ğŸ”—
        item["license"] = html_data.get("license", "")  # # âš–ï¸
        item["references"] = html_data.get("references", [])  # # ğŸ“š
        item["references_dois"] = html_data.get("references_dois", [])  # # ğŸ”—

        base_url = html_data.get("html_base_url", "")  # # ğŸŒ
        if base_url and base_url.startswith("http"):  # # âœ…
            item["html_url"] = base_url  # # ğŸŒ

        item["missing_fields"] = compute_missing_fields(item)  # # ğŸš©
        if item["missing_fields"]:  # # âœ…
            item["url_hint_if_missing"] = (  # # ğŸ§­
                f"Champs manquants: {', '.join(item['missing_fields'])}. "  # # ğŸ§¾
                f"VÃ©rifie HTML: {item.get('html_url','')} | ABS: {item.get('abs_url','')} | PDF: {item.get('pdf_url','')}"  # # ğŸ”—
            )  # # âœ…
        else:  # # âœ…
            item["url_hint_if_missing"] = ""  # # âœ…

        sleep_polite(min_s=polite_min_s, max_s=polite_max_s)  # # ğŸ˜‡

    hit_limit_100 = (max_results == MAX_RESULTS_HARD_LIMIT)  # # ğŸš§
    message_if_limit = "Limite 100 atteinte (max_results)." if hit_limit_100 else ""  # # ğŸ§¾

    bundle_html = "\n".join(bundle_parts)  # # ğŸ§¾
    html_name = f"arxiv_bundle_{ts}.html"  # # ğŸ§¾
    html_path = save_text_file(data_lake_raw_dir, html_name, bundle_html)  # # ğŸ’¾

    result: Dict[str, Any] = {  # # ğŸ“¦ JSON final
        "ok": True,  # # âœ…
        "query": query,  # # ğŸ”
        "sort": sort,  # # ğŸ”€
        "count": len(collected),  # # ğŸ”¢
        "max_results": max_results,  # # ğŸ¯
        "hit_limit_100": hit_limit_100,  # # ğŸš§
        "message_if_limit": message_if_limit,  # # ğŸ§¾
        "items": collected,  # # ğŸ“š
        "bundle_html_file": html_path,  # # ğŸ’¾
        "supported_fields": SUPPORTED_FIELDS,  # # âœ…
    }  # # âœ…

    json_name = f"arxiv_raw_{ts}.json"  # # ğŸ§¾
    json_path = os.path.join(data_lake_raw_dir, json_name)  # # ğŸ“
    with open(json_path, "w", encoding="utf-8") as f:  # # âœï¸
        json.dump(result, f, ensure_ascii=False, indent=2)  # # ğŸ§¾

    result["saved_to"] = json_path  # # ğŸ“Œ
    return result  # # ğŸ“¤

# ============================================================  # # ğŸ“Œ SÃ©parateur
# ğŸ§ª TEST LOCAL (1 ligne ON/OFF)  # # âœ…
# ============================================================  # # ğŸ“Œ SÃ©parateur

RUN_LOCAL_TEST = True  # # âœ… True = test ON | False = test OFF

if __name__ == "__main__" and RUN_LOCAL_TEST:  # # â–¶ï¸
    print("ğŸš€ Lancement du scraping arXiv (test local)...")  # # ğŸ–¨ï¸
    results = scrape_arxiv_cs(query="multimodal transformer", max_results=5, sort="relevance")  # # ğŸ•·ï¸
    print(f"âœ… OK: {results.get('count')} articles rÃ©cupÃ©rÃ©s")  # # ğŸ–¨ï¸
    print(f"ğŸ’¾ JSON sauvegardÃ©: {results.get('saved_to')}")  # # ğŸ–¨ï¸
    print(f"ğŸ’¾ HTML bundle sauvegardÃ©: {results.get('bundle_html_file')}")  # # ğŸ–¨ï¸
    items = results.get("items", [])  # # ğŸ“¦
    if items:  # # âœ…
        print("ğŸ§¾ AperÃ§u item[0] :")  # # ğŸ–¨ï¸
        print(json.dumps(items[0], ensure_ascii=False, indent=2))  # # ğŸ§¾
