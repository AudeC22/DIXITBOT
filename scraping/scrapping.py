# ============================================================  # # ğŸ“Œ DÃ©but du script
# ğŸ•·ï¸ arXiv Scraper (search/cs) -> JSON + sauvegarde data_lake/raw  # # ğŸ¯ Objectif du script
# ============================================================  # # ğŸ“Œ SÃ©parateur visuel

import os  # # ğŸ“ GÃ©rer les chemins et dossiers
import re  # # ğŸ” Extraire des infos via regex (ID, dates, versions)
import json  # # ğŸ§¾ Exporter en JSON
import time  # # â±ï¸ Pause polie entre requÃªtes
import random  # # ğŸ² Jitter pour Ã©viter un rythme trop "robot"
import datetime  # # ğŸ•’ GÃ©nÃ©rer timestamps pour fichiers
from typing import Dict, Any, List, Optional  # # ğŸ§© Typage pour clartÃ©
import requests  # # ğŸŒ Faire des requÃªtes HTTP GET
from bs4 import BeautifulSoup  # # ğŸ² Parser HTML et sÃ©lectionner des balises

ARXIV_BASE = "https://arxiv.org"  # # ğŸŒ Domaine arXiv
ARXIV_SEARCH_CS = "https://arxiv.org/search/cs"  # # ğŸ” Endpoint recherche Computer Science

DEFAULT_RAW_DIR = os.path.join("data_lake", "raw")  # # ğŸ“¦ Dossier de stockage raw
DEFAULT_META_DIR = os.path.join("data_lake", "metadata")  # # ğŸ§¾ Dossier metadata sources

MAX_RESULTS_HARD_LIMIT = 100  # # ğŸš§ Limite globale demandÃ©e (max 100)
PAGE_SIZE = 50  # # ğŸ“„ arXiv permet size=50 en gÃ©nÃ©ral (pratique pour paginer)
