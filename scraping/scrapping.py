# ============================================================  # # ğŸ“Œ DÃ©but du script
# ğŸ•·ï¸ arXiv Scraper (search -> abs -> html -> pdf fallback) -> JSON + sauvegarde data_lake/raw  # # ğŸ¯ Objectif
# âœ… But: rÃ©cupÃ©rer "tous les champs" (mÃªme si certains restent vides => on trace lâ€™erreur + URL possible)  # # ğŸ§¾ RÃ¨gle
# âœ… Pipeline: SEARCH (liste) -> ABS (mÃ©tadonnÃ©es fiables) -> HTML (si dispo) -> PDF (fallback)  # # ğŸ§  Logique
# âœ… Sortie: JSON + flag missing_fields[] + url_hint_if_missing  # # ğŸ§¾ RÃ©sultat
# âœ… Politesse: sleep 1.5â€“2.0s entre requÃªtes  # # ğŸ˜‡
# âœ… Limite: max_results <= 100 + message si atteint  # # ğŸš§
# ============================================================  # # ğŸ“Œ SÃ©parateur visuel

import os  # # ğŸ“ GÃ©rer chemins et dossiers
import re  # # ğŸ” Regex (ID, dates, repÃ©rage sections "References")
import json  # # ğŸ§¾ Export JSON
import time  # # â±ï¸ Pause polie
import random  # # ğŸ² Jitter anti-robot
import datetime  # # ğŸ•’ Timestamp pour fichiers
from typing import Dict, Any, List, Optional, Tuple  # # ğŸ§© Typage
import requests  # # ğŸŒ HTTP GET
from bs4 import BeautifulSoup  # # ğŸ² Parsing HTML + select

# ğŸ§  PDF parsing (fallback)  # # ğŸ“„
# - pypdf = simple et rapide  # # âœ…
# - pdfminer.six = plus robuste quand pypdf Ã©choue  # # ğŸ§°
from pypdf import PdfReader  # # ğŸ“„ Extraction texte PDF (rapide)
from pdfminer.high_level import extract_text as pdfminer_extract_text  # # ğŸ“„ Extraction texte PDF (robuste)
from io import BytesIO  # # ğŸ§ª PDF en mÃ©moire (pas de stockage sur disque)

ARXIV_BASE = "https://arxiv.org"  # # ğŸŒ Domaine
ARXIV_SEARCH_ALL = "https://arxiv.org/search"  # # ğŸ” Search tous domaines
ARXIV_SEARCH_CS = "https://arxiv.org/search/cs"  # # ğŸ” Search Computer Science

DEFAULT_RAW_DIR = os.path.join("data_lake", "raw")  # # ğŸ“¦ Stockage JSON (et HTML si activÃ©)
MAX_RESULTS_HARD_LIMIT = 100  # # ğŸš§ Hard limit
PAGE_SIZE = 50  # # ğŸ“„ Pagination (50 est un bon compromis)

SAVE_RAW_HTML = True  # # ğŸ’¾ Sauver HTML search + abs + html (debug/traÃ§abilitÃ©) | False = pas de HTML
SAVE_ABS_PAGES = True  # # ğŸ’¾ Sauver /abs (utile pour debug)
SAVE_HTML_PAGES = True  # # ğŸ’¾ Sauver /html si dispo (utile pour debug)


# ============================================================
# âœ… 0) DÃ©finition du "vide" (rÃ¨gle utilisateur) + missing_fields
# ============================================================

def is_empty(value: Any) -> bool:  # # ğŸ§ª DÃ©finir si une valeur est considÃ©rÃ©e "vide"
    if value is None:  # # âœ… None = vide
        return True  # # âœ…
    if isinstance(value, str):  # # âœ… Si string
        v = value.strip()  # # ğŸ§¹ Nettoyage
        if v == "":  # # âœ… "" = vide
            return True  # # âœ…
        if v.lower() in ("n/a", "null", "none"):  # # âœ… "N/A" / "null" / "None" (string) = vide
            return True  # # âœ…
        return False  # # âœ… Non vide
    if isinstance(value, list):  # # âœ… Si liste
        if len(value) == 0:  # # âœ… Liste vide = vide
            return True  # # âœ…
        return False  # # âœ… Non vide
    if isinstance(value, dict):  # # âœ… Si dict
        if len(value) == 0:  # # âœ… Dict vide = vide
            return True  # # âœ…
        return False  # # âœ… Non vide
    return False  # # âœ… Par dÃ©faut, on considÃ¨re non vide


def compute_missing_fields(item: Dict[str, Any], required_fields: List[str]) -> List[str]:  # # ğŸ§¾ Calculer la liste des champs manquants
    missing: List[str] = []  # # ğŸ“¦ Liste manquants
    for f in required_fields:  # # ğŸ” Pour chaque champ attendu
        if is_empty(item.get(f)):  # # âŒ Si vide selon la rÃ¨gle
            missing.append(f)  # # â• Ajouter
    return missing  # # ğŸ“¤ Retour


# ============================================================
# ğŸŒ A) Utils â€” dossiers + politesse + timestamps
# ============================================================

def ensure_dir(path: str) -> None:  # # ğŸ“ Assurer que le dossier existe
    os.makedirs(path, exist_ok=True)  # # âœ… CrÃ©e le dossier si absent

def now_iso_for_filename() -> str:  # # ğŸ•’ Timestamp format fichier
    return datetime.datetime.now().strftime("%Y%m%d_%H%M%S")  # # ğŸ§¾ Exemple: 20260113_113012

def sleep_polite(min_s: float = 1.5, max_s: float = 2.0) -> None:  # # ğŸ˜‡ Pause polie
    time.sleep(random.uniform(min_s, max_s))  # # â³ Attente alÃ©atoire

def save_text_file(folder: str, filename: str, content: str) -> str:  # # ğŸ’¾ Sauver texte (HTML/JSON)
    ensure_dir(folder)  # # ğŸ“ Dossier
    path = os.path.join(folder, filename)  # # ğŸ§© Chemin
    with open(path, "w", encoding="utf-8") as f:  # # âœï¸ Ouvrir
        f.write(content)  # # ğŸ§¾ Ã‰crire
    return path  # # ğŸ“Œ Retour


# ============================================================
# ğŸŒ B) GET â€” robuste (HTML + bytes) + erreurs tracÃ©es
# ============================================================

def http_get_text(session: requests.Session, url: str, timeout_s: int = 30) -> Tuple[str, str]:  # # ğŸŒ GET HTML
    headers = {"User-Agent": "Mozilla/5.0 (compatible; DIXITBOT-arXivScraper/2.0)"}  # # ğŸªª UA
    resp = session.get(url, headers=headers, timeout=timeout_s)  # # ğŸš€ GET
    resp.raise_for_status()  # # âŒ HTTP error
    return resp.text, resp.url  # # ğŸ“„ HTML + URL finale

def http_get_bytes(session: requests.Session, url: str, timeout_s: int = 60) -> Tuple[bytes, str, str]:  # # ğŸŒ GET bytes (PDF)
    headers = {"User-Agent": "Mozilla/5.0 (compatible; DIXITBOT-arXivScraper/2.0)"}  # # ğŸªª UA
    resp = session.get(url, headers=headers, timeout=timeout_s)  # # ğŸš€ GET
    resp.raise_for_status()  # # âŒ HTTP error
    content_type = resp.headers.get("Content-Type", "")  # # ğŸ§¾ Content-Type
    return resp.content, resp.url, content_type  # # ğŸ“¦ bytes + URL finale + type


# ============================================================
# ğŸ” C) URL builder (search + sort)
# ============================================================

def normalize_sort_to_order(sort: str) -> str:  # # ğŸ§­ Convertir sort -> param order arXiv
    sort = (sort or "").strip().lower()  # # ğŸ§¹ Normaliser
    if sort in ("relevance", "pertinence", ""):  # # âœ… "relevance" => order vide (sinon 400)
        return ""  # # âœ…
    if sort in ("submitted_date", "submission_date", "recent", "recent_submitted"):  # # âœ…
        return "-submitted_date"  # # âœ…
    if sort in ("announced_date", "announcement_date", "announced", "recent_announced"):  # # âœ…
        return "-announced_date_first"  # # âœ…
    if sort in ("last_updated_date", "updated", "last_updated"):  # # âœ… tri local (aprÃ¨s /abs)
        return ""  # # âœ…
    return ""  # # âœ… default relevance

def build_search_url(query: str, start: int, size: int, sort: str, archive: str) -> str:  # # ğŸ”— Construire URL search
    q = requests.utils.quote(query)  # # ğŸ” Encoder query
    base = ARXIV_SEARCH_CS if (archive or "").lower() == "cs" else ARXIV_SEARCH_ALL  # # ğŸ§­ Endpoint
    order = normalize_sort_to_order(sort)  # # ğŸ§­ order
    order_part = f"&order={requests.utils.quote(order)}" if order != "" else "&order="  # # âœ… order vide si relevance
    return f"{base}?query={q}&searchtype=all&abstracts=show{order_part}&size={size}&start={start}"  # # ğŸŒ URL

def build_abs_url(arxiv_id: str) -> str:  # # ğŸ”— URL /abs
    return f"{ARXIV_BASE}/abs/{arxiv_id}"  # # âœ…

def build_pdf_url(arxiv_id: str) -> str:  # # ğŸ“„ URL /pdf
    return f"{ARXIV_BASE}/pdf/{arxiv_id}"  # # âœ…

def build_html_url(arxiv_id: str) -> str:  # # ğŸŒ URL /html (pas toujours dispo)
    return f"{ARXIV_BASE}/html/{arxiv_id}"  # # âœ…


# ============================================================
# ğŸ§© D) Parsing SEARCH page (liste)
# ============================================================

def safe_text(el) -> str:  # # ğŸ§¼ Texte propre
    return el.get_text(" ", strip=True) if el else ""  # # âœ…

def abs_id_from_abs_url(abs_url: str) -> str:  # # ğŸ†” Extraire ID depuis /abs/...
    m = re.search(r"/abs/([^?#/]+)", abs_url or "")  # # ğŸ”
    return m.group(1) if m else ""  # # âœ…

def parse_submitted_line(text_line: str) -> Tuple[str, str]:  # # ğŸ—“ï¸ Extraire submitted_date / announced
    text_line = (text_line or "").strip()  # # ğŸ§¹
    submitted = ""  # # ğŸ—“ï¸
    announced = ""  # # ğŸ—“ï¸
    m1 = re.search(r"Submitted\s+(.+?);", text_line, flags=re.IGNORECASE)  # # ğŸ”
    if m1:  # # âœ…
        submitted = m1.group(1).strip()  # # ğŸ—“ï¸
    m2 = re.search(r"originally announced\s+(.+?)\.", text_line, flags=re.IGNORECASE)  # # ğŸ”
    if m2:  # # âœ…
        announced = m2.group(1).strip()  # # ğŸ—“ï¸
    return submitted, announced  # # ğŸ“¤

def parse_search_page(html: str) -> List[Dict[str, Any]]:  # # ğŸ§© HTML -> items
    soup = BeautifulSoup(html, "lxml")  # # ğŸ²
    items: List[Dict[str, Any]] = []  # # ğŸ“¦

    for li in soup.select("ol.breathe-horizontal li.arxiv-result"):  # # ğŸ“š Result
        title_el = li.select_one("p.title")  # # ğŸ·ï¸
        authors_links = li.select("p.authors a")  # # ğŸ‘¥
        abstract_el = li.select_one("span.abstract-full")  # # ğŸ§¾
        abs_link_el = li.select_one('p.list-title a[href*="/abs/"]')  # # ğŸ”—
        pdf_link_el = li.select_one('p.list-title a[href*="/pdf/"]')  # # ğŸ“„
        submitted_el = li.select_one("p.is-size-7")  # # ğŸ—“ï¸
        tag_els = li.select("div.tags span.tag")  # # ğŸ·ï¸

        title = safe_text(title_el)  # # ğŸ·ï¸

        authors: List[str] = []  # # ğŸ‘¥
        for a in authors_links:  # # ğŸ”
            t = safe_text(a)  # # ğŸ§¾
            if t:  # # âœ…
                authors.append(t)  # # â•

        abstract = safe_text(abstract_el).replace("â–³ Less", "").strip()  # # ğŸ§¾

        abs_url = abs_link_el.get("href", "").strip() if abs_link_el else ""  # # ğŸ”—
        pdf_url = pdf_link_el.get("href", "").strip() if pdf_link_el else ""  # # ğŸ“„

        if abs_url and abs_url.startswith("/"):  # # âœ…
            abs_url = ARXIV_BASE + abs_url  # # ğŸ”—
        if pdf_url and pdf_url.startswith("/"):  # # âœ…
            pdf_url = ARXIV_BASE + pdf_url  # # ğŸ”—

        arxiv_id = abs_id_from_abs_url(abs_url)  # # ğŸ†”

        submitted_line = safe_text(submitted_el)  # # ğŸ—“ï¸
        submitted_date, announced = parse_submitted_line(submitted_line)  # # ğŸ—“ï¸

        categories: List[str] = []  # # ğŸ·ï¸
        for t in tag_els:  # # ğŸ”
            tag_txt = safe_text(t)  # # ğŸ§¾
            if tag_txt:  # # âœ…
                categories.append(tag_txt)  # # â•

        # âœ… Structure "tous champs" (mÃªme si vide)  # # ğŸ§¾
        items.append({  # # ğŸ“¦
            "arxiv_id": arxiv_id or "N/A",  # # ğŸ†”
            "title": title or "N/A",  # # ğŸ·ï¸
            "authors": authors or [],  # # ğŸ‘¥
            "abstract": abstract or "N/A",  # # ğŸ§¾
            "categories": categories or [],  # # ğŸ·ï¸
            "submitted_date": submitted_date or "N/A",  # # ğŸ—“ï¸
            "announced": announced or "N/A",  # # ğŸ—“ï¸
            "abs_url": abs_url or "N/A",  # # ğŸ”—
            "pdf_url": pdf_url or "N/A",  # # ğŸ“„
            # --- Champs enrichis plus tard (on prÃ©-crÃ©e pour "rien d'optionnel")  # # ğŸ§¾
            "doi": "N/A",  # # ğŸ”—
            "license": "N/A",  # # âš–ï¸
            "journal_ref": "N/A",  # # ğŸ“š
            "comments": "N/A",  # # ğŸ’¬
            "versions": [],  # # ğŸ—“ï¸
            "last_updated_raw": "N/A",  # # ğŸ—“ï¸
            "subjects_raw": "N/A",  # # ğŸ·ï¸
            "html_url": "N/A",  # # ğŸŒ
            "content_html": "N/A",  # # ğŸ§¾
            "references": [],  # # ğŸ“š
            "affiliations": [],  # # ğŸ§¾ (pas toujours dispo)
            "full_text": "N/A",  # # ğŸ§¾ (texte extrait)
            "source": "arxiv_search",  # # ğŸ§¾
            "errors": [],  # # ğŸ§¾
            "fill_sources": [],  # # ğŸ§  Liste des sources qui ont rempli des champs (abs/html/pdf/pypdf/pdfminer)
        })  # # âœ…

    return items  # # ğŸ“¤


# ============================================================
# ğŸ“„ E) Parse ABS page (dÃ©tails fiables)
# ============================================================

def parse_abs_page(html: str) -> Dict[str, Any]:  # # ğŸ§© /abs -> dict
    soup = BeautifulSoup(html, "lxml")  # # ğŸ²
    data: Dict[str, Any] = {}  # # ğŸ“¦

    title_el = soup.select_one("h1.title")  # # ğŸ·ï¸
    data["title_abs"] = safe_text(title_el).replace("Title:", "").strip() if title_el else "N/A"  # # ğŸ·ï¸

    authors: List[str] = []  # # ğŸ‘¥
    for a in soup.select("div.authors a"):  # # ğŸ”
        t = safe_text(a)  # # ğŸ§¾
        if t:  # # âœ…
            authors.append(t)  # # â•
    data["authors_abs"] = authors  # # ğŸ‘¥

    abs_el = soup.select_one("blockquote.abstract")  # # ğŸ§¾
    data["abstract_abs"] = safe_text(abs_el).replace("Abstract:", "").strip() if abs_el else "N/A"  # # ğŸ§¾

    subj_el = soup.select_one("td.tablecell.subjects")  # # ğŸ·ï¸
    data["subjects_raw"] = safe_text(subj_el) if subj_el else "N/A"  # # ğŸ·ï¸

    def meta_text(selector: str) -> str:  # # ğŸ§° Lire texte d'un champ metadata
        el = soup.select_one(selector)  # # ğŸ”
        return safe_text(el) if el else "N/A"  # # âœ…

    data["comments"] = meta_text("td.tablecell.comments")  # # ğŸ’¬
    data["journal_ref"] = meta_text("td.tablecell.jref")  # # ğŸ“š
    data["doi"] = meta_text("td.tablecell.doi")  # # ğŸ”—

    license_el = soup.select_one("td.tablecell.license a")  # # âš–ï¸
    data["license"] = license_el.get("href", "").strip() if license_el else meta_text("td.tablecell.license")  # # âš–ï¸

    history_el = soup.select_one("div.submission-history")  # # ğŸ—“ï¸
    versions: List[Dict[str, str]] = []  # # ğŸ—“ï¸
    if history_el:  # # âœ…
        for li in history_el.select("li"):  # # ğŸ”
            line = safe_text(li)  # # ğŸ§¾
            m = re.search(r"\[(v\d+)\]\s*(.+)$", line)  # # ğŸ”
            if m:  # # âœ…
                versions.append({"version": m.group(1), "raw": m.group(2).strip()})  # # â•
            elif line:  # # âœ…
                versions.append({"version": "", "raw": line})  # # â•
    data["versions"] = versions  # # ğŸ—“ï¸
    data["last_updated_raw"] = versions[-1]["raw"] if versions else "N/A"  # # ğŸ—“ï¸

    return data  # # ğŸ“¤


# ============================================================
# ğŸŒ F) Parse HTML arXiv /html (si dispo)
# ============================================================

def parse_html_page(html: str) -> Dict[str, Any]:  # # ğŸ§© /html -> content + refs best effort
    soup = BeautifulSoup(html, "lxml")  # # ğŸ²
    data: Dict[str, Any] = {}  # # ğŸ“¦

    # ğŸ§¾ Contenu texte (best effort)  # # ğŸ§¾
    main = soup.select_one("main") or soup.select_one("body")  # # ğŸ§¾
    text = safe_text(main) if main else ""  # # ğŸ§¾
    data["content_html"] = text if text else "N/A"  # # ğŸ§¾

    # ğŸ“š RÃ©fÃ©rences (best effort)  # # ğŸ“š
    refs: List[str] = []  # # ğŸ“¦
    # - certains /html ont un bloc References / Bibliography  # # ğŸ§ 
    for h in soup.select("h1, h2, h3"):  # # ğŸ”
        t = safe_text(h).lower()  # # ğŸ§¾
        if "reference" in t or "bibliograph" in t:  # # âœ…
            # On prend le texte du parent proche comme approximation  # # ğŸ§ 
            parent = h.parent  # # ğŸ“Œ
            parent_text = safe_text(parent) if parent else ""  # # ğŸ§¾
            if parent_text:  # # âœ…
                refs = extract_references_from_text(parent_text)  # # ğŸ“š
            break  # # âœ…
    data["references_from_html"] = refs  # # ğŸ“š

    return data  # # ğŸ“¤


# ============================================================
# ğŸ“„ G) PDF parsing en mÃ©moire (pypdf puis pdfminer)
# ============================================================

def extract_references_from_text(text: str) -> List[str]:  # # ğŸ“š Heuristique simple pour sortir une liste de rÃ©fÃ©rences
    if not text:  # # ğŸš«
        return []  # # âœ…
    # ğŸ” On repÃ¨re une section References/Bibliography  # # ğŸ§ 
    idx = -1  # # ğŸ“
    m = re.search(r"\b(references|bibliography)\b", text, flags=re.IGNORECASE)  # # ğŸ”
    if m:  # # âœ…
        idx = m.start()  # # ğŸ“
    if idx == -1:  # # âŒ
        return []  # # âœ…

    tail = text[idx:]  # # ğŸ§¾ Texte Ã  partir de "References"
    # âœ‚ï¸ On coupe si on voit une section suivante trÃ¨s probable (Appendix/Acknowledgements)  # # ğŸ§ 
    cut = re.split(r"\b(appendix|acknowledg|supplementary)\b", tail, flags=re.IGNORECASE)  # # âœ‚ï¸
    refs_block = cut[0] if cut else tail  # # ğŸ§¾

    # ğŸ§¹ Nettoyage et split en lignes  # # ğŸ§¼
    lines = [l.strip() for l in refs_block.splitlines() if l.strip()]  # # ğŸ§¾
    # ğŸ§  Filtre: on enlÃ¨ve le titre "References" lui-mÃªme  # # ğŸ§¹
    lines = [l for l in lines if l.lower() not in ("references", "bibliography")]  # # ğŸ§¹
    # âœ… Limite soft pour Ã©viter JSON Ã©norme  # # ğŸš§
    return lines[:200]  # # ğŸ“š


def parse_pdf_with_pypdf(pdf_bytes: bytes) -> str:  # # ğŸ“„ Extraire texte via pypdf (rapide)
    try:  # # ğŸ§¯
        reader = PdfReader(BytesIO(pdf_bytes))  # # ğŸ“„ Lire PDF en mÃ©moire
        chunks: List[str] = []  # # ğŸ“¦
        for page in reader.pages:  # # ğŸ”
            t = page.extract_text() or ""  # # ğŸ§¾
            if t.strip():  # # âœ…
                chunks.append(t)  # # â•
        return "\n".join(chunks).strip()  # # ğŸ“¤ Texte
    except Exception:  # # âŒ
        return ""  # # ğŸ›Ÿ

def parse_pdf_with_pdfminer(pdf_bytes: bytes) -> str:  # # ğŸ“„ Extraire texte via pdfminer (robuste)
    try:  # # ğŸ§¯
        return (pdfminer_extract_text(BytesIO(pdf_bytes)) or "").strip()  # # ğŸ“¤
    except Exception:  # # âŒ
        return ""  # # ğŸ›Ÿ


# ============================================================
# ğŸ§  H) Enrich item: ABS -> HTML -> PDF fallback pour champs manquants
# ============================================================

REQUIRED_FIELDS = [  # # âœ… Liste "tous les champs" que tu veux (aucun optionnel)
    "arxiv_id", "title", "authors", "abstract", "categories",
    "submitted_date", "abs_url", "pdf_url",
    "doi", "license", "journal_ref", "comments",
    "versions", "last_updated_raw", "subjects_raw",
    "html_url", "content_html",
    "references", "affiliations",
    "full_text",
]  # # âœ…

def ensure_all_keys(item: Dict[str, Any]) -> None:  # # ğŸ§¾ Garantir que tous les champs existent (mÃªme vides)
    for k in REQUIRED_FIELDS:  # # ğŸ”
        if k not in item:  # # âœ…
            # ğŸ§  Valeurs par dÃ©faut cohÃ©rentes selon type  # # ğŸ§ 
            if k in ("authors", "categories", "versions", "references", "affiliations"):  # # âœ… List
                item[k] = []  # # âœ…
            else:  # # âœ… String
                item[k] = "N/A"  # # âœ…

def add_url_hint_for_missing(item: Dict[str, Any], missing_fields: List[str]) -> None:  # # ğŸ§­ Ajouter un message si champ manquant
    if not missing_fields:  # # âœ…
        item["url_hint_if_missing"] = ""  # # âœ… Rien
        return  # # âœ…
    # âœ… RÃ¨gle: "si erreur, Ã©crire que la rÃ©ponse est peut-Ãªtre dans cette page: URL"  # # ğŸ“Œ
    abs_url = item.get("abs_url", "N/A")  # # ğŸ”—
    pdf_url = item.get("pdf_url", "N/A")  # # ğŸ“„
    html_url = item.get("html_url", "N/A")  # # ğŸŒ
    item["url_hint_if_missing"] = f"Certains champs sont manquants ({', '.join(missing_fields)}). La rÃ©ponse est peut-Ãªtre dans: abs={abs_url} | html={html_url} | pdf={pdf_url}"  # # ğŸ§¾

def enrich_item(session: requests.Session, item: Dict[str, Any], polite_min_s: float, polite_max_s: float, raw_dir: str, ts: str, prefer_html_then_pdf: bool = True) -> Dict[str, Any]:  # # ğŸ§  Enrichir 1 item
    ensure_all_keys(item)  # # âœ… Tous champs prÃ©sents

    arxiv_id = (item.get("arxiv_id") or "").strip()  # # ğŸ†”
    if is_empty(arxiv_id):  # # âŒ
        item["errors"].append("missing_arxiv_id")  # # ğŸ§¾
        item["missing_fields"] = compute_missing_fields(item, REQUIRED_FIELDS)  # # ğŸ§¾
        add_url_hint_for_missing(item, item["missing_fields"])  # # ğŸ§­
        return item  # # ğŸ“¤

    # ğŸ”— Fix URLs  # # ğŸ”—
    item["abs_url"] = item.get("abs_url") if not is_empty(item.get("abs_url")) else build_abs_url(arxiv_id)  # # ğŸ”—
    item["pdf_url"] = item.get("pdf_url") if not is_empty(item.get("pdf_url")) else build_pdf_url(arxiv_id)  # # ğŸ“„
    item["html_url"] = build_html_url(arxiv_id)  # # ğŸŒ

    # =========================
    # 1) GET /abs + parse  # # ğŸ§¾
    # =========================
    try:  # # ğŸ§¯
        abs_html, final_abs = http_get_text(session=session, url=item["abs_url"])  # # ğŸŒ GET
        item["abs_url_final"] = final_abs  # # ğŸ”
        if SAVE_RAW_HTML and SAVE_ABS_PAGES:  # # ğŸ’¾
            item["abs_page_saved"] = save_text_file(raw_dir, f"arxiv_abs_{ts}_{arxiv_id}.html", abs_html)  # # ğŸ’¾
        abs_data = parse_abs_page(abs_html)  # # ğŸ” SELECT
        # ğŸ§  Remplir champs (sans Ã©craser si dÃ©jÃ  rempli)  # # ğŸ§ 
        if is_empty(item.get("title")):  # # âœ…
            item["title"] = abs_data.get("title_abs", "N/A")  # # ğŸ·ï¸
        if is_empty(item.get("abstract")):  # # âœ…
            item["abstract"] = abs_data.get("abstract_abs", "N/A")  # # ğŸ§¾
        if is_empty(item.get("authors")):  # # âœ…
            item["authors"] = abs_data.get("authors_abs", [])  # # ğŸ‘¥

        # âœ… Champs mÃ©ta  # # ğŸ§¾
        item["doi"] = abs_data.get("doi", item.get("doi", "N/A"))  # # ğŸ”—
        item["license"] = abs_data.get("license", item.get("license", "N/A"))  # # âš–ï¸
        item["journal_ref"] = abs_data.get("journal_ref", item.get("journal_ref", "N/A"))  # # ğŸ“š
        item["comments"] = abs_data.get("comments", item.get("comments", "N/A"))  # # ğŸ’¬
        item["versions"] = abs_data.get("versions", item.get("versions", []))  # # ğŸ—“ï¸
        item["last_updated_raw"] = abs_data.get("last_updated_raw", item.get("last_updated_raw", "N/A"))  # # ğŸ—“ï¸
        item["subjects_raw"] = abs_data.get("subjects_raw", item.get("subjects_raw", "N/A"))  # # ğŸ·ï¸

        item["fill_sources"].append("abs")  # # ğŸ§ 
    except Exception as e:  # # âŒ
        item["errors"].append(f"abs_fetch_failed: {type(e).__name__}")  # # ğŸ§¾

    sleep_polite(min_s=polite_min_s, max_s=polite_max_s)  # # ğŸ˜‡

    # =========================
    # 2) GET /html (B)  # # ğŸŒ
    # =========================
    html_ok = False  # # ğŸ§ª
    if prefer_html_then_pdf:  # # âœ…
        try:  # # ğŸ§¯
            html_page, final_html = http_get_text(session=session, url=item["html_url"])  # # ğŸŒ GET
            html_ok = True  # # âœ…
            item["html_url_final"] = final_html  # # ğŸ”
            if SAVE_RAW_HTML and SAVE_HTML_PAGES:  # # ğŸ’¾
                item["html_page_saved"] = save_text_file(raw_dir, f"arxiv_html_{ts}_{arxiv_id}.html", html_page)  # # ğŸ’¾
            html_data = parse_html_page(html_page)  # # ğŸ” SELECT
            if is_empty(item.get("content_html")):  # # âœ…
                item["content_html"] = html_data.get("content_html", "N/A")  # # ğŸ§¾
            # ğŸ“š RÃ©fÃ©rences depuis HTML  # # ğŸ“š
            refs_html = html_data.get("references_from_html", [])  # # ğŸ“š
            if is_empty(item.get("references")) and refs_html:  # # âœ…
                item["references"] = refs_html  # # ğŸ“š
            item["fill_sources"].append("html")  # # ğŸ§ 
        except requests.exceptions.HTTPError as e:  # # âŒ (ex: 404)
            item["errors"].append(f"html_fetch_failed: HTTPError")  # # ğŸ§¾
            item["html_unavailable"] = True  # # âœ…
        except Exception as e:  # # âŒ
            item["errors"].append(f"html_fetch_failed: {type(e).__name__}")  # # ğŸ§¾
            item["html_unavailable"] = True  # # âœ…

        sleep_polite(min_s=polite_min_s, max_s=polite_max_s)  # # ğŸ˜‡

    # =========================
    # 3) Fallback PDF (A) si champs manquants  # # ğŸ“„
    # =========================
    # âœ… On ne tÃ©lÃ©charge PAS le PDF sur disque : parsing en mÃ©moire uniquement  # # âœ…
    missing_before_pdf = compute_missing_fields(item, REQUIRED_FIELDS)  # # ğŸ§¾
    need_pdf = len(missing_before_pdf) > 0  # # âœ…
    if need_pdf:  # # âœ…
        try:  # # ğŸ§¯
            pdf_bytes, final_pdf, content_type = http_get_bytes(session=session, url=item["pdf_url"])  # # ğŸ“„ GET bytes
            item["pdf_url_final"] = final_pdf  # # ğŸ”
            item["pdf_content_type"] = content_type  # # ğŸ§¾
            # 3.1 pypdf  # # ğŸ“„
            text_pypdf = parse_pdf_with_pypdf(pdf_bytes)  # # ğŸ“„
            if text_pypdf:  # # âœ…
                if is_empty(item.get("full_text")):  # # âœ…
                    item["full_text"] = text_pypdf  # # ğŸ§¾
                if is_empty(item.get("references")):  # # âœ…
                    refs = extract_references_from_text(text_pypdf)  # # ğŸ“š
                    if refs:  # # âœ…
                        item["references"] = refs  # # ğŸ“š
                item["fill_sources"].append("pdf:pypdf")  # # ğŸ§ 

            # 3.2 pdfminer.six (uniquement si encore des champs manquants)  # # ğŸ§°
            missing_after_pypdf = compute_missing_fields(item, REQUIRED_FIELDS)  # # ğŸ§¾
            if len(missing_after_pypdf) > 0:  # # âœ…
                text_pdfminer = parse_pdf_with_pdfminer(pdf_bytes)  # # ğŸ“„
                if text_pdfminer:  # # âœ…
                    # âœ… On comble uniquement ce qui manque  # # ğŸ§ 
                    if is_empty(item.get("full_text")):  # # âœ…
                        item["full_text"] = text_pdfminer  # # ğŸ§¾
                    if is_empty(item.get("references")):  # # âœ…
                        refs2 = extract_references_from_text(text_pdfminer)  # # ğŸ“š
                        if refs2:  # # âœ…
                            item["references"] = refs2  # # ğŸ“š
                    item["fill_sources"].append("pdf:pdfminer")  # # ğŸ§ 
        except Exception as e:  # # âŒ
            item["errors"].append(f"pdf_fetch_or_parse_failed: {type(e).__name__}")  # # ğŸ§¾

        sleep_polite(min_s=polite_min_s, max_s=polite_max_s)  # # ğŸ˜‡

    # âœ… Calcul final des champs manquants + hint URL  # # ğŸ§¾
    item["missing_fields"] = compute_missing_fields(item, REQUIRED_FIELDS)  # # ğŸ§¾
    add_url_hint_for_missing(item, item["missing_fields"])  # # ğŸ§­

    return item  # # ğŸ“¤


# ============================================================
# ğŸš€ I) Fonction principale scrape_arxiv (multi-pages + enrich)
# ============================================================

def scrape_arxiv(  # # ğŸš€ Fonction principale
    query: str,  # # ğŸ” RequÃªte utilisateur
    max_results: int = 20,  # # ğŸ¯ Limite
    sort: str = "relevance",  # # ğŸ§­ relevance | submitted_date | last_updated_date
    archive: str = "cs",  # # ğŸ§­ cs | all
    polite_min_s: float = 1.5,  # # ğŸ˜‡
    polite_max_s: float = 2.0,  # # ğŸ˜‡
    data_lake_raw_dir: str = DEFAULT_RAW_DIR,  # # ğŸ’¾ JSON output
) -> Dict[str, Any]:  # # ğŸ§¾ JSON

    max_results = int(max_results)  # # ğŸ”¢
    if max_results < 1:  # # ğŸš«
        max_results = 1  # # âœ…
    if max_results > MAX_RESULTS_HARD_LIMIT:  # # ğŸš§
        max_results = MAX_RESULTS_HARD_LIMIT  # # âœ…

    ensure_dir(data_lake_raw_dir)  # # ğŸ“
    session = requests.Session()  # # ğŸ”Œ
    ts = now_iso_for_filename()  # # ğŸ•’

    collected: List[Dict[str, Any]] = []  # # ğŸ“¦
    raw_search_pages: List[str] = []  # # ğŸ’¾
    start = 0  # # ğŸ“„

    # 1) SEARCH pages  # # ğŸ”
    while len(collected) < max_results:  # # ğŸ”
        url = build_search_url(query=query, start=start, size=PAGE_SIZE, sort=sort, archive=archive)  # # ğŸ”—
        html, final_url = http_get_text(session=session, url=url)  # # ğŸŒ GET
        if SAVE_RAW_HTML:  # # ğŸ’¾
            p = save_text_file(data_lake_raw_dir, f"arxiv_search_{ts}_start_{start}.html", html)  # # ğŸ’¾
            raw_search_pages.append(p)  # # ğŸ“Œ
        page_items = parse_search_page(html)  # # ğŸ” SELECT
        if not page_items:  # # ğŸ›‘
            break  # # âœ…
        collected.extend(page_items)  # # â•
        start += PAGE_SIZE  # # â¡ï¸
        sleep_polite(min_s=polite_min_s, max_s=polite_max_s)  # # ğŸ˜‡
        if start > 2000:  # # ğŸ›¡ï¸
            break  # # âœ…

    collected = collected[:max_results]  # # âœ‚ï¸

    # 2) Enrich each item  # # ğŸ§ 
    enriched: List[Dict[str, Any]] = []  # # ğŸ“¦
    for it in collected:  # # ğŸ”
        enriched.append(enrich_item(session=session, item=it, polite_min_s=polite_min_s, polite_max_s=polite_max_s, raw_dir=data_lake_raw_dir, ts=ts, prefer_html_then_pdf=True))  # # ğŸ§ 

    # 3) Tri local si last_updated_date  # # ğŸ§­
    sort_norm = (sort or "").strip().lower()  # # ğŸ§¹
    if sort_norm in ("last_updated_date", "updated", "last_updated"):  # # âœ…
        enriched.sort(key=lambda x: (x.get("last_updated_raw") or ""), reverse=True)  # # ğŸ”
    elif sort_norm in ("submitted_date", "submission_date", "recent", "recent_submitted"):  # # âœ…
        enriched.sort(key=lambda x: (x.get("submitted_date") or ""), reverse=True)  # # ğŸ”

    hit_limit_100 = (max_results == MAX_RESULTS_HARD_LIMIT)  # # ğŸš§
    message_if_limit = "Limite 100 atteinte (max_results)." if hit_limit_100 else ""  # # ğŸ§¾

    result: Dict[str, Any] = {  # # ğŸ§¾
        "ok": True,  # # âœ…
        "query": query,  # # ğŸ”
        "sort": sort,  # # ğŸ§­
        "archive": archive,  # # ğŸ§­
        "count": len(enriched),  # # ğŸ”¢
        "max_results": max_results,  # # ğŸ¯
        "hit_limit_100": hit_limit_100,  # # ğŸš§
        "message_if_limit": message_if_limit,  # # ğŸ§¾
        "items": enriched,  # # ğŸ“š
        "raw_html_files": raw_search_pages,  # # ğŸ’¾
        "why_html_files_exist": "On sauvegarde les HTML bruts pour debug/traÃ§abilitÃ©. Mets SAVE_RAW_HTML=False si tu ne veux pas ces fichiers.",  # # ğŸ§¾
        "required_fields_definition": "Un champ est considÃ©rÃ© vide si: None, '', 'N/A', 'null', 'None' (string). missing_fields[] liste ces champs.",  # # ğŸ§¾
    }  # # âœ…

    out_json_path = os.path.join(data_lake_raw_dir, f"arxiv_raw_{ts}.json")  # # ğŸ“
    with open(out_json_path, "w", encoding="utf-8") as f:  # # âœï¸
        json.dump(result, f, ensure_ascii=False, indent=2)  # # ğŸ§¾

    result["saved_to"] = out_json_path  # # ğŸ“Œ
    return result  # # ğŸ“¤


# ============================================================
# ğŸ§ª J) Test local (1 variable)
# ============================================================

RUN_LOCAL_TEST = True  # # âœ… True = test ON | False = test OFF

if __name__ == "__main__" and RUN_LOCAL_TEST:  # # â–¶ï¸
    print("ğŸš€ Lancement du scraping arXiv (test local)...")  # # ğŸ–¨ï¸
    results = scrape_arxiv(query="multimodal transformer", max_results=5, sort="relevance", archive="cs")  # # ğŸ•·ï¸
    print(f"âœ… OK: {results.get('count')} articles rÃ©cupÃ©rÃ©s")  # # ğŸ–¨ï¸
    print(f"ğŸ’¾ JSON sauvegardÃ©: {results.get('saved_to')}")  # # ğŸ“Œ
    items = results.get("items", [])  # # ğŸ“¦
    if items:  # # âœ…
        print("ğŸ§¾ AperÃ§u 1er article (missing_fields + hint):")  # # ğŸ–¨ï¸
        print(json.dumps({  # # ğŸ§¾
            "arxiv_id": items[0].get("arxiv_id"),  # # ğŸ†”
            "missing_fields": items[0].get("missing_fields"),  # # ğŸ§¾
            "url_hint_if_missing": items[0].get("url_hint_if_missing"),  # # ğŸ§­
            "fill_sources": items[0].get("fill_sources"),  # # ğŸ§ 
        }, indent=2, ensure_ascii=False))  # # ğŸ§¾
