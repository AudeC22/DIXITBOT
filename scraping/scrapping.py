# ============================================================
# ğŸ•·ï¸ arXiv Scraper (search/cs) -> JSON + sauvegarde data_lake/raw
# ============================================================

import os  # # ğŸ“ Gestion des chemins/dossiers
import re  # # ğŸ” Regex pour extraire les versions/dates
import json  # # ğŸ§¾ Export JSON
import time  # # â±ï¸ Politesse (sleep 1.5â€“2s)
import random  # # ğŸ² Jitter pour Ã©viter un rythme trop â€œrobotâ€
import datetime  # # ğŸ•’ Timestamp ISO pour logs + fichiers
from typing import Dict, Any, List, Optional, Tuple  # # ğŸ§© Typage pour clartÃ©
import requests  # # ğŸŒ RequÃªtes HTTP
from bs4 import BeautifulSoup  # # ğŸ§  Parsing HTML (simple et robuste)

ARXIV_BASE = "https://arxiv.org"  # # ğŸŒ Base URL arXiv
ARXIV_SEARCH_CS = "https://arxiv.org/search/cs"  # # ğŸ” Page de recherche (computer science)

# ------------------------------
# ğŸ§  Helpers
# ------------------------------

def _sleep_polite(min_s: float = 1.5, max_s: float = 2.0) -> None:  # # ğŸ˜‡ Pause polie entre requÃªtes
    time.sleep(random.uniform(min_s, max_s))  # # â³ Attendre un temps alÃ©atoire dans lâ€™intervalle

def _now_iso() -> str:  # # ğŸ•’ Timestamp ISO
    return datetime.datetime.now(datetime.timezone.utc).isoformat()  # # âœ… Heure UTC ISO 8601

def _safe_mkdir(path: str) -> None:  # # ğŸ“ CrÃ©e un dossier si nÃ©cessaire
    os.makedirs(path, exist_ok=True)  # # âœ… Ne plante pas si dÃ©jÃ  existant

def _http_get(url: str, session: requests.Session, timeout_s: int = 30) -> str:  # # ğŸŒ GET robuste
    headers = {  # # ğŸªª Headers â€œnavigateurâ€ pour Ã©viter certains refus basiques
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) arXivScraper/1.0",  # # ğŸ‘¤ UA simple
        "Accept": "text/html,application/xhtml+xml",  # # ğŸ“¥ On veut du HTML
    }  # # âœ… Fin headers
    resp = session.get(url, headers=headers, timeout=timeout_s)  # # ğŸš€ Appel HTTP
    resp.raise_for_status()  # # âŒ LÃ¨ve une erreur si 4xx/5xx
    return resp.text  # # ğŸ“„ Retourne le HTML

def _parse_search_results(html: str) -> List[Dict[str, Any]]:  # # ğŸ§© Parse la liste de rÃ©sultats
    soup = BeautifulSoup(html, "lxml")  # # ğŸ² Parse HTML via lxml
    items = []  # # ğŸ“¦ Liste finale des papers
    for li in soup.select("ol.breathe-horizontal li.arxiv-result"):  # # ğŸ“š Chaque rÃ©sultat arXiv
        title_el = li.select_one("p.title")  # # ğŸ·ï¸ Titre
        authors_el = li.select_one("p.authors")  # # ğŸ‘¥ Auteurs
        abstract_el = li.select_one("span.abstract-full")  # # ğŸ§¾ Abstract (souvent â€œfullâ€ cachÃ©)
        link_abs_el = li.select_one('p.list-title a[href^="/abs/"]')  # # ğŸ”— Lien /abs/xxxx
        link_pdf_el = li.select_one('a[href^="/pdf/"]')  # # ğŸ“„ Lien PDF

        title = (title_el.get_text(" ", strip=True) if title_el else "")  # # âœ… Texte titre
        authors_txt = (authors_el.get_text(" ", strip=True) if authors_el else "")  # # âœ… Texte auteurs
        authors = [a.strip() for a in authors_txt.replace("Authors:", "").split(",") if a.strip()]  # # ğŸ§  Split simple

        abstract = ""  # # ğŸ§¾ Abstract
        if abstract_el:  # # âœ… Si trouvÃ©
            abstract = abstract_el.get_text(" ", strip=True).replace("â–³ Less", "").strip()  # # ğŸ§¹ Nettoyage minimal

        abs_url = ""  # # ğŸ”— URL abstract
        if link_abs_el and link_abs_el.get("href"):  # # âœ… Lien existant
            abs_url = ARXIV_BASE + link_abs_el["href"]  # # ğŸŒ Absolutise

        pdf_url = ""  # # ğŸ“„ URL PDF
        if link_pdf_el and link_pdf_el.get("href"):  # # âœ… Lien existant
            pdf_url = ARXIV_BASE + link_pdf_el["href"]  # # ğŸŒ Absolutise

        arxiv_id = ""  # # ğŸ†” arXiv ID
        m = re.search(r"/abs/([^/]+)$", abs_url)  # # ğŸ” Extrait lâ€™ID depuis /abs/
        if m:  # # âœ… Match
            arxiv_id = m.group(1)  # # ğŸ†” ID

        items.append({  # # â• Ajoute un item â€œrÃ©sultatâ€
            "arxiv_id": arxiv_id,  # # ğŸ†” Identifiant
            "title": title,  # # ğŸ·ï¸ Titre
            "authors": authors,  # # ğŸ‘¥ Liste auteurs
            "abstract": abstract,  # # ğŸ§¾ RÃ©sumÃ©
            "abs_url": abs_url,  # # ğŸ”— Page abstract
            "pdf_url": pdf_url,  # # ğŸ“„ PDF direct (si besoin, on renvoie le lien)
        })  # # âœ… Fin dict item
    return items  # # ğŸ“¤ Renvoie la liste

def _parse_abs_page_for_dates(html: str) -> Tuple[Optional[str], Optional[str], Optional[int]]:  # # ğŸ—“ï¸ Soumission + update + version
    soup = BeautifulSoup(html, "lxml")  # # ğŸ² Parse
    history = soup.select_one("div.submission-history")  # # ğŸ§¾ Bloc historique versions
    if not history:  # # â“ Pas trouvÃ©
        return None, None, None  # # ğŸ§¯ On renvoie des None

    txt = history.get_text(" ", strip=True)  # # ğŸ“„ Texte complet du bloc
    # Exemple typique: "Submitted on 9 Jan 2026 (v1), last revised 10 Jan 2026 (v2)"  # # ğŸ“ Exemple
    submitted = None  # # ğŸ—“ï¸ Date soumission
    last_updated = None  # # ğŸ” Date derniÃ¨re mise Ã  jour
    last_version = None  # # ğŸ”¢ DerniÃ¨re version

    m_sub = re.search(r"Submitted\s+on\s+([0-9]{1,2}\s+\w+\s+[0-9]{4})", txt)  # # ğŸ” Soumission
    if m_sub:  # # âœ… Match
        submitted = m_sub.group(1)  # # ğŸ—“ï¸ Valeur brute

    m_rev = re.search(r"last\s+revised\s+([0-9]{1,2}\s+\w+\s+[0-9]{4})\s+\(v(\d+)\)", txt)  # # ğŸ” DerniÃ¨re rÃ©vision
    if m_rev:  # # âœ… Match
        last_updated = m_rev.group(1)  # # ğŸ” Date
        last_version = int(m_rev.group(2))  # # ğŸ”¢ Version

    if last_version is None:  # # ğŸ¤” Si pas de â€œlast revisedâ€, on rÃ©cupÃ¨re la derniÃ¨re (vX) prÃ©sente
        m_ver = re.findall(r"\(v(\d+)\)", txt)  # # ğŸ” Toutes les versions
        if m_ver:  # # âœ… Au moins une
            last_version = int(m_ver[-1])  # # ğŸ”¢ Prend la derniÃ¨re

    return submitted, last_updated, last_version  # # ğŸ“¤ Renvoie

# ------------------------------
# ğŸ•·ï¸ Scrape principal
# ------------------------------

def scrape_arxiv_cs(  # # ğŸš€ Fonction appelÃ©e par lâ€™endpoint FastAPI
    query: str,  # # ğŸ” Mots-clÃ©s utilisateur
    max_results: int = 50,  # # ğŸ¯ Limite totale (capÃ©e Ã  100)
    sort: str = "relevance",  # # ğŸ§­ relevance | submitted_date | last_updated_date
    subcategory: Optional[str] = None,  # # ğŸ§© ex: cs.LG (si fourni)
    polite_min_s: float = 1.5,  # # ğŸ˜‡ Politesse min
    polite_max_s: float = 2.0,  # # ğŸ˜‡ Politesse max
    data_lake_raw_dir: str = "data_lake/raw",  # # ğŸ“ Dossier raw
) -> Dict[str, Any]:  # # ğŸ§¾ Retour JSON final

    max_results = min(int(max_results), 100)  # # ğŸ§± Hard cap 100 comme demandÃ©
    fetched_at = _now_iso()  # # ğŸ•’ Timestamp
    _safe_mkdir(data_lake_raw_dir)  # # ğŸ“ CrÃ©e le dossier raw si besoin

    # âš™ï¸ Mapping tri -> paramÃ¨tres arXiv (le site a un tri natif)  # # ğŸ§ 
    order = ""  # # ğŸ§­ ParamÃ¨tre â€œorderâ€
    if sort == "submitted_date":  # # ğŸ—“ï¸ Plus rÃ©cents soumis
        order = "-announced_date_first"  # # âœ… Tri cÃ´tÃ© arXiv (announced date)
    elif sort == "relevance":  # # ğŸ¯ Pertinence
        order = ""  # # âœ… arXiv est souvent â€œrelevanceâ€ par dÃ©faut
    elif sort == "last_updated_date":  # # ğŸ” DerniÃ¨res mises Ã  jour
        order = ""  # # âš ï¸ arXiv ne donne pas toujours ce tri directement sur search -> on le recalculera aprÃ¨s
    else:  # # âŒ Sort inconnu
        sort = "relevance"  # # âœ… Fallback

    # ğŸ§© Construction URL base (arXiv search utilise: query, searchtype, size, start, orderâ€¦)  # # ğŸ§ 
    size = 50  # # ğŸ“„ On pagine par 50 (standard pratique)
    start = 0  # # ğŸ“Œ Offset pagination
    collected: List[Dict[str, Any]] = []  # # ğŸ“¦ Accumulateur
    hit_limit = False  # # ğŸš§ Flag â€œmax_results atteintâ€

    session = requests.Session()  # # ğŸ”Œ Session HTTP (rÃ©utilise connexions)

    while len(collected) < max_results:  # # ğŸ” Tant quâ€™on nâ€™a pas assez de rÃ©sultats
        url = (  # # ğŸ”— URL de recherche paginÃ©e
            f"{ARXIV_SEARCH_CS}"
            f"?query={requests.utils.quote(query)}"  # # ğŸ” query encodÃ©e
            f"&searchtype=all"  # # âœ… â€œallâ€ comme tu veux
            f"&abstracts=show"  # # ğŸ§¾ Afficher les abstracts
            f"&order={requests.utils.quote(order)}"  # # ğŸ§­ Tri si applicable
            f"&size={size}"  # # ğŸ“„ Taille page
            f"&start={start}"  # # ğŸ“Œ Pagination
        )  # # âœ… Fin URL

        if subcategory:  # # ğŸ§© Si une sous-catÃ©gorie est donnÃ©e
            url += f"&classification-computer_science=y&classification-physics_archives=all&classification-q_finance=all&classification-statistics=all&classification=q_biology=all&classification=q_economics=all&classification=q_eess=all&classification-mathematics=all&classification={requests.utils.quote(subcategory)}"  # # ğŸ§© Ajout filtre (simple)

        try:  # # ğŸ§¯ Gestion dâ€™erreur rÃ©seau
            html = _http_get(url, session=session)  # # ğŸŒ RÃ©cupÃ¨re HTML
        except Exception as e:  # # âŒ Si erreur HTTP/parsing
            return {  # # ğŸ“¤ Retourne une erreur structurÃ©e
                "ok": False,  # # âŒ
                "error": str(e),  # # ğŸ§¾ Message
                "query": query,  # # ğŸ” Contexte
                "sort": sort,  # # ğŸ§­ Contexte
                "max_results": max_results,  # # ğŸ¯ Contexte
                "fetched_at": fetched_at,  # # ğŸ•’ Contexte
            }  # # âœ… Fin erreur

        page_items = _parse_search_results(html)  # # ğŸ§© Parse les rÃ©sultats de la page
        if not page_items:  # # ğŸ›‘ Plus rien Ã  scraper
            break  # # âœ… Stop pagination

        # â• Ajout en respectant max_results  # # ğŸ§ 
        for it in page_items:  # # ğŸ” Chaque item
            if len(collected) >= max_results:  # # ğŸš§ Si on a atteint la limite
                hit_limit = True  # # âœ… Flag
                break  # # ğŸ›‘ Stop
            collected.append(it)  # # â• Ajoute

        start += size  # # â¡ï¸ Page suivante
        _sleep_polite(polite_min_s, polite_max_s)  # # ğŸ˜‡ Pause polie

    # ğŸ§  RÃ©cupÃ©ration â€œcontenu des pagesâ€ : on visite /abs/ pour dates/versions  # # ğŸ§ 
    for it in collected:  # # ğŸ” Pour chaque paper collectÃ©
        abs_url = it.get("abs_url", "")  # # ğŸ”— URL abstract
        if not abs_url:  # # â“ Si pas de lien
            continue  # # â­ï¸

        try:  # # ğŸ§¯ ProtÃ¨ge lâ€™appel
            html_abs = _http_get(abs_url, session=session)  # # ğŸŒ HTML page abstract
            sub_date, last_upd, last_ver = _parse_abs_page_for_dates(html_abs)  # # ğŸ—“ï¸ Extrait dates/versions
            it["submitted_date"] = sub_date  # # ğŸ—“ï¸ Ajoute champ
            it["last_updated_date"] = last_upd  # # ğŸ” Ajoute champ
            it["last_version"] = last_ver  # # ğŸ”¢ Ajoute champ
        except Exception as e:  # # âŒ Si Ã§a plante sur un article
            it["submitted_date"] = None  # # ğŸ§¯ Valeur neutre
            it["last_updated_date"] = None  # # ğŸ§¯ Valeur neutre
            it["last_version"] = None  # # ğŸ§¯ Valeur neutre
            it["warning"] = f"abs_fetch_failed: {str(e)}"  # # âš ï¸ Trace courte

        _sleep_polite(polite_min_s, polite_max_s)  # # ğŸ˜‡ Pause polie

    # ğŸ” Tri â€œlast_updated_dateâ€ si demandÃ© (re-tri local)  # # ğŸ§ 
    if sort == "last_updated_date":  # # ğŸ” Si lâ€™utilisateur veut les derniÃ¨res mises Ã  jour
        def _key(it: Dict[str, Any]) -> str:  # # ğŸ§© ClÃ© de tri simple
            return it.get("last_updated_date") or ""  # # âœ… None -> ""
        collected.sort(key=_key, reverse=True)  # # ğŸ” Tri descendant

    # ğŸ§¾ Construction rÃ©sultat final  # # ğŸ§ 
    result = {  # # ğŸ“¦ JSON final
        "ok": True,  # # âœ… SuccÃ¨s
        "query": query,  # # ğŸ” Mots-clÃ©s
        "subcategory": subcategory,  # # ğŸ§© Sous-catÃ©gorie Ã©ventuelle
        "sort": sort,  # # ğŸ§­ Tri demandÃ©
        "max_results": max_results,  # # ğŸ¯ Limite
        "count": len(collected),  # # ğŸ”¢ Nombre obtenu
        "hit_limit_100": bool(hit_limit or len(collected) >= 100),  # # ğŸš§ Indique si limite 100 touchÃ©e
        "message_if_limit": "Limite 100 atteinte (max_results)." if (hit_limit or len(collected) >= 100) else "",  # # ğŸ§¾ Message demandÃ©
        "fetched_at": fetched_at,  # # ğŸ•’ Timestamp
        "items": collected,  # # ğŸ“š RÃ©sultats
    }  # # âœ… Fin JSON

    # ğŸ’¾ Sauvegarde dans raw format JSON  # # ğŸ§ 
    safe_name = re.sub(r"[^a-zA-Z0-9_-]+", "_", query)[:60]  # # ğŸ§¹ Nom de fichier propre
    out_path = os.path.join(data_lake_raw_dir, f"arxiv_cs_{safe_name}_{int(time.time())}.json")  # # ğŸ“ Chemin
    with open(out_path, "w", encoding="utf-8") as f:  # # âœï¸ Ouvre fichier
        json.dump(result, f, ensure_ascii=False, indent=2)  # # ğŸ§¾ Ã‰crit JSON lisible

    result["saved_to"] = out_path  # # ğŸ“Œ On renvoie aussi oÃ¹ on a sauvegardÃ©
    return result  # # ğŸ“¤ Retour final
