# ============================================================  # # ğŸ“Œ DÃ©but du script
# ğŸ•·ï¸ arXiv Scraper (CS search -> /abs -> /html) -> 1 HTML bundle + 1 JSON  # # ğŸ¯ Objectif
# âœ… FIX v2: extraction /abs et /pdf ULTRA-robuste (liens relatifs OU absolus, plusieurs layouts).  # # âœ…
# ============================================================  # # ğŸ“Œ SÃ©parateur visuel

import os  # # ğŸ“ Gestion des chemins/dossiers
import re  # # ğŸ” Regex (ID, versions, watermark)
import json  # # ğŸ§¾ Export JSON
import time  # # â±ï¸ Politesse (sleep)
import random  # # ğŸ² Jitter (Ã©viter rythme robot)
import datetime  # # ğŸ•’ Timestamp fichiers
from typing import Dict, Any, List, Tuple  # # ğŸ§© Typage

import requests  # # ğŸŒ HTTP (GET)
from bs4 import BeautifulSoup  # # ğŸ² Parsing HTML (select)

ARXIV_BASE = "https://arxiv.org"  # # ğŸŒ Domaine arXiv
ARXIV_SEARCH_CS = "https://arxiv.org/search/cs"  # # ğŸ” Recherche Computer Science
DEFAULT_RAW_DIR = os.path.join("data_lake", "raw")  # # ğŸ“¦ Stockage raw (HTML bundle + JSON)
MAX_RESULTS_HARD_LIMIT = 100  # # ğŸš§ Max global demandÃ©
PAGE_SIZE = 50  # # ğŸ“„ Pagination arXiv (50)

# ============================================================  # # ğŸ“Œ SÃ©parateur
# âœ… A) Helpers (dossiers, timestamps, â€œvideâ€, politesse, GET)
# ============================================================  # # ğŸ“Œ SÃ©parateur

def ensure_dir(path: str) -> None:  # # ğŸ“ CrÃ©er dossier si besoin
    os.makedirs(path, exist_ok=True)  # # âœ…

def now_iso_for_filename() -> str:  # # ğŸ•’ Timestamp pour nom fichier
    return datetime.datetime.now().strftime("%Y%m%d_%H%M%S")  # # ğŸ§¾ Exemple: 20260113_154500

def is_empty(value: Any) -> bool:  # # ğŸ§ª DÃ©finition du â€œvideâ€
    if value is None:  # # âœ… None
        return True  # # âœ…
    if isinstance(value, str):  # # ğŸ§¾ Si string
        v = value.strip()  # # ğŸ§¹
        if v == "":  # # âœ… vide si ""
            return True  # # âœ…
        if v.lower() in {"n/a", "null", "none"}:  # # âœ… vide si "N/A", "null", "None" (string)
            return True  # # âœ…
    if isinstance(value, list):  # # ğŸ“¦ Si liste
        return len(value) == 0  # # âœ… vide si liste vide
    return False  # # âŒ sinon non vide

def sleep_polite(min_s: float = 1.5, max_s: float = 2.0) -> None:  # # ğŸ˜‡ Pause polie
    time.sleep(random.uniform(min_s, max_s))  # # â³

def http_get_text(session: requests.Session, url: str, timeout_s: int = 30) -> Tuple[str, int]:  # # ğŸŒ GET HTML (texte)
    headers = {"User-Agent": "Mozilla/5.0 DIXITBOT-arXivScraper/Final/1.2"}  # # ğŸªª UA simple
    resp = session.get(url, headers=headers, timeout=timeout_s)  # # ğŸš€ GET
    return resp.text, resp.status_code  # # ğŸ“„ HTML + code HTTP

def save_text_file(folder: str, filename: str, content: str) -> str:  # # ğŸ’¾ Sauver un fichier texte
    ensure_dir(folder)  # # ğŸ“
    path = os.path.join(folder, filename)  # # ğŸ§©
    with open(path, "w", encoding="utf-8") as f:  # # âœï¸
        f.write(content)  # # ğŸ§¾
    return path  # # ğŸ“Œ

def normalize_to_abs_url(href: str) -> str:  # # ğŸ”— Normaliser href -> URL complÃ¨te /abs
    if not href:  # # ğŸš«
        return ""  # # âœ…
    h = href.strip()  # # ğŸ§¹
    if h.startswith("//"):  # # ğŸŒ URL sans schÃ©ma
        h = "https:" + h  # # âœ…
    if h.startswith("/"):  # # âœ… relatif
        return ARXIV_BASE + h  # # ğŸ”—
    return h  # # âœ… dÃ©jÃ  absolu

def normalize_to_pdf_url(href: str) -> str:  # # ğŸ“„ Normaliser href -> URL complÃ¨te /pdf
    return normalize_to_abs_url(href)  # # ğŸ“„ mÃªme logique

# ============================================================  # # ğŸ“Œ SÃ©parateur
# ğŸ” B) URL builder (tri compatible arXiv)
# ============================================================  # # ğŸ“Œ SÃ©parateur

def build_search_url(query: str, start: int, size: int, sort: str) -> str:  # # ğŸ”— Construire URL search/cs
    q = requests.utils.quote(query)  # # ğŸ” Encoder requÃªte
    base = f"{ARXIV_SEARCH_CS}?query={q}&searchtype=all&abstracts=show&size={size}&start={start}"  # # ğŸ”— Base
    s = (sort or "relevance").strip().lower()  # # ğŸ§ 
    if s in {"submitted_date", "submitted", "recent"}:  # # ğŸ—“ï¸
        return base + "&order=-announced_date_first"  # # âœ…
    return base  # # âœ… relevance

def abs_url(arxiv_id: str) -> str:  # # ğŸ”— URL /abs
    return f"{ARXIV_BASE}/abs/{arxiv_id}"  # # âœ…

def pdf_url(arxiv_id: str) -> str:  # # ğŸ“„ URL /pdf
    return f"{ARXIV_BASE}/pdf/{arxiv_id}"  # # âœ…

def html_url(arxiv_id: str, version: str = "") -> str:  # # ğŸŒ URL /html
    return f"{ARXIV_BASE}/html/{arxiv_id}{version}"  # # âœ…

# ============================================================  # # ğŸ“Œ SÃ©parateur
# ğŸ§© C) Parsing SEARCH page (liste) â€” âœ… FIX v2 ULTRA robuste
# ============================================================  # # ğŸ“Œ SÃ©parateur

def find_abs_and_pdf_hrefs(li: BeautifulSoup) -> Tuple[str, str]:  # # ğŸ” Trouver href /abs et /pdf (tous layouts)
    # âœ… 1) Essais rapides via sÃ©lecteurs connus
    candidates = []  # # ğŸ“¦
    for sel in [  # # ğŸ§  SÃ©lecteurs souvent utilisÃ©s
        'p.list-title a[href*="/abs/"]',  # # ğŸ”
        'p.list-title a[title*="Abstract"]',  # # ğŸ”
        'span.list-identifier a[href*="/abs/"]',  # # ğŸ” ancien layout
        'a[href*="/abs/"]',  # # ğŸ” fallback global
    ]:  # # âœ…
        a = li.select_one(sel)  # # ğŸ”
        if a and (a.get("href") or "").strip():  # # âœ…
            candidates.append((a.get("href") or "").strip())  # # â•

    abs_href = ""  # # ğŸ”—
    pdf_href = ""  # # ğŸ“„

    # âœ… 2) Si pas trouvÃ©, on parcourt TOUS les <a href> et on match avec regex (absolu OU relatif)
    all_hrefs = [(a.get("href") or "").strip() for a in li.select("a[href]")]  # # ğŸ”—
    all_hrefs = [h for h in all_hrefs if h]  # # ğŸ§¹
    all_hrefs = candidates + all_hrefs  # # âœ… on met les â€œbonsâ€ candidats en premier

    # âœ… Regex: accepte /abs/... OU https://arxiv.org/abs/... OU http(s)://.../abs/...
    for h in all_hrefs:  # # ğŸ”
        if not abs_href and re.search(r"/abs/[^?#/]+", h):  # # ğŸ”—
            abs_href = h  # # âœ…
        if not pdf_href and re.search(r"/pdf/[^?#/]+", h):  # # ğŸ“„
            pdf_href = h  # # âœ…
        if abs_href and pdf_href:  # # âœ…
            break  # # âœ…

    return abs_href, pdf_href  # # ğŸ“¤

def extract_arxiv_id_from_any(href: str) -> str:  # # ğŸ†” Extraire lâ€™ID depuis un href /abs ou /pdf
    if not href:  # # ğŸš«
        return ""  # # âœ…
    m = re.search(r"/abs/([^?#/]+)", href)  # # ğŸ”
    if m:  # # âœ…
        return m.group(1).strip()  # # ğŸ†”
    m2 = re.search(r"/pdf/([^?#/]+)", href)  # # ğŸ”
    if m2:  # # âœ…
        return m2.group(1).strip()  # # ğŸ†”
    return ""  # # âŒ

def parse_search_page(html: str) -> List[Dict[str, Any]]:  # # ğŸ§© HTML -> items
    soup = BeautifulSoup(html, "lxml")  # # ğŸ²
    items: List[Dict[str, Any]] = []  # # ğŸ“¦

    for li in soup.select("ol.breathe-horizontal li.arxiv-result"):  # # ğŸ“š
        title_el = li.select_one("p.title")  # # ğŸ·ï¸
        authors_el = li.select_one("p.authors")  # # ğŸ‘¥
        abstract_el = li.select_one("span.abstract-full")  # # ğŸ§¾
        submitted_el = li.select_one("p.is-size-7")  # # ğŸ—“ï¸

        abs_href, pdf_href = find_abs_and_pdf_hrefs(li)  # # âœ… FIX v2
        arxiv_id = extract_arxiv_id_from_any(abs_href or pdf_href)  # # ğŸ†”

        title = title_el.get_text(" ", strip=True) if title_el else ""  # # ğŸ·ï¸
        authors_txt = authors_el.get_text(" ", strip=True) if authors_el else ""  # # ğŸ‘¥
        authors = [a.strip() for a in authors_txt.replace("Authors:", "").split(",") if a.strip()]  # # ğŸ‘¥
        abstract = abstract_el.get_text(" ", strip=True) if abstract_el else ""  # # ğŸ§¾
        abstract = abstract.replace("â–³ Less", "").strip()  # # ğŸ§¹

        submitted_date = ""  # # ğŸ—“ï¸
        if submitted_el:  # # âœ…
            txt = submitted_el.get_text(" ", strip=True)  # # ğŸ§¾
            m3 = re.search(r"Submitted\s+(.+?)(?:;|$)", txt, flags=re.IGNORECASE)  # # ğŸ”
            if m3:  # # âœ…
                submitted_date = m3.group(1).strip()  # # ğŸ—“ï¸

        abs_full = normalize_to_abs_url(abs_href)  # # ğŸ”—
        pdf_full = normalize_to_pdf_url(pdf_href)  # # ğŸ“„

        # âœ… Si on a lâ€™ID, on GARANTIT les URLs (mÃªme si arXiv nâ€™a pas mis les liens)
        if arxiv_id and is_empty(abs_full):  # # âœ…
            abs_full = abs_url(arxiv_id)  # # ğŸ”—
        if arxiv_id and is_empty(pdf_full):  # # âœ…
            pdf_full = pdf_url(arxiv_id)  # # ğŸ“„

        items.append({  # # ğŸ“¦
            "arxiv_id": arxiv_id,  # # ğŸ†”
            "title": title,  # # ğŸ·ï¸
            "authors": authors,  # # ğŸ‘¥
            "abstract": abstract,  # # ğŸ§¾
            "submitted_date": submitted_date,  # # ğŸ—“ï¸
            "abs_url": abs_full,  # # ğŸ”—
            "pdf_url": pdf_full,  # # ğŸ“„
        })  # # âœ…

    return items  # # ğŸ“¤

# ============================================================  # # ğŸ“Œ SÃ©parateur
# ğŸ“Œ D) Parsing /abs (versions + doi + lien HTML experimental)
# ============================================================  # # ğŸ“Œ SÃ©parateur

def parse_abs_page(html: str) -> Dict[str, Any]:  # # ğŸ§© /abs -> dict
    soup = BeautifulSoup(html, "lxml")  # # ğŸ²
    out: Dict[str, Any] = {"doi": "", "versions": [], "last_updated_raw": "", "html_experimental_url": ""}  # # ğŸ“¦

    doi_a = soup.select_one('td.tablecell.doi a[href*="doi.org"]')  # # ğŸ”
    if doi_a:  # # âœ…
        out["doi"] = doi_a.get_text(" ", strip=True)  # # ğŸ§¾

    html_a = soup.select_one('div.full-text a[href*="/html/"]')  # # ğŸ”
    if html_a:  # # âœ…
        href = (html_a.get("href") or "").strip()  # # ğŸ§¾
        out["html_experimental_url"] = (ARXIV_BASE + href) if href.startswith("/") else href  # # âœ…

    versions: List[Dict[str, str]] = []  # # ğŸ“¦
    for li in soup.select("div.submission-history li"):  # # ğŸ”
        txt = li.get_text(" ", strip=True)  # # ğŸ§¾
        m = re.search(r"\[(v\d+)\]\s*(.*)$", txt)  # # ğŸ”
        if m:  # # âœ…
            versions.append({"version": m.group(1), "raw": m.group(2).strip()})  # # ğŸ“¦
    out["versions"] = versions  # # ğŸ”
    out["last_updated_raw"] = versions[-1]["raw"] if versions else ""  # # ğŸ—“ï¸

    return out  # # ğŸ“¤

# ============================================================  # # ğŸ“Œ SÃ©parateur
# ğŸŒ E) Parsing /html (watermark date + contenu + rÃ©fÃ©rences)
# ============================================================  # # ğŸ“Œ SÃ©parateur

def parse_html_page(html: str) -> Dict[str, Any]:  # # ğŸ§© /html -> dict
    soup = BeautifulSoup(html, "lxml")  # # ğŸ²
    out: Dict[str, Any] = {"published_date": "", "content_text": "", "references": [], "references_dois": []}  # # ğŸ“¦

    wm = soup.select_one("#watermark-tr")  # # ğŸ”
    if wm:  # # âœ…
        wm_text = wm.get_text(" ", strip=True)  # # ğŸ§¾
        m = re.search(r"\]\s*([0-9]{1,2}\s+\w+\s+[0-9]{4})", wm_text)  # # ğŸ”
        if m:  # # âœ…
            out["published_date"] = m.group(1).strip()  # # ğŸ—“ï¸

    doc = soup.select_one("article.ltx_document")  # # ğŸ”
    if doc:  # # âœ…
        out["content_text"] = doc.get_text("\n", strip=True)  # # ğŸ§¾
    else:  # # ğŸ›Ÿ
        main = soup.select_one("main") or soup.select_one("body")  # # ğŸ§¾
        out["content_text"] = main.get_text("\n", strip=True) if main else ""  # # ğŸ§¾

    bib_container = soup.select_one(".ltx_bibliography")  # # ğŸ”
    if bib_container:  # # âœ…
        references: List[Dict[str, Any]] = []  # # ğŸ“¦
        doi_list: List[str] = []  # # ğŸ”—
        for bi in bib_container.select(".ltx_bibitem, li, div"):  # # ğŸ”
            txt = bi.get_text(" ", strip=True)  # # ğŸ§¾
            if not txt:  # # ğŸš«
                continue  # # âœ…
            links = [a.get("href", "").strip() for a in bi.select("a[href]")]  # # ğŸ”—
            links = [l for l in links if l]  # # ğŸ§¹
            dois = [l for l in links if "doi.org/" in l]  # # ğŸ”—
            for d in dois:  # # ğŸ”
                if d not in doi_list:  # # âœ…
                    doi_list.append(d)  # # â•
            pdf_links = [l for l in links if ("/doi/pdf" in l) or l.lower().endswith(".pdf")]  # # ğŸ“„
            references.append({"raw_text": txt, "urls": links, "dois": dois, "pdf_links": pdf_links})  # # ğŸ“¦
        out["references"] = references  # # ğŸ“š
        out["references_dois"] = doi_list  # # ğŸ”—

    return out  # # ğŸ“¤

# ============================================================  # # ğŸ“Œ SÃ©parateur
# ğŸš€ F) Fonction principale (1 HTML bundle + 1 JSON)
# ============================================================  # # ğŸ“Œ SÃ©parateur

SUPPORTED_FIELDS = [  # # âœ… Champs supportÃ©s
    "arxiv_id", "title", "authors", "abstract", "submitted_date",
    "abs_url", "pdf_url",
    "doi", "versions", "last_updated_raw",
    "html_url", "published_date", "content_text",
    "references", "references_dois",
]  # # âœ…

def compute_missing_fields(item: Dict[str, Any]) -> List[str]:  # # ğŸš© Champs vides
    missing: List[str] = []  # # ğŸ“¦
    for f in SUPPORTED_FIELDS:  # # ğŸ”
        if is_empty(item.get(f)):  # # ğŸ§ª
            missing.append(f)  # # â•
    return missing  # # ğŸ“¤

def scrape_arxiv_cs(  # # ğŸš€
    query: str,
    max_results: int = 20,
    sort: str = "relevance",
    polite_min_s: float = 1.5,
    polite_max_s: float = 2.0,
    data_lake_raw_dir: str = DEFAULT_RAW_DIR,
) -> Dict[str, Any]:

    max_results = int(max_results)  # # ğŸ”¢
    if max_results < 1:  # # ğŸš«
        max_results = 1  # # âœ…
    if max_results > MAX_RESULTS_HARD_LIMIT:  # # ğŸš§
        max_results = MAX_RESULTS_HARD_LIMIT  # # âœ…

    ts = now_iso_for_filename()  # # ğŸ•’
    ensure_dir(data_lake_raw_dir)  # # ğŸ“
    session = requests.Session()  # # ğŸ”Œ
    bundle_parts: List[str] = []  # # ğŸ§¾ HTML bundle

    collected: List[Dict[str, Any]] = []  # # ğŸ“¦
    start = 0  # # ğŸ“„

    while len(collected) < max_results:  # # ğŸ”
        search_url = build_search_url(query=query, start=start, size=PAGE_SIZE, sort=sort)  # # ğŸ”—
        search_html, code = http_get_text(session=session, url=search_url)  # # ğŸŒ
        bundle_parts.append(f"<!-- ===== SEARCH URL: {search_url} | HTTP {code} ===== -->\n")  # # ğŸ§¾
        bundle_parts.append(search_html)  # # ğŸ§¾
        bundle_parts.append("\n<!-- ===== END SEARCH ===== -->\n")  # # ğŸ§¾
        if code != 200:  # # âŒ
            break  # # ğŸ›‘
        page_items = parse_search_page(search_html)  # # ğŸ”
        if not page_items:  # # ğŸ›‘
            break  # # âœ…
        collected.extend(page_items)  # # â•
        start += PAGE_SIZE  # # â¡ï¸
        sleep_polite(min_s=polite_min_s, max_s=polite_max_s)  # # ğŸ˜‡

    collected = collected[:max_results]  # # âœ‚ï¸

    for item in collected:  # # ğŸ”
        arxiv_id = item.get("arxiv_id", "")  # # ğŸ†”
        item["doi"] = ""  # # ğŸ”—
        item["versions"] = []  # # ğŸ”
        item["last_updated_raw"] = ""  # # ğŸ—“ï¸
        item["html_url"] = ""  # # ğŸŒ
        item["published_date"] = ""  # # ğŸ—“ï¸
        item["content_text"] = ""  # # ğŸ§¾
        item["references"] = []  # # ğŸ“š
        item["references_dois"] = []  # # ğŸ”—
        item["fallback_urls"] = []  # # ğŸ”—
        item["errors"] = []  # # ğŸ§¾

        if arxiv_id:  # # âœ…
            item["abs_url"] = item.get("abs_url") or abs_url(arxiv_id)  # # ğŸ”—
            item["pdf_url"] = item.get("pdf_url") or pdf_url(arxiv_id)  # # ğŸ“„

        # ===== /abs =====
        if item.get("abs_url"):  # # âœ…
            abs_html, abs_code = http_get_text(session=session, url=item["abs_url"])  # # ğŸŒ
            bundle_parts.append(f"<!-- ===== ABS URL: {item['abs_url']} | HTTP {abs_code} ===== -->\n")  # # ğŸ§¾
            bundle_parts.append(abs_html)  # # ğŸ§¾
            bundle_parts.append("\n<!-- ===== END ABS ===== -->\n")  # # ğŸ§¾
            if abs_code == 200:  # # âœ…
                abs_data = parse_abs_page(abs_html)  # # ğŸ”
                item["doi"] = abs_data.get("doi", "")  # # ğŸ”—
                item["versions"] = abs_data.get("versions", [])  # # ğŸ”
                item["last_updated_raw"] = abs_data.get("last_updated_raw", "")  # # ğŸ—“ï¸
                item["html_url"] = abs_data.get("html_experimental_url", "")  # # ğŸŒ
            else:  # # âŒ
                item["errors"].append(f"abs_http_{abs_code}")  # # ğŸ§¾
                item["fallback_urls"].append(item["abs_url"])  # # ğŸ”—
        else:  # # âŒ
            item["errors"].append("missing_abs_url")  # # ğŸ§¾

        sleep_polite(min_s=polite_min_s, max_s=polite_max_s)  # # ğŸ˜‡

        # ===== /html =====
        if not item.get("html_url") and arxiv_id:  # # âœ…
            item["html_url"] = html_url(arxiv_id)  # # ğŸŒ tentative simple
        if item.get("html_url"):  # # âœ…
            h_html, h_code = http_get_text(session=session, url=item["html_url"])  # # ğŸŒ
            bundle_parts.append(f"<!-- ===== HTML URL: {item['html_url']} | HTTP {h_code} ===== -->\n")  # # ğŸ§¾
            bundle_parts.append(h_html)  # # ğŸ§¾
            bundle_parts.append("\n<!-- ===== END HTML ===== -->\n")  # # ğŸ§¾
            if h_code == 200:  # # âœ…
                html_data = parse_html_page(h_html)  # # ğŸ”
                item["published_date"] = html_data.get("published_date", "")  # # ğŸ—“ï¸
                item["content_text"] = html_data.get("content_text", "")  # # ğŸ§¾
                item["references"] = html_data.get("references", [])  # # ğŸ“š
                item["references_dois"] = html_data.get("references_dois", [])  # # ğŸ”—
            else:  # # âŒ
                item["errors"].append(f"html_http_{h_code}")  # # ğŸ§¾
                item["fallback_urls"].append(item["html_url"])  # # ğŸ”—

        sleep_polite(min_s=polite_min_s, max_s=polite_max_s)  # # ğŸ˜‡

        item["missing_fields"] = compute_missing_fields(item)  # # ğŸš©
        if item["missing_fields"]:  # # âœ…
            item["url_hint_if_missing"] = (
                f"Champs manquants: {', '.join(item['missing_fields'])}. "
                f"Tu peux vÃ©rifier ici: abs={item.get('abs_url','')} | html={item.get('html_url','')} | pdf={item.get('pdf_url','')}"
            )  # # ğŸ§¾
        else:  # # âœ…
            item["url_hint_if_missing"] = ""  # # âœ…

    bundle_html = "\n".join(bundle_parts)  # # ğŸ§¾
    html_name = f"arxiv_bundle_{ts}.html"  # # ğŸ§¾
    html_path = save_text_file(data_lake_raw_dir, html_name, bundle_html)  # # ğŸ’¾

    result: Dict[str, Any] = {
        "ok": True,
        "query": query,
        "sort": sort,
        "count": len(collected),
        "max_results": max_results,
        "hit_limit_100": (max_results == MAX_RESULTS_HARD_LIMIT),
        "message_if_limit": "Limite 100 atteinte (max_results)." if (max_results == MAX_RESULTS_HARD_LIMIT) else "",
        "items": collected,
        "bundle_html_file": html_path,
        "supported_fields": SUPPORTED_FIELDS,
    }  # # âœ…

    json_name = f"arxiv_raw_{ts}.json"  # # ğŸ§¾
    json_path = os.path.join(data_lake_raw_dir, json_name)  # # ğŸ“
    with open(json_path, "w", encoding="utf-8") as f:  # # âœï¸
        json.dump(result, f, ensure_ascii=False, indent=2)  # # ğŸ§¾

    result["saved_to"] = json_path  # # ğŸ“Œ
    return result  # # ğŸ“¤

# ============================================================  # # ğŸ“Œ SÃ©parateur
# ğŸ§ª TEST LOCAL (1 ligne ON/OFF)
# ============================================================  # # ğŸ“Œ SÃ©parateur

RUN_LOCAL_TEST = True  # # âœ… True = test ON | False = test OFF

if __name__ == "__main__" and RUN_LOCAL_TEST:  # # â–¶ï¸
    print("ğŸš€ Lancement du scraping arXiv (test local)...")  # # ğŸ–¨ï¸
    results = scrape_arxiv_cs(query="multimodal transformer", max_results=5, sort="relevance")  # # ğŸ•·ï¸
    print(f"âœ… OK: {results.get('count')} articles rÃ©cupÃ©rÃ©s")  # # ğŸ–¨ï¸
    print(f"ğŸ’¾ JSON sauvegardÃ©: {results.get('saved_to')}")  # # ğŸ–¨ï¸
    print(f"ğŸ’¾ HTML bundle sauvegardÃ©: {results.get('bundle_html_file')}")  # # ğŸ–¨ï¸
