{
  "ok": true,
  "query": "multimodal transformer",
  "sort": "relevance",
  "count": 5,
  "max_results": 5,
  "hit_limit_100": false,
  "message_if_limit": "",
  "items": [
    {
      "arxiv_id": "2601.07581",
      "title": "BenchSeg: A Large-Scale Dataset and Benchmark for Multi-View Food Video Segmentation",
      "authors": [
        "Ahmad AlMughrabi",
        "Guillermo Rivo",
        "Carlos JimÃ©nez-FarfÃ¡n",
        "Umair Haroon",
        "Farid Al-Areqi",
        "Hyunjun Jung",
        "Benjamin Busam",
        "Ricardo Marques",
        "Petia Radeva"
      ],
      "abstract": "Food image segmentation is a critical task for dietary analysis, enabling accurate estimation of food volume and nutrients. However, current methods suffer from limited multi-view data and poor generalization to new viewpoints. We introduce BenchSeg, a novel multi-view food video segmentation dataset and benchmark. BenchSeg aggregates 55 dish scenes (from Nutrition5k, Vegetables & Fruits, MetaFood3D, and FoodKit) with 25,284 meticulously annotated frames, capturing each dish under free 360Â° camera motion. We evaluate a diverse set of 20 state-of-the-art segmentation models (e.g., SAM-based, transformer , CNN, and large multimodal ) on the existing FoodSeg103 dataset and evaluate them (alone and combined with video-memory modules) on BenchSeg. Quantitative and qualitative results demonstrate that while standard image segmenters degrade sharply under novel viewpoints, memory-augmented methods maintain temporal consistency across frames. Our best model based on a combination of SeTR-MLA+XMem2 outperforms prior work (e.g., improving over FoodMem by ~2.63% mAP), offering new insights into food segmentation and tracking for dietary analysis. We release BenchSeg to foster future research. The project page including the dataset annotations and the food segmentation models can be found at https://amughrabi.github.io/benchseg.",
      "submitted_date": "12 January, 2026",
      "abs_url": "https://arxiv.org/abs/2601.07581",
      "pdf_url": "https://arxiv.org/pdf/2601.07581",
      "doi": "",
      "versions": [],
      "last_updated_raw": "",
      "html_url": "https://arxiv.org/html/2601.07581",
      "published_date": "",
      "content_text": "",
      "references": [],
      "references_dois": [],
      "fallback_urls": [
        "https://arxiv.org/html/2601.07581"
      ],
      "errors": [
        "html_http_404"
      ],
      "missing_fields": [
        "doi",
        "versions",
        "last_updated_raw",
        "published_date",
        "content_text",
        "references",
        "references_dois"
      ],
      "url_hint_if_missing": "Champs manquants: doi, versions, last_updated_raw, published_date, content_text, references, references_dois. Tu peux vÃ©rifier ici: abs=https://arxiv.org/abs/2601.07581 | html=https://arxiv.org/html/2601.07581 | pdf=https://arxiv.org/pdf/2601.07581"
    },
    {
      "arxiv_id": "2601.07516",
      "title": "Controlling Multimodal Conversational Agents with Coverage-Enhanced Latent Actions",
      "authors": [
        "Yongqi Li",
        "Hao Lang",
        "Tieyun Qian",
        "Yongbin Li"
      ],
      "abstract": "Vision-language models are increasingly employed as multimodal conversational agents (MCAs) for diverse conversational tasks. Recently, reinforcement learning (RL) has been widely explored for adapting MCAs to various human-AI interaction scenarios. Despite showing great enhancement in generalization performance, fine-tuning MCAs via RL still faces challenges in handling the extremely large text token space. To address this, we learn a compact latent action space for RL fine-tuning instead. Specifically, we adopt the learning from observation mechanism to construct the codebook for the latent action space, where future observations are leveraged to estimate current latent actions that could further be used to reconstruct future observations. However, the scarcity of paired image-text data hinders learning a codebook with sufficient coverage. Thus, we leverage both paired image-text data and text-only data to construct the latent action space, using a cross-modal projector for transforming text embeddings into image-text embeddings. We initialize the cross-modal projector on paired image-text data, and further train it on massive text-only data with a novel cycle consistency loss to enhance its robustness. We show that our latent action based method outperforms competitive baselines on two conversation tasks across various RL algorithms.",
      "submitted_date": "12 January, 2026",
      "abs_url": "https://arxiv.org/abs/2601.07516",
      "pdf_url": "https://arxiv.org/pdf/2601.07516",
      "doi": "",
      "versions": [],
      "last_updated_raw": "",
      "html_url": "https://arxiv.org/html/2601.07516v1",
      "published_date": "",
      "content_text": "Controlling Multimodal Conversational Agents\nwith Coverage-Enhanced Latent Actions\nYongqi Li\n1,2,âˆ—,â€ \n,\nHao Lang\n2,â€ \n,\nTieyun Qian\n1,3,â€¡\n,\nYongbin Li\n2,â€¡\n1\nSchool of Computer Science, Wuhan University,\n2\nTongyi Lab\n3\nZhongguancun Academy\n{liyongqi,qty}@whu.edu.cn\n,\n{hao.lang,shuide.lyb}@alibaba-inc.com\nAbstract\nVision-language models are increasingly employed as multimodal conversational agents (MCAs) for diverse conversational tasks.\nRecently, reinforcement learning (RL) has been widely explored for adapting MCAs to various human-AI interaction scenarios. Despite showing great enhancement in generalization performance, fine-tuning MCAs via RL still faces challenges in handling the extremely large text token space.\nTo address this, we learn a compact latent action space for RL fine-tuning instead.\nSpecifically, we adopt the learning from observation mechanism to construct the codebook for the latent action space, where future observations are leveraged to estimate current latent actions that could further be used to reconstruct future observations.\nHowever, the scarcity of paired image-text data hinders learning a codebook with sufficient coverage.\nThus, we leverage both paired image-text data and text-only data to construct the latent action space, using a cross-modal projector for transforming text embeddings into image-text embeddings.\nWe initialize the cross-modal projector on paired image-text data, and further train it on massive text-only data with a novel cycle consistency loss to enhance its robustness.\nWe show that our latent action based method outperforms competitive baselines on two conversation tasks across various RL algorithms.\nControlling Multimodal Conversational Agents\nwith Coverage-Enhanced Latent Actions\nYongqi Li\n1,2,âˆ—,â€ \n,\nHao Lang\n2,â€ \n,\nTieyun Qian\n1,3,â€¡\n,\nYongbin Li\n2,â€¡\n1\nSchool of Computer Science, Wuhan University,\n2\nTongyi Lab\n3\nZhongguancun Academy\n{liyongqi,qty}@whu.edu.cn\n,\n{hao.lang,shuide.lyb}@alibaba-inc.com\n1\n1\nfootnotetext:\nWork done while the author was interning at Tongyi Lab.\n2\n2\nfootnotetext:\nEqual contributions.\n3\n3\nfootnotetext:\nCorresponding authors.\n1\nIntroduction\nVision-language models (VLMs)\nyin-2024-MLLMsurvey\nlike Qwen-VL\nbai-2025-qwen3vl\nand GPT-4o\nhurst-2024-gpt4ocard\nare increasingly employed as multimodal conversational agents (MCAs) for various conversation tasks\nyao-2025-MLLMAgentsurvey\n.\nMCAs enable emotionally rich and contextually grounded dialogues based on understanding both input images and texts, and thus become particularly valuable in fields like entertainment\nmehta-2022-exploring\n, online education\ngriol-2014-developing\n, and personalized assistants\nnguyen-2024-yo\n.\nRecently, reinforcement learning (RL)\nsutton-1998-reinforcement\nhas been widely explored for adapting MCAs to diverse real-world human-AI interaction scenarios\nzhou-2025-reinforcedMLLM\n.\nGenerally, RL algorithms frame response token generation in MCAs as a sequential decision-making process\nchen-2021-decision\n, which optimize the policy to maximize cumulative rewards through interacting with environments.\nDespite showing great enhancement in generalization performance\nchu-2025-sftRL\n, fine-tuning MCAs via RL still faces challenges in dealing with large exploration spaces.\nFor instance, with token vocabulary size\n|\nğ’±\n|\n|\\mathcal{V}|\nand maximum response length\nm\nm\n, the sampling space for RL scales exponentially as\n|\nğ’±\n|\nm\n|\\mathcal{V}|^{m}\n.\nTo address the challenge of large text token space, we learn a compact latent action space for RL fine-tuning instead, following previous works\njia-2025-controlling\n.\nSpecifically, we adopt the learning from observation mechanism\njiang-2023-efficient\n;\nye-2025-latent\nto construct the codebook for the latent action space, where future observations are leveraged to estimate current latent actions that could be further used to reconstruct future observations.\nAs a result, the action sampling space at each step is reduced from the token vocabulary size\n|\nğ’±\n|\n|\\mathcal{V}|\n(e.g., 152K for Qwen2.5-VL\nbai-2025-qwen25vl\n) to the latent action codebook size\n|\nğ’\n|\n|\\mathcal{C}|\n(e.g., 128).\nGenerally, the codebook has to be learned from diverse data with sufficient coverage, which is a prerequisite for effective RL exploration in latent spaces\nchen-2025-coverage\n.\nNote that VLMs in MCAs are typically pre-trained on paired image-text corpora\n(\nV\n,\nT\n)\n(V,T)\n, which implicitly convey complementary and partially redundant information between visual and textual modalities\nradford-2021-clip\n.\nUnfortunately, while unpaired image collections and text corpora are abundant on the web, curating them into aligned image-text corpora remains prohibitively costly\ngupta-2025-better\n, posing a dilemma in constructing latent spaces.\nOn one hand, using limited paired data and abundant unpaired data would introduce\nunimodal bias\nzhang-2024-UniModalBias\n, where a model would overly rely on one modality and ignore others.\nOn the other hand, training the codebook solely on limited paired data may result in insufficient coverage, thereby impairing the agentâ€™s generalization ability when handling diverse unseen conversation scenarios.\nIn this paper, we leverage both paired image-text data\n(\nV\n,\nT\n)\n(V,T)\nand unpaired text-only data\nT\nT\nto learn the coodbook for the latent space.\nTo improve the coverage of latent actions while avoiding potentially unimodal bias, we attempt to construct pseudo paired data\n(\nV\nâ€²\n,\nT\n)\n(V^{\\prime},T)\nbased on text-only data\nT\nT\n, and use the pseudo data\n(\nV\nâ€²\n,\nT\n)\n(V^{\\prime},T)\nand the collected data\n(\nV\n,\nT\n)\n(V,T)\nto learn the cookbook.\nHowever, training a conditional image generator\nG\nâ€‹\n(\nV\n|\nT\n)\nG(V|T)\nfor this purpose is computationally expensive due to the high dimension nature of images\npope-2021-dimension\n.\nThus, we learn a cross-modal projector\nP\nP\ninstead, which transforms an input text\ne\nT\ne^{T}\nto an image-text pair\ne\nV\n,\nT\ne^{V,T}\nin the embedding space, based on the cross-modal redundancy assumption\nradford-2021-clip\n.\nConcretely, for each item in the paired image-text data\n(\nV\n,\nT\n)\n(V,T)\n, we compute the text embedding\ne\nT\ne^{T}\nand image-text embedding\ne\nV\n,\nT\ne^{V,T}\nusing an existing encoder, and train the projector\nP\nP\nto imitate the projection between these two kinds of embeddings.\nTo enhance the robustness of the projector\nP\nP\n, we further train it on massive text-only data\nT\nT\nusing a cycle consistency loss\nzhu-2017-unpaired\n.\nWe introduce an additional projector\nP\nâ€²\nP^{\\prime}\nthat can transform image-text embedding\ne\nV\n,\nT\ne^{V,T}\nback to text embedding\ne\nT\ne^{T}\n.\nIn this way, we can optimize the projector\nP\nP\nby enforcing cycle consistency on text-only data\nT\nT\nsuch that\nP\nâ€²\nâ€‹\n(\nP\nâ€‹\n(\ne\nT\n)\n)\nâ‰ˆ\ne\nT\nP^{\\prime}(P(e^{T}))\\approx e^{T}\n.\nWe evaluate our method on two conversation tasks, namely multimodal role-playing conversation\ndai-2025-mmrole\nand multimodal personalized conversation\nli-2025-aligning\n.\nTo evaluate the generalizability of latent actions, we conduct experiments using various RL algorithms, such as GRPO\nshao-2024-GRPO\nand Dr.GRPO\nliu-2025-DRGRPO\n.\nWe construct the latent action space using paired image-text data\n(\nV\n,\nT\n)\n(V,T)\nand text-only data\nT\nT\n.\nThe\n(\nV\n,\nT\n)\n(V,T)\ndata are comprised of image-caption pairs, multimodal news articles, and multimodal Wikipedia pages, totaling 14M images and 1B text tokens.\nThe text-only data are mainly derived from SlimPajama\ncerebras-2023-slimpajama\n, which contains 627B text tokens.\nExperimental results show that our method outperforms competitive baselines.\nIn summary, our work makes the following three key contributions.\n1) We are the first to introduce latent actions for fine-tuning multimodal conversational agents via RL, which significantly reduces the exploration space.\n2) We construct the latent action space with both paired image-text data and text-only data, using a cross-modal projector trained with a novel cycle consistency loss.\n3) We evaluate our latent action based method on two multimodal conversation tasks and demonstrate that our method outperforms competitive baselines, and further show that the cross-modal projector is critical for improving the coverage of latent actions.\n2\nPreliminary\nReinforcement Learning for VLM Agents\nIn reinforcement learning (RL), problems are framed by a Markov Decision Process (MDP)\nâ„³\n=\nâŸ¨\nğ’®\n,\nğ’œ\n,\nğ’¯\n,\nâ„›\nâŸ©\n\\mathcal{M}=\\langle\\mathcal{S},\\mathcal{A},\\mathcal{T},\\mathcal{R}\\rangle\n.\nFor VLMs, the state at step\nt\nt\nis the contextual information\ns\nt\n=\n(\nx\nV\n,\nx\nT\n1\n:\nt\n)\nâˆˆ\nğ’®\ns_{t}=(x^{V},x^{T_{1:t}})\\in\\mathcal{S}\n, which includes the input image\nx\nV\nx^{V}\nand the current token sequence\nx\nT\n1\n:\nt\nx^{T_{1:t}}\n.\nğ’œ\n\\mathcal{A}\nis the action space containing all possible actions\na\nt\na_{t}\nat each step.\nğ’¯\n\\mathcal{T}\nis the state transition function, governing the transition from\ns\nt\ns_{t}\nto\ns\nt\n+\n1\ns_{t+1}\n, i.e.,\nP\nâ€‹\n(\ns\nt\n+\n1\nâˆ£\ns\nt\n,\na\nt\n)\nP\\big(s_{t+1}\\mid s_{t},a_{t}\\big)\n.\nThe reward function\nâ„›\nâ€‹\n(\nx\nT\np\n+\n1\n:\nm\n)\n\\mathcal{R}\\big(x^{T_{p+1:m}}\\big)\nassigns a scalar reward to the response\nx\nT\np\n+\n1\n:\nm\nx^{T_{p+1:m}}\n, conditioned on the input\n(\nx\nV\n,\nx\nT\n1\n:\np\n)\n(x^{V},x^{T_{1:p}})\n, with prompt length\np\np\nand maximum sequence length\nm\nm\n, following common practice in RL for VLMs\nshen-2025-vlmR1\n.\nLatent Actions for Reinforcement Learning\nIn traditional token-level RL, each action\na\nt\na_{t}\ncorresponds to selecting the next text token\nx\nT\nt\n+\n1\nx^{T_{t+1}}\nfrom the token vocabulary\nğ’±\n\\mathcal{V}\n, i.e.,\nğ’œ\n=\nğ’±\n\\mathcal{A}=\\mathcal{V}\n. While in latent action RL, at each step\nt\nt\n, the policy\nÏ€\nÎ¸\nâ€‹\n(\na\nt\n|\nx\nV\n,\nx\nT\n1\n:\nt\n)\n\\pi_{\\theta}(a_{t}|x^{V},x^{T_{1:t}})\nselects a latent action\na\nt\na_{t}\nfrom a compact codebook\nğ’\n\\mathcal{C}\n, i.e.,\nğ’œ\n=\nğ’\n\\mathcal{A}=\\mathcal{C}\n.\nDuring RL exploration, the latent action policy samples a latent action at each step, ultimately yielding the terminal state\ns\nm\ns_{m}\n.\nDuring exploitation, the latent action policy is refined to maximize expected rewards using RL algorithms such as GRPO\nshao-2024-GRPO\n.\nFigure 1:\nIllustrations of integrating latent actions with vision-language models.\nFigure 2:\nPipeline for constructing the latent action space. (a)\nInverse dynamics learning\n: Given future observations, the inverse dynamics model infers a discrete latent action from a learnable codebook; the language world model then uses this latent action and current observations to reconstruct the next token\nx\nT\nt\n+\n1\nx^{T_{t+1}}\n. The language world model, inverse dynamics model, and codebook are jointly trained. (b)\nPolicy behavior cloning\n: A policy model is trained to predict the same latent actions as those inferred by the inverse dynamics model, using only current observations.\n3\nMethodology\nIn this section, we first describe the overall model design for incorporating latent actions into VLMs (Sec.\n3.1\n).\nNext, we detail the unsupervised construction of the latent action space (Sec.\n3.2\n).\nFinally, we introduce the procedure of latent action based RL fine-tuning (Sec.\n3.3\n).\n3.1\nModel Design\nTo fine-tune MCAs via latent action RL, we introduce three new modules, as illustrated in Figure\n1\n. These modules are designed to share a base VLM while adding a small number of additional parameters, thereby introducing only marginal computational overhead. For further details on the model design, please refer to the Appendix\nA\n.\nLanguage World Model\nf\nworld\nf_{\\text{world}}\nThe language world model\nf\nworld\nâ€‹\n(\nx\nT\nt\n+\n1\n|\nx\nV\n,\nx\nT\n1\n:\nt\n,\na\nt\n)\nf_{\\text{world}}(x^{T_{t+1}}|x^{V},x^{T_{1:t}},a_{t})\ntakes current observations\n(\nx\nV\n,\nx\nT\n1\n:\nt\n)\n(x^{V},x^{T_{1:t}})\nand a latent action\na\nt\na_{t}\nas input, and auto-regressively outputs the next token\nx\nT\nt\n+\n1\nx^{T_{t+1}}\n.\nThe latent action\na\nt\na_{t}\nis provided by the inverse dynamics model\nf\ninverse\nf_{\\text{inverse}}\nduring constructing the latent action space, and by the policy\nÏ€\nÎ¸\n\\pi_{\\theta}\nduring inference and RL phases.\nInverse Dynamics Model\nf\ninverse\nf_{\\text{inverse}}\nThe inverse dynamics model\nf\ninverse\nâ€‹\n(\na\nt\n|\nx\nV\n,\nx\nT\n1\n:\nt\n+\n1\n)\nf_{\\text{inverse}}(a_{t}|x^{V},x^{T_{1:t+1}})\ntakes future observations\n(\nx\nV\n,\nx\nT\n1\n:\nt\n+\n1\n)\n(x^{V},x^{T_{1:t+1}})\nas input, and outputs a discrete latent action index\na\nt\nâˆˆ\n{\n1\n,\nâ€¦\n,\n|\nğ’\n|\n}\na_{t}\\in\\{1,\\dots,|\\mathcal{C}|\\}\nfor the current step.\nThe corresponding latent action embedding\nc\na\nt\n=\nğ’\nâ€‹\n[\na\nt\n]\nâˆˆ\nâ„\nd\nc_{a_{t}}=\\mathcal{C}[a_{t}]\\in\\mathbb{R}^{d}\nis then retrieved from the trainable codebook\nğ’\nâˆˆ\nâ„\n|\nğ’\n|\nÃ—\nd\n\\mathcal{C}\\in\\mathbb{R}^{|\\mathcal{C}|\\times d}\nand used by\nf\nworld\nf_{\\text{world}}\nto reconstruct the next token\nx\nT\nt\n+\n1\nx^{T_{t+1}}\n.\nNote that\nf\ninverse\nf_{\\text{inverse}}\nonly assists training and does not serve for the inference phase.\nPolicy Model\nÏ€\nÎ¸\n\\pi_{\\theta}\nThe latent action policy model\nÏ€\nÎ¸\nâ€‹\n(\na\nt\n|\nx\nV\n,\nx\nT\n1\n:\nt\n)\n\\pi_{\\theta}(a_{t}|x^{V},x^{T_{1:t}})\ntakes the current observations\n(\nx\nV\n,\nx\nT\n1\n:\nt\n)\n(x^{V},x^{T_{1:t}})\nas input, and predicts latent action\na\nt\na_{t}\nfor the current step.\nSince the language world model\nf\nworld\nf_{\\text{world}}\nis controlled by latent actions, we can optimize the latent action distribution of\nÏ€\nÎ¸\n\\pi_{\\theta}\nfor steering\nf\nworld\nf_{\\text{world}}\nto generate responses toward higher rewards.\n3.2\nLatent Action Space Learning\nFollowing\njia-2025-controlling\n, we construct the latent action space using large-scale corpora in two steps. 1)\ninverse dynamics learning\n, which trains the\nf\nworld\nf_{\\text{world}}\n,\nf\ninverse\nf_{\\text{inverse}}\n, and\nğ’\n\\mathcal{C}\nin an unsupervised manner (Fig.\n2\n(a)); 2)\npolicy behavior cloning\n, which trains the policy model\nÏ€\nÎ¸\n\\pi_{\\theta}\nto mimic the latent action\na\nt\na_{t}\ninferred by\nf\ninverse\nf_{\\text{inverse}}\n(Fig.\n2\n(b)).\n3.2.1\nInverse Dynamics Learning\nWe first outline the overall objective of inverse dynamics learning, followed by the training procedure of the introduced cross-modal projector.\nOverview\nAs shown in Fig.\n2\n(a), we jointly train the inverse dynamics model\nf\ninverse\nf_{\\text{inverse}}\n, language world model\nf\nworld\nf_{\\text{world}}\n, and the latent action codebook\nğ’\n\\mathcal{C}\n, using the mixed corpus\nğ’Ÿ\nV\nâ€‹\nT\nâˆª\nğ’Ÿ\nT\n\\mathcal{D}^{VT}\\cup\\mathcal{D}^{T}\n(paired image-text data and text-only data).\nThe loss is as:\nâ„’\ninverse\n=\nğ”¼\nğ’Ÿ\nV\nâ€‹\nT\nâˆª\nğ’Ÿ\nT\nâ€‹\n[\nâˆ’\nâˆ‘\nt\n=\n1\nm\nâˆ’\n1\nlog\nâ¡\nf\nworld\nâ€‹\n(\nx\nT\nt\n+\n1\n|\ne\nt\nV\n,\nT\n,\na\nt\n)\n]\n,\n\\mathcal{L}_{\\text{inverse}}=\\mathbb{E}_{\\mathcal{D}^{VT}\\cup\\mathcal{D}^{T}}\\left[-\\sum_{t=1}^{m-1}\\log f_{\\text{world}}\\big(x^{T_{t+1}}|e^{V,T}_{t},a_{t}\\big)\\right],\n(1)\nwhere the expectation is taken over sequences\n(\nx\nV\n,\nx\nT\n1\n:\nm\n)\n(x^{V},x^{T_{1:m}})\nsampled from the mixed corpus\nğ’Ÿ\nV\nâ€‹\nT\nâˆª\nğ’Ÿ\nT\n\\mathcal{D}^{VT}\\cup\\mathcal{D}^{T}\n, with\na\nt\n=\nf\ninverse\nâ€‹\n(\ne\nt\n+\n1\nV\n,\nT\n)\nâˆˆ\n{\n1\n,\nâ€¦\n,\n|\nğ’\n|\n}\na_{t}=f_{\\text{inverse}}(e^{V,T}_{t+1})\\in\\{1,...,|\\mathcal{C}|\\}\n.\nThe embedding\ne\nt\nV\n,\nT\ne^{V,T}_{t}\nis obtained via:\ne\nt\nV\n,\nT\n=\n{\nf\nVLM\nâ€‹\n(\nx\nV\n,\nx\nT\n1\n:\nt\n)\n,\nif\nâ€‹\nx\nV\nâ‰ \nâˆ…\n(\nfrom\nâ€‹\nğ’Ÿ\nV\nâ€‹\nT\n)\n;\nP\nâ€‹\n(\nf\nVLM\nâ€‹\n(\nx\nT\n1\n:\nt\n)\n)\n,\nif\nâ€‹\nx\nV\n=\nâˆ…\n(\nfrom\nâ€‹\nğ’Ÿ\nT\n)\n,\ne^{V,T}_{t}=\\begin{cases}f_{\\text{VLM}}(x^{V},x^{T_{1:t}}),&\\text{if }x^{V}\\neq\\emptyset\\quad(\\text{from }\\mathcal{D}^{VT});\\\\\nP\\big(f_{\\text{VLM}}(x^{T_{1:t}})\\big),&\\text{if }x^{V}=\\emptyset\\quad(\\text{from }\\mathcal{D}^{T}),\\end{cases}\n(2)\nwhere\nf\nVLM\nf_{\\text{VLM}}\ndenotes the encoding module based on VLMs.\nP\nP\ndenotes the cross-modal projector for transforming text embeddings into image-text embeddings, and its training procedure is as follows.\nCross-modal Projector Training\nLet\nP\nP\ndenote the forward cross-modal projector, which maps text embeddings\ne\nt\nT\ne^{T}_{t}\nto the parameters of a diagonal Gaussian distribution over the image-text embedding space, i.e.,\n(\nÎ¼\nt\n,\nÏƒ\nt\n)\n=\nP\nâ€‹\n(\ne\nt\nT\n)\n(\\mu_{t},\\sigma_{t})=P(e^{T}_{t})\n.\nLet\nP\nâ€²\nP^{\\prime}\ndenote the reverse projector, which maps image-text embeddings back to the text embedding space.\nWe train\nP\nP\nand\nP\nâ€²\nP^{\\prime}\nin the following two steps.\nStep 1: Initialization on paired image-text data.\nWe first train the forward projector\nP\nP\non paired image-text data\nğ’Ÿ\nV\nâ€‹\nT\n\\mathcal{D}^{VT}\n, where the loss is defined as:\nâ„’\nt2vt\n=\nğ”¼\nğ’Ÿ\nV\nâ€‹\nT\nâ€‹\n[\nâˆ‘\nt\n=\n1\nm\nâˆ’\n1\n1\n2\nâ€‹\n(\nâ€–\ne\nt\nV\n,\nT\nâˆ’\nÎ¼\nt\nÏƒ\nt\nâ€–\n2\n+\nâ€–\nlog\nâ¡\nÏƒ\nt\n2\nâ€–\n1\n)\n]\n,\n\\mathcal{L}_{\\text{t2vt}}=\\mathbb{E}_{\\mathcal{D}^{VT}}\\left[\\sum_{t=1}^{m-1}\\frac{1}{2}\\left(\\left\\|\\frac{e^{V,T}_{t}-\\mu_{t}}{\\sigma_{t}}\\right\\|^{2}+\\|\\log\\sigma_{t}^{2}\\|_{1}\\right)\\right],\n(3)\nwhere the expectation is taken over sequences\n(\nx\nV\n,\nx\nT\n1\n:\nm\n)\nâˆ¼\nğ’Ÿ\nV\nâ€‹\nT\n(x^{V},x^{T_{1:m}})\\sim\\mathcal{D}^{VT}\n,\nand\ne\nt\nV\n,\nT\n=\nf\nVLM\nâ€‹\n(\nx\nV\n,\nx\nT\n1\n:\nt\n)\ne^{V,T}_{t}=f_{\\text{VLM}}(x^{V},x^{T_{1:t}})\n, and\n(\nÎ¼\nt\n,\nÏƒ\nt\n)\n=\nP\nâ€‹\n(\ne\nt\nT\n=\nf\nVLM\nâ€‹\n(\nx\nT\n1\n:\nt\n)\n)\n(\\mu_{t},\\sigma_{t})=P(e^{T}_{t}=f_{\\text{VLM}}(x^{T_{1:t}}))\n.\nSimilarly,\nP\nâ€²\nP^{\\prime}\nis trained on\nğ’Ÿ\nV\nâ€‹\nT\n\\mathcal{D}^{VT}\nusing the symmetric loss\nâ„’\nvt2t\n\\mathcal{L}_{\\text{vt2t}}\n, defined as:\nâ„’\nvt2t\n=\nğ”¼\nğ’Ÿ\nV\nâ€‹\nT\nâ€‹\n[\nâˆ‘\nt\n=\n1\nm\nâˆ’\n1\n1\n2\nâ€‹\n(\nâ€–\ne\nt\nT\nâˆ’\nÎ½\nt\nÏ„\nt\nâ€–\n2\n+\nâ€–\nlog\nâ¡\nÏ„\nt\n2\nâ€–\n1\n)\n]\n,\n\\mathcal{L}_{\\text{vt2t}}=\\mathbb{E}_{\\mathcal{D}^{VT}}\\left[\\sum_{t=1}^{m-1}\\frac{1}{2}\\left(\\left\\|\\frac{e^{T}_{t}-\\nu_{t}}{\\tau_{t}}\\right\\|^{2}+\\|\\log\\tau_{t}^{2}\\|_{1}\\right)\\right],\n(4)\nwhere the expectation is taken over sequences\n(\nx\nV\n,\nx\nT\n1\n:\nm\n)\nâˆ¼\nğ’Ÿ\nV\nâ€‹\nT\n(x^{V},x^{T_{1:m}})\\sim\\mathcal{D}^{VT}\n,\ne\nt\nT\n=\nf\nVLM\nâ€‹\n(\nx\nT\n1\n:\nt\n)\ne^{T}_{t}=f_{\\text{VLM}}(x^{T_{1:t}})\ndenotes the text embedding,\nand\n(\nÎ½\nt\n,\nÏ„\nt\n)\n=\nP\nâ€²\nâ€‹\n(\ne\nt\nV\n,\nT\n=\nf\nVLM\nâ€‹\n(\nx\nV\n,\nx\nT\n1\n:\nt\n)\n)\n(\\nu_{t},\\tau_{t})=P^{\\prime}(e^{V,T}_{t}=f_{\\text{VLM}}(x^{V},x^{T_{1:t}}))\n.\nThe total loss for\nStep 1\nis:\nâ„’\nproj\n1\n=\nâ„’\nt2vt\n+\nâ„’\nvt2t\n.\n\\mathcal{L}_{\\text{proj}_{\\text{1}}}=\\mathcal{L}_{\\text{t2vt}}+\\mathcal{L}_{\\text{vt2t}}.\n(5)\nStep 2: Jointly training on paired image-text data and text-only data\nWe now jointly train\nP\nP\nand\nP\nâ€²\nP^{\\prime}\non paired data\nğ’Ÿ\nV\nâ€‹\nT\n\\mathcal{D}^{VT}\nand text-only data\nğ’Ÿ\nT\n\\mathcal{D}^{T}\n. The total objective is:\nâ„’\nproj\n2\n=\nâ„’\nt2vt\n+\nâ„’\nvt2t\n+\nâ„’\ncycle\n\\mathcal{L}_{\\text{proj}_{\\text{2}}}=\\mathcal{L}_{\\text{t2vt}}+\\mathcal{L}_{\\text{vt2t}}+\\mathcal{L}_{\\text{cycle}}\n(6)\nwhere\nâ„’\nt2vt\n\\mathcal{L}_{\\text{t2vt}}\n(Eq.\n3\n) and\nâ„’\nvt2t\n\\mathcal{L}_{\\text{vt2t}}\n(Eq.\n4\n) are computed over\nğ’Ÿ\nV\nâ€‹\nT\n\\mathcal{D}^{VT}\n, and\nâ„’\ncycle\n\\mathcal{L}_{\\text{cycle}}\ndenotes a novel cycle consistency loss computed on text-only data\nğ’Ÿ\nT\n\\mathcal{D}^{T}\n.\nThe cycle consistency loss\nâ„’\ncycle\n\\mathcal{L}_{\\text{cycle}}\nis defined as:\nâ„’\ncycle\n=\nğ”¼\nğ’Ÿ\nT\nâ€‹\n[\nâˆ‘\nt\n=\n1\nm\nâˆ’\n1\n1\n2\nâ€‹\n(\nâ€–\ne\nt\nT\nâˆ’\nÎ½\nt\nÏ„\nt\nâ€–\n2\n+\nâ€–\nlog\nâ¡\nÏ„\nt\n2\nâ€–\n1\n)\n]\n,\n\\mathcal{L}_{\\text{cycle}}=\\mathbb{E}_{\\mathcal{D}^{T}}\\left[\\sum_{t=1}^{m-1}\\frac{1}{2}\\left(\\left\\|\\frac{e^{T}_{t}-\\nu_{t}}{\\tau_{t}}\\right\\|^{2}+\\|\\log\\tau_{t}^{2}\\|_{1}\\right)\\right],\n(7)\nwhere the expectation is taken over text-only sequences\nx\nT\n1\n:\nm\nâˆ¼\nğ’Ÿ\nT\nx^{T_{1:m}}\\sim\\mathcal{D}^{T}\n,\ne\nt\nT\n=\nf\nVLM\nâ€‹\n(\nx\nT\n1\n:\nt\n)\ne^{T}_{t}=f_{\\text{VLM}}(x^{T_{1:t}})\n, and\n(\nÎ¼\nt\n,\nÏƒ\nt\n)\n=\nP\nâ€‹\n(\ne\nt\nT\n)\n(\\mu_{t},\\sigma_{t})=P(e^{T}_{t})\n, and\n(\nÎ½\nt\n,\nÏ„\nt\n)\n=\nP\nâ€²\nâ€‹\n(\nÎ¼\nt\n)\n(\\nu_{t},\\tau_{t})=P^{\\prime}(\\mu_{t})\n.\n3.2.2\nPolicy Behavior Cloning\nDuring RL exploration and inference, future observations are unavailable, making the inverse dynamics model\nf\ninverse\nf_{\\text{inverse}}\ninapplicable.\nThus, we train a policy model\nÏ€\nÎ¸\n\\pi_{\\theta}\nvia behavior cloning to mimic latent actions inferred by\nf\ninverse\nf_{\\text{inverse}}\n(Fig.\n2\n(b)).\nSpecifically, for samples from the mixed corpus\nğ’Ÿ\nmix\n=\nğ’Ÿ\nV\nâ€‹\nT\nâˆª\nğ’Ÿ\nT\n\\mathcal{D}^{\\text{mix}}=\\mathcal{D}^{VT}\\cup\\mathcal{D}^{T}\n,\nwe compute the loss as:\nâ„’\nbc\n=\nğ”¼\nğ’Ÿ\nmix\nâ€‹\n[\nâˆ’\nâˆ‘\nt\n=\n1\nm\nâˆ’\n1\nlog\nâ¡\nÏ€\nÎ¸\nâ€‹\n(\na\nt\nâˆ—\n=\nf\ninverse\nâ€‹\n(\ne\nt\n+\n1\nV\n,\nT\n)\nâˆ£\ne\nt\nV\n,\nT\n)\n]\n,\n\\mathcal{L}_{\\text{bc}}=\\mathbb{E}_{\\mathcal{D}^{\\text{mix}}}\\left[-\\sum_{t=1}^{m-1}\\log\\pi_{\\theta}\\big(a_{t}^{\\ast}=f_{\\text{inverse}}(e^{V,T}_{t+1})\\mid e^{V,T}_{t}\\big)\\right],\n(8)\nwhere the expectation is taken over sequences\n(\nx\nV\n,\nx\nT\n1\n:\nm\n)\nâˆ¼\nğ’Ÿ\nmix\n(x^{V},x^{T_{1:m}})\\sim\\mathcal{D}^{\\text{mix}}\n,\nwith\ne\nt\nV\n,\nT\ne^{V,T}_{t}\ndefined as in Eq.\n2\n.\n3.3\nLatent Action Reinforcement Learning\nOn downstream multimodal conversational tasks, we perform reinforcement learning at the policy model level, as illustrated in Fig.\n3\n.\nFor each prompt\n(\nx\nV\n,\nx\nT\n1\n:\np\n)\nâˆ¼\nğ’Ÿ\nrl\n(x^{V},x^{T_{1:p}})\\sim\\mathcal{D}_{\\text{rl}}\nwith the prompt length\np\np\n, the policy\nÏ€\nÎ¸\n\\pi_{\\theta}\nand the world model\nf\nworld\nf_{\\text{world}}\njointly generate response\nx\nT\np\n+\n1\n:\nm\nx^{T_{p+1:m}}\nauto-regressively, i.e., at each step\nt\n=\np\n,\nâ€¦\n,\nm\nâˆ’\n1\nt=p,\\dots,m-1\n,\na\nt\nâˆ¼\nÏ€\nÎ¸\n(\nâ‹…\n|\nx\nV\n,\nx\nT\n1\n:\nt\n)\n,\nx\nT\nt\n+\n1\n=\nf\nworld\n(\nx\nV\n,\nx\nT\n1\n:\nt\n,\na\nt\n)\na_{t}\\sim\\pi_{\\theta}(\\cdot|x^{V},x^{T_{1:t}}),x^{T_{t+1}}=f_{\\text{world}}(x^{V},x^{T_{1:t}},a_{t})\n, with maximum length\nm\nm\n.\nWe optimize\nÏ€\nÎ¸\n\\pi_{\\theta}\nby maximizing the expected rewards:\nğ’¥\nâ€‹\n(\nÎ¸\n)\n=\nğ”¼\n(\nx\nV\n,\nx\nT\n1\n:\np\n)\nâˆ¼\nğ’Ÿ\nrl\nâ€‹\n[\nR\nâ€‹\n(\nx\nT\np\n+\n1\n:\nm\n)\n]\n,\n\\mathcal{J}(\\theta)=\\mathbb{E}_{(x^{V},x^{T_{1:p}})\\sim\\mathcal{D}_{\\text{rl}}}\\left[R\\big(x^{T_{p+1:m}}\\big)\\right],\n(9)\nwhere\nR\nâ€‹\n(\nâ‹…\n)\nR(\\cdot)\ndenotes the reward function.\nFigure 3:\nIllustrations of latent action RL. The language world model is frozen, while the policy model is optimized to select latent actions from the codebook that steer the generated responses toward higher rewards.\nWe summarize our framework in Algorithm\n1\n.\nAlgorithm 1\nLatent Action Space Learning and Latent Action RL\n1:\nStage 1: Latent Action Space Learning\n2:\nInitialize\nf\nworld\n,\nf\ninverse\n,\nğ’\nf_{\\text{world}},f_{\\text{inverse}},\\mathcal{C}\nby minimizing\nâ„’\ninverse\n\\mathcal{L}_{\\text{inverse}}\n(Eq.\n1\n) on\nğ’Ÿ\nV\nâ€‹\nT\n\\mathcal{D}^{VT}\n.\n3:\nInitialize the cross-modal projectors\nP\n,\nP\nâ€²\nP,P^{\\prime}\nby minimizing\nâ„’\nproj\n1\n\\mathcal{L}_{\\text{proj}_{1}}\n(Eq.\n5\n) on\nğ’Ÿ\nV\nâ€‹\nT\n\\mathcal{D}^{VT}\n.\n4:\nJointly optimize\nf\nworld\n,\nf\ninverse\n,\nğ’\n,\nP\n,\nP\nâ€²\nf_{\\text{world}},f_{\\text{inverse}},\\mathcal{C},P,P^{\\prime}\nby minimizing\nâ„’\ninverse\n\\mathcal{L}_{\\text{inverse}}\n(Eq.\n1\n) and\nâ„’\nproj\n2\n\\mathcal{L}_{\\text{proj}_{2}}\n(Eq.\n6\n) on\nğ’Ÿ\nV\nâ€‹\nT\nâˆª\nğ’Ÿ\nT\n\\mathcal{D}^{VT}\\cup\\mathcal{D}^{T}\n.\n5:\nInitialize the policy model\nÏ€\nÎ¸\n\\pi_{\\theta}\nby minimizing\nâ„’\nbc\n\\mathcal{L}_{\\text{bc}}\n(Eq.\n8\n) on\nğ’Ÿ\nV\nâ€‹\nT\nâˆª\nğ’Ÿ\nT\n\\mathcal{D}^{VT}\\cup\\mathcal{D}^{T}\n.\n6:\nStage 2: Latent Action RL\n7:\nSample\n(\nx\nV\n,\nx\nT\n1\n:\np\n)\nâˆ¼\nğ’Ÿ\nrl\n(x^{V},x^{T_{1:p}})\\sim\\mathcal{D}_{\\text{rl}}\n:\n8:\nRoll out\nx\nT\np\n+\n1\n:\nm\nx^{T_{p+1:m}}\nvia\na\nt\nâˆ¼\nÏ€\nÎ¸\n(\nâ‹…\n|\nx\nV\n,\nx\nT\n1\n:\nt\n)\na_{t}\\sim\\pi_{\\theta}(\\cdot|x^{V},x^{T_{1:t}})\n,\nx\nT\nt\n+\n1\n=\nf\nworld\nâ€‹\n(\nx\nV\n,\nx\nT\n1\n:\nt\n,\na\nt\n)\nx^{T_{t+1}}=f_{\\text{world}}(x^{V},x^{T_{1:t}},a_{t})\n,\nt\n=\np\n,\nâ€¦\n,\nm\nâˆ’\n1\nt=p,...,m-1\n.\n9:\nCompute reward\nR\nâ€‹\n(\nx\nT\np\n+\n1\n:\nm\n)\nR(x^{T_{p+1:m}})\n.\n10:\nOptimize\nÏ€\nÎ¸\n\\pi_{\\theta}\nby maximizing\nğ’¥\nâ€‹\n(\nÎ¸\n)\n\\mathcal{J}(\\theta)\n(Eq.\n9\n).\n4\nExperiments\n4.1\nExperimental Setup\nModels\nWe build the language world model, inverse dynamics model, and policy model upon the same foundation vision-language model. Specifically, we use\nQwen2.5-VL-3B-Instruct\nand\nQwen2.5-VL-7B-Instruct\nbai-2025-qwen25vl\nfor main experiments.\nThe latent action space is implemented as a codebook with size\n|\nğ’\n|\n=\n128\n|\\mathcal{C}|=128\n.\nDatasets\nDuring the latent action space construction stage (Section\n3.2\n), we use a mixture of paired image-text corpora\nğ’Ÿ\nV\nâ€‹\nT\n\\mathcal{D}^{VT}\nand text-only corpora\nğ’Ÿ\nT\n\\mathcal{D}^{T}\n.\nFor\nğ’Ÿ\nV\nâ€‹\nT\n\\mathcal{D}^{VT}\n, we collect image-caption pairs from\nConceptual-12M\nchangpinyo-2021-Conceptual\n, multimodal news articles from\nN24News\nwang-2022-N24News\n, and multimodal Wikipedia data from\nWikiWeb2M\nburns-2023-wiki\n, totaling 14 million images and 1 billion text tokens.\nFor\nğ’Ÿ\nT\n\\mathcal{D}^{T}\n, we collect text-only data mainly from the\nSlimPajama-627B\ndataset\ncerebras-2023-slimpajama\n, which contains 627 billion text tokens.\nFor latent action RL (Section\n3.3\n), we evaluate our method on two downstream tasks: 1) multimodal role-playing conversation on\nMMRole\ndai-2025-mmrole\n, where we focus on the challenging\nComment\nsubset; we train on the in-distribution (ID) split and evaluate on ID and out-of-distribution (OOD) test sets; 2) multimodal personalized conversation on\nPCogAlignBench\nli-2025-aligning\n, where we train the agent on the\nLS1\nset and evaluate on\nLS1\nand\nLS2\ntest sets.\nEvaluation Metrics\nWe adopt the\nLLM-as-a-Judge\nmetric to evaluate model performance, using prompt templates validated by\ndai-2025-mmrole\n;\nli-2025-aligning\n, which show high correlation with human judgments.\nFor each sample, the LLM judge scores both the model and ground-truth responses across benchmark-specific dimensions, with scores ranging 1-10.\nThen, following\ndai-2025-mmrole\n, we report the ratio of the modelâ€™s average score to the ground-truth responseâ€™s average score across all evaluation dimensions.\nWe report the mean and standard deviation across three evaluation runs.\nMethod\nMMRole\nPCogAlignBench\nAverage\nID\nOOD\nLS1\nLS2\nQwen2.5-VL-3B-Instruct\nPrompt\n0.728\nÂ±0.005\n0.687\nÂ±0.025\n0.678\nÂ±0.003\n0.676\nÂ±0.002\n0.692\nÂ±0.009\nSFT\n0.843\nÂ±0.002\n0.809\nÂ±0.012\n0.808\nÂ±0.009\n0.810\nÂ±0.005\n0.817\nÂ±0.007\nGRPO (Token)\n0.838\nÂ±0.017\n0.796\nÂ±0.027\n0.845\nÂ±0.007\n0.845\nÂ±0.004\n0.831\nÂ±0.014\nGRPO (Latent Action)\n0.949\nÂ±0.007\n0.915\nÂ±0.065\n0.871\nÂ±0.011\n0.837\nÂ±0.010\n0.893\nÂ±0.023\nDr.GRPO (Token)\n0.867\nÂ±0.011\n0.823\nÂ±0.002\n0.835\nÂ±0.008\n0.834\nÂ±0.012\n0.840\nÂ±0.008\nDr.GRPO (Latent Action)\n0.953\nÂ±0.016\n0.916\nÂ±0.038\n0.874\nÂ±0.009\n0.840\nÂ±0.009\n0.896\nÂ±0.018\nDAPO (Token)\n0.856\nÂ±0.003\n0.805\nÂ±0.033\n0.835\nÂ±0.008\n0.828\nÂ±0.008\n0.831\nÂ±0.013\nDAPO (Latent Action)\n0.941\nÂ±0.016\n0.889\nÂ±0.009\n0.879\nÂ±0.011\n0.835\nÂ±0.006\n0.886\nÂ±0.010\nBNPO (Token)\n0.860\nÂ±0.012\n0.801\nÂ±0.038\n0.849\nÂ±0.008\n0.836\nÂ±0.007\n0.836\nÂ±0.016\nBNPO (Latent Action)\n0.940\nÂ±0.004\n0.901\nÂ±0.014\n0.872\nÂ±0.007\n0.836\nÂ±0.008\n0.887\nÂ±0.008\nQwen2.5-VL-7B-Instruct\nPrompt\n0.839\nÂ±0.006\n0.821\nÂ±0.024\n0.721\nÂ±0.003\n0.710\nÂ±0.003\n0.773\nÂ±0.009\nSFT\n0.885\nÂ±0.003\n0.856\nÂ±0.013\n0.808\nÂ±0.005\n0.799\nÂ±0.004\n0.837\nÂ±0.006\nGRPO (Token)\n0.892\nÂ±0.004\n0.840\nÂ±0.014\n0.870\nÂ±0.016\n0.851\nÂ±0.012\n0.863\nÂ±0.011\nGRPO (Latent Action)\n0.920\nÂ±0.005\n0.872\nÂ±0.016\n0.898\nÂ±0.009\n0.852\nÂ±0.010\n0.885\nÂ±0.010\nDr.GRPO (Token)\n0.892\nÂ±0.006\n0.854\nÂ±0.009\n0.854\nÂ±0.006\n0.839\nÂ±0.004\n0.860\nÂ±0.006\nDr.GRPO (Latent Action)\n0.916\nÂ±0.010\n0.864\nÂ±0.020\n0.897\nÂ±0.008\n0.851\nÂ±0.015\n0.882\nÂ±0.013\nDAPO (Token)\n0.892\nÂ±0.004\n0.842\nÂ±0.025\n0.844\nÂ±0.013\n0.828\nÂ±0.007\n0.852\nÂ±0.012\nDAPO (Latent Action)\n0.920\nÂ±0.009\n0.863\nÂ±0.017\n0.903\nÂ±0.012\n0.850\nÂ±0.005\n0.884\nÂ±0.011\nBNPO (Token)\n0.894\nÂ±0.004\n0.859\nÂ±0.029\n0.850\nÂ±0.007\n0.836\nÂ±0.004\n0.860\nÂ±0.011\nBNPO (Latent Action)\n0.916\nÂ±0.006\n0.842\nÂ±0.018\n0.901\nÂ±0.009\n0.852\nÂ±0.012\n0.878\nÂ±0.011\nTable 1:\nPerformance comparison on MMRole and PCogAlignBench, using the\nLLM-as-a-Judge\nmetric. Results are averaged over three runs. We conduct experiments using various VLMs, including Qwen2.5-VL-3B-Instruct and Qwen2.5-VL-7B-Instruct. Best results are in\nbold\non each RL algorithm.\nBaselines\nWe consider two categories of baselines: 1) Non-RL baselines: the naive\nPrompt\nand supervised fine-tuning (\nSFT\n); 2) RL-based methods, where we compare two optimization strategies, token-level and latent action RL, using four algorithms: a) Group Relative Policy Optimization (\nGRPO\n)\nshao-2024-GRPO\n, b)\nDr. GRPO\nliu-2025-DRGRPO\n, c) Decoupled Clip and Dynamic Sampling Policy Optimization (\nDAPO\n)\nyu-2025-dapo\n, and d) Beta Normalization Policy Optimization (\nBNPO\n)\nxiao-2025-BNPO\n. The reward functions are kept the same for methods. Please refer to the Appendix\nB\nfor more experimental details.\n4.2\nMain Results\nOverall Performance\nTable\n1\nreports the experimental results of token-level baselines and our proposed latent action level RL. Based on these results, we have made the following observations.\n1) Our method achieves superior performance across diverse tasks and datasets. On average, it outperforms token-level RL by 4% (averaged over all settings).\n2) Our latent action framework is RL-agnostic and readily compatible with diverse policy optimization algorithms, including GRPO, Dr. GRPO, DAPO, and BNPO, yielding consistent gains over baselines.\n3) The improvements brought by latent actions are consistently observed in both 3B and 7B models, demonstrating the scalability of our approach.\nPerformance on Fine-grained Dimensions\nTo thoroughly evaluate the performance of multimodal conversational agents trained with latent actions across various fine-grained conversational dimensions, following prior work\ndai-2025-mmrole\n;\nli-2025-aligning\n, we assess eight dimensions on\nMMRole\n: 1) Instruction Adherence (IA), 2) Fluency (Flu), 3) Coherency (Coh), 4) Image-Text Relevance (ITR), 5) Response Accuracy (RA), 6) Personality Consistency (OC), 7) Knowledge Consistency (KC), and 8) Tone Consistency (TC).\nOn\nPCogAlignBench\n, we evaluate: 1) Role-Set Awareness (RSA), 2) Body Behavior Awareness (BBA), 3) Mind Feelings Awareness (MFA), 4) Contextual Awareness (CA), and 5) Conversational Flow (CF).\nWe present the comparison results in Fig.\n4\n, with detailed results provided in Appendix\nC.2\n.\nAs shown in Figure 4, we make the following observations:\n1) Overall, our methods outperform token-level baselines across all evaluated dimensions.\n2) While both our method and the baselines achieve strong performance on basic conversational capabilities, such as Fluency (Flu) and Conversational Flow (CF), our approach demonstrates substantially more pronounced improvements on more challenging personalized dimensions, such as Tone Consistency (TC) on\nMMRole\n.\nFigure 4:\nFine-grained performance comparison on (a) MMRole and (b) PCogAlignBench. Results using latent actions are shown with dashed lines, while results using token-level RL are plotted with solid lines.\nMethod\nMMRole\nPCogAlignBench\nAvg.\nID\nOOD\nLS1\nLS2\nOurs\n0.949\nÂ±0.007\n0.915\nÂ±0.065\n0.871\nÂ±0.011\n0.837\nÂ±0.010\n0.893\nÂ±0.023\nOurs\nw/o cycle consistency\n0.921\nÂ±0.005\n0.878\nÂ±0.023\n0.858\nÂ±0.007\n0.825\nÂ±0.013\n0.870\nÂ±0.012\nOurs\nw/o cross-modal projector\n0.944\nÂ±0.014\n0.901\nÂ±0.014\n0.858\nÂ±0.010\n0.819\nÂ±0.013\n0.880\nÂ±0.013\nOurs\nw/o text-only data\n0.932\nÂ±0.010\n0.861\nÂ±0.036\n0.851\nÂ±0.007\n0.817\nÂ±0.006\n0.865\nÂ±0.015\nTable 2:\nAblation study on main components of our method. We evaluate on MMRole and PCogAlignBench using the\nLLM-as-a-Judge\nmetric. Results are averaged over three runs. All variants are fine-tuned with GRPO based on Qwen2.5-VL-3B-Instruct. Best results are in\nbold\n.\n4.3\nAblation Study\nTo assess the contribution of main components in our method, we conduct ablation study using three variants. 1)\nOurs w/o cycle consistency\n: We remove the cycle consistency loss during cross-modal projector training, and instead directly apply the projector trained only on paired image-text data, i.e., removing\nâ„’\ncycle\n\\mathcal{L}_{\\text{cycle}}\nin Eq.\n6\n; 2)\nOurs w/o cross-modal projector\n: We remove the cross-modal projector entirely, and learn the latent action codebook directly from text-only representations\ne\nT\ne^{T}\n; 3)\nOurs w/o text-only data\n: We construct the latent action space using only the limited paired multimodal corpus, excluding all text-only data. The results of ablation study are shown in Table\n2\n.\nFrom Table\n2\n, we can make the following observations.\n1) Removing the cycle consistency loss leads to an average performance drop of 2.3%, indicating that fine-tuning the projector on large-scale text-only data via cycle consistency loss is crucial for improving its robustness.\n2) Eliminating the cross-modal projector causes a noticeable decline in performance. This suggests that directly learning the latent action space from text-only embeddings may introduce a unimodal bias, i.e., the trained latent action policy model overly relies textual representations and fail to effectively handle multimodal scenarios.\n3) Solely leveraging paired multimodal data results in the largest performance degradation, particularly in out-of-distribution settings (e.g.,\nOOD\non\nMMRole\nand\nLS2\non\nPCogAlignBench\n). This highlights that the limited diversity and coverage of paired image-text corpora constrain the generalization capability of latent action policy models.\n4.4\nAnalysis\nRollout Diversity with Latent Actions\nBenefiting from the reduced action space, the constructed latent action space is expected to improve the agentâ€™s rollout diversity during RL exploration, i.e., generating more diverse responses. Prior work has shown that such diversity is critical for improving the upper bound of RL performance\nli-2025-preserving\n;\nyu-2025-dapo\n.\nFollowing\njia-2025-controlling\n, we quantify rollout diversity via\nsemantic diversity\n, as it reflects both linguistic diversity and response quality.\nConcretely, as shown in Fig.\n3\n, for each prompt\n(\nx\nT\n,\nx\nT\n1\n:\np\n)\n(x^{T},x^{T_{1:p}})\nin the RL training set\nğ’Ÿ\nRL\n\\mathcal{D}_{\\text{RL}}\n, the agent generates\nG\nG\nresponses\n{\nx\nT\np\n+\n1\n:\nm\n,\ni\n}\ni\n=\n1\nG\n\\{x^{T_{p+1:m,i}}\\}_{i=1}^{G}\n, with\np\np\nas the prompt length and\nm\nm\nas the maximum length.\nWe calculate the semantic diversity as:\nG\nâ€‹\n(\nG\nâˆ’\n1\n)\nâˆ‘\ni\n=\n1\nG\nâˆ‘\nj\n=\n1\n,\nj\nâ‰ \ni\nG\nSim\nâ€‹\n(\nx\nT\np\n+\n1\n:\nm\n,\ni\n,\nx\nT\np\n+\n1\n:\nm\n,\nj\n)\n,\n\\frac{G(G-1)}{\\sum_{i=1}^{G}\\sum_{\\begin{subarray}{c}j=1,j\\neq i\\end{subarray}}^{G}\\text{Sim}(x^{T_{p+1:m,i}},x^{T_{p+1:m,j}})},\n(10)\nwhere\nSim\nâ€‹\n(\nâ‹…\n,\nâ‹…\n)\n\\text{Sim}(\\cdot,\\cdot)\ndenotes the embedding similarity between two responses and we adopt BGE-M3\nchen-2024-m3\nas the embedding model.\nWe report the average semantic diversity over all samples in the RL training set.\nIn Table\n3\n, we compare the rollout diversity of token based and latent action based RL algorithms. From Table\n3\n, we observe that latent action RL consistently and significantly outperforms token-level RL in rollout diversity, demonstrating the superior exploration efficiency.\nWe also provide a case study in Appendix\nC.3\nto illustrate the improvements in rollout diversity intuitively.\nMethod\nMMRole\nPCogAlignBench\nGRPO (Token)\n1.079\nÂ±0.230\n1.043\nÂ±0.197\nGRPO (Latent Action)\n1.256\nÂ±0.302\n1.200\nÂ±0.326\nDr.GRPO (Token)\n1.074\nÂ±0.224\n1.259\nÂ±0.249\nDr.GRPO (Latent Action)\n1.252\nÂ±0.297\n1.323\nÂ±0.278\nDAPO (Token)\n1.075\nÂ±0.230\n1.040\nÂ±0.182\nDAPO (Latent Action)\n1.254\nÂ±0.302\n1.131\nÂ±0.253\nBNPO (Token)\n1.078\nÂ±0.224\n1.261\nÂ±0.257\nBNPO (Latent Action)\n1.297\nÂ±0.303\n1.321\nÂ±0.286\nTable 3:\nRollout diversity during RL exploration. Higher values indicate better rollout diversity. Best results are in\nbold\n.\nComputational Budget\nTo assess the computational overhead introduced by our latent action framework, we analyze the time cost during RL training.\nSpecifically, we consider the time cost in two stages: 1)\nRollout\n: generating multiple candidate responses per prompt; 2)\nPolicy update\n: updating the policy model using the computed rewards.\nWe present the time cost per RL step of our method and the baseline in Fig.\n5\n, using GRPO as an example with a rollout batch size of 8.\nAs illustrated in Fig.\n5\n, our latent action based method incurs a 1.13Ã— slowdown in rollout time, due to the additional latent action prediction step.\nHowever, policy updates in latent action RL require only 0.86Ã— the time of the baseline, as the optimization involves adjusting the policyâ€™s output distribution over a compact latent action space, rather than the full token vocabulary.\nOverall, the total RL training time is only 1.08Ã— that of token-level RL.\nFigure 5:\nTime cost per step during RL training, including rollout, policy update, and total time.\n5\nRelated Work\nMultimodal Conversational Agents\nRecent advances in vision-language models (VLMs)\nbai-2025-qwen25vl\nhave enabled increasingly capable multimodal conversational agents (MCAs)\nyao-2025-MLLMAgentsurvey\n, such as multimodal role-playing agents\ndai-2025-mmrole\nand personalized assistants\nnguyen-2024-yo\n;\nli-2025-aligning\n, which hold significant promise in fields like entertainment\nmehta-2022-exploring\nand personalized education\ngriol-2014-developing\n.\nInitial efforts to build MCAs primarily rely on supervised fine-tuning\nlillava-2024-TMLR\n, but often suffer from poor generalization.\nRecently, RL has been widely explored for fine-tuning MCAs and has demonstrated strong generalization performance\nzhou-2025-reinforcedMLLM\n;\nchu-2025-sftRL\n.\nHowever, fine-tuning MCAs via RL faces challenges in handling the extremely large text token space.\nTo address this, we propose constructing a compact latent action space for RL fine-tuning, which enables efficient policy learning.\nReinforcement Learning with Latent Actions\nIn many real-world scenarios, only observation-only data are available, such as expert demonstration videos of robots where explicit action labels are missing\nTorabi-2019-RecentImitation\n.\nTo address this challenge, prior works leverage the learning from observation mechanism\nseo-2022-reinforcement\n;\nbaker-2022-video\nto infer latent actions from observation-only data, which are then used for RL fine-tuning of agents.\nFor instance,\nzhang-2024-whale\n;\ngao-2025-adaworld\nlearn latent actions from videos to control video generation, while\nye-2025-latent\n;\nbu-2025-univla\nextract latent actions from robot manipulation videos and use them for robot policy learning.\nThese constructed latent actions not only enhance controllability\nbruce-2024-genie\nbut also enable better transferability across different tasks due to their higher-level nature\njang-2025-dreamgen\n.\nThe most relevant work to ours is CoLA\njia-2025-controlling\n, which introduces latent actions into RL fine-tuning of LLMs.\nHowever, when constructing the latent action space for multimodal conversational agents, the scarcity of paired image-text data hinders learning a latent space with sufficient coverage.\nTo overcome this, we leverage both paired image-text data and massive text-only data to construct the latent space, using a cross-modal projector trained with a novel cycle-consistency loss.\n6\nConclusion\nIn this work, we propose to learn a compact latent action space for reinforcement learning (RL) fine-tuning of multimodal conversational agents (MCAs).\nTo construct this latent space, we leverage both paired image-text data and abundant text-only data, using a cross-modal projector trained with a novel cycle-consistency loss, which improves the coverage of latent actions while avoiding potentially unimodal bias.\nWe evaluate our approach on two tasks, including multimodal role-playing and multimodal personalized conversation, and demonstrate significant improvements over competitive baselines across various RL algorithms.\nLimitations\nWe acknowledge the following limitations in our work.\nFirst, the additional latent action prediction step increases RL training time by 1.08Ã— and inference latency by 1.13Ã—.\nSecond, due to constraints of computational resources, we evaluate our latent action based approach on multimodal conversational tasks, and leave validation on more tasks to future work, such as visual mathematical reasoning.\nEthics Considerations\nOur work is entirely at the methodological level, which means that there will not be any negative social impacts.\nAppendix A\nDetails on Model Design\nA.1\nLanguage World Model\nThe language world model\nf\nworld\nâ€‹\n(\nx\nT\nt\n+\n1\nâˆ£\nx\nV\n,\nx\nT\n1\n:\nt\n,\na\nt\n)\nf_{\\text{world}}(x^{T_{t+1}}\\mid x^{V},x^{T_{1:t}},a_{t})\npredicts the next token\nx\nT\nt\n+\n1\nx^{T_{t+1}}\nautoregressively given the current multimodal context\n(\nx\nV\n,\nx\nT\n1\n:\nt\n)\n(x^{V},x^{T_{1:t}})\nand a latent action\na\nt\na_{t}\npredicted by the inverse dynamics model (during the latent action space learning) or the policy model (during latent action RL and inference). It consists of two core modules, reusing some components from the original VLM:\nEncode Module\nThis module encodes the input\n(\nx\nV\n,\nx\nT\n1\n:\nt\n)\n(x^{V},x^{T_{1:t}})\ninto a context embedding\ne\nt\nV\n,\nT\nâˆˆ\nâ„\nd\ne^{V,T}_{t}\\in\\mathbb{R}^{d}\n, using the transformer blocks of the original VLM.\nMerge Module\nThis module fuses the context embedding\ne\nt\nV\n,\nT\ne^{V,T}_{t}\nand the latent action embedding\nc\na\nt\nâˆˆ\nâ„\nd\nc_{a_{t}}\\in\\mathbb{R}^{d}\n(where\nc\na\nt\nc_{a_{t}}\nis the code vector in\nğ’\n\\mathcal{C}\ncorresponding to the latent action\na\nt\na_{t}\n) to produce the next-token prediction. Specifically, a two-layer MLP\nf\nmlp\n:\nâ„\n2\nâ€‹\nd\nâ†’\nâ„\nd\nf_{\\text{mlp}}:\\mathbb{R}^{2d}\\to\\mathbb{R}^{d}\ntakes the concatenation\n[\ne\nt\nV\n,\nT\n;\nc\na\nt\n]\n[e^{V,T}_{t};c_{a_{t}}]\nas input and outputs a merged representation\ne\nt\nmlp\n=\nf\nmlp\nâ€‹\n(\n[\ne\nt\nV\n,\nT\n;\nc\na\nt\n]\n)\ne^{\\text{mlp}}_{t}=f_{\\text{mlp}}([e^{V,T}_{t};c_{a_{t}}])\n. Then, the merged vector\ne\nt\nmlp\ne^{\\text{mlp}}_{t}\nis fed into the original VLMâ€™s language modeling head\nf\nhead\nf_{\\text{head}}\n, yielding the token prediction distribution\np\nâ€‹\n(\nx\nT\nt\n+\n1\nâˆ£\nâ‹…\n)\n=\nf\nhead\nâ€‹\n(\ne\nt\nmlp\n)\np(x^{T_{t+1}}\\mid\\cdot)=f_{\\text{head}}(e^{\\text{mlp}}_{t})\n. The next token\nx\nT\nt\n+\n1\nx^{T_{t+1}}\nis selected from this distribution.\nA.2\nInverse Dynamics Model\nThe inverse dynamics model\nf\ninverse\n(\na\nt\n|\nx\nV\n,\nx\nT\n1\n:\nt\n+\n1\n)\n)\nf_{\\text{inverse}}(a_{t}|x^{V},x^{T_{1:t+1}}))\nis designed to take future observations\n(\nx\nV\n,\nx\nT\n1\n:\nt\n+\n1\n)\n(x^{V},x^{T_{1:t+1}})\nas input, and extracts the latent action\na\nt\na_{t}\nfor the current step\nt\nt\n.\nIt consists of three core modules.\nEncode Module\nThe input\n(\nx\nV\n,\nx\nT\n1\n:\nt\n+\n1\n)\n(x^{V},x^{T_{1:t+1}})\nis encoded into\ne\nt\n+\n1\nV\n,\nT\nâˆˆ\nâ„\nd\ne^{V,T}_{t+1}\\in\\mathbb{R}^{d}\nusing the transformer blocks of the original VLM.\nWhen\nx\nV\n=\nâˆ…\nx^{V}=\\emptyset\n(text-only sequences), the text embedding\ne\nt\n+\n1\nT\n=\nf\nVLM\nâ€‹\n(\nx\nT\n1\n:\nt\n+\n1\n)\ne^{T}_{t+1}=f_{\\text{VLM}}(x^{T_{1:t+1}})\nis projected to the image-text embedding via the cross-modal projector\nP\nP\n, i.e.,\ne\n^\nt\n+\n1\nV\n,\nT\n=\nP\nâ€‹\n(\ne\nt\n+\n1\nT\n)\n\\hat{e}^{V,T}_{t+1}=P(e^{T}_{t+1})\n, as illustrated in Fig.\n2\n.\nInverse Transformer Layers\nTo adapt the VLM embedding to the latent action space, the obtained embedding\ne\nt\n+\n1\nV\n,\nT\ne^{V,T}_{t+1}\nis processed by 4-layer Transformer blocks, yielding a representation\ne\n~\nt\n+\n1\nV\n,\nT\nâˆˆ\nâ„\nd\n\\tilde{e}^{V,T}_{t+1}\\in\\mathbb{R}^{d}\n.\nInverse Action Head\nFollowing\njia-2025-controlling\n, we adopt a\ndirect code assignment\nstrategy to avoid code collapse. Specifically, a linear head (inverse action head) maps\ne\n~\nt\n+\n1\nV\n,\nT\n\\tilde{e}^{V,T}_{t+1}\nto logits\nğ¥\nt\nâˆˆ\nâ„\n|\nğ’\n|\n\\mathbf{l}_{t}\\in\\mathbb{R}^{|\\mathcal{C}|}\nover the codebook indices. During inverse dynamics learning, we apply the Gumbel-Softmax and a reparameterization trick to obtain a differentiable soft assignment:\nğ \nt\n=\nGumbelSoftmax\nâ€‹\n(\nğ¥\nt\n)\n,\nğ¨\n^\nt\n=\n(\nğ¨\nt\nâˆ’\nğ \nt\n)\nsg\n+\nğ \nt\n,\n\\mathbf{g}_{t}=\\text{GumbelSoftmax}(\\mathbf{l}_{t}),\\quad\\hat{\\mathbf{o}}_{t}=(\\mathbf{o}_{t}-\\mathbf{g}_{t})_{\\text{sg}}+\\mathbf{g}_{t},\nwhere\nğ¨\nt\n\\mathbf{o}_{t}\nis the hard one-hot vector (\narg\nâ¡\nmax\n\\arg\\max\nof\nğ¥\nt\n\\mathbf{l}_{t}\n), and\n(\nâ‹…\n)\nsg\n(\\cdot)_{\\text{sg}}\ndenotes stop-gradient. The final latent action embedding is\nc\na\nt\n=\nğ¨\n^\nt\nâŠ¤\nâ€‹\nğ’\nc_{a_{t}}=\\hat{\\mathbf{o}}_{t}^{\\top}\\mathcal{C}\n, which is then used by the language world model.\nA.3\nPolicy Model\nThe policy\nÏ€\nÎ¸\nâ€‹\n(\na\nt\nâˆ£\nx\nV\n,\nx\nT\n1\n:\nt\n)\n\\pi_{\\theta}(a_{t}\\mid x^{V},x^{T_{1:t}})\npredicts the latent action\na\nt\na_{t}\nfrom the current context\n(\nx\nV\n,\nx\nT\n1\n:\nt\n)\n(x^{V},x^{T_{1:t}})\n.\nIts architecture mirrors\nf\ninverse\nf_{\\text{inverse}}\n, which includes: 1) the encode module, 2) policy transformer layers (8-layer), and 3) policy action head.\nA.4\nCodebook for the Latent Action Space\nThe latent action space is defined by a codebook\nğ’\n=\n{\nc\n1\n,\nâ€¦\n,\nc\nK\n}\nâŠ‚\nâ„\nd\n\\mathcal{C}=\\{c_{1},\\dots,c_{K}\\}\\subset\\mathbb{R}^{d}\nwith\nK\n=\n128\nK=128\n. Each code vector\nc\nk\nc_{k}\nis initialized independently via Kaiming uniform initialization\nhe-2015-delving\n. Given a latent action index\na\nt\nâˆˆ\n{\n1\n,\nâ€¦\n,\nK\n}\na_{t}\\in\\{1,\\dots,K\\}\n, the corresponding latent action embedding is retrieved as\nc\na\nt\nâˆˆ\nğ’\nc_{a_{t}}\\in\\mathcal{C}\n.\nA.5\nCross-modal Projector\nThe cross-modal projector\nP\nP\nis implemented as a dual-MLP module: given a text embedding\ne\nt\nT\ne^{T}_{t}\n, the first MLP outputs the mean vector\nÎ¼\nt\n\\mu_{t}\n, and the second MLP outputs the log standard deviation vector\nlog\nâ¡\nÏƒ\nt\n\\log\\sigma_{t}\n(for numerical stability), forming a diagonal Gaussian distribution\nğ’©\nâ€‹\n(\nÎ¼\nt\n,\ndiag\nâ€‹\n(\nÏƒ\nt\n2\n)\n)\n\\mathcal{N}(\\mu_{t},\\mathrm{diag}(\\sigma_{t}^{2}))\nin the image-text embedding space.\nAppendix B\nExperimental Details\nB.1\nDetails on Datasets\nCorpora for Constructing the Latent Action Space\nTo construct the latent action space in an unsupervised manner, we collect large-scale paired image-text and text-only corpora.\nFor paired image-text data, we use: (1) image-caption pairs from\nConceptual-12M\nchangpinyo-2021-Conceptual\n; (2) multimodal news articles from\nN24News\nwang-2022-N24News\n; and (3) multimodal Wikipedia articles from\nWikiWeb2M\nburns-2023-wiki\n, comprising 14M images and 1B text tokens in total.\nFor text-only data, we primarily sample 500K sequences from\nSlimPajama-627B\ncerebras-2023-slimpajama\ndue to computational constraints, and additionally include 40K alignment corpora from\nHelpSteer3\nwang-2025-helpsteer3\nto preserve the original VLMâ€™s safety and preference alignment during latent space learning.\nTo ensure fair comparison, we analyze data exposure in Appendix\nC.1\nand find that downstream task performance does not benefit from the above corpora, confirming that observed improvements stem from methodological advances.\nB.2\nDetails on Evaluation Metric\nWe adopt\nLLM-as-a-Judge\nmetrics to evaluate model performance, using prompt templates validated by\ndai-2025-mmrole\n;\nli-2025-aligning\n, which show high correlation with human judgments.\nThe evaluation prompt templates used on\nMMRole\nand\nPCogAlignBench\nare shown in Table\n4\n. We adopt the\nQwen3-235B-A22B\nby the Qwen3 API platform as the judge model.\nB.3\nTraining Details\nBaseline Methods\nFor the SFT baseline, we fine-tune the VLM with a learning rate of\n5\nÃ—\n10\nâˆ’\n6\n5\\times 10^{-6}\nfor 2 epochs.\nFor token-level RL baselines, we use a rollout size of 8, a per-step batch size of 32, and train for 100 RL steps with a constant learning rate of\n1\nÃ—\n10\nâˆ’\n6\n1\\times 10^{-6}\n. For all RL methods, we use 50% of the training data to initialize the model via SFT, followed by RL fine-tuning on the remaining 50%.\nDuring RL rollouts, we set the sampling temperature to 1.0 for all methods.\nLatent Action Space Learning\nAs outlined in Algorithm\n1\n, the latent action space learning procedure consists of the following four stages:\n1.\nInitialize\nf\nworld\n,\nf\ninverse\n,\nğ’\nf_{\\text{world}},f_{\\text{inverse}},\\mathcal{C}\nby minimizing\nâ„’\ninverse\n\\mathcal{L}_{\\text{inverse}}\n(Eq.\n1\n) on\nğ’Ÿ\nV\nâ€‹\nT\n\\mathcal{D}^{VT}\n.\nTraining details\n: learning rate =\n1\nÃ—\n10\nâˆ’\n4\n1\\!\\times\\!10^{-4}\n, cosine decay with minimum learning rate\n1\nÃ—\n10\nâˆ’\n5\n1\\!\\times\\!10^{-5}\n, batch size = 16, max sequence length = 2048, 1 epoch.\n2.\nInitialize the cross-modal projectors\nP\n,\nP\nâ€²\nP,P^{\\prime}\nby minimizing\nâ„’\nproj\n1\n\\mathcal{L}_{\\text{proj}_{1}}\n(Eq.\n5\n) on\nğ’Ÿ\nV\nâ€‹\nT\n\\mathcal{D}^{VT}\n.\nTraining details\n: learning rate =\n1\nÃ—\n10\nâˆ’\n3\n1\\!\\times\\!10^{-3}\n, cosine decay, batch size = 16, 1 epoch.\n3.\nJointly optimize\nf\nworld\n,\nf\ninverse\n,\nğ’\n,\nP\n,\nP\nâ€²\nf_{\\text{world}},f_{\\text{inverse}},\\mathcal{C},P,P^{\\prime}\nby minimizing\nâ„’\ninverse\n\\mathcal{L}_{\\text{inverse}}\n(Eq.\n1\n) and\nâ„’\nproj\n2\n\\mathcal{L}_{\\text{proj}_{2}}\n(Eq.\n6\n) on\nğ’Ÿ\nV\nâ€‹\nT\nâˆª\nğ’Ÿ\nT\n\\mathcal{D}^{VT}\\cup\\mathcal{D}^{T}\n.\nTraining details\n: learning rate =\n1\nÃ—\n10\nâˆ’\n4\n1\\!\\times\\!10^{-4}\n, cosine decay with minimum learning rate\n1\nÃ—\n10\nâˆ’\n5\n1\\!\\times\\!10^{-5}\n, batch size = 16, max sequence length = 2048, 1 epoch.\n4.\nInitialize the policy model\nÏ€\nÎ¸\n\\pi_{\\theta}\nby minimizing\nâ„’\nbc\n\\mathcal{L}_{\\text{bc}}\n(Eq.\n8\n) on\nğ’Ÿ\nV\nâ€‹\nT\nâˆª\nğ’Ÿ\nT\n\\mathcal{D}^{VT}\\cup\\mathcal{D}^{T}\n.\nTraining details\n: learning rate =\n1\nÃ—\n10\nâˆ’\n4\n1\\!\\times\\!10^{-4}\n, cosine decay, batch size = 16, max sequence length = 2048, 1 epoch.\nLatent Action RL\nWe adopt the same RL hyperparameters as the token-level baselines: rollout size of 8, per-step batch size of 32, 100 RL steps, and constant learning rate of\n1\nÃ—\n10\nâˆ’\n6\n1\\times 10^{-6}\n.\nTo prevent code collapse and excessive deviation from the initial policy, we incorporate a KL regularization term between the current policyâ€™s action distribution and its initialization, with a coefficient of 0.01.\nDuring RL fine-tuning, only the policy transformer layers and the policy head in the policy model (Sec.\nA.3\n) are optimized.\nSince all token-level RL methods build upon an SFT-initialized model, for fair comparison, we also perform SFT before latent action RL. Specifically, we fine-tune the transformer blocks in VLMs (shared by the policy model and the language world model) and the language modeling head in VLMs (used by the language world model) using the same SFT data as the baselines.\nDuring RL rollouts, we set the sampling temperature for the latent action level policy model as 1.0.\nReward Function\nFor all methods, we employ a generative reward model for fair comparison, where responses are scored by\nQwen3-235B-A22B\nusing the evaluation prompt templates in Table\n4\n.\nImplementation Details\nAll experiments are conducted on a single machine equipped with 4 Nvidia A100-80G GPU.\nFor the baseline SFT and RL algorithms, as well as our newly proposed latent action RL methods, we adapt the framework based on the TRL library\nvonwerra-2022-trl\n.\nB.4\nInference Details\nFor all methods, we use a sampling temperature of 0.1 during inference, i.e., for token-based baselines, this temperature is applied to the token logits; for our latent action based methods, it is applied to the latent action logits.\nAdditionally, following\njia-2025-controlling\n, for our latent action based methods, token generation by the language world model is deterministic, i.e., tokens are selected via\nargmax\nover the output token logits.\nPrompt Template for Evaluation on MMRole\n## [Question Start]\n{question}\n## [Question End]\n## [Model Aâ€™s Response Start]\n{evaluated_answer}\n## [Model Aâ€™s Response End]\n## [Model Bâ€™s Response Start]\n{groundtruth_answer}\n## [Model Bâ€™s Response End]\n## [Instruction]\nThe task instruction of the two models is to directly role-play as {role_name} and talk with a curious human about the given image using the distinctive tone, manner and vocabulary of {role_name}.\nHere is the detailed character information about {role_name}:\n{role_info}\nPlease evaluate the following aspects of each modelâ€™s response:\n1. Instruction Adherence: Do the responses accurately adhere to the task instruction, directly role-playing as {role_name} and only including words that {role_name} should say, without any additional explanatory prefixes or suffixes?\n2. Fluency: Are the responses grammatically correct and smoothly articulated?\n3. Coherency: Do the responses maintain a coherent thread of dialogue without contradicting earlier parts of the conversation or previously established facts?\n4. Image-Text Relevance: Are the responses closely related to the visual content of the image?\n5. Response Accuracy: Do the responses accurately answer the curious humanâ€™s words or appropriately initiate a conversation based on the image?\n6. Personality Consistency: Do the responses accurately and sufficiently reflect the personality of {role_name}?\n7. Knowledge Consistency: Are the responses consistent with the factual knowledge that {role_name} should possess, including experiences, abilities, and relationships?\n8. Tone Consistency: Do the responses maintain a consistent tone that aligns with {role_name}â€™s typical manner of speaking and catchphrases, rather than resembling the style of AI assistants?\nFor each aspect, provide a brief qualitative evaluation for the relative performance of the two models, followed by paired quantitative scores from 1 to 10, where 1 indicates poor performance and 10 indicates excellent performance.\nThe output should be in the following format:\n1. Instruction Adherence: {{Qualitative Evaluation}}, [Scores]: ({{the score of Model A}}, {{the score of Model B}})\n2. Fluency: {{Qualitative Evaluation}}, [Scores]: ({{the score of Model A}}, {{the score of Model B}})\netc.\nPlease ensure that your evaluations are unbiased and that the order in which the responses were presented does not affect your judgment.\nFormat requirement: Please ensure that your evaluations only include 8 score pairs, which means that there can only be eight pairs of [Scores]: () in your output text.\nPrompt Template for Evaluation on PCogAlignBench\nPersonalizedAI Company is developing a personalized AI service robot that aims to better serve each individual. The service is currently being trialed with a small group of users. In order to improve the level of personalization in the responses provided by the AI service robot, our company plans to conduct surveys and interviews with participants in the trial. We will first provide historical interview records, which include the feedback and preferences expressed by the test users regarding AI responses in a certain scenario. During the interview, the interviewee needs to refer to these historical records to answer questions posed by the interviewer. The interview will be conducted in an online Q&A format, and interviewees must strictly follow the format requirements provided in system instructions.\n# Historical Interview Records\nInterviewer: Hello, could you please briefly describe your role set?\nInterviewee: OK. {individual_RoleSet_str}\nInterviewer: In the \"{visual_scene_text}\" scenario at {location} location, what kind of responses would you like the AI to provide?\nInterviewee: Okay, I will describe what kind of AI responses would satisfy me in this scenario. {EvalHelp_str}\n# Interview\nInterviewer: Hello, and thank you for trialing the personalized AI responses from our company.\nInterviewee: Youâ€™re welcome.\nInterviewer: Alright, we will now present you with a question you posed in a particular scenario along with two generated responses from the AI. We would like you to choose which response is better.\nInterviewee: Sure, I understand. Please go ahead.\nInterviewer: According to our cloud records, in a \"{visual_scene_text}\" scenario, you asked the personalized AI robot the question: \"{query}\". Here are the generated responses from the AI.\n> **Response A**: {response_A}\n> **Response B**: {response_B}\n> System Instruction: Interviewee, please note that you should not choose a response as better just because itâ€™s long. Instead, select the response that best considers your physical and mental state and helps you to achieve better body behavior and mind feelings.\n> System Instruction: For each aspect, provide a brief qualitative evaluation for the relative performance of the two models, followed by paired quantitative scores from 1 to 10, where 1 indicates poor performance and 10 indicates excellent performance.\nThe output should be in the following format:\n1. Role-Set Sensitivity: {{Qualitative Evaluation}}, [Scores]: ({{the score of Response A}}, {{the score of Response B}})\n2. Body Behavior Awareness: {{Qualitative Evaluation}}, [Scores]: ({{the score of Response A}}, {{the score of Response B}})\n3. Mind Feelings Awareness: {{Qualitative Evaluation}}, [Scores]: ({{the score of Response A}}, {{the score of Response B}})\n4. Contextual Awareness: {{Qualitative Evaluation}}, [Scores]: ({{the score of Response A}}, {{the score of Response B}})\n5. Conversational Flow: {{Qualitative Evaluation}}, [Scores]: ({{the score of Response A}}, {{the score of Response B}})\netc.\nPlease ensure that your evaluations are unbiased and that the order in which the responses were presented does not affect your judgment.\nFormat requirement: Please ensure that your evaluations only include 5 score pairs, which means that there can only be 5 pairs of [Scores]: () in your output text.\nTable 4:\nPrompt templates used for LLM-as-a-Judge evaluation on\nMMRole\nand\nPCogAlignBench\n. These templates follow established designs from\ndai-2025-mmrole\n;\nli-2025-aligning\nand have been shown to achieve high correlation with human judgments.\nData\nMMRole\nPCogAlignBench\nAvg.\nID\nOOD\nLS1\nLS2\nQwen2.5-VL-3B-Instruct\nSFT Data\n0.843\nÂ±0.002\n0.809\nÂ±0.012\n0.808\nÂ±0.009\n0.810\nÂ±0.005\n0.817\nÂ±0.007\nw/ Extra Corpora\n0.836\nÂ±0.010\n0.822\nÂ±0.014\n0.797\nÂ±0.010\n0.802\nÂ±0.012\n0.814\nÂ±0.011\nQwen2.5-VL-7B-Instruct\nSFT Data\n0.885\nÂ±0.003\n0.856\nÂ±0.013\n0.808\nÂ±0.005\n0.799\nÂ±0.004\n0.837\nÂ±0.006\nw/ Extra Corpora\n0.881\nÂ±0.007\n0.895\nÂ±0.021\n0.797\nÂ±0.006\n0.757\nÂ±0.006\n0.832\nÂ±0.010\nTable 5:\nPerformance comparison of models fine-tuned with: 1) only SFT data and 2) SFT data and extra corpora (used for constructing the latent action space). Results are averaged over three runs. Best results within each model size are in\nbold\n.\nAppendix C\nAdditional Empirical Results\nC.1\nAnalysis on Data Exposure\nTo verify that gains arise from our latent action design, not merely from exposure to extra corpora that are used for constructing the latent action space, we conduct continued pre-training on Qwen2.5-VL-3B/7B using the same corpora, followed by SFT.\nAs shown in Table\n5\n, this approach yields no consistent improvement, and even slight degradation on average. This confirms that the benefits of our latent action approach arise from the action space design, not from exposure to the extra corpora.\nC.2\nDetailed Results on Fine-grained Dimensions\nWe report the fine-grained performance across each evaluation dimensions, previously summarized in Fig.\n4\n. Specifically, Tables\n6\nand\n7\npresent results on the in-distribution (ID) and out-of-distribution (OOD) splits of MMRole, respectively. Tables\n8\nand\n9\nshow results on the LS1 and LS2 subsets of PCogAlignBench. All results are obtained using the Qwen2.5-VL-3B-Instruct model.\nC.3\nCase Study\nTo intuitively illustrate the improvements in diversity and response quality achieved by our latent action RL during rollout, we present case studies on MMRole (Fig.\n6\n) and PCogAlignBench (Fig.\n7\n), respectively.\nMethod\nMMRole (ID)\nIA\nFlu\nCoh\nITR\nRA\nPC\nKC\nTC\nBase\n0.721\n0.897\n0.802\n0.743\n0.734\n0.629\n0.674\n0.628\nSFT\n0.837\n0.936\n0.894\n0.858\n0.858\n0.776\n0.822\n0.760\nGRPO (Token)\n0.837\n0.916\n0.866\n0.847\n0.848\n0.789\n0.828\n0.773\nGRPO (Latent Action)\n0.937\n0.963\n0.951\n0.967\n0.965\n0.926\n0.965\n0.919\nDr.GRPO (Token)\n0.861\n0.946\n0.907\n0.871\n0.883\n0.816\n0.857\n0.794\nDr.GRPO (Latent Action)\n0.947\n0.966\n0.956\n0.960\n0.968\n0.931\n0.967\n0.928\nDAPO (Token)\n0.852\n0.940\n0.900\n0.863\n0.868\n0.797\n0.842\n0.783\nDAPO (Latent Action)\n0.932\n0.962\n0.948\n0.943\n0.952\n0.920\n0.960\n0.912\nBNPO (Token)\n0.853\n0.941\n0.899\n0.874\n0.876\n0.803\n0.846\n0.787\nBNPO (Latent Action)\n0.930\n0.959\n0.944\n0.950\n0.951\n0.919\n0.957\n0.908\nTable 6:\nFine-grained performance on MMRole (ID set), using the LLM-as-a-Judge metric. Results are averaged over three runs. We conduct experiments using Qwen2.5-VL-3B-Instruct. Dimensions: Instruction Adherence (IA); Fluency (Flu); Coherency (Coh); Image-Text Relevance (ITR); Response Accuracy (RA); Personality Consistency (OC); Knowledge Consistency (KC); Tone Consistency (TC).\nMethod\nMMRole (OOD)\nIA\nFlu\nCoh\nITR\nRA\nPC\nKC\nTC\nBase\n0.682\n0.887\n0.754\n0.704\n0.693\n0.588\n0.595\n0.594\nSFT\n0.816\n0.924\n0.867\n0.804\n0.823\n0.749\n0.760\n0.729\nGRPO (Token)\n0.798\n0.873\n0.812\n0.825\n0.834\n0.735\n0.764\n0.728\nGRPO (Latent Action)\n0.904\n0.960\n0.917\n0.983\n0.962\n0.859\n0.877\n0.856\nDr.GRPO (Token)\n0.844\n0.933\n0.878\n0.783\n0.812\n0.770\n0.798\n0.766\nDr.GRPO (Latent Action)\n0.902\n0.945\n0.930\n0.932\n0.934\n0.892\n0.908\n0.887\nDAPO (Token)\n0.825\n0.911\n0.845\n0.785\n0.799\n0.756\n0.770\n0.751\nDAPO (Latent Action)\n0.883\n0.946\n0.909\n0.931\n0.915\n0.842\n0.843\n0.840\nBNPO (Token)\n0.814\n0.907\n0.848\n0.775\n0.800\n0.754\n0.762\n0.746\nBNPO (Latent Action)\n0.893\n0.931\n0.898\n0.942\n0.930\n0.862\n0.879\n0.868\nTable 7:\nFine-grained performance on MMRole (OOD set), using the LLM-as-a-Judge metric. Results are averaged over three runs. We conduct experiments using Qwen2.5-VL-3B-Instruct. Dimensions: Instruction Adherence (IA); Fluency (Flu); Coherency (Coh); Image-Text Relevance (ITR); Response Accuracy (RA); Personality Consistency (OC); Knowledge Consistency (KC); Tone Consistency (TC).\nMethod\nPCogAlignBench (LS1)\nRSA\nBBA\nMFA\nCA\nCF\nBase\n0.697\n0.698\n0.599\n0.700\n0.696\nSFT\n0.775\n0.791\n0.801\n0.808\n0.864\nGRPO (Token)\n0.803\n0.832\n0.855\n0.841\n0.896\nGRPO (Latent Action)\n0.825\n0.864\n0.884\n0.863\n0.920\nDr.GRPO (Token)\n0.797\n0.821\n0.839\n0.834\n0.882\nDr.GRPO (Latent Action)\n0.830\n0.871\n0.889\n0.864\n0.918\nDAPO (Token)\n0.794\n0.829\n0.832\n0.832\n0.890\nDAPO (Latent Action)\n0.833\n0.878\n0.897\n0.863\n0.922\nBNPO (Token)\n0.806\n0.845\n0.853\n0.838\n0.901\nBNPO (Latent Action)\n0.826\n0.872\n0.880\n0.862\n0.920\nTable 8:\nFine-grained performance on PCogAlignBench (LS1 set), using the LLM-as-a-Judge metric. Results are averaged over three runs. We conduct experiments using Qwen2.5-VL-3B-Instruct. Dimensions: Role-Set Awareness (RSA); Body Behavior Awareness (BBA); Mind Feelings Awareness (MFA); Contextual Awareness (CA); Conversational Flow (CF).\nMethod\nPCogAlignBench (LS2)\nRSA\nBBA\nMFA\nCA\nCF\nBase\n0.690\n0.751\n0.582\n0.671\n0.686\nSFT\n0.781\n0.802\n0.806\n0.796\n0.863\nGRPO (Token)\n0.815\n0.845\n0.857\n0.815\n0.893\nGRPO (Latent Action)\n0.797\n0.839\n0.850\n0.814\n0.901\nDr.GRPO (Token)\n0.802\n0.839\n0.833\n0.818\n0.878\nDr.GRPO (Latent Action)\n0.793\n0.838\n0.845\n0.806\n0.894\nDAPO (Token)\n0.799\n0.825\n0.827\n0.804\n0.884\nDAPO (Latent Action)\n0.790\n0.832\n0.843\n0.802\n0.895\nBNPO (Token)\n0.800\n0.846\n0.836\n0.815\n0.885\nBNPO (Latent Action)\n0.791\n0.835\n0.841\n0.809\n0.895\nTable 9:\nFine-grained performance on PCogAlignBench (LS2 set), using the LLM-as-a-Judge metric. Results are averaged over three runs. We conduct experiments using Qwen2.5-VL-3B-Instruct. Dimensions: Role-Set Awareness (RSA); Body Behavior Awareness (BBA); Mind Feelings Awareness (MFA); Contextual Awareness (CA); Conversational Flow (CF).\nFigure 6:\nA case study on the MMRole dataset. From this example, we observe that latent-action RL yields more diverse responses during rollout compared to token-level RL. Moreover, the generated responses using latent actions better align with the emotional traits expected of the given character. The RL algorithm used here is GRPO, with Qwen2.5-VL-3B-Instruct as the base model.\nFigure 7:\nA case study on the PCogAlignBench dataset. As shown in this example, latent action RL produces more diverse responses during rollout compared to token-level RL. Moreover, the generated responses using latent actions better incorporate personalized elements tailored to the userâ€™s background. The RL algorithm used here is GRPO, with Qwen2.5-VL-3B-Instruct as the base model.",
      "references": [],
      "references_dois": [],
      "fallback_urls": [],
      "errors": [],
      "missing_fields": [
        "doi",
        "versions",
        "last_updated_raw",
        "published_date",
        "references",
        "references_dois"
      ],
      "url_hint_if_missing": "Champs manquants: doi, versions, last_updated_raw, published_date, references, references_dois. Tu peux vÃ©rifier ici: abs=https://arxiv.org/abs/2601.07516 | html=https://arxiv.org/html/2601.07516v1 | pdf=https://arxiv.org/pdf/2601.07516"
    },
    {
      "arxiv_id": "2601.07235",
      "title": "Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges",
      "authors": [
        "Agnivo Gosai",
        "Shuvodeep De",
        "Karun Thankachan"
      ],
      "abstract": "This paper presents a comprehensive survey of sentiment analysis methods for movie reviews, a benchmark task that has played a central role in advancing natural language processing. We review the evolution of techniques from early lexicon-based and classical machine learning approaches to modern deep learning architectures and large language models, covering widely used datasets such as IMDb, Rotten Tomatoes, and SST-2, and models ranging from Naive Bayes and support vector machines to LSTM networks, BERT, and attention-based transformers . Beyond summarizing prior work, this survey differentiates itself by offering a comparative, challenge-driven analysis of how these modeling paradigms address domain-specific issues such as sarcasm, negation, contextual ambiguity, and domain shift, which remain open problems in existing literature. Unlike earlier reviews that focus primarily on text-only pipelines, we also synthesize recent advances in multimodal sentiment analysis that integrate textual, audio, and visual cues from movie trailers and clips. In addition, we examine emerging concerns related to interpretability, fairness, and robustness that are often underexplored in prior surveys, and we outline future research directions including zero-shot and few-shot learning, hybrid symbolic--neural models, and real-time deployment considerations. Overall, this abstract provides a domain-focused roadmap that highlights both established solutions and unresolved challenges toward building more accurate, generalizable, and explainable sentiment analysis systems for movie review data.",
      "submitted_date": "12 January, 2026",
      "abs_url": "https://arxiv.org/abs/2601.07235",
      "pdf_url": "https://arxiv.org/pdf/2601.07235",
      "doi": "",
      "versions": [],
      "last_updated_raw": "",
      "html_url": "https://arxiv.org/html/2601.07235v1",
      "published_date": "",
      "content_text": "Sentiment Analysis on Movie Reviews: A Deep Dive into Modern Techniques and Open Challenges\nAgnivo Gosai\n1\nShuvodeep De\n2,âˆ—\nKarun Thankachan\n3\n1\nIndependent Researcher; agnivo2007@gmail.com\n2\nTexas State University; vvg26@txstate.edu\n3\nCarnegie Mellon University; kthankac@alumni.cmu.edu\nâˆ—\nCorresponding author: vvg26@txstate.edu\nAbstract\nThis paper presents a comprehensive survey of sentiment analysis methods for movie reviews, a benchmark task that has played a central role in advancing natural language processing. We review the evolution of techniques from early lexicon-based and classical machine learning approaches to modern deep learning architectures and large language models, covering widely used datasets such as IMDb, Rotten Tomatoes, and SST-2, and models ranging from Naive Bayes and support vector machines to LSTM networks, BERT, and attention-based transformers. Beyond summarizing prior work, this survey differentiates itself by offering a comparative, challenge-driven analysis of how these modeling paradigms address domain-specific issues such as sarcasm, negation, contextual ambiguity, and domain shift, which remain open problems in existing literature. Unlike earlier reviews that focus primarily on text-only pipelines, we also synthesize recent advances in multimodal sentiment analysis that integrate textual, audio, and visual cues from movie trailers and clips. In addition, we examine emerging concerns related to interpretability, fairness, and robustness that are often underexplored in prior surveys, and we outline future research directions including zero-shot and few-shot learning, hybrid symbolicâ€“neural models, and real-time deployment considerations. Overall, this abstract provides a domain-focused roadmap that highlights both established solutions and unresolved challenges toward building more accurate, generalizable, and explainable sentiment analysis systems for movie review data.\nKeywords:\nSentiment analysis, movie reviews, opinion mining, natural language processing (NLP), text classification, machine learning, deep learning, emotion detection, feature extraction, polarity classification\n1\nIntroduction\n1.1\nBackground\nSentiment analysis of movie reviews, that is, the task of determining whether a piece of text conveys a positive, negative, or neutral opinion, is one of the most enduring and influential test beds in natural language processing (NLP)\n[\n1\n,\n2\n]\n. What began a couple of decades ago as a seemingly straightforward binary classification problem has evolved into a sophisticated domain that exemplifies the full spectrum of challenges facing modern computational linguistics: from handling linguistic creativity and cultural nuance to ensuring robust deployment in resource-constrained environments\n[\n3\n,\n4\n]\n. The literature on movie review sentiment analysis is anchored by Pang and Leeâ€™s seminal 2008 monograph â€œOpinion Mining and Sentiment Analysisâ€ in Foundations and Trends in Information Retrieval\n[\n5\n]\n, which established the definitive theoretical framework and extensively utilized movie review classification as the primary case study, building upon their pioneering 2002 EMNLP paper\n[\n1\n]\nthat first applied machine learning techniques to movie review sentiment classification. This foundational work, along with Maas et al.â€™s 2011 ACL paper\n[\n6\n]\nintroducing the Large Movie Review Dataset (IMDB), created the benchmark standards that continue to dominate the field today.\nRecent comprehensive surveys (from 2020â€“2025) have documented the methodological evolution toward deep learning approaches, including Wankhade et al.\n[\n7\n]\nand Jain et al.\n[\n8\n]\n, which provide extensive coverage of neural architectures and transformer models, while Raghunathan et al.\n[\n9\n]\noffers systematic literature review methodology focusing on contemporary challenges. Domain-specific surveys such as the conference paper by Tetteh et al. on sentiment analysis tools for movie review evaluation\n[\n10\n]\nand the recent publication examining the applications of BERT and XLNet\n[\n11\n]\nto movie sentiment analysis demonstrate the fieldâ€™s continued focus on performance optimization using state-of-the-art models. However, existing reviews\n[\n12\n,\n13\n]\non the analysis of movie review sentiment have notable gaps, particularly in addressing multilingual and cross-cultural contexts, bias and fairness, and the detection of sarcasm / irony. The coverage of aspect-based sentiment analysis is somewhat fragmented\n[\n14\n,\n15\n,\n16\n]\n, with limited attention to methods targeting specific movie elements, and real-world deployment challenges are underexplored compared to academic benchmark performance. These omissions hinder a comprehensive understanding of the current state of the field and its practical applicability.\nThe evaluation landscape reveals inconsistent protocols and limited standardization across studies, with existing surveys\n[\n17\n,\n18\n]\nfailing to establish comprehensive frameworks for comparing methodological approaches. Recent surveys have also inadequately addressed emerging challenges including AI-generated fake reviews, cross-modal sentiment analysis that combines text with visual movie content, and privacy-preserving analysis techniques. These gaps indicate substantial opportunities for novel survey work that could provide more systematic coverage of multilingual approaches, comprehensive bias analysis, standardized evaluation frameworks, and practical deployment considerations that have been largely unexplored in the existing literature on this topic.\n1.2\nSignificance and Motivation\nSentiment analysis of movie reviews has evolved from a niche academic pursuit into a critical infrastructure that today shapes the entertainment industry and influences billions of viewers around the world. In an era where digital platforms generate millions of user reviews daily, automated sentiment analysis has become the invisible but essential technology that powers recommendation systems on major streaming services such as Netflix, Amazon Prime and Disney+\n[\n19\n]\n. Studios use these systems to gauge audience reception before and after theatrical releases, marketing teams refine promotional strategies based on sentiment insights, and critics increasingly find their influence mediated through algorithmic aggregation systems\n[\n2\n]\nthat must parse the subtle distinctions between professional critique and casual opinion. Movie reviews presented by audiences has an exceptionally rich linguistic landscape from the measured prose of professional critics to the enthusiastic hyperbole of fan communities, from the sardonic wit of satirical posts to the technical analysis of film school graduates\n[\n6\n,\n20\n]\n, making this domain an ideal testbed for evaluating NLP systems across varied linguistic challenges, cultural contexts, and temporal periods\n[\n21\n]\n. As automated sentiment understanding increasingly shapes commercial decisions and cultural discussions, improving its accuracy, robustness, and interpretability becomes a practical necessity as well as a significant scientific goal.\nHowever, despite achieving near-perfect performance on standard benchmarks\n[\n22\n]\n, real-world deployment of sentiment analysis systems continues to reveal fundamental limitations that transcend simple accuracy measurements.\nSarcasm and irony\nremain particularly challenging\n[\n23\n]\n, as rhetorical expressions such as â€œGreat, another 3-hour snooze-festâ€ fundamentally invert the relationship between surface-level sentiment indicators and actual meaning.\nDomain and temporal drift\nintroduce ongoing stability issues when models confront new platforms, evolving slang, and cultural references absent from their training data\n[\n21\n]\n.\nLong-form context processing\nchallenges arise when detailed reviews exceed standard transformer input limitations\n[\n24\n]\n, while\nexplainability and bias concerns\nbecome critical as these systems move from research prototypes to production deployments affecting real commercial outcomes\n[\n25\n]\n. Finally,\nresource efficiency\ndemands create fundamental trade-offs between model sophistication and computational constraints of mobile and edge environments\n[\n26\n,\n27\n]\n.\nThese challenges are deeply interconnected rather than isolated: effective sarcasm detection often requires extensive context\n[\n28\n]\n, which conflicts directly with efficiency constraints\n[\n26\n]\n. Robust domain adaptation must continually account for changing cultural expressions\n[\n21\n]\n; and explainability methods must generalize across domains while operating under strict resource budgets\n[\n25\n]\n. This interconnected nature suggests that future progress will depend on holistic approaches, ones that jointly address multiple challenges instead of optimizing individual components in isolation\n[\n29\n]\n. In the context of movie review analysis, nuanced language and diverse author voices around such integrated solutions are essential to deliver the reliable, interpretable, and efficient sentiment insights that underpin both commercial success and scholarly advancement.\n1.3\nThe Evolution of Sentiment Analysis\nThe foundational work by Pang and Lee (2002)\n[\n1\n]\nmarked a pivotal moment in computational sentiment analysis, introducing the Movie Reviews (MR) corpus and demonstrating that machine learning techniques could significantly outperform simple lexicon-based sentiment scoring methods\n[\n30\n]\n. Their work established movie review sentiment analysis as a benchmark problem and created the methodological framework that would guide research for the next two decades\n[\n31\n]\n. The subsequent introduction of the IMDB-Large dataset\n[\n6\n]\nwith its 50,000 balanced reviews further standardized evaluation practices and enabled more robust comparison across approaches.\nThe field has since progressed through distinct methodological eras, each bringing significant advances in both techniques and performance. From early bag-of-words approaches\n[\n1\n,\n32\n]\nachieving\nâˆ¼\n\\sim\n82% accuracy to static word embeddings\n[\n33\n,\n34\n]\nreaching 86â€“91%, followed by RNN and attention mechanisms\n[\n35\n,\n36\n]\nthat reached 90â€“93%, transformer architectures\n[\n24\n,\n22\n,\n37\n]\npushing performance to 94â€“97%, and modern large language models\n[\n38\n,\n39\n]\nreaching 97â€“98% on standard benchmarks\n[\n40\n]\n, this progression represents one of the most successful long-term research trajectories in NLP.\nHowever, this apparent success reveals a more complex reality: as benchmark performance has approached theoretical limits, persistent challenges have emerged that cannot be solved through optimizing for better classification accuracy alone\n[\n25\n,\n29\n]\n. Contemporary research has identified five core challenges that continue to define the frontier of movie review sentiment analysis\n[\n2\n,\n3\n]\n: sarcasm and irony detection, domain and temporal drift adaptation, long-form context processing, explainability and bias mitigation, and resource-efficient deployment. These challenges are not merely technical hurdles, but represent fundamental aspects of human communication that computational systems must learn to navigate.\n1.4\nLimitations and Challenges\nDespite achieving near-human performance on benchmark datasets, current sentiment analysis methods encounter persistent limitations that prevent seamless real-world application. These limitations include:\nâ€¢\nSarcasm and Irony Detection\n: Recognizing nuanced or indirect sentiment expressions remains problematic, significantly affecting accuracy.\nâ€¢\nDomain and Temporal Drift\n: Models trained on historical data or single-source reviews often fail when deployed in evolving or cross-platform contexts.\nâ€¢\nLong-form Context Processing\n: Contemporary models struggle to consistently interpret sentiment across extended texts, missing context-dependent meanings.\nâ€¢\nInterpretability and Bias Mitigation\n: Sentiment predictions by deep learning models frequently lack transparency, undermining trust in real-world deployments.\nâ€¢\nResource Efficiency\n: High-performing transformer models require computational resources, limiting practical deployment scenarios, especially in real-time settings.\nThus, addressing these intertwined challenges is essential to bridge the gap between theoretical model performance and practical usability in the dynamic domain of movie review sentiment analysis.\n1.5\nNovelty and Scope of this Review\nThis review systematically synthesizes two decades of research from foundational lexicon-based approaches to cutting-edge transformer and multimodal models, uniquely emphasizing practical deployment considerations. Its primary contributions are:\nâ€¢\nComprehensive Methodological Evolution\n: A structured analysis of sentiment analysis methodologies from early rule-based systems to modern large language models, contextualizing their strengths, limitations, and practical implications.\nâ€¢\nCritical Analysis of Persistent Challenges\n: Explicitly identifying and evaluating ongoing research limitations: sarcasm detection, domain adaptation, contextual understanding, interpretability, and computational efficiency, which previous reviews have only addressed separately or superficially.\nâ€¢\nIntegrated Benchmark Critique and Recommendations\n: Proposing next-generation evaluation frameworks that incorporate multidimensional, real-world performance metrics, including temporal robustness, cross-platform adaptability, multimodal understanding, and resource-aware assessments.\nâ€¢\nFuture-Oriented Research Roadmap\n: Highlighting promising future directions in multimodal fusion, zero-shot and hybrid model training, interactive explainability, and efficient deployment, which collectively address the identified limitations in an integrative manner.\nWe provide a structured analysis of movie review sentiment analysis that extends beyond traditional accuracy-focused evaluation to address the complex challenges facing modern deployment scenarios. We synthesize the complete methodological evolution of the field while maintaining focus on practical applicability and real-world deployment considerations, with particular emphasis on the five persistent challenges that define the current research frontier: sarcasm and irony detection, domain and temporal drift, long-form context processing, explainability and bias mitigation, and resource-efficient deployment.\nFigure 1:\nSentiment Analysis on Movie Review: Organization of the Article. Figure created with AI assistance.\nOur analysis is organized into seven complementary sections that build systematically from foundational concepts to cutting-edge research directions.\nSection 2\nestablishes the theoretical foundation by examining different granularities of sentiment analysis and categorizing types of movie review data.\nSection 3\ntraces the evolution of evaluation frameworks from foundational datasets all to multimodal sentiment extensions, critically analyzing how benchmarks have shaped research while potentially constraining real-world understanding.\nSection 4\nprovides comprehensive coverage of the methodological progression from rule-based approaches to transformer models, documenting how it evolved from 82 percent accuracy to 97â€“98 percent accuracy.\nSection 5\nprovides a detailed examination of the persistent challenges that define the current research in this area, highlighting how these interconnected problems require holistic solutions.\nSection 6\nexplores some of the most promising research avenues, including improved benchmarks, few-shot learning, cross-lingual transfer, explainable models, multimodal analysis, and bias mitigation, while\nSection 7\nsynthesizes key insights to develop robust and deployable systems. Throughout this analysis, we maintain focus on practical implications of research developments, bridging the gap between academic research and real-world deployment to serve both as a comprehensive resource for researchers and a strategic guide for practitioners. This field currently stands at a critical juncture where traditional accuracy-focused approaches have reached their limits, and this review provides the foundation for navigating the transition toward systems that can truly understand, explain, and generalize across the rich diversity of human expression in film criticism.\n2\nProblem Formulation and Taxonomy\nSentiment analysis is often referred to as opinion mining since it is essentially the computational study of peopleâ€™s opinions, attitudes, and emotions expressed in text. At its core, sentiment analysis seeks to infer the underlying sentiment polarity: positive, negative, or neutral conveyed by a writer about an entity or event. Early work in this field framed the problem as a binary or multiclass text classification task, leveraging hand-crafted lexicons or shallow statistical features to assign a polarity label to an entire document or sentence\n[\n1\n]\n. As the field matured, researchers broadened this scope to capture richer sentiment dimensions, such as intensity (e.g., â€œvery happyâ€ vs. â€œslightly happyâ€), emotion categories (e.g., joy, anger, sadness), and even finer-grained opinions towards specific aspects of an entity\n[\n41\n]\n.\n2.1\nGranularity of Sentiment Analysis\nSentiment analysis can be organized along several levels of granularity. In\ndocument-level\nanalysis, the goal is to predict the overall sentiment of a full review or article. This is often the simplest formulation, but may obscure conflicting sentiments directed at different aspects (for example, praising acting but criticizing the plot).\nSentence-level\nanalysis addresses this by assigning sentiment labels to individual sentences, thereby allowing more localized sentiment detection within longer texts.\nAspect-level\n(or feature-level) sentiment analysis goes a step further by first identifying specific attributes of entity (such as â€œacting,â€ â€œdirection,â€ or â€œcinematographyâ€ in movie reviews) and then determining the sentiment expressed toward each attribute. This finer granularity supports nuanced insights, such as simultaneously capturing praise for special effects and disappointment with story pacing\n[\n20\n]\n.\n2.2\nTypes of Movie Review Data\nMovie reviews come in diverse formats and levels of annotation, each providing different signals for sentiment analysis. In this subsection, we distinguish three principal types of movie review data that have driven research in the field.\n2.2.1\nUnstructured Textual Reviews\nThe most prevalent form of movie review data consists of free-form text written by users or professional critics. These unstructured reviews appear on platforms such as IMDb and Rotten Tomatoes, where authors express nuanced opinions, personal anecdotes, and rhetorical devices that convey their affective stance\n[\n1\n]\n. Unlike short social-media posts, movie reviews often span multiple paragraphs and include both explicit sentiment (â€œI loved the stunning visualsâ€) and implicit cues (â€œIt took me a half hour to recover from that endingâ€). The richness of this text makes natural language processing both challenging and rewarding, as models must learn to parse context, resolve coreference, and interpret discourse structure to accurately gauge sentiment intensity and direction.\n2.2.2\nStructured Ratings and Metadata\nComplementing unstructured reviews are structured signals such as numerical star ratings (e.g., 1â€“10) and categorical labels (e.g., â€˜Freshâ€™ vs. â€˜Rottenâ€™ on Rotten Tomatoes). These coarse-grained annotations provide readily available polarity information that can serve as distant supervision for training classifiers\n[\n41\n]\n. In many datasets, each review is accompanied by metadata such as reviewer ID, timestamp, and genre labels, which enables temporal analysis and user-level modeling of sentiment trends. However, structured ratings often mask the rationale behind the score, motivating hybrid approaches that combine metadata with text to improve interpretability and performance.\n2.2.3\nAnnotated Corpora for Fine-Grained Analysis\nTo capture sentiment at sub-sentential levels, researchers have developed richly annotated corpora. The Stanford Sentiment Treebank (SST) is a paradigmatic example: it provides phrase-level polarity labels across the parse tree of each sentence, allowing models to learn compositional sentiment patterns (e.g., â€˜not goodâ€™ vs. â€˜goodâ€™)\n[\n20\n]\n. Such corpora facilitate sentence-level sentiment classification as well as aspect-level analysis, where individual entities or attributes (like â€˜acting,â€™ â€˜plot,â€™ or â€˜soundtrackâ€™) are annotated with their own sentiment tags. Even though such annotations are labor-intensive to produce, they have lead to significant advances in deep learning architectures, particularly recursive and attention-based models that can reason over tree structures and isolate sentiment-bearing subphrases.\n3\nBenchmark Datasets and Metrics\nThe evolution of movie review sentiment analysis has been fundamentally shaped by the benchmark datasets that define evaluation standards and research directions\n[\n6\n,\n20\n]\n. While these datasets have enabled remarkable progress in classification accuracy, their design choices and inherent limitations have also constrained our understanding of real-world performance across the five persistent challenges identified in this review\n[\n1\n,\n2\n]\n. This section examines the major benchmark corpora, their contributions to the fieldâ€™s development, and the emerging recognition that traditional evaluation frameworks may inadequately capture the complexity of deploying sentiment analysis systems in production environments.\n3.1\nFoundational Datasets: Establishing the Benchmark Paradigm\n3.1.1\nThe Movie Reviews Corpus: Pioneering Binary Classification\nThe seminal work by Pang and Lee (2002)\n[\n1\n]\nintroduced the Movie Reviews (MR) corpus, which became the foundational benchmark for sentiment analysis research. This dataset, containing 2,000 movie reviews equally split between positive and negative sentiment, established several conventions that would influence the field for decades: binary classification as the primary task, balanced class distribution as the evaluation standard, and accuracy as the dominant performance metric.\nThe MR corpus demonstrated that machine learning approaches could significantly outperform lexicon-based methods, with Support Vector Machines achieving approximately 82% accuracy using unigram features\n[\n1\n]\n. This result established sentiment analysis as a viable machine learning problem and created the methodological framework that would guide subsequent research. However, the relatively small size of the dataset (2,000 documents) and its focus on professional reviews from a specific time period would later prove limiting for understanding model robustness across the challenges of sarcasm detection, domain adaptation, and temporal drift.\nThe balanced 50/50 polarity split, while useful for controlled evaluation, may not reflect the natural distribution of sentiments in real-world movie review platforms\n[\n6\n]\n. This design choice, replicated across subsequent benchmarks, potentially oversimplifies the nuanced nature of movie criticism where mixed sentiments, qualified opinions, and contextual judgments are common\n[\n20\n]\n.\n3.1.2\nExpansion and Standardization: The IMDB-Large Dataset\nThe introduction of the IMDB-Large dataset by Maas et al. (2011)\n[\n6\n]\nrepresented a significant leap forward in benchmark sophistication, providing 50,000 reviews split into 25,000 training and 25,000 test samples. This dataset became the de facto standard for movie review sentiment analysis, hosted on platforms like Kaggle and Stanford AI, and enabled more robust evaluation of deep learning approaches as they emerged throughout the 2010s.\nThe IMDB-Large datasetâ€™s larger scale allowed for more sophisticated model architectures and training procedures, supporting the transition from traditional machine learning to deep learning approaches\n[\n42\n,\n36\n]\n. The datasetâ€™s availability and standardized train/test splits enabled meaningful comparison across different methodological approaches, contributing to the fieldâ€™s rapid progress from 86â€“91% accuracy with static word embeddings to 94â€“97% with transformer architectures\n[\n22\n,\n37\n]\n.\nHowever, the IMDB-Large dataset also inherited and amplified certain limitations from its predecessor. The binary classification paradigm, while computationally convenient, may inadequately capture the complexity of sentiment expression in movie reviews\n[\n20\n]\n. Professional and amateur reviewers often express nuanced opinions that resist simple positive/negative categorization, particularly when discussing different aspects of films or comparing works within specific genres or cultural contexts.\n3.2\nDiversification and Multimodal Extensions\n3.2.1\nStanford Sentiment Treebank: Phrase-Level Granularity\nThe Stanford Sentiment Treebank (SST)\n[\n20\n]\nintroduced a different perspective on sentiment analysis with 10,000 documents focused on short snippets and phrase-level sentiment analysis tasks. This dataset provided fine-grained sentiment annotations at the phrase level, enabling models to learn compositional sentiment understanding rather than document-level classification alone.\nThe SSTâ€™s contribution extends beyond simple scale to methodological sophistication, as it enables evaluation of modelsâ€™ ability to understand how sentiment composes across linguistic structures\n[\n20\n]\n. This capability is particularly relevant for handling complex rhetorical constructions common in movie reviews, where sentiment may shift or be qualified across different parts of a text. The phrase-level annotations provide insights into model behavior that document-level accuracy measurements cannot capture.\nHowever, the focus on short snippets may limit the SSTâ€™s ability to evaluate models on the long-form context processing challenge identified in this review. Movie reviews often develop arguments over extended passages, and phrase-level sentiment understanding, while valuable, may not capture the full complexity of document-level sentiment development\n[\n36\n]\n.\n3.2.2\nMultimodal Sentiment: CMU-MOSI and MOSEI\nThe emergence of multimodal datasets like CMU-MOSI and CMU-MOSEI\n[\n43\n,\n44\n]\nbrought additional complexity with 2,200â€“22,800 YouTube clips incorporating text, video, and audio sentiment analysis. These datasets convert 7-point sentiment scales to binary classifications, enabling comparison with traditional text-only benchmarks while introducing the possibility of multimodal sentiment understanding.\nThe multimodal datasets represent a significant expansion of the evaluation paradigm, acknowledging that movie reviews often reference visual and auditory elements of films that pure text analysis cannot capture\n[\n43\n]\n. A reviewer discussing â€œhaunting cinematographyâ€ or â€œjarring sound designâ€ makes references that could benefit from access to actual visual and audio content, suggesting directions for more comprehensive sentiment understanding.\nHowever, the conversion from 7-point sentiment scales to binary classifications, while enabling comparison with traditional datasets, may lose important nuances in sentiment expression that are particularly relevant for movie reviews\n[\n44\n]\n. The multimodal nature of these datasets also introduces additional complexity to evaluation, requiring metrics that can assess performance across text, video, and audio modalities simultaneously.\n3.2.3\nRecent Benchmark Expansions: GoEmotions and TweetEval\nThe year 2020 marked significant advances in sentiment benchmark development with the introduction of GoEmotions and TweetEval. GoEmotions\n[\n45\n]\n, developed by Google Research, introduced the largest manually annotated dataset for fine-grained emotion classification, comprising 58,000 Reddit comments labeled across 27 emotion categories plus neutral. Unlike previous datasets limited to basic sentiment polarity or six basic emotions, GoEmotions captures nuanced emotional states including admiration, amusement, curiosity, and disappointment, enabling more sophisticated emotion-aware applications. The datasetâ€™s taxonomy was developed in collaboration with psychologists, covering 12 positive, 11 negative, and 4 ambiguous emotion categories.\nTweetEval\n[\n46\n]\naddressed the fragmentation problem in social media NLP by unifying seven heterogeneous Twitter-specific classification tasks including sentiment analysis, emotion recognition, irony detection, and hate speech detection into a standardized evaluation framework. With over 150,000 tweets and consistent train/validation/test splits, TweetEval enables meaningful comparison across models and has become the de facto benchmark for social media sentiment analysis. The accompanying Twitter-RoBERTa model, pre-trained on 58 million tweets, established strong baselines that continue to inform research on domain-specific language model adaptation.\nThese recent datasets reflect a broader trend toward more comprehensive evaluation frameworks that capture the complexity of real-world sentiment expression beyond simple binary classification.\nTable 1:\nEvolution of Major Sentiment Analysis Benchmark Datasets.\nDataset\nYear\nSize\nSplit\nSource\nModality\nKey Innovation\nMovie Reviews (MR)\n2002\n2k\n50/50\nProf. reviews\nText\nBinary classification\nIMDB-Large\n2011\n50k\n25k/25k\nUser reviews\nText\nLarge-scale evaluation\nStanford SST\n2013\n10k\nVariable\nProf. snippets\nText\nPhrase-level granularity\nRotten Tomatoes\n2013\n10k\n50/50\nMixed\nText\nShort snippet focus\nCMU-MOSI\n2016\n2.2k\n7pt\nâ†’\n\\to\nbin\nYouTube\nT+V+A\nMultimodal integration\nCMU-MOSEI\n2018\n22.8k\n7pt\nâ†’\n\\to\nbin\nYouTube\nT+V+A\nLarge-scale multimodal\nGoEmotions\n2020\n58k\nTrain/Val/Test\nReddit\nText\n27 fine-grained emotions\nTweetEval\n2020\n150k+\nStandardized\nTwitter\nText\nUnified social media benchmark\n2002\n2005\n2008\n2011\n2014\n2017\n2020\nMovie Reviews\n2k docs\nBinary paradigm\nIMDB-Large\n50k docs\nDeep learning era\nStanford SST\nPhrase-level\ngranularity\nCMU-MOSI\nMultimodal\nintegration\nCMU-MOSEI\nLarge-scale\nmultimodal\nFoundation Era\nEstablishing paradigms\nExpansion Era\nScale and sophistication\nDiversification Era\nMultimodal integration\nFigure 2:\nTimeline of major benchmark dataset development in movie review sentiment analysis, showing the evolution from simple binary classification to complex multimodal evaluation frameworks.\n3.3\nEvaluation Metrics\n3.3.1\nThe Dominance of Accuracy and Macro-F1\nThe standardization of evaluation metrics has been crucial for the fieldâ€™s development, with accuracy and macro-F1 remaining the dominant measures across most benchmark studies\n[\n2\n]\n. Many works additionally report ROC-AUC (Receiver Operating Characteristic - Area Under Curve) or MCC (Matthews Correlation Coefficient) to handle class imbalance issues, though the balanced nature of most movie review datasets makes this less critical than in other domains.\nThe choice of evaluation metrics reflects the fieldâ€™s evolution from simple binary classification to more nuanced understanding of model performance\n[\n3\n]\n. While accuracy provides an intuitive measure of overall performance, macro-F1 offers better insights into model behavior across both positive and negative classes. ROC-AUC provides valuable information about model discrimination ability across different threshold settings, which is particularly relevant for deployment scenarios where operating points may need adjustment.\nHowever, these traditional metrics have significant limitations when evaluating models against the five persistent challenges identified in this review\n[\n25\n]\n. Standard accuracy measurements may not capture a modelâ€™s ability to handle sarcastic reviews, adapt to temporal drift, or maintain performance across different review lengths. This limitation has led to increased interest in developing more comprehensive evaluation frameworks that assess model robustness across multiple dimensions simultaneously.\n3.3.2\nThe Limitations of Binary Classification Paradigms\nThe focus on binary classification in most benchmarks, including the conversion of multi-point scales to binary labels in multimodal datasets, may oversimplify the nuanced nature of movie criticism\n[\n20\n]\n. Many reviews express mixed sentiments or qualified opinions that are difficult to capture in binary classifications. Professional critics often provide sophisticated analyses that appreciate certain aspects of films while criticizing others, creating sentiment expressions that resist simple categorization.\nThe binary paradigm also limits evaluation of modelsâ€™ ability to handle the explainability challenge, as it provides limited insight into whether models are making predictions for appropriate reasons\n[\n25\n]\n. A model might achieve high accuracy while relying on spurious correlations or biased patterns in training data, leading to unreliable performance when deployed in different domains or time periods.\nTable 2:\nCommon Evaluation Metrics and Their Limitations for Persistent Challenges.\nMetric\nFormula/Description\nAdvantages\nLimitations for Challenges\nAccuracy\nT\nâ€‹\nP\n+\nT\nâ€‹\nN\nT\nâ€‹\nP\n+\nT\nâ€‹\nN\n+\nF\nâ€‹\nP\n+\nF\nâ€‹\nN\n\\frac{TP+TN}{TP+TN+FP+FN}\nIntuitive, widely used\nCannot detect sarcasm failures, domain drift, or bias issues\nMacro-F1\n1\n2\nâ€‹\n(\nF\nâ€‹\n1\np\nâ€‹\no\nâ€‹\ns\n+\nF\nâ€‹\n1\nn\nâ€‹\ne\nâ€‹\ng\n)\n\\frac{1}{2}(F1_{pos}+F1_{neg})\nBalanced class performance\nIgnores explainability and resource efficiency\nROC-AUC\nArea under ROC curve\nThreshold-independent\nNo insight into robustness across domains/time\nMCC\nT\nâ€‹\nP\nâ‹…\nT\nâ€‹\nN\nâˆ’\nF\nâ€‹\nP\nâ‹…\nF\nâ€‹\nN\n(\nT\nâ€‹\nP\n+\nF\nâ€‹\nP\n)\nâ€‹\n(\nT\nâ€‹\nP\n+\nF\nâ€‹\nN\n)\nâ€‹\n(\nT\nâ€‹\nN\n+\nF\nâ€‹\nP\n)\nâ€‹\n(\nT\nâ€‹\nN\n+\nF\nâ€‹\nN\n)\n\\frac{TP\\cdot TN-FP\\cdot FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\nHandles imbalance well\nMissing context processing and efficiency assessment\n3.4\nEmerging Recognition of Benchmark Limitations\n3.4.1\nThe Saturation Effect and Diminishing Returns\nThe progression from approximately 82% accuracy with bag-of-words approaches to 97â€“98% with large language models represents remarkable technical achievement, yet this performance increase has revealed a critical insight: traditional benchmarks may be approaching their utility limits for driving meaningful research progress\n[\n38\n,\n39\n]\n. As models achieve near-perfect performance on standard benchmarks, the five persistent challenges become more apparent and more critical for real-world deployment success.\nThis performance saturation suggests that future progress requires moving beyond accuracy optimization toward holistic approaches that address multiple challenges simultaneously\n[\n29\n]\n. The diminishing returns from pure accuracy improvements have redirected research attention toward robustness, interpretability, and practical deployment considerations that existing benchmarks may not adequately capture.\n3.4.2\nDomain and Temporal Representation Gaps\nMost benchmark datasets represent snapshots from particular time periods and platforms, potentially limiting their ability to capture temporal drift and domain variation that characterize real-world deployment scenarios\n[\n21\n]\n. The IMDB-Large dataset, despite its size and influence, reflects language patterns and cultural references from its collection period, which may not generalize to contemporary movie criticism or different platforms.\nThe focus on English-language reviews from primarily Western cultural contexts also limits benchmark utility for understanding cross-cultural sentiment expression and the challenges of deploying sentiment analysis systems globally\n[\n2\n]\n. Movie criticism reflects diverse cultural perspectives and linguistic patterns that may not be captured in existing benchmarks, suggesting need for more diverse and culturally representative evaluation frameworks.\n3.4.3\nInadequate Evaluation of Persistent Challenges\nTraditional benchmarks provide limited assessment of model performance on the five persistent challenges identified in this review. Sarcasm detection capabilities are not systematically evaluated, domain adaptation robustness is not measured across different platforms or time periods, and explainability is not assessed through standard accuracy metrics\n[\n25\n,\n23\n]\n.\nThe resource efficiency challenge is particularly underserved by current benchmarks, which typically focus on maximizing accuracy without considering computational constraints, latency requirements, or energy consumption that characterize real-world deployment scenarios\n[\n26\n,\n27\n]\n. This gap between benchmark evaluation and deployment reality has led to systems that perform excellently in research settings but struggle in production environments.\n3.5\nToward More Comprehensive Evaluation Frameworks\n3.5.1\nMulti-Dimensional Assessment Needs\nThe fieldâ€™s maturation demands evaluation approaches that move beyond traditional accuracy measurements toward comprehensive assessment of model capabilities across multiple dimensions\n[\n29\n]\n. Future benchmark development should incorporate explicit evaluation of sarcasm detection, domain adaptation, temporal robustness, explainability, and resource efficiency alongside traditional accuracy metrics.\nTemporal robustness testing, where models are evaluated on reviews from different time periods, would provide insights into adaptation capabilities that current benchmarks cannot capture\n[\n21\n]\n. Cross-domain evaluation using reviews from different platforms and contexts would assess generalization abilities that are critical for real-world deployment but underexplored in current research.\n3.5.2\nAdversarial and Stress Testing\nSystematic evaluation of model robustness through adversarial examples and stress testing represents another direction for benchmark evolution\n[\n25\n]\n. Current benchmarks provide limited assessment of how models handle edge cases, deliberate attempts at deception, or inputs that are designed to reveal failure modes.\nSarcasm-specific evaluation metrics that go beyond simple accuracy to assess whether models are identifying sarcastic content for appropriate reasons would provide more meaningful insights into model capabilities\n[\n23\n]\n. Similarly, explainability metrics that evaluate whether model explanations align with human reasoning about sentiment would be valuable for building trust in deployed systems.\n3.5.3\nResource-Aware Evaluation\nThe development of evaluation frameworks that explicitly consider resource constraints represents a critical need for bridging the gap between research and deployment\n[\n26\n]\n. These frameworks should assess model performance across different computational budgets, enabling direct comparison of accuracy-efficiency trade-offs that are central to real-world decision making.\nLatency-aware evaluation, energy consumption measurement, and model size constraints should be incorporated into benchmark standards to ensure that research progress translates to deployable systems\n[\n27\n]\n. This shift would encourage development of models that balance multiple performance dimensions rather than optimizing accuracy alone.\n3.6\nThe Path Forward: Integrated Benchmark Design\nThe evolution of movie review sentiment analysis benchmarks reflects the fieldâ€™s maturation from simple classification problems to complex real-world deployment challenges\n[\n2\n,\n3\n]\n. While traditional datasets like MR and IMDB-Large have been invaluable for standardizing evaluation and enabling reproducible research, their limitations have become apparent as the field approaches the frontiers defined by the five persistent challenges.\nFuture benchmark development should embrace integrated evaluation frameworks that assess multiple dimensions of model performance simultaneously, moving beyond accuracy optimization toward holistic system design\n[\n29\n]\n. This evolution requires collaboration between academic researchers and industry practitioners to ensure that evaluation standards reflect both theoretical advances and practical deployment realities.\nThe success of movie review sentiment analysis as a research domain demonstrates the power of well-designed benchmarks to drive sustained progress over decades. As the field continues to evolve, the development of more comprehensive, challenging, and realistic evaluation frameworks will be essential for addressing the persistent challenges that define the next frontier of sentiment analysis research and deployment.\n4\nCurrent Sentiment Analysis Models and Methods\nSentiment analysis on movie reviews has progressed from lexicon-based heuristics that barely outperformed chance to transformers surpassing 96% accuracy on the 50k-review IMDb benchmark. Accuracy gains came in four waves lexicons, classic ML, deep neural networks, and large-scale pre-trained transformers each wave improving speed, data efficiency, and interpretability.\n4.1\nRule-Based and Lexicon-Based Methods\nRule-based and lexicon-based methods form the earliest category of sentiment analysis techniques. These methods operate by leveraging sentiment lexicons precompiled lists of words annotated with polarity values to assess the sentiment of a given text without requiring supervised learning. Despite being conceptually simple and interpretable, these methods face limitations in handling domain-specific expressions, sarcasm, and compositionality.\n4.1.1\nSentiWordNet\nSentiWordNet\n[\n47\n]\nis a lexical resource derived from WordNet, where each synset (set of synonyms) is assigned three sentiment scores: positivity, negativity, and objectivity. These scores are determined through semi-supervised learning techniques and allow for a graded view of sentiment intensity. In the context of movie reviews, SentiWordNet has been used to compute sentiment scores by aggregating the polarity of terms found in the review text\n[\n48\n]\n. However, its effectiveness depends heavily on accurate word sense disambiguation a nontrivial challenge, especially in informal or figurative language common in user-generated movie reviews.\n4.1.2\nVADER\nVADER (Valence Aware Dictionary and sEntiment Reasoner) is a rule-based sentiment analysis tool specifically tuned for social media and short text contexts\n[\n49\n]\n. It combines a human-validated lexicon with grammatical heuristics (e.g., punctuation, capitalization, degree modifiers) to infer sentiment. Unlike traditional lexicon-based approaches, VADER performs well even without prior domain adaptation, making it suitable for lightweight applications such as movie review summarization or preliminary filtering\n[\n49\n]\n. Nonetheless, its performance degrades when faced with domain-specific jargon or multi-aspect sentiment common in longer movie reviews.\n4.1.3\nLIWC\nThe Linguistic Inquiry and Word Count (LIWC) tool is a psycholinguistic lexicon that maps words to psychologically meaningful categories, including affective processes such as positive and negative emotion\n[\n50\n]\n. While not originally designed for sentiment classification, LIWC has been widely applied in the analysis of movie reviews and online discourse to reveal patterns of emotional expression\n[\n51\n]\n. LIWCâ€™s strength lies in its interpretability and grounding in psychological theory, but it lacks coverage for modern slang, idiomatic expressions, and evolving linguistic trends, which limits its applicability in dynamic domains like film critique.\n4.1.4\nAdvantages and Limitations\nLexicon-based methods are advantageous due to their interpretability, domain-independence (at least initially), and low computational cost. They are particularly useful in low-resource settings where labeled data is scarce. However, their major limitations include:\nâ€¢\nLack of domain adaptation:\nLexicons do not capture domain-specific usage (e.g., â€œdarkâ€ may be neutral in general language but positive in the context of horror films).\nâ€¢\nInability to model context:\nThese methods typically ignore syntactic and semantic context, leading to incorrect sentiment predictions in the presence of negation or sarcasm\n[\n1\n]\n.\nâ€¢\nStatic word representations:\nLexicon scores are fixed and do not account for polysemy or word sense variation across contexts.\nDespite these limitations, rule-based approaches continue to serve as strong baselines and are often integrated with machine learning systems to provide interpretable explanations or initial polarity signals.\n4.2\nTraditional Machine Learning Approaches\nPrior to the advent of deep learning, sentiment analysis on movie reviews was predominantly approached as a supervised text classification problem using traditional machine learning algorithms. These methods required careful feature engineering to transform raw text into structured representations, followed by the application of classification algorithms such as NaÃ¯ve Bayes\n[\n52\n]\n, Support Vector Machines (SVMs)\n[\n53\n]\n, and Logistic Regression\n[\n54\n]\n. Though largely surpassed by neural methods in recent years, traditional models remain relevant for their interpretability, speed, and competitive performance on smaller or well-curated datasets.\n4.2.1\nFeature Engineering for Sentiment Representation\nOne of the foundational steps in traditional sentiment analysis pipelines is the transformation of text into numerical features. The simplest and most common representations include bag-of-words (BoW) and\nn\nn\n-gram models. Pang et al.\n[\n1\n]\ndemonstrated that unigram and bigram features, when combined with frequency-based weighting schemes, can achieve high accuracy on movie reviews.\nTerm Frequency-Inverse Document Frequency (TF-IDF) is another widely used weighting technique, which downweights commonly occurring words while emphasizing informative terms\n[\n55\n]\n. TF-IDF has been shown to outperform raw frequency counts by better capturing the discriminative power of specific sentiment-bearing words.\nIn addition to frequency-based features, part-of-speech (POS) tags have been incorporated to emphasize sentiment-heavy word classes such as adjectives and adverbs. Whitelaw et al.\n[\n56\n]\nproposed enriching BoW models with POS-tagged phrases, leading to improved performance in polarity classification tasks. Feature engineering in traditional pipelines may also include stemming or lemmatization to reduce word form variation, and syntactic parsing to extract structured linguistic patterns.\n4.2.2\nClassification Algorithms\nAmong the earliest and most effective classifiers for sentiment analysis is the Multinomial NaÃ¯ve Bayes (MNB) algorithm. Despite its simplistic conditional independence assumption, MNB performs competitively on textual data due to its robustness and efficiency\n[\n57\n]\n. Pang et al.\n[\n1\n]\nfound NaÃ¯ve Bayes to perform reasonably well on movie reviews, though it was often outperformed by more discriminative models.\nSupport Vector Machines (SVMs) quickly became the dominant method for sentiment classification due to their ability to handle high-dimensional sparse data and to find optimal separating hyperplanes in feature space\n[\n55\n]\n. Linear SVMs, in particular, are well-suited for text classification tasks where data points lie in a high-dimensional feature space. In experiments on the IMDb dataset, SVMs consistently outperformed NaÃ¯ve Bayes and Logistic Regression when paired with appropriate feature selection\n[\n1\n]\n.\nLogistic Regression is another frequently used classifier due to its probabilistic output and ability to be interpreted in terms of feature weights. It is often preferred in settings where decision thresholds or confidence scores are required. While generally competitive with SVMs, Logistic Regression may suffer when the classes are not linearly separable, especially in noisy review corpora\n[\n58\n]\n.\n4.2.3\nStrengths and Limitations\nTraditional machine learning methods offer several advantages. They are computationally efficient and interpretable, allowing for fine control over the modeling pipeline. Feature importance can be directly extracted from model coefficients, making them suitable for use cases that require explainability.\nHowever, these methods suffer from notable limitations. Their performance is heavily dependent on the quality of feature engineering and cannot easily model long-distance dependencies, negation, or compositionality in language. For instance, detecting that â€œnot a great movieâ€ carries negative sentiment requires more than just counting positive words. Furthermore, traditional classifiers struggle with domain adaptation and sarcasm detection due to their inability to capture contextual semantics\n[\n1\n]\n.\nDespite these drawbacks, traditional models laid the foundational groundwork for sentiment analysis and continue to serve as strong baselines and components in hybrid systems.\n4.3\nDeep Learning-Based Methods\nDeep learning has significantly advanced the state of sentiment analysis by enabling end-to-end learning of hierarchical and contextual representations from raw text. Unlike traditional machine learning approaches that rely heavily on manual feature engineering, deep neural networks can automatically learn semantic and syntactic features that are critical for sentiment classification. In the domain of movie reviews, deep learning methods have demonstrated superior performance across multiple benchmarks by capturing compositional sentiment, long-range dependencies, and subtle linguistic cues.\n4.3.1\nConvolutional Neural Networks (CNNs)\nConvolutional Neural Networks (CNNs), initially developed for computer vision, have been successfully adapted for text classification tasks such as sentiment analysis. Kim\n[\n42\n]\nshowed that a simple CNN with a single convolutional layer applied to pre-trained word vectors (e.g., Word2Vec) can achieve competitive results on multiple sentiment benchmarks, including the IMDb movie review dataset. CNNs are particularly effective at capturing local patterns such as sentiment-bearing phrases (e.g., â€œutterly disappointingâ€, â€œbrilliant performanceâ€) by applying filters over word sequences. Their parallelizable architecture and relatively shallow depth also make them computationally efficient.\n4.3.2\nRecurrent Neural Networks (RNNs), LSTMs, and BiLSTMs\nRecurrent Neural Networks (RNNs) are designed to model sequential dependencies in text, making them well-suited for tasks where word order and context matter. However, traditional RNNs suffer from vanishing and exploding gradient problems, which limit their ability to capture long-term dependencies\n[\n59\n]\n. To overcome this, Long Short-Term Memory (LSTM) networks were introduced\n[\n35\n]\n. LSTMs use gating mechanisms to regulate the flow of information, enabling them to remember sentiment-relevant features over longer spans of text.\nBidirectional LSTMs (BiLSTMs) further enhance performance by processing the input sequence in both forward and backward directions, thereby incorporating both past and future context\n[\n60\n]\n. In sentiment classification of movie reviews, where sentiment may depend on distant contextual cues (e.g., â€œAlthough the film starts slow, it ultimately delivers a powerful messageâ€), BiLSTMs provide a robust mechanism for modeling such dependencies.\n4.3.3\nWord Embeddings: Word2Vec and GloVe\nA crucial component of deep learning-based sentiment models is the use of distributed word representations or embeddings. Word2Vec\n[\n61\n]\n, trained using skip-gram or CBOW objectives, captures semantic similarity by placing similar words in nearby vector space. GloVe\n[\n34\n]\n, on the other hand, leverages global co-occurrence statistics to produce embeddings that better capture linear substructures. These embeddings are often used as input to CNNs or RNNs and can be fine-tuned during training for improved task-specific performance. Pre-trained embeddings alleviate data sparsity issues and help models generalize better on small or imbalanced movie review datasets.\n4.3.4\nAttention Mechanisms\nWhile RNNs and CNNs are capable of learning useful features, they often compress long sequences into a single vector, potentially losing important information. Attention mechanisms address this limitation by allowing the model to focus selectively on relevant parts of the input when making predictions\n[\n62\n]\n. In sentiment analysis, attention helps identify sentiment-bearing phrases or clauses even if they are distant from each other in the input. Yang et al.\n[\n36\n]\nproposed a Hierarchical Attention Network (HAN) that applies attention at both the word and sentence levels, achieving strong performance on document-level sentiment classification tasks, including movie reviews.\n4.3.5\nSummary and Limitations\nDeep learning methods significantly outperform traditional approaches in sentiment classification, particularly on large datasets. They offer the ability to model complex linguistic phenomena such as negation, sarcasm, and compositionality. However, they require substantial computational resources and are often data-hungry. Moreover, deep models are typically less interpretable than their traditional counterparts, making it challenging to understand their decision-making processes without auxiliary explanation methods.\n4.4\nTransformer-Based Models\nThe introduction of transformer architectures\n[\n24\n]\nhas revolutionized the field of natural language processing, including sentiment analysis of movie reviews. Unlike traditional RNN-based models, transformers leverage self-attention mechanisms to model long-range dependencies in text efficiently, without relying on sequential data processing.\n4.4.1\nPretrained Transformer Models\nOne of the earliest and most impactful transformer-based models is BERT (Bidirectional Encoder Representations from Transformers)\n[\n22\n]\n. BERT is pretrained on large-scale corpora using masked language modeling and next sentence prediction, and can be fine-tuned on downstream tasks such as sentiment classification with minimal architectural modifications. In the context of movie reviews, fine-tuning BERT on datasets like IMDb has yielded state-of-the-art results in binary sentiment classification\n[\n63\n]\n.\nVariants such as RoBERTa\n[\n37\n]\nimproved upon BERT by optimizing pretraining strategies, including removal of the next sentence prediction objective and training on larger corpora with dynamic masking. These modifications have led to improved sentiment classification performance across benchmarks. DistilBERT\n[\n64\n]\n, a compressed version of BERT, provides a trade-off between inference speed and accuracy, making it suitable for real-time applications without substantial degradation in accuracy. XLNet\n[\n65\n]\n, a generalized autoregressive pretraining model, overcomes some limitations of BERT by modeling bidirectional context while retaining autoregressive properties, demonstrating competitive performance on sentiment tasks.\n4.4.2\nDomain-Specific Transformers\nAlthough general-purpose pretrained transformers offer strong baselines, domain-adapted transformer models have emerged to capture nuances specific to movie reviews. BERT models for sentiment analysis in different contexts have gained prominence including sentiment analysis for microblogging platforms like Twitter, demonstrating that BERT combined with neural network architectures (CNN, RNN, BiLSTM) achieves superior performance compared to traditional Word2vec approaches\n[\n66\n]\n. Batra et al.\n[\n67\n]\nhave applied BERT-based models to software engineering contexts, analyzing sentiment in GitHub comments, Jira comments, and Stack Overflow posts, where ensemble BERT models and compressed BERT variants showed 6â€“12% improvement in F1-scores over existing tools like SentiCR and SentiStrength-SE. Penha et al.\n[\n68\n]\ninvestigated how much factual knowledge about recommendation items (books, movies, music) is stored in pre-trained BERT models and whether this knowledge can be leveraged for Conversational Recommender Systems (CRS). Their study finds that BERT contains substantial content-based knowledge about items but has limited collaborative knowledge, and while it can perform basic recommendation tasks, it struggles with conversational recommendations when faced with challenging data. More recently, BERT has been combined with BiLSTM for movie review sentiment analysis\n[\n69\n]\n, achieving better accuracy than existing state-of-the-art methods and demonstrating how the approach can predict overall movie sentiment for recommendation systems.\n4.4.3\nPrompt-Based and Zero-Shot Models\nWith the advent of generative pretrained transformers like T5\n[\n70\n]\nand GPT\n[\n38\n]\n, few-shot and zero-shot sentiment classification has become increasingly feasible. These models can be prompted using task-specific instructions, enabling them to perform classification tasks without extensive fine-tuning. For example, a prompt like â€œClassify the sentiment of this review: â€˜The movie was breathtaking and unforgettable.â€â€™ can elicit accurate sentiment predictions even with minimal supervision. However, prompt-based performance is often sensitive to prompt wording and may require careful calibration\n[\n71\n]\n.\n4.4.4\nStrengths and Limitations\nTransformer-based models offer several advantages for sentiment classification, including superior accuracy, transferability to low-resource settings, and the ability to model complex syntactic and semantic relationships. However, they are not without limitations. Large models such as BERT and GPT-3 are computationally expensive to train and deploy, and their performance may degrade when faced with domain-specific jargon or informal language unless further fine-tuned. Additionally, explainability remains a significant challenge, especially in high-stakes applications where interpretability of predictions is critical.\n4.5\nLarge Language Models and the New Paradigm\nThe emergence of Large Language Models (LLMs) such as GPT-4, Llama, and Claude has fundamentally transformed the landscape of sentiment analysis, introducing capabilities that extend far beyond traditional fine-tuning paradigms.\n4.5.1\nZero-Shot and Few-Shot Sentiment Classification\nA comprehensive evaluation by Zhang et al.\n[\n72\n]\nacross 13 sentiment analysis tasks on 26 datasets revealed that while LLMs demonstrate satisfactory performance in simpler sentiment classification tasks, they lag behind fine-tuned small language models (SLMs) in more complex tasks requiring deeper understanding of specific sentiment phenomena. However, LLMs significantly outperform SLMs in few-shot learning settings, suggesting their potential when annotation resources are limited. The study introduced SentiEval, a benchmark for more comprehensive evaluation of LLM sentiment capabilities.\nKrugmann and Hartmann\n[\n73\n]\nbenchmarked GPT-3.5, GPT-4, and Llama 2 against established transfer learning models for marketing research applications. Their findings indicate that despite their zero-shot nature, LLMs can not only compete with but in some cases surpass traditional transfer learning methods in sentiment classification accuracy. The study also highlighted that prompt specificity significantly affects prediction consistency, with explicitly structured prompts yielding greater reliability.\nRecent work on multilingual sentiment analysis has demonstrated GPTâ€™s effectiveness across 12 languages, with performance comparable to or exceeding top-performing fine-tuned models from previous years\n[\n74\n]\n. GPT-4 achieved correlations of\nr\n=\n0.59\nr=0.59\nto\n0.77\n0.77\nwith human annotators, substantially outperforming traditional dictionary-based methods (\nr\n=\n0.20\nr=0.20\nto\n0.30\n0.30\n).\n4.5.2\nChain-of-Thought Prompting for Sentiment\nChain-of-thought (CoT) prompting has emerged as a powerful technique for improving LLM performance on sentiment tasks requiring reasoning. Fei et al.\n[\n75\n]\nintroduced the Three-hop Reasoning (THOR) framework for implicit sentiment analysis, which mimics human-like reasoning by step-by-step inducing the implicit aspect, opinion, and finally the sentiment polarity. This approach addresses the challenge of detecting sentiment where opinion cues are implicit and obscure, requiring common-sense and multi-hop reasoning to infer latent intent.\nThe THOR framework demonstrates that structured reasoning chains can significantly improve performance on nuanced sentiment tasks, particularly for sarcasm and implicit sentiment where traditional classification approaches struggle. This represents a paradigm shift from pattern matching to genuine reasoning about sentiment.\n4.5.3\nInstruction-Tuned Models for Aspect-Based Sentiment\nInstructABSA\n[\n76\n]\nintroduced an instruction learning paradigm for Aspect-Based Sentiment Analysis (ABSA) that achieves state-of-the-art results across multiple subtasks. By incorporating positive, negative, and neutral examples into training samples and instruction-tuning the Tk-Instruct model, InstructABSA outperformed previous SOTA on the Rest14 ATE subtask by 5.69% points and the Rest15 ATSC subtask by 9.59% points, surpassing models 7\nÃ—\n\\times\nlarger. Notably, just 50% of training data was sufficient to achieve competitive results, demonstrating remarkable sample efficiency.\n4.5.4\nLimitations and Emerging Challenges\nDespite their impressive capabilities, LLMs present new challenges for sentiment analysis. Model outputs exhibit sensitivity to prompt wording, with even small changes in phrasing leading to different sentiment classifications\n[\n72\n]\n. Reproducibility remains a concern, as temperature settings and model versions can affect result consistency. Furthermore, the computational cost of deploying LLMs at scale poses practical limitations for real-time sentiment monitoring applications.\nThe question of whether LLMs truly â€œunderstandâ€ sentiment or merely exploit statistical patterns remains open. While chain-of-thought prompting improves interpretability, the black-box nature of these models continues to challenge deployment in high-stakes domains requiring explainable predictions.\nYear\nAccuracy (%)\n80\n85\n90\n95\n100\n2005\n2010\n2015\n2020\n2025\nPerformance\nSaturation Zone\nPeak Accuracy\n- -\nChallenge Awareness\nBag-of-Words\n82%\nRNNs\n90â€“93%\nTransformers\n94â€“97%\nLLMs\n97â€“98%\nFigure 3:\nThe progression of peak accuracy on IMDB benchmarks versus growing awareness of persistent challenges. As accuracy approaches saturation, attention shifts to robustness, explainability, and deployment considerations.\n5\nDomain-Specific Challenges\nDespite the significant advancements in sentiment analysis, a number of domain-specific challenges continue to hinder the accuracy and robustness of models. These challenges are particularly pronounced in real-world applications such as e-commerce, movie reviews, and social media analytics, where language use is highly dynamic and context-dependent.\n5.1\nSarcasm and Irony Detection\nOne of the most persistent challenges in sentiment classification is the accurate detection of sarcasm and irony. Sarcasm often conveys a sentiment opposite to the literal meaning of the text, making traditional sentiment classifiers fail when relying on lexical cues alone. For example, the statement\nâ€œGreat, another three-hour delay!â€\nexpresses negative sentiment despite containing the positive word â€œgreat.â€ Studies have shown that sarcasm significantly reduces classification accuracy when left unaddressed\n[\n77\n]\n. Recent work leverages contextual embeddings and attention mechanisms to better capture incongruity between expected and actual sentiment\n[\n78\n]\n. However, sarcasm detection remains challenging due to its dependence on pragmatic cues, cultural background, and shared speaker-listener knowledge\n[\n79\n]\n.\n5.2\nDomain Drift\nDomain drift refers to the deterioration in model performance when applying sentiment classifiers trained on one domain to another. For instance, models trained on product reviews may not generalize effectively to movie or restaurant reviews due to domain-specific vocabulary and context\n[\n21\n]\n. Domain adaptation techniques, including adversarial learning and pivot-based feature transfer, have been proposed to mitigate this issue\n[\n80\n]\n. Nevertheless, achieving robustness across domains is particularly challenging when labeled data in the target domain is scarce.\n5.3\nTemporal Drift\nLanguage evolves over time, and with it, the expressions of sentiment also change. Temporal drift refers to the decline in model accuracy as a result of evolving slang, memes, and cultural references\n[\n81\n]\n. For example, terms like â€œsickâ€ or â€œfireâ€ may shift from negative to positive sentiment over time. Research has demonstrated that models trained on static data often underperform on more recent corpora\n[\n82\n]\n. Addressing this requires continual learning approaches that update model parameters in response to new linguistic trends, though such solutions often face the risk of catastrophic forgetting as first explained in the seminal 1989 paper by McCloskey et al.\n[\n83\n]\n.\n5.4\nLong-form Context\nAnother key challenge is sentiment analysis over long-form content such as blogs, articles, and movie scripts. Unlike short reviews or tweets, longer texts often contain mixed sentiments across different sections, requiring hierarchical models that capture local and global dependencies\n[\n36\n]\n. Models such as Hierarchical Attention Networks (HANs) have been shown to improve performance by aggregating sentence-level embeddings into document-level representations\n[\n36\n]\n. However, accurately modeling sentiment across long documents remains computationally intensive and often struggles with discourse-level phenomena such as contrastive statements and sentiment shifts within the same text\n[\n84\n]\n.\n5.5\nCultural and Language Diversity\nSentiment expression varies widely across languages and cultures. Words, idioms, and symbols that carry strong sentiment in one culture may not translate equivalently into another\n[\n85\n]\n. For instance, the interpretation of emojis varies significantly between cultural contexts\n[\n86\n]\n. Cross-lingual sentiment analysis approaches, including multilingual embeddings and machine translation-based methods, attempt to address this challenge\n[\n87\n]\n. However, even state-of-the-art multilingual transformers such as XLM-R struggle with low-resource languages and culture-specific idiomatic expressions\n[\n88\n]\n. This diversity highlights the need for culturally adaptive sentiment models that integrate both linguistic and socio-cultural knowledge.\nIn practice, these domain-specific challenges rarely occur in isolation. Sarcasm may intertwine with cultural idioms, temporal drift may alter how irony is expressed, and domain drift may exacerbate the failure to detect nuanced sentiment in multimodal reviews. For instance, a sarcastic movie review written in Hinglish (mixture of Hindi and English) on YouTube in 2023 might be nearly impossible to classify correctly using a model trained on English-only IMDB reviews from 2010. This interplay underscores the necessity for holistic approaches that combine sarcasm detection, domain adaptation, temporal robustness, and multilingual modeling into unified frameworks\n[\n87\n]\n. Addressing these intertwined challenges is a key direction for future research in sentiment analysis of movie reviews.\n6\nFuture Research Directions\nThe field of sentiment analysis on movie reviews continues to evolve rapidly, with several emerging research challenges and opportunities shaping its future trajectory. In this section, we discuss key areas that demand attention, including improved benchmarks, few-shot and zero-shot learning, cross-lingual and cross-domain transfer, explainability, multimodal extensions, and fairness in sentiment analysis.\n6.1\nImproving Benchmarks\nAlthough numerous benchmark datasets exist for sentiment analysis, they often fail to capture nuanced linguistic phenomena such as sarcasm, irony, or evolving cultural expressions\n[\n89\n,\n77\n]\n. Benchmark datasets also rarely incorporate domain and temporal drift\n[\n6\n]\n, which limits the generalizability of models when exposed to changing review trends and movie discourse. Future benchmarks need to incorporate long-form contextual reviews\n[\n90\n]\nand scenarios that require deeper discourse understanding rather than sentence-level polarity predictions. Moreover, evaluation frameworks should integrate dimensions of explainability\n[\n91\n]\nand computational efficiency\n[\n92\n]\nto ensure real-world applicability in resource-constrained environments.\nTable 3:\nProposed Requirements for Next-Generation Movie Review Sentiment Analysis Benchmarks.\nChallenge\nCurrent Gap\nProposed Evaluation\nSarcasm Detection\nNo systematic evaluation of sarcastic content\nDedicated sarcasm subset with explanation requirements\nDomain Drift\nSingle platform/source evaluation only\nMulti-platform evaluation with cross-domain generalization\nTemporal Drift\nStatic datasets from specific time periods\nTemporal robustness testing across multiple time periods\nLong-form Context\nTruncation or chunking of longer reviews\nFull document processing evaluation metrics\nExplainability\nNo explanation assessment in standard metrics\nHuman-aligned explanation evaluation frameworks\nResource Efficiency\nAccuracy optimization without cost consideration\nPareto frontier evaluation (accuracy vs. efficiency)\n6.2\nBeyond Few-shot: Emerging LLM Challenges\nWhile few-shot and zero-shot learning approaches have rapidly matured with the advent of large language models\n[\n38\n,\n72\n,\n93\n]\n, new challenges have emerged. Current LLMs demonstrate strong performance on standard sentiment benchmarks but struggle with complex tasks requiring deeper understanding of specific sentiment phenomena\n[\n72\n]\n. Key open problems include: (1) reducing sensitivity to prompt wording, where minor phrasing changes can dramatically alter predictions\n[\n73\n]\n; (2) ensuring reproducibility across model versions and temperature settings; (3) developing efficient inference strategies for real-time applications given the computational cost of LLM deployment; and (4) addressing the gap between benchmark performance and real-world robustness on domain-specific jargon, evolving language, and adversarial inputs. Chain-of-thought prompting\n[\n75\n,\n94\n]\nand prompt tuning approaches\n[\n95\n]\noffer promising directions for improving reasoning on implicit sentiment, but systematic evaluation frameworks for such approaches remain underdeveloped.\n6.3\nCross-lingual and Cross-domain Transfer\nMovie reviews are inherently multilingual and cross-cultural, yet much of the sentiment analysis research has focused on English-only datasets, like the seminal product review paper by Hu et al.\n[\n96\n]\n. Cross-lingual transfer learning using multilingual models such as XLM-R\n[\n88\n]\nor mBERT\n[\n22\n]\noffers promising directions for extending sentiment analysis systems to underrepresented languages. Furthermore, cross-domain transfer is essential for adapting models trained on professional critic reviews to informal user reviews or social media commentary\n[\n80\n]\n. Addressing these challenges requires robust domain adaptation and multilingual embedding techniques to ensure inclusivity and applicability across diverse populations.\n6.4\nExplainable Sentiment Models\nExplainability remains a crucial area of research for sentiment analysis, especially in high-stakes applications such as content recommendation or censorship\n[\n91\n]\n. While attention-based mechanisms\n[\n24\n]\nand post-hoc interpretability tools\n[\n25\n]\nhave been explored, they often provide incomplete or misleading explanations. Future research must focus on building inherently interpretable sentiment models that allow end-users to understand why a particular review is classified as positive, negative, or mixed. Such approaches would also enable the identification of biases in training data and the reduction of spurious correlations.\n6.5\nMultimodal and Conversational Sentiment\nAs user-generated reviews increasingly take multimodal forms, incorporating audio, visual, and textual signals into sentiment analysis has become essential\n[\n43\n,\n97\n,\n98\n]\n. Multimodal transformers such as MMBERT\n[\n99\n]\nrepresent promising directions for fusing diverse modalities. Additionally, conversational sentiment analysis, where reviewers discuss movies in interactive settings (e.g., podcasts, live streams, or interviews), demands models that can track sentiment across multiple speakers and temporal contexts\n[\n100\n]\n. These directions open new opportunities for richer, context-aware sentiment understanding.\n6.6\nBias and Fairness in Sentiment Analysis Systems\nBias and fairness present pressing ethical challenges in sentiment analysis\n[\n101\n]\n. Models trained on imbalanced review datasets may propagate stereotypes, amplify biases, or underperform on minority languages and cultural expressions. For instance, sentiment polarity markers may vary across dialects or sociolects\n[\n102\n]\n, leading to systematic misclassification. Future research must focus on fairness-aware learning frameworks, debiasing strategies, and representative datasets to ensure equitable performance across demographic and linguistic groups.\n7\nConclusion\nThe domain of movie reviews has consistently served as a proving ground for advances in sentiment analysis, pushing the boundaries of natural language understanding. Since the seminal work by Pang et al.\n[\n1\n]\n, movie review datasets have acted as the canonical benchmark for testing supervised and unsupervised methods, ranging from early bag-of-words and SVM classifiers\n[\n31\n,\n30\n]\nto modern pre-trained transformers, multimodal models, and large language models\n[\n22\n,\n103\n,\n37\n,\n72\n]\n. The diversity and richness of movie reviews ranging from concise star ratings to nuanced long-form critiques have forced researchers to grapple with fundamental challenges such as sarcasm, irony, domain drift, temporal shifts, and cultural variation\n[\n89\n,\n104\n,\n105\n,\n77\n]\n. These challenges not only stress-test algorithms but also advance the state of the art in natural language processing more broadly.\nThe emergence of large language models has fundamentally transformed the sentiment analysis landscape. While LLMs demonstrate impressive zero-shot and few-shot capabilities that rival or exceed fine-tuned models on standard benchmarks\n[\n73\n,\n74\n]\n, they also introduce new challenges around prompt sensitivity, reproducibility, and computational cost. Chain-of-thought prompting\n[\n75\n]\nand instruction-tuned models\n[\n76\n]\nrepresent promising directions for improving reasoning on nuanced sentiment tasks, particularly for implicit sentiment and aspect-based analysis. However, comprehensive evaluation frameworks like SentiEval\n[\n72\n]\nreveal that LLMs still lag behind specialized models on complex structured sentiment tasks, suggesting that the field has not yet reached a definitive solution.\nLooking ahead, sentiment understanding in the context of creative media remains uniquely positioned to influence the trajectory of research. The development of fine-grained emotion datasets like GoEmotions\n[\n45\n]\nand unified social media benchmarks like TweetEval\n[\n46\n]\nreflects a broader recognition that binary sentiment classification is insufficient for real-world applications. Cross-lingual transfer learning\n[\n88\n,\n106\n]\npromises to extend sentiment systems to under-resourced languages, while research in explainability\n[\n91\n,\n107\n]\nand bias mitigation\n[\n102\n,\n101\n]\nis critical for ensuring that sentiment models remain fair and interpretable.\nMoreover, as multimodal and conversational sentiment analysis continues to evolve\n[\n19\n,\n97\n,\n108\n]\n, the movie domain will provide a natural testbed for models that must integrate text, audio, and visual signals to capture audience reactions or understand narrative tone. Such advancements have implications that go beyond recommendation systems or review aggregation, influencing how we design human-centered AI capable of interpreting creativity, storytelling, and emotional resonance.\nFrom an industry perspective, the methodological advances discussed in this survey have practical implications for recommendation systems, content moderation, market research, and audience analytics within the film and streaming ecosystem. Fine-grained sentiment analysis can support more personalized content recommendations, early detection of audience dissatisfaction, and improved understanding of viewer engagement beyond coarse rating scores. The ability of modern LLMs to provide explanatory reasoning for their predictions\n[\n75\n]\nopens new possibilities for interpretable audience analytics.\nIn summary, the movie domain has shaped sentiment analysis into a rich interdisciplinary field that continually challenges researchers to develop methods that are robust, context-aware, and adaptable across cultures and modalities. As sentiment understanding becomes more deeply integrated into creative industries, it will not only enhance our ability to process media at scale, but also provide insights into the ways humans communicate and experience emotion. Future research must balance methodological innovation with cultural sensitivity, explainability, and fairness, ensuring that sentiment analysis contributes meaningfully to our understanding of human expression in creative media.\nAuthor Contributions\nA.G., S.D. and K.T. have equally contributed to conceptualization, methodology, validation and writing original draft preparation. All authors have read and agreed to the published version of the manuscript.\nFunding\nThis research received no external funding.\nData Availability\nNot applicable.\nConflicts of Interest\nThe authors declare no conflicts of interest.\nReferences\n[1]\nB.Â Pang and L.Â Lee, â€œThumbs up? sentiment classification using machine learning techniques,â€ in\nProceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing\n, vol.Â 10, pp.Â 79â€“86, 2002.\n[2]\nL.Â Zhang, S.Â Wang, and B.Â Liu, â€œDeep learning for sentiment analysis: A survey,â€\nWiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery\n, vol.Â 8, no.Â 4, p.Â e1253, 2018.\n[3]\nA.Â Yadav and D.Â K. Vishwakarma, â€œSentiment analysis using deep learning architectures: A review,â€\nArtificial Intelligence Review\n, vol.Â 53, no.Â 6, pp.Â 4335â€“4385, 2020.\n[4]\nD.Â Khurana, A.Â Koli, K.Â Khatter, and S.Â Singh, â€œNatural language processing: State of the art, current trends and challenges,â€\nMultimedia Tools and Applications\n, vol.Â 82, no.Â 3, pp.Â 3713â€“3744, 2023.\n[5]\nB.Â Pang and L.Â Lee, â€œOpinion mining and sentiment analysis,â€\nFoundations and Trends in Information Retrieval\n, vol.Â 2, pp.Â 1â€“135, 01 2008.\n[6]\nA.Â L. Maas, R.Â E. Daly, P.Â T. Pham, D.Â Huang, A.Â Y. Ng, and C.Â Potts, â€œLearning word vectors for sentiment analysis,â€ in\nProceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies\n, vol.Â 1, pp.Â 142â€“150, 2011.\n[7]\nM.Â Wankhade, A.Â Rao, and C.Â Kulkarni, â€œA survey on sentiment analysis methods, applications, and challenges,â€\nArtificial Intelligence Review\n, vol.Â 55, no.Â 7, pp.Â 5731â€“5780, 2022.\n[8]\nP.Â K. Jain, R.Â Pamula, and G.Â Srivastava, â€œSystematic reviews in sentiment analysis: A tertiary study,â€\nArtificial Intelligence Review\n, vol.Â 54, no.Â 7, pp.Â 4997â€“5053, 2022.\n[9]\nN.Â Raghunathan and K.Â Saravanakumar, â€œChallenges and issues in sentiment analysis: A comprehensive survey,â€\nIEEE Access\n, vol.Â 11, pp.Â 69626â€“69642, 2023.\n[10]\nM.Â Tetteh and M.Â Thushara, â€œSentiment analysis tools for movie review evaluation - a survey,â€ in\n2023 7th International Conference on Intelligent Computing and Control Systems (ICICCS)\n, pp.Â 816â€“823, 2023.\n[11]\nM.Â M. Danyal, S.Â S. Khan, M.Â Khan, S.Â Ullah, F.Â Mehmood, and I.Â Ali, â€œProposing sentiment analysis model based on bert and xlnet for movie reviews,â€\nMultimedia Tools and Applications\n, vol.Â 83, pp.Â 64315â€“64339, Jul 2024.\n[12]\nM.Â Birjali, M.Â Kasri, and A.Â Beni-Hssane, â€œA comprehensive survey on sentiment analysis: Approaches, challenges and trends,â€\nKnowledge-Based Systems\n, vol.Â 226, p.Â 107134, 2021.\n[13]\nN.Â C. Dang, M.Â N. Moreno-GarcÃ­a, and F.Â DeÂ la Prieta, â€œChallenges and future in deep learning for sentiment analysis: A comprehensive review and a proposed novel hybrid approach,â€\nArtificial Intelligence Review\n, vol.Â 57, pp.Â 1â€“54, 2024.\n[14]\nT.Â T. Thet, J.Â C. Na, and C.Â S.Â G. Khoo, â€œAspect-based sentiment analysis of movie reviews on discussion boards,â€\nJournal of Information Science\n, vol.Â 36, no.Â 6, pp.Â 823â€“848, 2010.\n[15]\nO.Â G. Horsa and K.Â K. Tune, â€œAspectâ€based sentiment analysis for afaan oromoo movie reviews using machine learning techniques,â€\nApplied Computational Intelligence and Soft Computing\n, vol.Â 2023, p.Â 3462691, 2023.\n[16]\nA.Â Musa, F.Â M. Adam, U.Â Ibrahim, and A.Â Y. Zandam, â€œHaubert: A transformer model for aspect-based sentiment analysis of hausa-language movie reviews,â€\nEngineering Proceedings\n, vol.Â 87, no.Â 1, 2025.\n[17]\nM.Â Bordoloi and S.Â K. Biswas, â€œSentiment analysis: A survey on design framework, applications and future scopes,â€\nArtificial Intelligence Review\n, vol.Â 56, pp.Â 12505â€“12560, Nov 2023.\n[18]\nY.Â Mao, Q.Â Liu, and Y.Â Zhang, â€œSentiment analysis methods, applications, and challenges: A systematic literature review,â€\nJournal of King Saud University - Computer and Information Sciences\n, vol.Â 36, no.Â 4, p.Â 102048, 2024.\n[19]\nS.Â Poria, E.Â Cambria, R.Â Bajpai, and A.Â Hussain, â€œA review of affective computing: From unimodal analysis to multimodal fusion,â€\nInformation Fusion\n, vol.Â 37, pp.Â 98â€“125, 2017.\n[20]\nR.Â Socher, A.Â Perelygin, J.Â Wu, J.Â Chuang, C.Â D. Manning, A.Â Y. Ng, and C.Â Potts, â€œRecursive deep models for semantic compositionality over a sentiment treebank,â€ in\nProceedings of the 2013 Conference on Empirical Methods in Natural Language Processing\n, pp.Â 1631â€“1642, 2013.\n[21]\nJ.Â Blitzer, M.Â Dredze, and F.Â Pereira, â€œBiographies, bollywood, boomâ€boxes and blenders: Domain adaptation for sentiment classification,â€ in\nProceedings of the 45th Annual Meeting of the Association of Computational Linguistics\n, (Prague, Czech Republic), pp.Â 440â€“447, Association for Computational Linguistics, 2007.\n[22]\nJ.Â Devlin, M.-W. Chang, K.Â Lee, and K.Â Toutanova, â€œBert: Pre-training of deep bidirectional transformers for language understanding,â€ in\nProceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\n, vol.Â 1, pp.Â 4171â€“4186, 2019.\n[23]\nT.Â Wilson, J.Â Wiebe, and P.Â Hoffmann, â€œRecognizing contextual polarity in phrase-level sentiment analysis,â€ in\nProceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing\n, pp.Â 347â€“354, 2005.\n[24]\nA.Â Vaswani, N.Â Shazeer, N.Â Parmar, J.Â Uszkoreit, L.Â Jones, A.Â N. Gomez, Å.Â Kaiser, and I.Â Polosukhin, â€œAttention is all you need,â€ in\nAdvances in Neural Information Processing Systems\n, vol.Â 30, pp.Â 5998â€“6008, Curran Associates, Inc., 2017.\n[25]\nM.Â T. Ribeiro, S.Â Singh, and C.Â Guestrin, â€œ\"why should i trust you?\" explaining the predictions of any classifier,â€ in\nProceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining\n, pp.Â 1135â€“1144, 2016.\n[26]\nE.Â J. Hu, Y.Â Shen, P.Â Wallis, Z.Â Allen-Zhu, Y.Â Li, S.Â Wang, L.Â Wang, and W.Â Chen, â€œLora: Low-rank adaptation of large language models,â€ in\nProceedings of the International Conference on Learning Representations\n, OpenReview.net, 2022.\nICLR 2022 Poster.\n[27]\nT.Â Dettmers, A.Â Pagnoni, A.Â Holtzman, and L.Â Zettlemoyer, â€œQlora: Efficient finetuning of quantized llms,â€\nAdvances in Neural Information Processing Systems\n, vol.Â 36, 2023.\n[28]\nY.Â Wang, M.Â Huang, X.Â Zhu, and L.Â Zhao, â€œAttention-based lstm for aspect-level sentiment classification,â€ in\nProceedings of the 2016 Conference on Empirical Methods in Natural Language Processing\n, pp.Â 606â€“615, 2016.\n[29]\nA.Â Rogers, O.Â Kovaleva, and A.Â Rumshisky, â€œA primer on neural network models for natural language processing,â€\nJournal of Artificial Intelligence Research\n, vol.Â 57, pp.Â 615â€“732, 2020.\n[30]\nP.Â D. Turney, â€œThumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews,â€ in\nProceedings of the 40th Annual Meeting of the Association for Computational Linguistics\n, pp.Â 417â€“424, 2002.\n[31]\nB.Â Pang and L.Â Lee, â€œA sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts,â€ in\nProceedings of the 42nd Annual Meeting of the Association for Computational Linguistics\n, pp.Â 271â€“278, 2004.\n[32]\nR.Â McDonald, K.Â Hannan, T.Â Neylon, M.Â Wells, and J.Â Reynar, â€œStructured models for fineâ€toâ€coarse sentiment analysis,â€ in\nProceedings of the 45th Annual Meeting of the Association of Computational Linguistics\n, (Prague, Czech Republic), pp.Â 432â€“439, Association for Computational Linguistics, 2007.\n[33]\nT.Â Mikolov, K.Â Chen, G.Â Corrado, and J.Â Dean, â€œEfficient estimation of word representations in vector space,â€\narXiv preprint arXiv:1301.3781\n, 2013.\n[34]\nJ.Â Pennington, R.Â Socher, and C.Â D. Manning, â€œGlove: Global vectors for word representation,â€ in\nProceedings of the 2014 Conference on Empirical Methods in Natural Language Processing\n, pp.Â 1532â€“1543, 2014.\n[35]\nS.Â Hochreiter and J.Â Schmidhuber, â€œLong short-term memory,â€\nNeural Computation\n, vol.Â 9, no.Â 8, pp.Â 1735â€“1780, 1997.\n[36]\nZ.Â Yang, D.Â Yang, C.Â Dyer, X.Â He, A.Â Smola, and E.Â Hovy, â€œHierarchical attention networks for document classification,â€ in\nProceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\n, pp.Â 1480â€“1489, 2016.\n[37]\nY.Â Liu, M.Â Ott, N.Â Goyal, J.Â Du, M.Â Joshi, D.Â Chen, O.Â Levy, M.Â Lewis, L.Â Zettlemoyer, and V.Â Stoyanov, â€œRoberta: A robustly optimized bert pretraining approach,â€\narXiv preprint arXiv:1907.11692\n, 2019.\n[38]\nT.Â B. Brown, B.Â Mann, N.Â Ryder, M.Â Subbiah, J.Â Kaplan, P.Â Dhariwal, A.Â Neelakantan, P.Â Shyam, G.Â Sastry, A.Â Askell, S.Â Agarwal, A.Â Herbert-Voss, G.Â Krueger, T.Â Henighan, R.Â Child, A.Â Ramesh, D.Â M. Ziegler, J.Â Wu, C.Â Winter, C.Â Hesse, M.Â Chen, E.Â Sigler, M.Â Litwin, S.Â Gray, B.Â Chess, J.Â Clark, C.Â Berner, S.Â McCandlish, A.Â Radford, I.Â Sutskever, and D.Â Amodei, â€œLanguage models are few-shot learners,â€\nAdvances in Neural Information Processing Systems (NeurIPS)\n, vol.Â 33, pp.Â 1877â€“1901, 2020.\n[39]\nH.Â Touvron, T.Â Lavril, G.Â Izacard, X.Â Martinet, M.-A. Lachaux, T.Â Lacroix, B.Â RoziÃ¨re, N.Â Goyal, E.Â Hambro, F.Â Azhar,\netÂ al.\n, â€œLlama: Open and efficient foundation language models,â€\narXiv preprint arXiv:2302.13971\n, 2023.\n[40]\nS.Â Stilwell and D.Â Inkpen, â€œExplainable Prompt-based Approaches for Sentiment Analysis of Movie Reviews,â€ may 27 2024.\n[41]\nB.Â Liu,\nSentiment Analysis and Opinion Mining\n.\nMorgan & Claypool Publishers, 2012.\n[42]\nY.Â Kim, â€œConvolutional neural networks for sentence classification,â€ in\nProceedings of the 2014 Conference on Empirical Methods in Natural Language Processing\n, pp.Â 1746â€“1751, 2014.\n[43]\nA.Â Zadeh, M.Â Chen, S.Â Poria, E.Â Cambria, and L.-P. Morency, â€œTensor fusion network for multimodal sentiment analysis,â€ in\nProceedings of the 2017 Conference on Empirical Methods in Natural Language Processing\n, pp.Â 1103â€“1114, 2017.\n[44]\nA.Â B. Zadeh, P.Â P. Liang, S.Â Poria, P.Â Vij, E.Â Cambria, and L.-P. Morency, â€œMulti-attention recurrent network for human communication comprehension,â€ in\nProceedings of the AAAI Conference on Artificial Intelligence\n, vol.Â 32, pp.Â 5642â€“5649, 2018.\n[45]\nD.Â Demszky, D.Â Movshovitz-Attias, J.Â Ko, A.Â Cowen, G.Â Nemade, and S.Â Ravi, â€œGoEmotions: A dataset of fine-grained emotions,â€ in\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics\n, (Online), pp.Â 4040â€“4054, Association for Computational Linguistics, July 2020.\n[46]\nF.Â Barbieri, J.Â Camacho-Collados, L.Â EspinosaÂ Anke, and L.Â Neves, â€œTweetEval: Unified benchmark and comparative evaluation for tweet classification,â€ in\nFindings of the Association for Computational Linguistics: EMNLP 2020\n, (Online), pp.Â 1644â€“1650, Association for Computational Linguistics, Nov. 2020.\n[47]\nS.Â Baccianella, A.Â Esuli, and F.Â Sebastiani, â€œSentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining,â€\nProceedings of the Seventh Conference on International Language Resources and Evaluation (LRECâ€™10)\n, 2010.\n[48]\nA.Â Esuli and F.Â Sebastiani, â€œSentiwordnet: A publicly available lexical resource for opinion mining,â€ in\nProceedings of LREC\n, pp.Â 417â€“422, 2006.\n[49]\nC.Â J. Hutto and E.Â Gilbert, â€œVader: A parsimonious rule-based model for sentiment analysis of social media text,â€ in\nProceedings of the International AAAI Conference on Weblogs and Social Media\n, 2014.\n[50]\nJ.Â W. Pennebaker, M.Â E. Francis, and R.Â J. Booth,\nLinguistic Inquiry and Word Count (LIWC): LIWC2001\n.\nLawrence Erlbaum Associates, 2001.\n[51]\nY.Â R. Tausczik and J.Â W. Pennebaker, â€œThe psychological meaning of words: Liwc and computerized text analysis methods,â€\nJournal of Language and Social Psychology\n, vol.Â 29, no.Â 1, pp.Â 24â€“54, 2010.\n[52]\nD.Â Jurafsky and J.Â Martin, â€œSpeech and language processing: an introduction to natural language processing, computational linguistics, and speech recognition,â€ 2000.\n[53]\nA.Â Kennedy and D.Â Inkpen, â€œSentiment classification of movie reviews using contextual valence shifters,â€\nComputational Intelligence\n, vol.Â 22, no.Â 2, pp.Â 110â€“125, 2006.\n[54]\nJ.Â Otterbacher, â€œInferring gender of movie reviewers: exploiting writing style, content and metadata,â€ in\nProceedings of the 19th ACM International Conference on Information and Knowledge Management\n, CIKM â€™10, (New York, NY, USA), p.Â 369â€“378, Association for Computing Machinery, 2010.\n[55]\nT.Â Joachims, â€œText categorization with support vector machines: Learning with many relevant features,â€ in\nEuropean Conference on Machine Learning\n, pp.Â 137â€“142, Springer, 1998.\n[56]\nC.Â Whitelaw, N.Â Garg, and S.Â Argamon, â€œUsing appraisal groups for sentiment analysis,â€ in\nProceedings of the 14th ACM International Conference on Information and Knowledge Management\n, pp.Â 625â€“631, 2005.\n[57]\nA.Â McCallum and K.Â Nigam, â€œA comparison of event models for naive bayes text classification,â€ in\nAAAI-98 Workshop on Learning for Text Categorization\n, 1998.\n[58]\nC.Â D. Manning, P.Â Raghavan, and H.Â SchÃ¼tze,\nIntroduction to Information Retrieval\n.\nCambridge University Press, 2008.\n[59]\nY.Â Bengio, P.Â Simard, and P.Â Frasconi, â€œLearning long-term dependencies with gradient descent is difficult,â€ vol.Â 5, pp.Â 157â€“166, IEEE, 1994.\n[60]\nK.Â S. Tai, R.Â Socher, and C.Â D. Manning, â€œImproved semantic representations from tree-structured long short-term memory networks,â€ in\nProceedings of the 53rd Annual Meeting of the Association for Computational Linguistics\n, pp.Â 1556â€“1566, 2015.\n[61]\nT.Â Mikolov, I.Â Sutskever, K.Â Chen, G.Â Corrado, and J.Â Dean, â€œDistributed representations of words and phrases and their compositionality,â€ in\nAdvances in Neural Information Processing Systems (NeurIPS)\n, pp.Â 3111â€“3119, 2013.\n[62]\nD.Â Bahdanau, K.Â Cho, and Y.Â Bengio, â€œNeural machine translation by jointly learning to align and translate,â€ in\nInternational Conference on Learning Representations (ICLR)\n, 2015.\n[63]\nC.Â Sun, X.Â Qiu, Y.Â Xu, and X.Â Huang, â€œHow to fine-tune bert for text classification?,â€ in\nChina National Conference on Chinese Computational Linguistics\n, pp.Â 194â€“206, Springer, 2019.\n[64]\nV.Â Sanh, L.Â Debut, J.Â Chaumond, and T.Â Wolf, â€œDistilbert, a distilled version of bert: smaller, faster, cheaper and lighter,â€\narXiv preprint arXiv:1910.01108\n, 2019.\n[65]\nZ.Â Yang, Z.Â Dai, Y.Â Yang, J.Â Carbonell, R.Â R. Salakhutdinov, and Q.Â V. Le, â€œXlnet: Generalized autoregressive pretraining for language understanding,â€ in\nAdvances in Neural Information Processing Systems\n(H.Â Wallach, H.Â Larochelle, A.Â Beygelzimer, F.Â d'AlchÃ©-Buc, E.Â Fox, and R.Â Garnett, eds.), vol.Â 32, Curran Associates, Inc., 2019.\n[66]\nA.Â Bello, S.-C. Ng, and M.-F. Leung, â€œA bert framework to sentiment analysis of tweets,â€\nSensors\n, vol.Â 23, no.Â 1, 2023.\n[67]\nH.Â Batra, N.Â S. Punn, S.Â K. Sonbhadra, and S.Â Agarwal,\nBERT-Based Sentiment Analysis: A Software Engineering Perspective\n, p.Â 138â€“148.\nSpringer International Publishing, 2021.\n[68]\nG.Â Penha and C.Â Hauff, â€œWhat does bert know about books, movies and music? probing bert for conversational recommendation,â€ in\nProceedings of the 14th ACM Conference on Recommender Systems\n, RecSys â€™20, (New York, NY, USA), p.Â 388â€“397, Association for Computing Machinery, 2020.\n[69]\nG.Â Nkhata, S.Â Gauch, U.Â Anjum, and J.Â Zhan, â€œFine-tuning BERT with bidirectional LSTM for fine-grained movie reviews sentiment analysis,â€\narXiv preprint arXiv:2502.20682\n, 2025.\n[70]\nC.Â Raffel, N.Â Shazeer, A.Â Roberts, K.Â Lee, S.Â Narang, M.Â Matena, Y.Â Zhou, W.Â Li, and P.Â J. Liu, â€œExploring the limits of transfer learning with a unified text-to-text transformer,â€\nJournal of Machine Learning Research\n, vol.Â 21, no.Â 140, pp.Â 1â€“67, 2020.\n[71]\nT.Â Gao, A.Â Fisch, and D.Â Chen, â€œMaking pre-trained language models better few-shot learners,â€ in\nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)\n(C.Â Zong, F.Â Xia, W.Â Li, and R.Â Navigli, eds.), (Online), pp.Â 3816â€“3830, Association for Computational Linguistics, Aug. 2021.\n[72]\nW.Â Zhang, Y.Â Deng, B.Â Liu, S.Â Pan, and L.Â Bing, â€œSentiment analysis in the era of large language models: A reality check,â€ in\nFindings of the Association for Computational Linguistics: NAACL 2024\n, (Mexico City, Mexico), pp.Â 3881â€“3906, Association for Computational Linguistics, June 2024.\n[73]\nJ.Â O. Krugmann and J.Â Hartmann, â€œSentiment analysis in the age of generative ai,â€\nCustomer Needs and Solutions\n, vol.Â 11, p.Â 3, Mar. 2024.\n[74]\nS.Â Rathje, D.-M. Mirea, I.Â Sucholutsky, R.Â Marjieh, C.Â E. Robertson, and J.Â J. VanÂ Bavel, â€œGpt is an effective tool for multilingual psychological text analysis,â€\nProceedings of the National Academy of Sciences\n, vol.Â 121, Aug. 2024.\n[75]\nH.Â Fei, B.Â Li, Q.Â Liu, L.Â Bing, F.Â Li, and T.-S. Chua, â€œReasoning implicit sentiment with chain-of-thought prompting,â€ in\nProceedings of the 61st Annual Meeting of the Association for Computational Linguistics\n, (Toronto, Canada), pp.Â 1171â€“1182, Association for Computational Linguistics, July 2023.\n[76]\nK.Â Scaria, H.Â Gupta, S.Â Goyal, S.Â Sawant, S.Â Mishra, and C.Â Baral, â€œInstructabsa: Instruction learning for aspect based sentiment analysis,â€ in\nProceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)\n, (Mexico City, Mexico), pp.Â 720â€“736, Association for Computational Linguistics, June 2024.\n[77]\nA.Â Joshi, P.Â Bhattacharyya, and M.Â J. Carman, â€œAutomatic sarcasm detection: A survey,â€\nACM Comput. Surv.\n, vol.Â 50, Sept. 2017.\n[78]\nA.Â Ghosh, G.Â Li, T.Â Veale, P.Â Rosso, E.Â Shutova, J.Â Barnden, and A.Â Reyes, â€œSemEval-2015 task 11: Sentiment analysis of figurative language in Twitter,â€ in\nProceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015)\n(P.Â Nakov, T.Â Zesch, D.Â Cer, and D.Â Jurgens, eds.), (Denver, Colorado), pp.Â 470â€“478, Association for Computational Linguistics, June 2015.\n[79]\nR.Â J. Kreuz and R.Â M. Roberts, â€œThe use of verbal irony: Cues and constraints,â€\nMetaphor and Symbol\n, vol.Â 11, no.Â 1, pp.Â 23â€“38, 1996.\n[80]\nX.Â Glorot, A.Â Bordes, and Y.Â Bengio, â€œDomain adaptation for large-scale sentiment classification: A deep learning approach,â€ in\nProceedings of the 28th International Conference on Machine Learning\n, pp.Â 513â€“520, 2011.\n[81]\nF.Â Diaz, B.Â Mitra, and N.Â Craswell, â€œQuery expansion with locally-trained word embeddings,â€ in\nProceedings of the 54th Annual Meeting of the Association for Computational Linguistics\n, pp.Â 367â€“377, 2016.\n[82]\nA.Â Lazaridou, A.Â Kuncoro, E.Â Gribovskaya, D.Â Agrawal, A.Â LiÅ¡ka, T.Â Terzi, M.Â Gimenez, C.Â d.Â M. dâ€™Autume, T.Â Kocisky, S.Â Ruder, D.Â Yogatama, K.Â Cao, S.Â Young, and P.Â Blunsom, â€œMind the gap: assessing temporal generalization in neural language models,â€ in\nProceedings of the 35th International Conference on Neural Information Processing Systems\n, NIPS â€™21, (Red Hook, NY, USA), Curran Associates Inc., 2021.\n[83]\nM.Â McCloskey and N.Â Cohen, â€œCatastrophic interference in connectionist networks: The sequential learning problem,â€\nPsychology of Learning and Motivation - Advances in Research and Theory\n, vol.Â 24, pp.Â 109â€“165, Jan. 1989.\n[84]\nP.Â Bhatia, Y.Â Ji, and J.Â Eisenstein, â€œBetter document-level sentiment analysis from RST discourse parsing,â€ in\nProceedings of the 2015 Conference on Empirical Methods in Natural Language Processing\n(L.Â MÃ rquez, C.Â Callison-Burch, and J.Â Su, eds.), (Lisbon, Portugal), pp.Â 2212â€“2218, Association for Computational Linguistics, Sept. 2015.\n[85]\nC.Â Banea, R.Â Mihalcea, J.Â Wiebe, and S.Â Hassan, â€œMultilingual subjectivity analysis using machine translation,â€ in\nProceedings of the 2008 Conference on Empirical Methods in Natural Language Processing\n(M.Â Lapata and H.Â T. Ng, eds.), (Honolulu, Hawaii), pp.Â 127â€“135, Association for Computational Linguistics, Oct. 2008.\n[86]\nP.Â KraljÂ Novak, J.Â SmailoviÄ‡, B.Â Sluban, and I.Â MozetiÄ, â€œSentiment of emojis,â€\nPLOS ONE\n, vol.Â 10, pp.Â 1â€“22, 12 2015.\n[87]\nJ.Â Barnes, R.Â Klinger, and S.Â SchulteÂ im Walde, â€œBilingual sentiment embeddings: Joint projection of sentiment across languages,â€ in\nProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\n(I.Â Gurevych and Y.Â Miyao, eds.), (Melbourne, Australia), pp.Â 2483â€“2493, Association for Computational Linguistics, July 2018.\n[88]\nA.Â Conneau, K.Â Khandelwal, N.Â Goyal, V.Â Chaudhary, G.Â Wenzek, F.Â GuzmÃ¡n, E.Â Grave, M.Â Ott, L.Â Zettlemoyer, and V.Â Stoyanov, â€œUnsupervised cross-lingual representation learning at scale,â€ in\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics\n(D.Â Jurafsky, J.Â Chai, N.Â Schluter, and J.Â Tetreault, eds.), (Online), pp.Â 8440â€“8451, Association for Computational Linguistics, July 2020.\n[89]\nR.Â GonzÃ¡lez-IbÃ¡Ã±ez, S.Â Muresan, and N.Â Wacholder, â€œIdentifying sarcasm in Twitter: A closer look,â€ in\nProceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies\n, (Portland, Oregon, USA), pp.Â 581â€“586, Association for Computational Linguistics, June 2011.\n[90]\nI.Â Beltagy, M.Â E. Peters, and A.Â Cohan, â€œLongformer: The long-document transformer,â€ 2020.\n[91]\nM.Â Danilevsky, K.Â Qian, R.Â Aharonov, Y.Â Katsis, B.Â Kawas, and P.Â Sen, â€œA survey of the state of explainable AI for natural language processing,â€ in\nProceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing\n(K.-F. Wong, K.Â Knight, and H.Â Wu, eds.), (Suzhou, China), pp.Â 447â€“459, Association for Computational Linguistics, Dec. 2020.\n[92]\nR.Â Schwartz, J.Â Dodge, N.Â A. Smith, and O.Â Etzioni, â€œGreen ai,â€ in\nCommunications of the ACM\n, 2020.\n[93]\nW.Â Yin, J.Â Hay, and D.Â Roth, â€œBenchmarking zero-shot text classification: Datasets, evaluation and entailment approach,â€ in\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)\n(K.Â Inui, J.Â Jiang, V.Â Ng, and X.Â Wan, eds.), (Hong Kong, China), pp.Â 3914â€“3923, Association for Computational Linguistics, Nov. 2019.\n[94]\nJ.Â Wei, X.Â Wang, D.Â Schuurmans, M.Â Bosma, F.Â Xia, E.Â Chi, Q.Â V. Le, and D.Â Zhou, â€œChain-of-thought prompting elicits reasoning in large language models,â€ in\nAdvances in Neural Information Processing Systems\n, vol.Â 35, pp.Â 24824â€“24837, 2022.\n[95]\nY.Â Gu, X.Â Han, Z.Â Liu, and M.Â Huang, â€œPPT: Pre-trained prompt tuning for few-shot learning,â€ in\nProceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\n(S.Â Muresan, P.Â Nakov, and A.Â Villavicencio, eds.), (Dublin, Ireland), pp.Â 8410â€“8423, Association for Computational Linguistics, May 2022.\n[96]\nM.Â Hu and B.Â Liu, â€œMining and summarizing customer reviews,â€ in\nProceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining\n, KDD â€™04, (New York, NY, USA), p.Â 168â€“177, Association for Computing Machinery, 2004.\n[97]\nA.Â BagherÂ Zadeh, P.Â P. Liang, S.Â Poria, E.Â Cambria, and L.-P. Morency, â€œMultimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph,â€ in\nProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\n(I.Â Gurevych and Y.Â Miyao, eds.), (Melbourne, Australia), pp.Â 2236â€“2246, Association for Computational Linguistics, July 2018.\n[98]\nS.Â Lai, X.Â Hu, H.Â Xu, Z.Â Ren, and Z.Â Liu, â€œMultimodal sentiment analysis: A survey,â€ 2023.\n[99]\nY.Â Khare, V.Â Bagal, M.Â Mathew, A.Â Devi, U.Â D. Priyakumar, and C.Â Jawahar, â€œMmbert: Multimodal bert pretraining for improved medical vqa,â€ in\n2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)\n, pp.Â 1033â€“1036, 2021.\n[100]\nS.Â Poria, N.Â Majumder, R.Â Mihalcea, and E.Â Hovy, â€œEmotion recognition in conversation: Research challenges, datasets, and recent advances,â€\nIEEE Access\n, vol.Â 7, pp.Â 100943â€“100953, 2019.\n[101]\nE.Â Sheng, K.-W. Chang, P.Â Natarajan, and N.Â Peng, â€œThe woman worked as a babysitter: On biases in language generation,â€ in\nProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)\n(K.Â Inui, J.Â Jiang, V.Â Ng, and X.Â Wan, eds.), (Hong Kong, China), pp.Â 3407â€“3412, Association for Computational Linguistics, Nov. 2019.\n[102]\nS.Â Kiritchenko and S.Â Mohammad, â€œExamining gender and race bias in two hundred sentiment analysis systems,â€ in\nProceedings of the Seventh Joint Conference on Lexical and Computational Semantics\n(M.Â Nissim, J.Â Berant, and A.Â Lenci, eds.), (New Orleans, Louisiana), pp.Â 43â€“53, Association for Computational Linguistics, June 2018.\n[103]\nA.Â Radford, J.Â W. Kim, C.Â Hallacy, A.Â Ramesh, G.Â Goh, S.Â Agarwal, G.Â Sastry, A.Â Askell, P.Â Mishkin, J.Â Clark, G.Â Krueger, and I.Â Sutskever, â€œLearning transferable visual models from natural language supervision,â€ in\nProceedings of the 38th International Conference on Machine Learning\n(M.Â Meila and T.Â Zhang, eds.), vol.Â 139 of\nProceedings of Machine Learning Research\n, pp.Â 8748â€“8763, PMLR, 18â€“24 Jul 2021.\n[104]\nJ.Â He, L.Â Wang, L.Â Liu, J.Â Feng, and H.Â Wu, â€œLong document classification from local word glimpses via recurrent attention learning,â€\nIEEE Access\n, vol.Â 7, pp.Â 40707â€“40718, 2019.\n[105]\nS.Â Amir, B.Â C. Wallace, H.Â Lyu, P.Â Carvalho, and M.Â J. Silva, â€œModelling context with user embeddings for sarcasm detection in social media,â€ in\nProceedings of the 20th SIGNLL Conference on Computational Natural Language Learning\n(S.Â Riezler and Y.Â Goldberg, eds.), (Berlin, Germany), pp.Â 167â€“177, Association for Computational Linguistics, Aug. 2016.\n[106]\nT.Â Ranasinghe and M.Â Zampieri, â€œMultilingual offensive language identification with cross-lingual embeddings,â€ in\nProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)\n(B.Â Webber, T.Â Cohn, Y.Â He, and Y.Â Liu, eds.), (Online), pp.Â 5838â€“5844, Association for Computational Linguistics, Nov. 2020.\n[107]\nN.Â F. Rajani, B.Â McCann, C.Â Xiong, and R.Â Socher, â€œExplain yourself! leveraging language models for commonsense reasoning,â€ in\nProceedings of the 57th Annual Meeting of the Association for Computational Linguistics\n(A.Â Korhonen, D.Â Traum, and L.Â MÃ rquez, eds.), (Florence, Italy), pp.Â 4932â€“4942, Association for Computational Linguistics, July 2019.\n[108]\nY.Â Cai, X.Â Li, Y.Â Zhang, J.Â Li, F.Â Zhu, and L.Â Rao, â€œMultimodal sentiment analysis based on multi-layer feature fusion and multi-task learning,â€\nScientific Reports\n, vol.Â 15, p.Â 2126, Jan 2025.",
      "references": [
        {
          "raw_text": "[1] B.Â Pang and L.Â Lee, â€œThumbs up? sentiment classification using machine learning techniques,â€ in Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing , vol.Â 10, pp.Â 79â€“86, 2002.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[2] L.Â Zhang, S.Â Wang, and B.Â Liu, â€œDeep learning for sentiment analysis: A survey,â€ Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery , vol.Â 8, no.Â 4, p.Â e1253, 2018.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[3] A.Â Yadav and D.Â K. Vishwakarma, â€œSentiment analysis using deep learning architectures: A review,â€ Artificial Intelligence Review , vol.Â 53, no.Â 6, pp.Â 4335â€“4385, 2020.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[4] D.Â Khurana, A.Â Koli, K.Â Khatter, and S.Â Singh, â€œNatural language processing: State of the art, current trends and challenges,â€ Multimedia Tools and Applications , vol.Â 82, no.Â 3, pp.Â 3713â€“3744, 2023.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[5] B.Â Pang and L.Â Lee, â€œOpinion mining and sentiment analysis,â€ Foundations and Trends in Information Retrieval , vol.Â 2, pp.Â 1â€“135, 01 2008.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[6] A.Â L. Maas, R.Â E. Daly, P.Â T. Pham, D.Â Huang, A.Â Y. Ng, and C.Â Potts, â€œLearning word vectors for sentiment analysis,â€ in Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies , vol.Â 1, pp.Â 142â€“150, 2011.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[7] M.Â Wankhade, A.Â Rao, and C.Â Kulkarni, â€œA survey on sentiment analysis methods, applications, and challenges,â€ Artificial Intelligence Review , vol.Â 55, no.Â 7, pp.Â 5731â€“5780, 2022.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[8] P.Â K. Jain, R.Â Pamula, and G.Â Srivastava, â€œSystematic reviews in sentiment analysis: A tertiary study,â€ Artificial Intelligence Review , vol.Â 54, no.Â 7, pp.Â 4997â€“5053, 2022.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[9] N.Â Raghunathan and K.Â Saravanakumar, â€œChallenges and issues in sentiment analysis: A comprehensive survey,â€ IEEE Access , vol.Â 11, pp.Â 69626â€“69642, 2023.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[10] M.Â Tetteh and M.Â Thushara, â€œSentiment analysis tools for movie review evaluation - a survey,â€ in 2023 7th International Conference on Intelligent Computing and Control Systems (ICICCS) , pp.Â 816â€“823, 2023.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[11] M.Â M. Danyal, S.Â S. Khan, M.Â Khan, S.Â Ullah, F.Â Mehmood, and I.Â Ali, â€œProposing sentiment analysis model based on bert and xlnet for movie reviews,â€ Multimedia Tools and Applications , vol.Â 83, pp.Â 64315â€“64339, Jul 2024.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[12] M.Â Birjali, M.Â Kasri, and A.Â Beni-Hssane, â€œA comprehensive survey on sentiment analysis: Approaches, challenges and trends,â€ Knowledge-Based Systems , vol.Â 226, p.Â 107134, 2021.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[13] N.Â C. Dang, M.Â N. Moreno-GarcÃ­a, and F.Â DeÂ la Prieta, â€œChallenges and future in deep learning for sentiment analysis: A comprehensive review and a proposed novel hybrid approach,â€ Artificial Intelligence Review , vol.Â 57, pp.Â 1â€“54, 2024.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[14] T.Â T. Thet, J.Â C. Na, and C.Â S.Â G. Khoo, â€œAspect-based sentiment analysis of movie reviews on discussion boards,â€ Journal of Information Science , vol.Â 36, no.Â 6, pp.Â 823â€“848, 2010.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[15] O.Â G. Horsa and K.Â K. Tune, â€œAspectâ€based sentiment analysis for afaan oromoo movie reviews using machine learning techniques,â€ Applied Computational Intelligence and Soft Computing , vol.Â 2023, p.Â 3462691, 2023.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[16] A.Â Musa, F.Â M. Adam, U.Â Ibrahim, and A.Â Y. Zandam, â€œHaubert: A transformer model for aspect-based sentiment analysis of hausa-language movie reviews,â€ Engineering Proceedings , vol.Â 87, no.Â 1, 2025.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[17] M.Â Bordoloi and S.Â K. Biswas, â€œSentiment analysis: A survey on design framework, applications and future scopes,â€ Artificial Intelligence Review , vol.Â 56, pp.Â 12505â€“12560, Nov 2023.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[18] Y.Â Mao, Q.Â Liu, and Y.Â Zhang, â€œSentiment analysis methods, applications, and challenges: A systematic literature review,â€ Journal of King Saud University - Computer and Information Sciences , vol.Â 36, no.Â 4, p.Â 102048, 2024.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[19] S.Â Poria, E.Â Cambria, R.Â Bajpai, and A.Â Hussain, â€œA review of affective computing: From unimodal analysis to multimodal fusion,â€ Information Fusion , vol.Â 37, pp.Â 98â€“125, 2017.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[20] R.Â Socher, A.Â Perelygin, J.Â Wu, J.Â Chuang, C.Â D. Manning, A.Â Y. Ng, and C.Â Potts, â€œRecursive deep models for semantic compositionality over a sentiment treebank,â€ in Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing , pp.Â 1631â€“1642, 2013.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[21] J.Â Blitzer, M.Â Dredze, and F.Â Pereira, â€œBiographies, bollywood, boomâ€boxes and blenders: Domain adaptation for sentiment classification,â€ in Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics , (Prague, Czech Republic), pp.Â 440â€“447, Association for Computational Linguistics, 2007.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[22] J.Â Devlin, M.-W. Chang, K.Â Lee, and K.Â Toutanova, â€œBert: Pre-training of deep bidirectional transformers for language understanding,â€ in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , vol.Â 1, pp.Â 4171â€“4186, 2019.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[23] T.Â Wilson, J.Â Wiebe, and P.Â Hoffmann, â€œRecognizing contextual polarity in phrase-level sentiment analysis,â€ in Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing , pp.Â 347â€“354, 2005.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[24] A.Â Vaswani, N.Â Shazeer, N.Â Parmar, J.Â Uszkoreit, L.Â Jones, A.Â N. Gomez, Å.Â Kaiser, and I.Â Polosukhin, â€œAttention is all you need,â€ in Advances in Neural Information Processing Systems , vol.Â 30, pp.Â 5998â€“6008, Curran Associates, Inc., 2017.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[25] M.Â T. Ribeiro, S.Â Singh, and C.Â Guestrin, â€œ\"why should i trust you?\" explaining the predictions of any classifier,â€ in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pp.Â 1135â€“1144, 2016.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[26] E.Â J. Hu, Y.Â Shen, P.Â Wallis, Z.Â Allen-Zhu, Y.Â Li, S.Â Wang, L.Â Wang, and W.Â Chen, â€œLora: Low-rank adaptation of large language models,â€ in Proceedings of the International Conference on Learning Representations , OpenReview.net, 2022. ICLR 2022 Poster.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[27] T.Â Dettmers, A.Â Pagnoni, A.Â Holtzman, and L.Â Zettlemoyer, â€œQlora: Efficient finetuning of quantized llms,â€ Advances in Neural Information Processing Systems , vol.Â 36, 2023.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[28] Y.Â Wang, M.Â Huang, X.Â Zhu, and L.Â Zhao, â€œAttention-based lstm for aspect-level sentiment classification,â€ in Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing , pp.Â 606â€“615, 2016.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[29] A.Â Rogers, O.Â Kovaleva, and A.Â Rumshisky, â€œA primer on neural network models for natural language processing,â€ Journal of Artificial Intelligence Research , vol.Â 57, pp.Â 615â€“732, 2020.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[30] P.Â D. Turney, â€œThumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews,â€ in Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics , pp.Â 417â€“424, 2002.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[31] B.Â Pang and L.Â Lee, â€œA sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts,â€ in Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics , pp.Â 271â€“278, 2004.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[32] R.Â McDonald, K.Â Hannan, T.Â Neylon, M.Â Wells, and J.Â Reynar, â€œStructured models for fineâ€toâ€coarse sentiment analysis,â€ in Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics , (Prague, Czech Republic), pp.Â 432â€“439, Association for Computational Linguistics, 2007.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[33] T.Â Mikolov, K.Â Chen, G.Â Corrado, and J.Â Dean, â€œEfficient estimation of word representations in vector space,â€ arXiv preprint arXiv:1301.3781 , 2013.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[34] J.Â Pennington, R.Â Socher, and C.Â D. Manning, â€œGlove: Global vectors for word representation,â€ in Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing , pp.Â 1532â€“1543, 2014.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[35] S.Â Hochreiter and J.Â Schmidhuber, â€œLong short-term memory,â€ Neural Computation , vol.Â 9, no.Â 8, pp.Â 1735â€“1780, 1997.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[36] Z.Â Yang, D.Â Yang, C.Â Dyer, X.Â He, A.Â Smola, and E.Â Hovy, â€œHierarchical attention networks for document classification,â€ in Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pp.Â 1480â€“1489, 2016.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[37] Y.Â Liu, M.Â Ott, N.Â Goyal, J.Â Du, M.Â Joshi, D.Â Chen, O.Â Levy, M.Â Lewis, L.Â Zettlemoyer, and V.Â Stoyanov, â€œRoberta: A robustly optimized bert pretraining approach,â€ arXiv preprint arXiv:1907.11692 , 2019.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[38] T.Â B. Brown, B.Â Mann, N.Â Ryder, M.Â Subbiah, J.Â Kaplan, P.Â Dhariwal, A.Â Neelakantan, P.Â Shyam, G.Â Sastry, A.Â Askell, S.Â Agarwal, A.Â Herbert-Voss, G.Â Krueger, T.Â Henighan, R.Â Child, A.Â Ramesh, D.Â M. Ziegler, J.Â Wu, C.Â Winter, C.Â Hesse, M.Â Chen, E.Â Sigler, M.Â Litwin, S.Â Gray, B.Â Chess, J.Â Clark, C.Â Berner, S.Â McCandlish, A.Â Radford, I.Â Sutskever, and D.Â Amodei, â€œLanguage models are few-shot learners,â€ Advances in Neural Information Processing Systems (NeurIPS) , vol.Â 33, pp.Â 1877â€“1901, 2020.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[39] H.Â Touvron, T.Â Lavril, G.Â Izacard, X.Â Martinet, M.-A. Lachaux, T.Â Lacroix, B.Â RoziÃ¨re, N.Â Goyal, E.Â Hambro, F.Â Azhar, etÂ al. , â€œLlama: Open and efficient foundation language models,â€ arXiv preprint arXiv:2302.13971 , 2023.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[40] S.Â Stilwell and D.Â Inkpen, â€œExplainable Prompt-based Approaches for Sentiment Analysis of Movie Reviews,â€ may 27 2024.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[41] B.Â Liu, Sentiment Analysis and Opinion Mining . Morgan & Claypool Publishers, 2012.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[42] Y.Â Kim, â€œConvolutional neural networks for sentence classification,â€ in Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing , pp.Â 1746â€“1751, 2014.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[43] A.Â Zadeh, M.Â Chen, S.Â Poria, E.Â Cambria, and L.-P. Morency, â€œTensor fusion network for multimodal sentiment analysis,â€ in Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing , pp.Â 1103â€“1114, 2017.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[44] A.Â B. Zadeh, P.Â P. Liang, S.Â Poria, P.Â Vij, E.Â Cambria, and L.-P. Morency, â€œMulti-attention recurrent network for human communication comprehension,â€ in Proceedings of the AAAI Conference on Artificial Intelligence , vol.Â 32, pp.Â 5642â€“5649, 2018.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[45] D.Â Demszky, D.Â Movshovitz-Attias, J.Â Ko, A.Â Cowen, G.Â Nemade, and S.Â Ravi, â€œGoEmotions: A dataset of fine-grained emotions,â€ in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , (Online), pp.Â 4040â€“4054, Association for Computational Linguistics, July 2020.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[46] F.Â Barbieri, J.Â Camacho-Collados, L.Â EspinosaÂ Anke, and L.Â Neves, â€œTweetEval: Unified benchmark and comparative evaluation for tweet classification,â€ in Findings of the Association for Computational Linguistics: EMNLP 2020 , (Online), pp.Â 1644â€“1650, Association for Computational Linguistics, Nov. 2020.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[47] S.Â Baccianella, A.Â Esuli, and F.Â Sebastiani, â€œSentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining,â€ Proceedings of the Seventh Conference on International Language Resources and Evaluation (LRECâ€™10) , 2010.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[48] A.Â Esuli and F.Â Sebastiani, â€œSentiwordnet: A publicly available lexical resource for opinion mining,â€ in Proceedings of LREC , pp.Â 417â€“422, 2006.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[49] C.Â J. Hutto and E.Â Gilbert, â€œVader: A parsimonious rule-based model for sentiment analysis of social media text,â€ in Proceedings of the International AAAI Conference on Weblogs and Social Media , 2014.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[50] J.Â W. Pennebaker, M.Â E. Francis, and R.Â J. Booth, Linguistic Inquiry and Word Count (LIWC): LIWC2001 . Lawrence Erlbaum Associates, 2001.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[51] Y.Â R. Tausczik and J.Â W. Pennebaker, â€œThe psychological meaning of words: Liwc and computerized text analysis methods,â€ Journal of Language and Social Psychology , vol.Â 29, no.Â 1, pp.Â 24â€“54, 2010.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[52] D.Â Jurafsky and J.Â Martin, â€œSpeech and language processing: an introduction to natural language processing, computational linguistics, and speech recognition,â€ 2000.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[53] A.Â Kennedy and D.Â Inkpen, â€œSentiment classification of movie reviews using contextual valence shifters,â€ Computational Intelligence , vol.Â 22, no.Â 2, pp.Â 110â€“125, 2006.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[54] J.Â Otterbacher, â€œInferring gender of movie reviewers: exploiting writing style, content and metadata,â€ in Proceedings of the 19th ACM International Conference on Information and Knowledge Management , CIKM â€™10, (New York, NY, USA), p.Â 369â€“378, Association for Computing Machinery, 2010.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[55] T.Â Joachims, â€œText categorization with support vector machines: Learning with many relevant features,â€ in European Conference on Machine Learning , pp.Â 137â€“142, Springer, 1998.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[56] C.Â Whitelaw, N.Â Garg, and S.Â Argamon, â€œUsing appraisal groups for sentiment analysis,â€ in Proceedings of the 14th ACM International Conference on Information and Knowledge Management , pp.Â 625â€“631, 2005.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[57] A.Â McCallum and K.Â Nigam, â€œA comparison of event models for naive bayes text classification,â€ in AAAI-98 Workshop on Learning for Text Categorization , 1998.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[58] C.Â D. Manning, P.Â Raghavan, and H.Â SchÃ¼tze, Introduction to Information Retrieval . Cambridge University Press, 2008.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[59] Y.Â Bengio, P.Â Simard, and P.Â Frasconi, â€œLearning long-term dependencies with gradient descent is difficult,â€ vol.Â 5, pp.Â 157â€“166, IEEE, 1994.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[60] K.Â S. Tai, R.Â Socher, and C.Â D. Manning, â€œImproved semantic representations from tree-structured long short-term memory networks,â€ in Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics , pp.Â 1556â€“1566, 2015.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[61] T.Â Mikolov, I.Â Sutskever, K.Â Chen, G.Â Corrado, and J.Â Dean, â€œDistributed representations of words and phrases and their compositionality,â€ in Advances in Neural Information Processing Systems (NeurIPS) , pp.Â 3111â€“3119, 2013.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[62] D.Â Bahdanau, K.Â Cho, and Y.Â Bengio, â€œNeural machine translation by jointly learning to align and translate,â€ in International Conference on Learning Representations (ICLR) , 2015.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[63] C.Â Sun, X.Â Qiu, Y.Â Xu, and X.Â Huang, â€œHow to fine-tune bert for text classification?,â€ in China National Conference on Chinese Computational Linguistics , pp.Â 194â€“206, Springer, 2019.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[64] V.Â Sanh, L.Â Debut, J.Â Chaumond, and T.Â Wolf, â€œDistilbert, a distilled version of bert: smaller, faster, cheaper and lighter,â€ arXiv preprint arXiv:1910.01108 , 2019.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[65] Z.Â Yang, Z.Â Dai, Y.Â Yang, J.Â Carbonell, R.Â R. Salakhutdinov, and Q.Â V. Le, â€œXlnet: Generalized autoregressive pretraining for language understanding,â€ in Advances in Neural Information Processing Systems (H.Â Wallach, H.Â Larochelle, A.Â Beygelzimer, F.Â d'AlchÃ©-Buc, E.Â Fox, and R.Â Garnett, eds.), vol.Â 32, Curran Associates, Inc., 2019.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[66] A.Â Bello, S.-C. Ng, and M.-F. Leung, â€œA bert framework to sentiment analysis of tweets,â€ Sensors , vol.Â 23, no.Â 1, 2023.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[67] H.Â Batra, N.Â S. Punn, S.Â K. Sonbhadra, and S.Â Agarwal, BERT-Based Sentiment Analysis: A Software Engineering Perspective , p.Â 138â€“148. Springer International Publishing, 2021.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[68] G.Â Penha and C.Â Hauff, â€œWhat does bert know about books, movies and music? probing bert for conversational recommendation,â€ in Proceedings of the 14th ACM Conference on Recommender Systems , RecSys â€™20, (New York, NY, USA), p.Â 388â€“397, Association for Computing Machinery, 2020.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[69] G.Â Nkhata, S.Â Gauch, U.Â Anjum, and J.Â Zhan, â€œFine-tuning BERT with bidirectional LSTM for fine-grained movie reviews sentiment analysis,â€ arXiv preprint arXiv:2502.20682 , 2025.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[70] C.Â Raffel, N.Â Shazeer, A.Â Roberts, K.Â Lee, S.Â Narang, M.Â Matena, Y.Â Zhou, W.Â Li, and P.Â J. Liu, â€œExploring the limits of transfer learning with a unified text-to-text transformer,â€ Journal of Machine Learning Research , vol.Â 21, no.Â 140, pp.Â 1â€“67, 2020.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[71] T.Â Gao, A.Â Fisch, and D.Â Chen, â€œMaking pre-trained language models better few-shot learners,â€ in Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) (C.Â Zong, F.Â Xia, W.Â Li, and R.Â Navigli, eds.), (Online), pp.Â 3816â€“3830, Association for Computational Linguistics, Aug. 2021.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[72] W.Â Zhang, Y.Â Deng, B.Â Liu, S.Â Pan, and L.Â Bing, â€œSentiment analysis in the era of large language models: A reality check,â€ in Findings of the Association for Computational Linguistics: NAACL 2024 , (Mexico City, Mexico), pp.Â 3881â€“3906, Association for Computational Linguistics, June 2024.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[73] J.Â O. Krugmann and J.Â Hartmann, â€œSentiment analysis in the age of generative ai,â€ Customer Needs and Solutions , vol.Â 11, p.Â 3, Mar. 2024.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[74] S.Â Rathje, D.-M. Mirea, I.Â Sucholutsky, R.Â Marjieh, C.Â E. Robertson, and J.Â J. VanÂ Bavel, â€œGpt is an effective tool for multilingual psychological text analysis,â€ Proceedings of the National Academy of Sciences , vol.Â 121, Aug. 2024.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[75] H.Â Fei, B.Â Li, Q.Â Liu, L.Â Bing, F.Â Li, and T.-S. Chua, â€œReasoning implicit sentiment with chain-of-thought prompting,â€ in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics , (Toronto, Canada), pp.Â 1171â€“1182, Association for Computational Linguistics, July 2023.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[76] K.Â Scaria, H.Â Gupta, S.Â Goyal, S.Â Sawant, S.Â Mishra, and C.Â Baral, â€œInstructabsa: Instruction learning for aspect based sentiment analysis,â€ in Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers) , (Mexico City, Mexico), pp.Â 720â€“736, Association for Computational Linguistics, June 2024.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[77] A.Â Joshi, P.Â Bhattacharyya, and M.Â J. Carman, â€œAutomatic sarcasm detection: A survey,â€ ACM Comput. Surv. , vol.Â 50, Sept. 2017.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[78] A.Â Ghosh, G.Â Li, T.Â Veale, P.Â Rosso, E.Â Shutova, J.Â Barnden, and A.Â Reyes, â€œSemEval-2015 task 11: Sentiment analysis of figurative language in Twitter,â€ in Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015) (P.Â Nakov, T.Â Zesch, D.Â Cer, and D.Â Jurgens, eds.), (Denver, Colorado), pp.Â 470â€“478, Association for Computational Linguistics, June 2015.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[79] R.Â J. Kreuz and R.Â M. Roberts, â€œThe use of verbal irony: Cues and constraints,â€ Metaphor and Symbol , vol.Â 11, no.Â 1, pp.Â 23â€“38, 1996.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[80] X.Â Glorot, A.Â Bordes, and Y.Â Bengio, â€œDomain adaptation for large-scale sentiment classification: A deep learning approach,â€ in Proceedings of the 28th International Conference on Machine Learning , pp.Â 513â€“520, 2011.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[81] F.Â Diaz, B.Â Mitra, and N.Â Craswell, â€œQuery expansion with locally-trained word embeddings,â€ in Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics , pp.Â 367â€“377, 2016.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[82] A.Â Lazaridou, A.Â Kuncoro, E.Â Gribovskaya, D.Â Agrawal, A.Â LiÅ¡ka, T.Â Terzi, M.Â Gimenez, C.Â d.Â M. dâ€™Autume, T.Â Kocisky, S.Â Ruder, D.Â Yogatama, K.Â Cao, S.Â Young, and P.Â Blunsom, â€œMind the gap: assessing temporal generalization in neural language models,â€ in Proceedings of the 35th International Conference on Neural Information Processing Systems , NIPS â€™21, (Red Hook, NY, USA), Curran Associates Inc., 2021.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[83] M.Â McCloskey and N.Â Cohen, â€œCatastrophic interference in connectionist networks: The sequential learning problem,â€ Psychology of Learning and Motivation - Advances in Research and Theory , vol.Â 24, pp.Â 109â€“165, Jan. 1989.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[84] P.Â Bhatia, Y.Â Ji, and J.Â Eisenstein, â€œBetter document-level sentiment analysis from RST discourse parsing,â€ in Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (L.Â MÃ rquez, C.Â Callison-Burch, and J.Â Su, eds.), (Lisbon, Portugal), pp.Â 2212â€“2218, Association for Computational Linguistics, Sept. 2015.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[85] C.Â Banea, R.Â Mihalcea, J.Â Wiebe, and S.Â Hassan, â€œMultilingual subjectivity analysis using machine translation,â€ in Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (M.Â Lapata and H.Â T. Ng, eds.), (Honolulu, Hawaii), pp.Â 127â€“135, Association for Computational Linguistics, Oct. 2008.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[86] P.Â KraljÂ Novak, J.Â SmailoviÄ‡, B.Â Sluban, and I.Â MozetiÄ, â€œSentiment of emojis,â€ PLOS ONE , vol.Â 10, pp.Â 1â€“22, 12 2015.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[87] J.Â Barnes, R.Â Klinger, and S.Â SchulteÂ im Walde, â€œBilingual sentiment embeddings: Joint projection of sentiment across languages,â€ in Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (I.Â Gurevych and Y.Â Miyao, eds.), (Melbourne, Australia), pp.Â 2483â€“2493, Association for Computational Linguistics, July 2018.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[88] A.Â Conneau, K.Â Khandelwal, N.Â Goyal, V.Â Chaudhary, G.Â Wenzek, F.Â GuzmÃ¡n, E.Â Grave, M.Â Ott, L.Â Zettlemoyer, and V.Â Stoyanov, â€œUnsupervised cross-lingual representation learning at scale,â€ in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (D.Â Jurafsky, J.Â Chai, N.Â Schluter, and J.Â Tetreault, eds.), (Online), pp.Â 8440â€“8451, Association for Computational Linguistics, July 2020.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[89] R.Â GonzÃ¡lez-IbÃ¡Ã±ez, S.Â Muresan, and N.Â Wacholder, â€œIdentifying sarcasm in Twitter: A closer look,â€ in Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies , (Portland, Oregon, USA), pp.Â 581â€“586, Association for Computational Linguistics, June 2011.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[90] I.Â Beltagy, M.Â E. Peters, and A.Â Cohan, â€œLongformer: The long-document transformer,â€ 2020.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[91] M.Â Danilevsky, K.Â Qian, R.Â Aharonov, Y.Â Katsis, B.Â Kawas, and P.Â Sen, â€œA survey of the state of explainable AI for natural language processing,â€ in Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing (K.-F. Wong, K.Â Knight, and H.Â Wu, eds.), (Suzhou, China), pp.Â 447â€“459, Association for Computational Linguistics, Dec. 2020.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[92] R.Â Schwartz, J.Â Dodge, N.Â A. Smith, and O.Â Etzioni, â€œGreen ai,â€ in Communications of the ACM , 2020.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[93] W.Â Yin, J.Â Hay, and D.Â Roth, â€œBenchmarking zero-shot text classification: Datasets, evaluation and entailment approach,â€ in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) (K.Â Inui, J.Â Jiang, V.Â Ng, and X.Â Wan, eds.), (Hong Kong, China), pp.Â 3914â€“3923, Association for Computational Linguistics, Nov. 2019.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[94] J.Â Wei, X.Â Wang, D.Â Schuurmans, M.Â Bosma, F.Â Xia, E.Â Chi, Q.Â V. Le, and D.Â Zhou, â€œChain-of-thought prompting elicits reasoning in large language models,â€ in Advances in Neural Information Processing Systems , vol.Â 35, pp.Â 24824â€“24837, 2022.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[95] Y.Â Gu, X.Â Han, Z.Â Liu, and M.Â Huang, â€œPPT: Pre-trained prompt tuning for few-shot learning,â€ in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (S.Â Muresan, P.Â Nakov, and A.Â Villavicencio, eds.), (Dublin, Ireland), pp.Â 8410â€“8423, Association for Computational Linguistics, May 2022.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[96] M.Â Hu and B.Â Liu, â€œMining and summarizing customer reviews,â€ in Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , KDD â€™04, (New York, NY, USA), p.Â 168â€“177, Association for Computing Machinery, 2004.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[97] A.Â BagherÂ Zadeh, P.Â P. Liang, S.Â Poria, E.Â Cambria, and L.-P. Morency, â€œMultimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph,â€ in Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (I.Â Gurevych and Y.Â Miyao, eds.), (Melbourne, Australia), pp.Â 2236â€“2246, Association for Computational Linguistics, July 2018.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[98] S.Â Lai, X.Â Hu, H.Â Xu, Z.Â Ren, and Z.Â Liu, â€œMultimodal sentiment analysis: A survey,â€ 2023.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[99] Y.Â Khare, V.Â Bagal, M.Â Mathew, A.Â Devi, U.Â D. Priyakumar, and C.Â Jawahar, â€œMmbert: Multimodal bert pretraining for improved medical vqa,â€ in 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI) , pp.Â 1033â€“1036, 2021.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[100] S.Â Poria, N.Â Majumder, R.Â Mihalcea, and E.Â Hovy, â€œEmotion recognition in conversation: Research challenges, datasets, and recent advances,â€ IEEE Access , vol.Â 7, pp.Â 100943â€“100953, 2019.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[101] E.Â Sheng, K.-W. Chang, P.Â Natarajan, and N.Â Peng, â€œThe woman worked as a babysitter: On biases in language generation,â€ in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) (K.Â Inui, J.Â Jiang, V.Â Ng, and X.Â Wan, eds.), (Hong Kong, China), pp.Â 3407â€“3412, Association for Computational Linguistics, Nov. 2019.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[102] S.Â Kiritchenko and S.Â Mohammad, â€œExamining gender and race bias in two hundred sentiment analysis systems,â€ in Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics (M.Â Nissim, J.Â Berant, and A.Â Lenci, eds.), (New Orleans, Louisiana), pp.Â 43â€“53, Association for Computational Linguistics, June 2018.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[103] A.Â Radford, J.Â W. Kim, C.Â Hallacy, A.Â Ramesh, G.Â Goh, S.Â Agarwal, G.Â Sastry, A.Â Askell, P.Â Mishkin, J.Â Clark, G.Â Krueger, and I.Â Sutskever, â€œLearning transferable visual models from natural language supervision,â€ in Proceedings of the 38th International Conference on Machine Learning (M.Â Meila and T.Â Zhang, eds.), vol.Â 139 of Proceedings of Machine Learning Research , pp.Â 8748â€“8763, PMLR, 18â€“24 Jul 2021.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[104] J.Â He, L.Â Wang, L.Â Liu, J.Â Feng, and H.Â Wu, â€œLong document classification from local word glimpses via recurrent attention learning,â€ IEEE Access , vol.Â 7, pp.Â 40707â€“40718, 2019.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[105] S.Â Amir, B.Â C. Wallace, H.Â Lyu, P.Â Carvalho, and M.Â J. Silva, â€œModelling context with user embeddings for sarcasm detection in social media,â€ in Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (S.Â Riezler and Y.Â Goldberg, eds.), (Berlin, Germany), pp.Â 167â€“177, Association for Computational Linguistics, Aug. 2016.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[106] T.Â Ranasinghe and M.Â Zampieri, â€œMultilingual offensive language identification with cross-lingual embeddings,â€ in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) (B.Â Webber, T.Â Cohn, Y.Â He, and Y.Â Liu, eds.), (Online), pp.Â 5838â€“5844, Association for Computational Linguistics, Nov. 2020.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[107] N.Â F. Rajani, B.Â McCann, C.Â Xiong, and R.Â Socher, â€œExplain yourself! leveraging language models for commonsense reasoning,â€ in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (A.Â Korhonen, D.Â Traum, and L.Â MÃ rquez, eds.), (Florence, Italy), pp.Â 4932â€“4942, Association for Computational Linguistics, July 2019.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[108] Y.Â Cai, X.Â Li, Y.Â Zhang, J.Â Li, F.Â Zhu, and L.Â Rao, â€œMultimodal sentiment analysis based on multi-layer feature fusion and multi-task learning,â€ Scientific Reports , vol.Â 15, p.Â 2126, Jan 2025.",
          "urls": [],
          "dois": [],
          "pdf_links": []
        }
      ],
      "references_dois": [],
      "fallback_urls": [],
      "errors": [],
      "missing_fields": [
        "doi",
        "versions",
        "last_updated_raw",
        "published_date",
        "references_dois"
      ],
      "url_hint_if_missing": "Champs manquants: doi, versions, last_updated_raw, published_date, references_dois. Tu peux vÃ©rifier ici: abs=https://arxiv.org/abs/2601.07235 | html=https://arxiv.org/html/2601.07235v1 | pdf=https://arxiv.org/pdf/2601.07235"
    },
    {
      "arxiv_id": "2601.06874",
      "title": "MVGGT: Multimodal Visual Geometry Grounded Transformer for Multiview 3D Referring Expression Segmentation",
      "authors": [
        "Changli Wu",
        "Haodong Wang",
        "Jiayi Ji",
        "Yutian Yao",
        "Chunsai Du",
        "Jihua Kang",
        "Yanwei Fu",
        "Liujuan Cao"
      ],
      "abstract": "Most existing 3D referring expression segmentation (3DRES) methods rely on dense, high-quality point clouds, while real-world agents such as robots and mobile phones operate with only a few sparse RGB views and strict latency constraints. We introduce Multi-view 3D Referring Expression Segmentation (MV-3DRES), where the model must recover scene structure and segment the referred object directly from sparse multi-view images. Traditional two-stage pipelines, which first reconstruct a point cloud and then perform segmentation, often yield low-quality geometry, produce coarse or degraded target regions, and run slowly. We propose the Multimodal Visual Geometry Grounded Transformer (MVGGT), an efficient end-to-end framework that integrates language information into sparse-view geometric reasoning through a dual-branch design. Training in this setting exposes a critical optimization barrier, termed Foreground Gradient Dilution (FGD), where sparse 3D signals lead to weak supervision. To resolve this, we introduce Per-view No-target Suppression Optimization (PVSO), which provides stronger and more balanced gradients across views, enabling stable and efficient learning. To support consistent evaluation, we build MVRefer, a benchmark that defines standardized settings and metrics for MV-3DRES. Experiments show that MVGGT establishes the first strong baseline and achieves both high accuracy and fast inference, outperforming existing alternatives. Code and models are publicly available at https://mvggt.github.io.",
      "submitted_date": "11 January, 2026",
      "abs_url": "https://arxiv.org/abs/2601.06874",
      "pdf_url": "https://arxiv.org/pdf/2601.06874",
      "doi": "",
      "versions": [],
      "last_updated_raw": "",
      "html_url": "https://arxiv.org/html/2601.06874v1",
      "published_date": "",
      "content_text": "MVGGT: Multimodal Visual Geometry Grounded Transformer for\nMultiview 3D Referring Expression Segmentation\nChangli Wu\n1,2,â€ \n,\nHaodong Wang\n1,â€ \n,\nJiayi Ji\n1\n,\nYutian Yao\n5\n,\nChunsai Du\n4\n,\nJihua Kang\n4\n,\nYanwei Fu\n3,2\n,\nLiujuan Cao\n1,âˆ—\n1\nXiamen University\n2\nShanghai Innovation Institute\n3\nFudan University\n4\nByteDance\n5\nTianjin University of Science and Technology\n{wuchangli, wahadon}@stu.xmu.edu.cn, jjyxmu@gmail.com, 22201316@mail.tust.edu.cn,\n{duchunsai, kangjihua}@bytedance.com, yanweifu@fudan.edu.cn, caoliujuan@xmu.edu.cn\nAbstract\nMost existing 3D referring expression segmentation (3DRES) methods rely on dense, high-quality point clouds, while real-world agents such as robots and mobile phones operate with only a few sparse RGB views and strict latency constraints. We introduce Multi-view 3D Referring Expression Segmentation (MV-3DRES), where the model must recover scene structure and segment the referred object directly from sparse multi-view images. Traditional two-stage pipelines, which first reconstruct a point cloud and then perform segmentation, often yield low-quality geometry, produce coarse or degraded target regions, and run slowly. We propose the Multimodal Visual Geometry Grounded Transformer (MVGGT), an efficient end-to-end framework that integrates language information into sparse-view geometric reasoning through a dual-branch design. Training in this setting exposes a critical optimization barrier, termed Foreground Gradient Dilution (FGD), where sparse 3D signals lead to weak supervision. To resolve this, we introduce Per-view No-target Suppression Optimization (PVSO), which provides stronger and more balanced gradients across views, enabling stable and efficient learning. To support consistent evaluation, we build MVRefer, a benchmark that defines standardized settings and metrics for MV-3DRES. Experiments show that MVGGT establishes the first strong baseline and achieves both high accuracy and fast inference, outperforming existing alternatives. Code and models are publicly available at\nhttps://mvggt.github.io\n.\nâ€ \nâ€ \nâ€ \nEqual Contribution.\nâˆ—\nCorresponding Author.\nFigure 1\n:\nThe Reality Gap: From Idealized 3D RES to Real-World MV-3DRES.\n(a)\nTraditional 3D RES depends on dense, high-quality point clouds produced by slow offline scanning and heavy reconstruction.\n(b)\nApplied to sparse, low-quality point clouds from real-world RGB views, these models fail to generalize.\n(c)\nWe introduce MV-3DRES, which uses sparse multi-view RGB inputs and text to achieve robust joint reconstruction and perception, enabled by our MVGGT model.\n1\n1\nfootnotetext:\nâ€ \nEqual Contribution.\n2\n2\nfootnotetext:\nâˆ—\nCorresponding Author.\n1\nIntroduction\nGrounding natural language in 3D physical scenes is fundamental to embodied AI. A prominent formulation is 3D referring expression segmentation (3DRES), where a model segments an object in a 3D scene given a textual description. Although recent methods\n[\n3\n,\n1\n,\n53\n,\n55\n,\n17\n,\n48\n,\n47\n,\n46\n]\nhave achieved strong results, they are built upon a rarely questioned assumption: the availability of dense, complete, and reliable point clouds. Such point clouds typically require LIDAR sensors or lengthy RGB-D SLAM pipelines like BundleFusion\n[\n5\n]\n, which demand deliberate scanning and heavy offline processing. This assumption stands in stark contrast to real-world agentsâ€”robots, AR glasses, mobile devicesâ€”that perceive environments through only a few casually captured RGB views.\nIn real settings, high-fidelity geometry is the exception. Sparse multi-view images produce 3D reconstructions that are noisy, incomplete, and often ambiguous (Figure\n1\n(b)). Existing 3DRES models, trained on idealized point clouds, collapse under such inputs. This exposes a fundamental limitation: current 3DRES is sensor-privileged and misaligned with the actual sensing conditions of embodied systems. It motivates a central question:\nHow can we achieve language-grounded 3D perception when complete geometry is no longer given but must be inferred from sparse, inconsistent views?\nWe address this by introducing Multi-view 3D Referring Expression Segmentation (MV-3DRES), a new setting where the model must jointly reconstruct the scene and segment the referred object directly from sparse RGB views (Figure\n1\n(c)). MV-3DRES is inherently challenging: the model must reason over missing structure, integrate information across misaligned viewpoints, and resolve linguistic ambiguities without access to dense 3D input.\nConventional pipelines fail in this regime. Purely 2D methods cannot enforce global 3D consistency, since they operate on isolated views and cannot resolve depth ordering, occlusion relationships, or spatial relations such as â€œin front ofâ€ or â€œon the left.â€ As a result, back-projecting their per-view masks produces fragmented or conflicting 3D predictions. Two-stage â€œreconstruct-then-segmentâ€ pipelines face a different failure mode: sparse inputs yield point clouds that are noisy, incomplete, and structurally distorted, making it difficult for 3DRES models to recover full object extents. Moreover, running a full reconstruction before segmentation incurs substantial latency, limiting practical deployment.\nTo this end, we propose the Multimodal Visual Geometry Grounded Transformer (MVGGT), the first end-to-end architecture designed specifically for MV-3DRES. MVGGT adopts a dual-branch paradigm: a frozen geometric branch provides camera poses, depth cues, and a coarse structural scaffold, while a multimodal branch injects linguistic cues into sparse-view visual features through cross-view, cross-modal attention. This design embodies a key conceptual shift: language is intertwined with geometric reasoning from the start, enabling it to guide evidence aggregation and scene disambiguation long before a complete 3D representation exists.\nFigure 2\n:\nIllustration of Foreground Gradient Dilusion Problem of Global 3D DICE loss and Per-view No-Target Suppression Optimization.\nHowever, sparse-view learning introduces a fundamental optimization challenge. Sparse multi-view reconstruction produces point clouds in which the target instance is represented by only a very small number of scattered points, far fewer than in the dense point clouds used by conventional 3DRES methods. Under such extreme foreground sparsity, standard 3D losses such as Dice become ineffective: gradients from the target region are overwhelmed by background points, leading to Foreground Gradient Dilution (FGD) and causing the optimization to stagnate in early trainingâ€”gradients are too small to escape poor local minima, as illustrated in Figure\n2\n. The problem is exacerbated by view-dependent visibilityâ€”some views contain clear target evidence, while others provide almost noneâ€”making uniform 3D supervision unstable and noisy. To mitigate FGD, we introduce Per-view No-target Suppression Optimization (PVSO), which shifts supervision back to 2D view space where the target occupies a larger and more reliable area. This per-view formulation amplifies meaningful gradients from informative views and suppresses misleading signals from target-absent views, resulting in significantly more stable and effective training.\nFinally, to standardize evaluation, we construct MVRefer, the first benchmark defining settings, metrics, and data protocol for MV-3DRES. Extensive experiments show that MVGGT provides a strong baseline and significantly outperforms existing alternatives. Taken together, our contributions are fourfold:\nâ€¢\nWe identify and formalize MV-3DRES, a new problem setting that aligns 3D grounding with realistic sensing conditions.\nâ€¢\nWe propose MVGGT, a novel dual-branch architecture unifying geometric scaffolding with cross-view, language-aware perception.\nâ€¢\nWe analyze and address the Foreground Gradient Dilution challenge via PVSO, offering a principled optimization strategy tailored for sparse 3D supervision.\nâ€¢\nWe construct the MVRefer benchmark, defining standardized settings and metrics for MV-3DRES and providing the first strong baseline.\n2\nRelated Work\n2.1\nTraditional 3D Referring Segmentation\n3D grounding\n[\n19\n,\n18\n,\n54\n,\n36\n,\n21\n,\n11\n,\n33\n,\n30\n]\naims to locate a specific object in a 3D scene based on a unique natural language description\n[\n3\n,\n1\n]\n,which is part of vision-language tasks\n[\n10\n,\n13\n,\n49\n,\n12\n,\n24\n]\n. Following this, the 3D-RES (3D Referring Segmentation) task aims to segment a specific object within a point cloud based on a textual query. The field has evolved from foundational two-stage paradigms\n[\n17\n,\n51\n,\n34\n]\n(relying on object proposals and language matching) to recent end-to-end architectures\n[\n30\n,\n15\n,\n26\n,\n14\n,\n34\n,\n47\n,\n27\n,\n48\n,\n46\n]\ndemonstrating high efficacy through advanced cross-modal fusion. Despite this progress, the sub-field shares a critical limitation: the requirement for high-quality, dense point clouds. This expensive geometric data is inaccessible to many real-world embodied agents that must rely on sparse, online RGB captures.\n2.2\nMulti-View Feed-forward Reconstruction\nReconstructing 3D geometry from multi-view RGB images provides a practical solution to the input ambiguity in sparse-view settings. Early feed-forward approaches such as DUSt3R and MASt3R\n[\n22\n,\n42\n]\nintroduced coupled scene representations but required heavy post-processing or integration with classical SfM/SLAM pipelines\n[\n8\n,\n9\n,\n31\n,\n32\n]\nfor unconstrained reconstruction. Later works improved efficiency and stability by replacing classical optimization with Transformer-based latent state propagation, as demonstrated by Spann3R, CUT3R, and MUSt3R\n[\n39\n,\n41\n,\n2\n]\n. Streaming models like WinT3R\n[\n45\n]\nfurther enabled real-time performance via sliding-window processing and global camera token pooling.\nMore recent architecturesâ€”VGGT and its successors\n[\n40\n,\n44\n,\n35\n,\n38\n,\n7\n]\nâ€”adopt alternating-attention designs to achieve robust generalized reconstruction, while extensions address semantic and multi-task perception\n[\n43\n,\n37\n,\n23\n,\n50\n]\n. This progress culminates in universal backbones such as MapAnything\n[\n20\n]\n, which produce fully factored, metric-aware scene representations.\n3\nMV-3DRES Task and MVRefer Benchmark\n3.1\nTask Formulation\nWe formalize the Multi-view 3D Referring Segmentation (MV-3DRES) task to align 3D language grounding with the sensing constraints of real-world agents. Instead of assuming access to pre-constructed dense point clouds, the model operates directly on sparse multi-view RGB images.\nGiven a set of\nN\nN\nRGB views\nI\n=\n{\nI\ni\n}\ni\n=\n1\nN\nI=\\{I_{i}\\}_{i=1}^{N}\nand a natural-language referring expression\nT\nT\n, the goal is to learn a function\nf\n:\n(\nI\n,\nT\n)\nâ†’\n(\nS\nâ€²\n,\nM\n)\n,\nf:(I,T)\\rightarrow(S^{\\prime},M),\n(1)\nwhere\nS\nâ€²\nâˆˆ\nâ„\nK\nÃ—\n3\nS^{\\prime}\\in\\mathbb{R}^{K\\times 3}\ndenotes the reconstructed 3D point cloud containing\nK\nK\npoints, and\nM\nâˆˆ\n{\n0\n,\n1\n}\nK\nM\\in\\{0,1\\}^{K}\nis the corresponding 3D binary mask marking the points that belong to the object referred to by\nT\nT\n. The model must infer both geometry and semantics from the same sparse observations, without any ground-truth 3D input at inference time.\nThis formulation introduces challenges not present in standard 3DRES. Sparse multi-view observations generate incomplete and noisy geometry, forcing the model to couple reconstruction and grounding. Spatial relations described in language, such as â€œon the left of the chair,â€ must be resolved across viewpoints with inconsistent visibility. Moreover, the target object often occupies only a small portion of the available views, yielding severe foreground sparsity and weak supervisory signals, which we later characterize as Foreground Gradient Dilution (FGD).\n3.2\nThe MVRefer Benchmark\nTo support systematic evaluation of MV-3DRES, we construct MVRefer, a benchmark built upon ScanRefer\n[\n3\n]\nand the underlying ScanNet sequences\n[\n4\n]\n. MVRefer is designed to emulate how an embodied agent perceives a scene through a limited number of casual views.\n3.2.1\nBenchmark Setting\nFor each languageâ€“object pair in ScanRefer\n[\n3\n]\n, we sample\nN\n=\n8\nN=8\nRGB frames from the raw ScanNet video stream\n[\n4\n]\nat uniform temporal intervals to approximate sparse, on-the-fly observations. Sparse sampling creates a solvability issue: the target may be absent from all selected frames. To ensure each sample remains resolvable, we perform a visibility validation step. If none of the initial eight images contain the target, we replace one no-target frame with a randomly chosen target-visible frame. This guarantees at least one positive view while naturally preserving a high proportion of no-target views, maintaining the difficulty inherent to sparse-view grounding.\n3.2.2\nEvaluation Metrics and Splits\nEvaluating MV-3DRES requires metrics that disentangle grounding quality from reconstruction quality. Since both outputs in\n(\nS\nâ€²\n,\nM\n)\n(S^{\\prime},M)\nare jointly predicted from sparse inputs, reconstruction errors can obscure the modelâ€™s true grounding ability.\nTraditional 3D Metric.\nWe report global 3D mean IoU,\nmIoU\nglobal\n=\nIoU\nâ€‹\n(\nM\n,\nM\nâˆ—\n)\n,\n\\text{mIoU}_{\\text{global}}=\\text{IoU}(M,M^{\\ast}),\n(2)\nwhere\nM\nâˆ—\nM^{\\ast}\ndenotes the ground-truth mask projected onto the reconstructed point cloud\nS\nâ€²\nS^{\\prime}\n. Although standard,\nmIoU\nglobal\nentangles segmentation performance with the fidelity of\nS\nâ€²\nS^{\\prime}\n, making it insufficient as the primary diagnostic measure.\nMulti-view Diagnostic Metrics.\nTo isolate grounding behaviors, we reproject the predicted 3D mask\nM\nM\ninto each view using the known camera intrinsics and extrinsics. Let\nP\ni\nâ€‹\n(\nM\n)\nP_{i}(M)\ndenote the projected 2D mask for view\ni\ni\n, and\nP\ni\nâ€‹\n(\nM\nâˆ—\n)\nP_{i}(M^{\\ast})\nits ground-truth counterpart. We compute:\nmIoU\nview\n=\n1\nN\nâ€‹\nâˆ‘\ni\n=\n1\nN\nIoU\nâ€‹\n(\nP\ni\nâ€‹\n(\nM\n)\n,\nP\ni\nâ€‹\n(\nM\nâˆ—\n)\n)\n,\n\\text{mIoU}_{\\text{view}}=\\frac{1}{N}\\sum_{i=1}^{N}\\text{IoU}\\left(P_{i}(M),P_{i}(M^{\\ast})\\right),\n(3)\nmIoU\npos\n=\n1\n|\nğ’±\n+\n|\nâ€‹\nâˆ‘\ni\nâˆˆ\nğ’±\n+\nIoU\nâ€‹\n(\nP\ni\nâ€‹\n(\nM\n)\n,\nP\ni\nâ€‹\n(\nM\nâˆ—\n)\n)\n,\n\\text{mIoU}_{\\text{pos}}=\\frac{1}{|\\mathcal{V}^{+}|}\\sum_{i\\in\\mathcal{V}^{+}}\\text{IoU}\\left(P_{i}(M),P_{i}(M^{\\ast})\\right),\n(4)\nmIoU\nneg\n=\n1\n|\nğ’±\nâˆ’\n|\nâ€‹\nâˆ‘\ni\nâˆˆ\nğ’±\nâˆ’\nIoU\nâ€‹\n(\nP\ni\nâ€‹\n(\nM\n)\n,\nP\ni\nâ€‹\n(\nM\nâˆ—\n)\n)\n,\n\\text{mIoU}_{\\text{neg}}=\\frac{1}{|\\mathcal{V}^{-}|}\\sum_{i\\in\\mathcal{V}^{-}}\\text{IoU}\\left(P_{i}(M),P_{i}(M^{\\ast})\\right),\n(5)\nwhere\nğ’±\n+\n\\mathcal{V}^{+}\nand\nğ’±\nâˆ’\n\\mathcal{V}^{-}\ndenote the sets of target-visible and no-target views, respectively. These metrics shed light on grounding precision (\nmIoU\npos\n) and suppression ability (\nmIoU\nneg\n), both of which are essential for robust performance under sparse supervision.\nDifficulty Splits.\nTo evaluate robustness under varying signal sparsity, we define two difficulty splits based on the targetâ€™s 2D pixel ratio. A sample is categorized as\nhard\nif the target occupies less than\n5\n%\n5\\%\nof pixels in all its visible views, and\neasy\nif at least one view contains at least\n5\n%\n5\\%\ntarget pixels. This separation allows us to isolate performance differences arising from the strength of view-specific supervision.\nFigure 3\n:\nArchitecture of MVGGT, which comprises a frozen Reconstruction Branch that establishes geometric structure and a trainable Multimodal Branch that integrates language into sparse-view visual reasoning.\n4\nMethod\nWe introduce the\nMultimodal Visual Geometry Grounded Transformer (MVGGT)\n, an end-to-end dual-branch framework tailored for the MV-3DRES task, as shown in Figure\n3\n.\n4.1\nThe proposed MVGGT\nMVGGT is designed to jointly recover 3D geometry and perform language-conditioned segmentation from sparse views. The separation into two branches allows the model to exploit a stable geometric scaffold while learning multimodal representations aligned with the text query.\nInputs and Encoders.\nGiven\nN\nN\ninput images\nI\n=\n{\nI\ni\n}\ni\n=\n1\nN\nI=\\{I_{i}\\}_{i=1}^{N}\nand a referring expression\nT\nT\n, each image is encoded by a frame-wise ViT, yielding patch embeddings\nF\ni\nvis\nâˆˆ\nâ„\nP\nÃ—\nD\nF_{i}^{\\text{vis}}\\in\\mathbb{R}^{P\\times D}\n, where\nP\nP\nis the number of patches and\nD\nD\nthe feature dimension. The text is tokenized and processed by a language encoder to produce word embeddings\nF\nlang\nâˆˆ\nâ„\nW\nÃ—\nD\nF^{\\text{lang}}\\in\\mathbb{R}^{W\\times D}\n, where\nW\nW\ndenotes token count.\nFrozen Reconstruction Branch.\nThe reconstruction branch is a geometry-aware transformer with\nL\nL\nblocks. Each block alternates between frame-level self-attention and global cross-view attention, progressively building view-consistent structural cues. Let\nF\nâ„“\ngeo\nF_{\\ell}^{\\text{geo}}\ndenote the features at block\nâ„“\nâˆˆ\n{\n1\n,\nâ€¦\n,\nL\n}\n\\ell\\in\\{1,\\dots,L\\}\n. These features are fed to a reconstruction decoder that predicts camera poses and depth maps, which are back-projected into a coarse point cloud\nS\nâ€²\nS^{\\prime}\n. All parameters in this branch remain frozen, ensuring a stable geometric prior across training and removing the need to re-learn 3D geometry from sparse images.\nTrainable Multimodal Branch.\nThe multimodal branch contains\nL\nmulti\n=\nL\n/\n3\nL_{\\text{multi}}=L/3\ntransformer blocks. Its goal is to fuse geometric cues with text-conditioned visual features. Since the two branches have different depths, we align their interactions by injecting geometric features from the final\nL\n/\n3\nL/3\nblocks of the reconstruction branch into all\nL\nmulti\nL_{\\text{multi}}\nblocks of the multimodal branch.\nGeometric Injection.\nThe multimodal branch contains\nL\nmulti\n=\nL\n/\n3\nL_{\\text{multi}}=L/3\nblocks, and each of them receives geometric guidance from the\nfinal\nL\nmulti\nL_{\\text{multi}}\nblocks of the reconstruction branch. Concretely, for the\nl\nâ€²\nl^{\\prime}\n-th multimodal block (\nl\nâ€²\n=\n1\n,\nâ€¦\n,\nL\nmulti\nl^{\\prime}=1,\\dots,L_{\\text{multi}}\n), we take the geometric feature\nF\nl\ngeo\nF_{l}^{\\text{geo}}\nfrom the\nl\nl\n-th reconstruction block, where\nl\nl\nruns over the last\nL\nmulti\nL_{\\text{multi}}\nlayers in order (i.e., the reconstruction branchâ€™s\n(\nL\nâˆ’\nL\nmulti\n+\n1\n)\n(L-L_{\\text{multi}}+1)\n-th layer provides geometry to the first multimodal block, the next layer to the second, and so on).\nThese geometric features are passed through a zero-initialized\n1\nÃ—\n1\n1\\times 1\nconvolution\nğ’µ\n\\mathcal{Z}\n[\n52\n]\n, which projects them into the multimodal feature space. The input to the\nl\nâ€²\nl^{\\prime}\n-th multimodal block is then\nF\nl\nâ€²\nin\n=\nF\nl\nâ€²\nâˆ’\n1\nout\n+\nğ’µ\nâ€‹\n(\nF\nl\ngeo\n)\n,\nF_{l^{\\prime}}^{\\text{in}}=F_{l^{\\prime}-1}^{\\text{out}}+\\mathcal{Z}(F_{l}^{\\text{geo}}),\n(6)\nwhere\nF\nl\nâ€²\nâˆ’\n1\nout\nF_{l^{\\prime}-1}^{\\text{out}}\nis the output of the preceding visual attention and cross attention. It allows the multimodal branch to progressively incorporate higher-level geometric cues without perturbing the pretrained reconstruction backbone.\nF\nl\nâ€²\nin\nF_{l^{\\prime}}^{\\text{in}}\nis then fed into the visual attention module to get\nF\nl\nâ€²\nvis\nF_{l^{\\prime}}^{\\text{vis}}\n.\nLanguage Injection.\nWithin each multimodal block, language injection is implemented through a standard cross-attention layer. Let\nF\nl\nâ€²\nin\nF_{l^{\\prime}}^{\\text{in}}\nbe the visual tokens entering block\nl\nâ€²\nl^{\\prime}\n. Query, key, and value projections are defined as\nQ\n=\nF\nl\nâ€²\nvis\nâ€‹\nW\nQ\n,\nK\n=\nF\nlang\nâ€‹\nW\nK\n,\nV\n=\nF\nlang\nâ€‹\nW\nV\n,\nQ=F_{l^{\\prime}}^{\\text{vis}}W_{Q},\\quad K=F^{\\text{lang}}W_{K},\\quad V=F^{\\text{lang}}W_{V},\n(7)\nwith\nW\nQ\nW_{Q}\n,\nW\nK\nW_{K}\n, and\nW\nV\nâˆˆ\nâ„\nD\nÃ—\nD\nW_{V}\\in\\mathbb{R}^{D\\times D}\nlearnable matrices. Attention is computed as\nF\nl\nâ€²\nout\n=\nsoftmax\nâ€‹\n(\nQ\nâ€‹\nK\nâŠ¤\nD\n)\nâ€‹\nV\n,\nF_{l^{\\prime}}^{\\text{out}}=\\mathrm{softmax}\\!\\left(\\frac{QK^{\\top}}{\\sqrt{D}}\\right)V,\n(8)\nand added back through a residual path. This multi-level injection allows text cues to guide feature aggregation across views.\nDecoders and Outputs.\nFinally, the multimodal features\nF\nL\nmulti\nmulti\nF_{L_{\\text{multi}}}^{\\text{multi}}\nare decoded into per-view masks\n{\nM\ni\n}\ni\n=\n1\nN\n\\{M_{i}\\}_{i=1}^{N}\n. Using the depths and camera parameters from the frozen reconstruction branch, these 2D masks are back-projected and aggregated on the reconstructed point cloud\nS\nâ€²\nS^{\\prime}\nto obtain the final 3D mask\nM\nM\n. The resulting multi-view predictions serve as the supervision targets in PVSO (Section\n4.3\n).\n4.2\nForeground Gradient Dilution\nTraining under the MV-3DRES setting is fundamentally hindered by the extreme sparsity of foreground points in the reconstructed 3D space. Let the Dice loss for 3D segmentation be\nâ„’\nDice\n=\n1\nâˆ’\n2\nâ€‹\nI\nU\n,\nI\n=\nâˆ‘\nj\np\nj\nâ€‹\ng\nj\n,\nU\n=\nâˆ‘\nj\np\nj\n+\nâˆ‘\nj\ng\nj\n,\n\\mathcal{L}_{\\text{Dice}}=1-\\frac{2I}{U},\\quad I=\\sum_{j}p_{j}g_{j},\\quad U=\\sum_{j}p_{j}+\\sum_{j}g_{j},\n(9)\nwhere\np\nj\np_{j}\nis the predicted probability at point\nj\nj\nand\ng\nj\nâˆˆ\n{\n0\n,\n1\n}\ng_{j}\\in\\{0,1\\}\ndenotes ground-truth labels. The gradient with respect to\np\nj\np_{j}\nis\nâˆ‚\nâ„’\nDice\nâˆ‚\np\nj\n=\n2\nâ€‹\n(\nI\nâˆ’\ng\nj\nâ€‹\nU\n)\nU\n2\n.\n\\frac{\\partial\\mathcal{L}_{\\text{Dice}}}{\\partial p_{j}}=\\frac{2(I-g_{j}U)}{U^{2}}.\n(10)\nDuring early training, predictions remain small and diffuse, yielding\nI\nâ‰ˆ\n0\nI\\approx 0\n. In MV-3DRES, reconstructed point clouds from sparse views are large (often\n10\n6\n10^{6}\nâ€“\n10\n7\n10^{7}\npoints) while the target instance typically occupies less than\n2\n%\n2\\%\nof them. Consequently, the union term\nU\nU\nis dominated by background, inflating by several orders of magnitude. For a foreground point (\ng\nj\n=\n1\ng_{j}=1\n),\nâˆ‚\nâ„’\nDice\nâˆ‚\np\nj\n|\ng\nj\n=\n1\nâ‰ˆ\nâˆ’\n2\nU\n,\n\\left.\\frac{\\partial\\mathcal{L}_{\\text{Dice}}}{\\partial p_{j}}\\right|_{g_{j}=1}\\approx-\\frac{2}{U},\n(11)\nwhose magnitude becomes extremely small when\nU\nU\nis large. Empirically, gradients fall to\n10\nâˆ’\n9\n10^{-9}\nâ€“\n10\nâˆ’\n11\n10^{-11}\n, far below the scale needed to drive meaningful updates. Although foreground points are present, their contribution to optimization becomes negligible, leading to stalled convergence. We refer to this failure mode as\nForeground Gradient Dilution (FGD)\n.\n4.3\nPer-view No-Target Suppression Optimization\nTo mitigate FGD, we propose\nPer-view No-Target Suppression Optimization (PVSO)\n, a view-wise supervision strategy that shifts early learning signals from sparse 3D space to the denser 2D image domain. This modification significantly reduces the imbalance between foreground and background, thereby amplifying effective gradients.\nPositive-aware Sampling.\nGiven the view set\nğ’±\n\\mathcal{V}\nof a scene, let\nğ’±\nt\n\\mathcal{V}_{t}\nand\nğ’±\nn\n\\mathcal{V}_{n}\ndenote target-visible and no-target views, respectively. PVSO samples a subset\nğ’±\nâ€²\n\\mathcal{V}^{\\prime}\nwhile enforcing a minimum foreground-view ratio\nÏ\nt\n=\n|\nğ’±\nt\n|\n|\nğ’±\nâ€²\n|\n,\n\\rho_{t}=\\frac{|\\mathcal{V}_{t}|}{|\\mathcal{V}^{\\prime}|},\n(12)\nensuring that each batch contains sufficient positive evidence. This prevents the optimization from collapsing to trivial background predictions.\n2D Gradient Concentration.\nFor each sampled view, the predicted 3D mask is projected onto the image plane, and a 2D Dice loss is applied. Since foreground regions typically occupy\n10\n10\nâ€“\n15\n%\n15\\%\nof pixels in visible viewsâ€”far larger than their\n<\n2\n%\n<2\\%\nproportion in 3D point cloudsâ€”the 2D Dice denominator\nU\n2D\nU_{\\text{2D}}\nbecomes substantially smaller:\nU\n2D\nâ‰ª\nU\n3D\n.\nU_{\\text{2D}}\\ll U_{\\text{3D}}.\n(13)\nGiven that the Dice gradient scales as\nğ’ª\nâ€‹\n(\n1\n/\nU\n)\n\\mathcal{O}(1/U)\n, the per-view supervision yields foreground gradients that are 1â€“3 orders of magnitude larger than in 3D. This concentrated 2D supervision strengthens early training signals.\nSuppression of No-Target Views.\nNo-target views often far outnumber target-visible ones. To avoid overwhelming the loss with trivial negatives, PVSO normalizes their contribution using\nw\ns\n=\n1\n|\nğ’±\nn\n|\n.\nw_{s}=\\frac{1}{|\\mathcal{V}_{n}|}.\n(14)\nThe complete PVSO objective is\nL\nPVSO\n=\n1\n|\nğ’±\nt\n|\n+\n1\n(\nâˆ‘\ni\nâˆˆ\nğ’±\nt\nL\nDice\nâ€‹\n(\nm\ni\n,\nM\ni\ngt\n)\n+\nw\ns\nâˆ‘\nj\nâˆˆ\nğ’±\nn\nL\nDice\n(\nm\nj\n,\nğŸ\n)\n)\n,\n\\begin{split}L_{\\text{PVSO}}=\\frac{1}{|\\mathcal{V}_{t}|+1}\\big(&\\sum_{i\\in\\mathcal{V}_{t}}L_{\\text{Dice}}(m_{i},M_{i}^{\\text{gt}})+\\\\\n&w_{s}\\sum_{j\\in\\mathcal{V}_{n}}L_{\\text{Dice}}(m_{j},\\mathbf{0})\\big),\\end{split}\n(15)\nwhere\nm\ni\nm_{i}\nis the predicted 2D mask for view\ni\ni\nand\nM\ni\ngt\nM_{i}^{\\text{gt}}\nis the corresponding ground-truth mask (empty for no-target views).\nJoint Objective.\nPVSO complements 3D supervision by providing dense, stable gradients during early training. The complete objective is\nL\ntotal\n=\nL\nBCE\n+\nÎ»\np\nâ€‹\nL\nPVSO\n,\nL_{\\text{total}}=L_{\\text{BCE}}+\\lambda_{p}L_{\\text{PVSO}},\n(16)\nwhere\nÎ»\np\n\\lambda_{p}\nbalances 2D and 3D signals. This formulation alleviates foreground gradient dilution and yields robust multimodal 3D grounding from sparse views.\nTable 1\n:\nPerformance comparison on the MVRefer benchmark.Metrics are all mIoU under different categories.\nMethod\nHard (\nâˆ¼\n\\sim\n40%)\nEasy (\nâˆ¼\n\\sim\n60%)\nUnique (\nâˆ¼\n\\sim\n19%)\nMultiple (\nâˆ¼\n\\sim\n81%)\nOverall\nglobal\nview\npos\nneg\nglobal\nview\npos\nneg\nglobal\nview\npos\nneg\nglobal\nview\npos\nneg\nglobal\nview\npos\nneg\ntwo-stage\n8.1\n8.6\n22.6\n10.0\n25.8\n28.2\n45.6\n21.6\n27.5\n41.1\n52.1\n30.1\n16.4\n15.4\n32.1\n13.8\n18.5\n20.3\n35.9\n16.9\n2D-Lift\n6.4\n15.0\n25.3\n11.9\n25.4\n24.1\n45.1\n12.3\n31.5\n30.9\n47.2\n19.2\n14.5\n17.9\n34.8\n10.4\n17.8\n20.4\n37.2\n12.1\nOurs\n24.4\n67.3\n31.6\n78.0\n50.1\n70.6\n52.2\n81.2\n65.2\n82.6\n64.5\n90.4\n33.8\n66.1\n39.0\n77.4\n39.9\n69.3\n44.0\n79.9\nTable 2\n:\nThe results of traditional 3D-RES and MV-3DRES tasks under original ScanRefer setting.\nMethod\nUnique (\nâˆ¼\n\\sim\n19%)\nMultiple (\nâˆ¼\n\\sim\n81%)\nOverall\nAcc@25\nAcc@50\nmIoU\nAcc@25\nAcc@50\nmIoU\nAcc@25\nAcc@50\nmIoU\nTraditional 3D-RES (with ground-truth point clouds)\nTGNN\n[\n17\n]\n69.3\n57.8\n50.7\n31.2\n26.6\n23.6\n38.6\n32.7\n28.8\n3D-STMN\n[\n48\n]\n89.3\n84.0\n74.5\n46.2\n29.2\n31.1\n54.6\n39.8\n39.5\nSegPoint\n[\n14\n]\n-\n-\n-\n-\n-\n-\n-\n-\n41.7\nReason3D\n[\n16\n]\n88.4\n84.2\n74.6\n50.5\n31.7\n34.1\n57.9\n41.9\n42.0\nRG-SAN\n[\n46\n]\n89.2\n84.3\n74.5\n55.0\n35.4\n37.4\n61.7\n44.9\n44.6\nLESS\n[\n27\n]\n-\n-\n-\n-\n-\n-\n53.2\n29.9\n33.7\n3D-LLaVA\n[\n6\n]\n-\n-\n-\n-\n-\n-\n-\n-\n43.3\nMV-3DRES (sparse RGB inputs only)\ntwo-stage\n43.8\n20.9\n27.5\n25.1\n12.2\n16.4\n28.7\n13.9\n18.5\n2D-Lift\n51.6\n26.0\n31.5\n21.5\n5.6\n14.5\n27.3\n9.6\n17.8\nOurs\n83.6\n74.5\n65.2\n49.2\n33.5\n33.8\n55.9\n41.5\n39.9\n5\nExperiments\n5.1\nImplementation Details and Setup\nMVGGT Configuration.\nWe adopt the Pi3\n[\n44\n]\nreconstruction backbone (36 blocks), which remains frozen throughout training. We use a frozen Roberta model\n[\n28\n]\nas the language encoder. The multimodal branch contains\nL\nmulti\n=\n12\nL_{\\text{multi}}=12\nblocks and is optimized end-to-end. Training uses AdamW\n[\n29\n]\nwith a\n1\nÃ—\n10\nâˆ’\n4\n1\\!\\times\\!10^{-4}\nlearning rate, batch size of 16, and 30 epochs on a single NVIDIA 4090 GPU. The PVSO weight\nÎ»\np\n\\lambda_{p}\nis fixed to\n1\n1\n.\nDataset and Metrics.\nAll evaluations are conducted on the proposed\nMVRefer\nbenchmark. We follow standard ScanRefer train/validation splits\n[\n3\n]\n. Alongside\nmIoU\nglobal\n\\text{mIoU}_{\\text{global}}\n, we report all diagnostic view-level metrics from Section\n3.2.2\n, with special focus on the\nHard\nand\nEasy\nsubsets, which most directly reflect the foreground sparsity challenge underlying FGD.\n5.2\nMain Results\nWe compare MVGGT with two representative MV-3DRES baselines:\n(1)\n2D-Lift\n, which lifts ReferDINO\n[\n25\n]\nmasks into 3D via Pi3\n[\n44\n]\n;\n(2)\ntwo-stage\n, which runs Pi3\n[\n44\n]\nreconstruction followed by LESS\n[\n27\n]\nfor referring segmentation.\nTable\n1\nshows that MVGGT consistently outperforms all baselines across difficulty levels. On the\nHard\nsplit, it achieves 24.4 global mIoUâ€”gains of 16.3 and 18.0 over two-stage and 2D-Liftâ€”and improves view mIoU by 52.3 over two-stage. The\nEasy\nsplit follows the same pattern, reaching 50.1 global mIoU and 70.6 view mIoU. Over the full benchmark, MVGGT attains 39.9 global mIoU and 69.3 view mIoU, exceeding the strongest baseline by 22.1 and 48.9.\nThese gains across all subsets confirm MVGGTâ€™s robustness to sparse evidence and ambiguity, and highlight PVSOâ€™s role in overcoming FGD.\nTable\n2\ncompares MVGGT with traditional 3D-RES and recent MV-3DRES baselines under the standard ScanRefer protocol. Despite using only sparse RGB inputs, MVGGT achieves 83.6 Acc@25, 74.5 Acc@50, and 65.2 mIoU on the\nUnique\nsplitâ€”substantially narrowing the gap with full 3D-resourced methods. The improvement is even more pronounced in the\nMultiple\nsplit, where MVGGT reaches 33.8 mIoU, exceeding the two-stage and 2D-Lift baselines by 17.4 and 19.3. These results underscore the capability of MVGGT to perform reliable 3D grounding without access to ground-truth point clouds.\n5.3\nAblation Studies\nWe conduct comprehensive ablation studies to validate the effectiveness of each proposed component. All experiments are performed on the MVRefer benchmark.\nImpact of Core Components.\nTable\n3\ncontrasts the complete model with partial variants. When neither MVGGT nor PVSO is used, performance drops sharply, reflecting the difficulty of grounding language with sparse and noisy geometry alone. Introducing PVSO optimization yields a clear improvement, raising overall\nmIoU\nglobal\n\\mathrm{mIoU}_{\\text{global}}\nfrom\n26.9\n26.9\nto\n32.0\n32.0\nand\nmIoU\nview\n\\mathrm{mIoU}_{\\text{view}}\nfrom\n41.1\n41.1\nto\n47.5\n47.5\n, indicating that rebalancing per-view gradients effectively addresses the Foreground Gradient Dilution problem and materially enhances the stability of sparse-view learning. MVGGT also increases stability, particularly on hard scenes (from\n12.9\n12.9\nto\n19.0\n19.0\nmIoU\nglobal\n\\mathrm{mIoU}_{\\text{global}}\n), showing that integrating language into geometric reasoning efficiently guides sparse-view aggregation. The full model combines both advantages, achieving\n39.9\n39.9\noverall\nmIoU\nglobal\n\\mathrm{mIoU}_{\\text{global}}\nand\n69.3\n69.3\nmIoU\nview\n\\mathrm{mIoU}_{\\text{view}}\n. This synergy reinforces our premise that effective MV-3DRES relies on both geometry-aware multimodal design and an optimization strategy resilient to sparse, uneven supervision.\nTable 3\n:\nAblation studies on the core components.\nEasy\nâ†‘\n\\uparrow\nHard\nâ†‘\n\\uparrow\nOverall\nâ†‘\n\\uparrow\nPVSO\nMVGGT\nmIoU\nglobal\n\\mathrm{mIoU}_{\\text{global}}\nmIoU\nview\n\\mathrm{mIoU}_{\\text{view}}\nmIoU\nglobal\n\\mathrm{mIoU}_{\\text{global}}\nmIoU\nview\n\\mathrm{mIoU}_{\\text{view}}\nmIoU\nglobal\n\\mathrm{mIoU}_{\\text{global}}\nmIoU\nview\n\\mathrm{mIoU}_{\\text{view}}\n2D-Lift\n25.4\n24.1\n6.4\n15.0\n17.8\n20.4\nÃ—\n\\times\nÃ—\n\\times\n36.3\n43.0\n12.9\n38.5\n26.9\n41.1\nâœ“\n\\checkmark\nÃ—\n\\times\n40.7\n48.9\n19.0\n45.4\n32.0\n47.5\nâœ“\n\\checkmark\nâœ“\n\\checkmark\n50.1\n70.6\n24.4\n67.3\n39.9\n69.3\nFigure 4\n:\nQualitative comparison on the MVRefer benchmark.\nPVSO Component Analysis.\nTable\n4\nfurther analyzes PVSO. Without no-target suppression, random view sampling produces unstable performance, especially on hard scenes. Enabling suppression consistently improves results (from\n32.4\n32.4\nto\n36.7\n36.7\noverall\nmIoU\nglobal\n\\mathrm{mIoU}_{\\text{global}}\n), confirming that reducing misleading gradients from target-absent views is essential. Hybrid sampling strengthens this effect by ensuring sufficient positive evidence in each batch. A no-target ratio of\n0.5\n0.5\nachieves the most balanced outcome, reaching\n39.9\n39.9\nmIoU\nglobal\n\\mathrm{mIoU}_{\\text{global}}\nand\n69.3\n69.3\nmIoU\nview\n\\mathrm{mIoU}_{\\text{view}}\n. Ratios that are too low underexploit discriminative negative pairs, while overly high ratios drown out positives, leading to degraded training dynamics. These patterns illustrate the core intuition behind PVSO: stable sparse-view learning emerges when each view contributes information in proportion to its actual visibility of the target.\nTable 4\n:\nAblation on PVSO components.\nSampling\nNo-Target\nNo-Target\nEasy\nâ†‘\n\\uparrow\nHard\nâ†‘\n\\uparrow\nOverall\nâ†‘\n\\uparrow\nStrategy\nRatio\nSuppression\nmIoU\nglobal\n\\mathrm{mIoU}_{\\text{global}}\nmIoU\nview\n\\mathrm{mIoU}_{\\text{view}}\nmIoU\nglobal\n\\mathrm{mIoU}_{\\text{global}}\nmIoU\nview\n\\mathrm{mIoU}_{\\text{view}}\nmIoU\nglobal\n\\mathrm{mIoU}_{\\text{global}}\nmIoU\nview\n\\mathrm{mIoU}_{\\text{view}}\nRandom\n-\nÃ—\n\\times\n40.8\n65.1\n19.7\n66.51\n32.4\n65.6\nRandom\n-\nâœ“\n46.2\n55.8\n22.4\n49.3\n36.7\n53.2\nHybrid\n0\nâœ“\n42.4\n24.3\n11.2\n10.3\n29.9\n18.7\nHybrid\n0.25\nâœ“\n46.5\n65.2\n19.4\n59.6\n35.7\n63.0\nHybrid\n0.5\nâœ“\n50.1\n70.6\n24.4\n67.3\n39.9\n69.3\nHybrid\n0.75\nâœ“\n30.3\n64.0\n18.7\n72.0\n25.7\n67.2\nMVGGT Fusion Architecture.\nTable\n5\nevaluates where multimodal fusion is best positioned within the encoder. Early fusion performs the weakest, suggesting that injecting language before geometric evidence has formed can disrupt structural reasoning. Middle fusion offers a modest improvement, but late fusion yields the strongest results (\n39.9\n39.9\noverall\nmIoU\nglobal\n\\mathrm{mIoU}_{\\text{global}}\n;\n69.3\n69.3\nmIoU\nview\n\\mathrm{mIoU}_{\\text{view}}\n). This trend suggests that spatial perception should be established first, with language guiding later refinement rather than early feature alignment. Such late fusion yields more stable and discriminative cross-view representations.\nTable 5\n:\nAblation on MVGGT fusion stage.\nFusion Stage\nEasy\nâ†‘\n\\uparrow\nHard\nâ†‘\n\\uparrow\nOverall\nâ†‘\n\\uparrow\nmIoU\nglobal\n\\mathrm{mIoU}_{\\text{global}}\nmIoU\nview\n\\mathrm{mIoU}_{\\text{view}}\nmIoU\nglobal\n\\mathrm{mIoU}_{\\text{global}}\nmIoU\nview\n\\mathrm{mIoU}_{\\text{view}}\nmIoU\nglobal\n\\mathrm{mIoU}_{\\text{global}}\nmIoU\nview\n\\mathrm{mIoU}_{\\text{view}}\nEarly\n45.7\n65.9\n21.6\n63.4\n36.1\n64.9\nMiddle\n47.5\n65.7\n22.5\n62.1\n37.5\n64.3\nLate\n50.1\n70.6\n24.4\n67.3\n39.9\n69.3\nMultimodal Branch Depth.\nTable\n6\nstudies the depth of the multimodal branch. A shallow configuration (6 layers) struggles on complex scenes, indicating insufficient capacity for cross-view alignment. Increasing to 12 layers delivers consistent improvements across easy and hard subsets, achieving the best overall accuracy. Expanding further to 16 layers causes a sharp decline, pointing to overfitting and unstable attention patterns under sparse supervision. The results reveal a practical design insight: moderate multimodal depth offers the right balance between expressiveness and regularity, enabling language cues to guide view aggregation without overwhelming the geometric signal.\nTable 6\n:\nAblation on layer number of multimodal branch.\nLayers\nEasy\nâ†‘\n\\uparrow\nHard\nâ†‘\n\\uparrow\nOverall\nâ†‘\n\\uparrow\nmIoU\nglobal\n\\mathrm{mIoU}_{\\text{global}}\nmIoU\nview\n\\mathrm{mIoU}_{\\text{view}}\nmIoU\nglobal\n\\mathrm{mIoU}_{\\text{global}}\nmIoU\nview\n\\mathrm{mIoU}_{\\text{view}}\nmIoU\nglobal\n\\mathrm{mIoU}_{\\text{global}}\nmIoU\nview\n\\mathrm{mIoU}_{\\text{view}}\n6\n47.1\n66.7\n22.5\n64.1\n37.3\n65.7\n12\n50.1\n70.6\n24.4\n67.3\n39.9\n69.3\n16\n35.6\n26.2\n10.1\n14.2\n25.4\n21.4\n5.4\nQualitative Analysis\nFigure\n3\nhighlights MVGGTâ€™s ability to maintain coherent 3D grounding under sparse and noisy views. While 2D lifting baselines frequently drift to nearby structures or collapse under occlusion and depth ambiguity, MVGGT produces stable segmentations that follow the intended targets across diverse conditions.\nIn example (a), the baseline confuses a thin whiteboard with adjacent planar surfaces, whereas MVGGT localizes the correct wall-aligned region by combining geometric cues with linguistic context. In example (b), MVGGT isolates the document organizer within a cluttered shelf, supported by PVSOâ€™s balanced per-view supervision. Example (c) contains severe depth noise; MVGGT still recovers the toilet bowl, enabled by late-stage multimodal fusion that refines geometry with textual cues. In example (d), the method distinguishes a narrow curtain from a visually similar wall segment. Finally, in example (e), MVGGT identifies the coffee table despite heavy clutter and partial visibility, demonstrating robust cross-view consistency.\n6\nConclusion\nWe introduced MV-3DRES, a new setting that aligns 3D language grounding with the sparse and view-limited conditions encountered by real-world agents. Conventional two-stage pipelines degrade severely under such sparsity, motivating MVGGT, a dual-branch architecture that integrates linguistic cues directly into sparse-view geometric reasoning. We further identified the Foreground Gradient Dilution problem and addressed it with Per-view No-target Suppression Optimization strategy, enabling stable and efficient training under extreme sparsity. Together with the MVRefer benchmark and the MVGGT model, this work establishes a unified framework for multimodal 3D grounding and paves a practical path toward more capable embodied perception systems.\nReferences\n[1]\nP. Achlioptas, A. Abdelreheem, F. Xia, M. Elhoseiny, and L. Guibas\n(2020)\nReferit3d: neural listeners for fine-grained 3d object identification in real-world scenes\n.\nIn\nECCV\n,\nCited by:\nÂ§1\n,\nÂ§2.1\n.\n[2]\nY. Cabon, L. Stoffl, L. Antsfeld, G. Csurka, B. Chidlovskii, J. Revaud, and V. Leroy\n(2025)\nMust3r: multi-view network for stereo 3d reconstruction\n.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n,\npp.Â 1050â€“1060\n.\nCited by:\nÂ§2.2\n.\n[3]\nD. Z. Chen, A. X. Chang, and M. NieÃŸner\n(2020)\nScanrefer: 3d object localization in rgb-d scans using natural language\n.\nIn\nECCV\n,\nCited by:\nÂ§1\n,\nÂ§2.1\n,\nÂ§3.2.1\n,\nÂ§3.2\n,\nÂ§5.1\n.\n[4]\nA. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. NieÃŸner\n(2017)\nScannet: richly-annotated 3d reconstructions of indoor scenes\n.\nIn\nCVPR\n,\nCited by:\nÂ§3.2.1\n,\nÂ§3.2\n.\n[5]\nA. Dai, M. NieÃŸner, M. ZollhÃ¶fer, S. Izadi, and C. Theobalt\n(2017)\nBundlefusion: real-time globally consistent 3d reconstruction using on-the-fly surface reintegration\n.\nACM Transactions on Graphics (ToG)\n36\n(\n4\n),\npp.Â 1\n.\nCited by:\nÂ§1\n.\n[6]\nJ. Deng, T. He, L. Jiang, T. Wang, F. Dayoub, and I. Reid\n(2025)\n3d-llava: towards generalist 3d lmms with omni superpoint transformer\n.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n,\npp.Â 3772â€“3782\n.\nCited by:\nTable 2\n.\n[7]\nK. Deng, Z. Ti, J. Xu, J. Yang, and J. Xie\n(2025)\nVGGT-long: chunk it, loop it, align itâ€“pushing vggtâ€™s limits on kilometer-scale long rgb sequences\n.\narXiv preprint arXiv:2507.16443\n.\nCited by:\nÂ§2.2\n.\n[8]\nB. P. Duisterhof, L. Zust, P. Weinzaepfel, V. Leroy, Y. Cabon, and J. Revaud\n(2025)\nMast3r-sfm: a fully-integrated solution for unconstrained structure-from-motion\n.\nIn\n2025 International Conference on 3D Vision (3DV)\n,\npp.Â 1â€“10\n.\nCited by:\nÂ§2.2\n.\n[9]\nS. Elflein, Q. Zhou, and L. Leal-TaixÃ©\n(2025)\nLight3R-sfm: towards feed-forward structure-from-motion\n.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n,\npp.Â 16774â€“16784\n.\nCited by:\nÂ§2.2\n.\n[10]\nH. Fei, S. Wu, H. Zhang, T. Chua, and S. Yan\n(2024)\nVitron: a unified pixel-level vision llm for understanding, generating, segmenting, editing\n.\nAdvances in neural information processing systems\n37\n,\npp.Â 57207â€“57239\n.\nCited by:\nÂ§2.1\n.\n[11]\nM. Feng, Z. Li, Q. Li, L. Zhang, X. Zhang, G. Zhu, H. Zhang, Y. Wang, and A. Mian\n(2021)\nFree-form description guided 3d visual graph network for object grounding in point cloud\n.\nIn\nICCV\n,\nCited by:\nÂ§2.1\n.\n[12]\nX. Ge, F. Chen, J. M. Jose, Z. Ji, Z. Wu, and X. Liu\n(2021)\nStructured multi-modal feature embedding and alignment for image-sentence retrieval\n.\nIn\nProceedings of the 29th ACM international conference on multimedia\n,\npp.Â 5185â€“5193\n.\nCited by:\nÂ§2.1\n.\n[13]\nY. Gong, L. Huang, and L. Chen\n(2022)\nPerson re-identification method based on color attack and joint defence\n.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n,\npp.Â 4313â€“4322\n.\nCited by:\nÂ§2.1\n.\n[14]\nS. He, H. Ding, X. Jiang, and B. Wen\n(2024)\nSegpoint: segment any point cloud via large language model\n.\nIn\nECCV\n,\nCited by:\nÂ§2.1\n,\nTable 2\n.\n[15]\nS. He and H. Ding\n(2024)\nRefMask3D: language-guided transformer for 3d referring segmentation\n.\nIn\nACM MM\n,\nCited by:\nÂ§2.1\n.\n[16]\nK. Huang, X. Li, L. Qi, S. Yan, and M. Yang\n(2025)\nReason3d: searching and reasoning 3d segmentation via large language model\n.\nIn\nInternational Conference on 3D Vision 2025\n,\nCited by:\nTable 2\n.\n[17]\nP. Huang, H. Lee, H. Chen, and T. Liu\n(2021)\nText-guided graph neural networks for referring 3d instance segmentation\n.\nIn\nAAAI\n,\nCited by:\nÂ§1\n,\nÂ§2.1\n,\nTable 2\n.\n[18]\nS. Huang, Y. Chen, J. Jia, and L. Wang\n(2022)\nMulti-view transformer for 3d visual grounding\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 15524â€“15533\n.\nCited by:\nÂ§2.1\n.\n[19]\nJ. Jain, J. Li, M. T. Chiu, A. Hassani, N. Orlov, and H. Shi\n(2023)\nOneformer: one transformer to rule universal image segmentation\n.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n,\npp.Â 2989â€“2998\n.\nCited by:\nÂ§2.1\n.\n[20]\nN. Keetha, N. MÃ¼ller, J. SchÃ¶nberger, L. Porzi, Y. Zhang, T. Fischer, A. Knapitsch, D. Zauss, E. Weber, N. Antunes,\net al.\n(2025)\nMapAnything: universal feed-forward metric 3d reconstruction\n.\narXiv preprint arXiv:2509.13414\n.\nCited by:\nÂ§2.2\n.\n[21]\nY. Kim, C. Chu, and S. Kurohashi\n(2022)\nFlexible visual grounding\n.\nIn\nProceedings of the 60th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop\n,\npp.Â 285â€“299\n.\nCited by:\nÂ§2.1\n.\n[22]\nV. Leroy, Y. Cabon, and J. Revaud\n(2024)\nGrounding image matching in 3d with mast3r\n.\nIn\nEuropean Conference on Computer Vision\n,\npp.Â 71â€“91\n.\nCited by:\nÂ§2.2\n.\n[23]\nH. Li, J. Qu, and L. Zhang\n(2025)\nOVSeg3R: learn open-vocabulary instance segmentation from 2d via 3d reconstruction\n.\narXiv preprint arXiv:2509.23541\n.\nCited by:\nÂ§2.2\n.\n[24]\nY. Li, X. Wang, J. Xiao, W. Ji, and T. Chua\n(2023)\nTransformer-empowered invariant grounding for video question answering\n.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n.\nCited by:\nÂ§2.1\n.\n[25]\nT. Liang, K. Lin, C. Tan, J. Zhang, W. Zheng, and J. Hu\n(2025)\nReferdino: referring video object segmentation with visual grounding foundations\n.\narXiv preprint arXiv:2501.14607\n.\nCited by:\nÂ§5.2\n.\n[26]\nH. Lin, Y. Luo, X. Zheng, L. Li, F. Chao, T. Jin, D. Luo, Y. Wang, L. Cao, and R. Ji\n(2023)\nA unified framework for 3d point cloud visual grounding\n.\narXiv:2308.11887\n.\nCited by:\nÂ§2.1\n.\n[27]\nX. Liu, X. Xu, J. Li, Q. Zhang, X. Wang, N. Sebe, and L. Ma\n(2024)\nLESS: label-efficient and single-stage referring 3d segmentation\n.\narXiv:2410.13294\n.\nCited by:\nÂ§2.1\n,\nTable 2\n,\nÂ§5.2\n.\n[28]\nY. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov\n(2019)\nRoberta: a robustly optimized bert pretraining approach\n.\narXiv preprint arXiv:1907.11692\n.\nCited by:\nÂ§5.1\n.\n[29]\nI. Loshchilov and F. Hutter\n(2017)\nDecoupled weight decay regularization\n.\narXiv preprint arXiv:1711.05101\n.\nCited by:\nÂ§5.1\n.\n[30]\nJ. Luo, J. Fu, X. Kong, C. Gao, H. Ren, H. Shen, H. Xia, and S. Liu\n(2022)\n3d-sps: single-stage 3d visual grounding via referred point progressive selection\n.\nIn\nCVPR\n,\nCited by:\nÂ§2.1\n.\n[31]\nR. Murai, E. Dexheimer, and A. J. Davison\n(2025)\nMASt3R-slam: real-time dense slam with 3d reconstruction priors\n.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n,\npp.Â 16695â€“16705\n.\nCited by:\nÂ§2.2\n.\n[32]\nZ. Pataki, P. Sarlin, J. L. SchÃ¶nberger, and M. Pollefeys\n(2025)\nMP-sfm: monocular surface priors for robust structure-from-motion\n.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n,\npp.Â 21891â€“21901\n.\nCited by:\nÂ§2.2\n.\n[33]\nC. R. Qi, O. Litany, K. He, and L. J. Guibas\n(2019)\nDeep hough voting for 3d object detection in point clouds\n.\nIn\nproceedings of the IEEE/CVF International Conference on Computer Vision\n,\npp.Â 9277â€“9286\n.\nCited by:\nÂ§2.1\n.\n[34]\nZ. Qian, Y. Ma, J. Ji, and X. Sun\n(2024)\nX-refseg3d: enhancing referring 3d instance segmentation via structured cross-modal graph neural networks\n.\nIn\nAAAI\n,\nCited by:\nÂ§2.1\n.\n[35]\nY. Shen, Z. Zhang, Y. Qu, and L. Cao\n(2025)\nFastvggt: training-free acceleration of visual geometry transformer\n.\narXiv preprint arXiv:2509.02560\n.\nCited by:\nÂ§2.2\n.\n[36]\nS. Subramanian, W. Merrill, T. Darrell, M. Gardner, S. Singh, and A. Rohrbach\n(2022)\nReclip: a strong zero-shot baseline for referring expression comprehension\n.\narXiv preprint arXiv:2204.05991\n.\nCited by:\nÂ§2.1\n.\n[37]\nG. Transformer\nIGGT: instance-grounded geometry trans-former for semantic 3d reconstruction\n.\nCited by:\nÂ§2.2\n.\n[38]\nC. B. Wang, C. Schmidt, J. Piekenbrinck, and B. Leibe\n(2025)\nFaster vggt with block-sparse global attention\n.\narXiv preprint arXiv:2509.07120\n.\nCited by:\nÂ§2.2\n.\n[39]\nH. Wang and L. Agapito\n(2024)\n3d reconstruction with spatial memory\n.\narXiv preprint arXiv:2408.16061\n.\nCited by:\nÂ§2.2\n.\n[40]\nJ. Wang, M. Chen, N. Karaev, A. Vedaldi, C. Rupprecht, and D. Novotny\n(2025)\nVggt: visual geometry grounded transformer\n.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n,\npp.Â 5294â€“5306\n.\nCited by:\nÂ§2.2\n.\n[41]\nQ. Wang, Y. Zhang, A. Holynski, A. A. Efros, and A. Kanazawa\n(2025)\nContinuous 3d perception model with persistent state\n.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n,\npp.Â 10510â€“10522\n.\nCited by:\nÂ§2.2\n.\n[42]\nS. Wang, V. Leroy, Y. Cabon, B. Chidlovskii, and J. Revaud\n(2024)\nDust3r: geometric 3d vision made easy\n.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n,\npp.Â 20697â€“20709\n.\nCited by:\nÂ§2.2\n.\n[43]\nY. R. Wang, Y. Zhao, H. Xu, S. Eppel, A. Aspuru-Guzik, F. Shkurti, and A. Garg\n(2023)\nMvtrans: multi-view perception of transparent objects\n.\narXiv preprint arXiv:2302.11683\n.\nCited by:\nÂ§2.2\n.\n[44]\nY. Wang, J. Zhou, H. Zhu, W. Chang, Y. Zhou, Z. Li, J. Chen, J. Pang, C. Shen, and T. He\n(2025)\nPi3: permutation-equivariant visual geometry learning\n.\narXiv preprint arXiv:2507.13347\n.\nCited by:\nÂ§2.2\n,\nÂ§5.1\n,\nÂ§5.2\n.\n[45]\nW. WinT3R\nWINT3R: window-based streaming recon-struction with camera token pool\n.\nCited by:\nÂ§2.2\n.\n[46]\nC. Wu, J. Ji, H. Wang, Y. Ma, Y. Huang, G. Luo, H. Fei, X. Sun, R. Ji,\net al.\n(2024)\nRg-san: rule-guided spatial awareness network for end-to-end 3d referring expression segmentation\n.\nAdvances in Neural Information Processing Systems\n37\n,\npp.Â 110972â€“110999\n.\nCited by:\nÂ§1\n,\nÂ§2.1\n,\nTable 2\n.\n[47]\nC. Wu, Y. Liu, J. Ji, Y. Ma, H. Wang, G. Luo, H. Ding, X. Sun, and R. Ji\n(2024)\n3d-gres: generalized 3d referring expression segmentation\n.\nIn\nACM MM\n,\nCited by:\nÂ§1\n,\nÂ§2.1\n.\n[48]\nC. Wu, Y. Ma, Q. Chen, H. Wang, G. Luo, J. Ji, and X. Sun\n(2024)\n3d-stmn: dependency-driven superpoint-text matching network for end-to-end 3d referring expression segmentation\n.\nIn\nAAAI\n,\nCited by:\nÂ§1\n,\nÂ§2.1\n,\nTable 2\n.\n[49]\nS. Wu, H. Fei, L. Qu, W. Ji, and T. Chua\n(2024)\nNext-gpt: any-to-any multimodal llm\n.\nIn\nForty-first International Conference on Machine Learning\n,\nCited by:\nÂ§2.1\n.\n[50]\nQ. Xu, D. Wei, L. Zhao, W. Li, Z. Huang, S. Ji, and P. Liu\n(2025)\nSIU3R: simultaneous scene understanding and 3d reconstruction beyond feature alignment\n.\narXiv preprint arXiv:2507.02705\n.\nCited by:\nÂ§2.2\n.\n[51]\nZ. Yuan, X. Yan, Y. Liao, R. Zhang, S. Wang, Z. Li, and S. Cui\n(2021)\nInstancerefer: cooperative holistic understanding for visual grounding on point clouds through instance multi-level contextual referring\n.\nIn\nICCV\n,\nCited by:\nÂ§2.1\n.\n[52]\nL. Zhang, A. Rao, and M. Agrawala\n(2023)\nAdding conditional control to text-to-image diffusion models\n.\nIn\nProceedings of the IEEE/CVF international conference on computer vision\n,\npp.Â 3836â€“3847\n.\nCited by:\nÂ§4.1\n.\n[53]\nY. Zhang, Z. Gong, and A. X. Chang\n(2023)\nMulti3drefer: grounding text description to multiple 3d objects\n.\nIn\nICCV\n,\nCited by:\nÂ§1\n.\n[54]\nZ. Zhang, H. Yannakoudakis, X. Zhen, and E. Shutova\n(2023)\nCK-transformer: commonsense knowledge enhanced transformers for referring expression comprehension\n.\narXiv preprint arXiv:2302.09027\n.\nCited by:\nÂ§2.1\n.\n[55]\nL. Zhao, D. Cai, L. Sheng, and D. Xu\n(2021)\n3dvg-transformer: relation modeling for visual grounding on point clouds\n.\nIn\nICCV\n,\nCited by:\nÂ§1\n.",
      "references": [
        {
          "raw_text": "[1] P. Achlioptas, A. Abdelreheem, F. Xia, M. Elhoseiny, and L. Guibas (2020) Referit3d: neural listeners for fine-grained 3d object identification in real-world scenes . In ECCV , Cited by: Â§1 , Â§2.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S1.p1.1",
            "https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[2] Y. Cabon, L. Stoffl, L. Antsfeld, G. Csurka, B. Chidlovskii, J. Revaud, and V. Leroy (2025) Must3r: multi-view network for stereo 3d reconstruction . In Proceedings of the Computer Vision and Pattern Recognition Conference , pp.Â 1050â€“1060 . Cited by: Â§2.2 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[3] D. Z. Chen, A. X. Chang, and M. NieÃŸner (2020) Scanrefer: 3d object localization in rgb-d scans using natural language . In ECCV , Cited by: Â§1 , Â§2.1 , Â§3.2.1 , Â§3.2 , Â§5.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S1.p1.1",
            "https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1",
            "https://arxiv.org/html/2601.06874v1#S3.SS2.SSS1.p1.1",
            "https://arxiv.org/html/2601.06874v1#S3.SS2.p1.1",
            "https://arxiv.org/html/2601.06874v1#S5.SS1.SSS0.Px2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[4] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. NieÃŸner (2017) Scannet: richly-annotated 3d reconstructions of indoor scenes . In CVPR , Cited by: Â§3.2.1 , Â§3.2 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S3.SS2.SSS1.p1.1",
            "https://arxiv.org/html/2601.06874v1#S3.SS2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[5] A. Dai, M. NieÃŸner, M. ZollhÃ¶fer, S. Izadi, and C. Theobalt (2017) Bundlefusion: real-time globally consistent 3d reconstruction using on-the-fly surface reintegration . ACM Transactions on Graphics (ToG) 36 ( 4 ), pp.Â 1 . Cited by: Â§1 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[6] J. Deng, T. He, L. Jiang, T. Wang, F. Dayoub, and I. Reid (2025) 3d-llava: towards generalist 3d lmms with omni superpoint transformer . In Proceedings of the Computer Vision and Pattern Recognition Conference , pp.Â 3772â€“3782 . Cited by: Table 2 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S4.T2.2.2.11.9.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[7] K. Deng, Z. Ti, J. Xu, J. Yang, and J. Xie (2025) VGGT-long: chunk it, loop it, align itâ€“pushing vggtâ€™s limits on kilometer-scale long rgb sequences . arXiv preprint arXiv:2507.16443 . Cited by: Â§2.2 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS2.p2.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[8] B. P. Duisterhof, L. Zust, P. Weinzaepfel, V. Leroy, Y. Cabon, and J. Revaud (2025) Mast3r-sfm: a fully-integrated solution for unconstrained structure-from-motion . In 2025 International Conference on 3D Vision (3DV) , pp.Â 1â€“10 . Cited by: Â§2.2 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[9] S. Elflein, Q. Zhou, and L. Leal-TaixÃ© (2025) Light3R-sfm: towards feed-forward structure-from-motion . In Proceedings of the Computer Vision and Pattern Recognition Conference , pp.Â 16774â€“16784 . Cited by: Â§2.2 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[10] H. Fei, S. Wu, H. Zhang, T. Chua, and S. Yan (2024) Vitron: a unified pixel-level vision llm for understanding, generating, segmenting, editing . Advances in neural information processing systems 37 , pp.Â 57207â€“57239 . Cited by: Â§2.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[11] M. Feng, Z. Li, Q. Li, L. Zhang, X. Zhang, G. Zhu, H. Zhang, Y. Wang, and A. Mian (2021) Free-form description guided 3d visual graph network for object grounding in point cloud . In ICCV , Cited by: Â§2.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[12] X. Ge, F. Chen, J. M. Jose, Z. Ji, Z. Wu, and X. Liu (2021) Structured multi-modal feature embedding and alignment for image-sentence retrieval . In Proceedings of the 29th ACM international conference on multimedia , pp.Â 5185â€“5193 . Cited by: Â§2.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[13] Y. Gong, L. Huang, and L. Chen (2022) Person re-identification method based on color attack and joint defence . In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp.Â 4313â€“4322 . Cited by: Â§2.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[14] S. He, H. Ding, X. Jiang, and B. Wen (2024) Segpoint: segment any point cloud via large language model . In ECCV , Cited by: Â§2.1 , Table 2 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1",
            "https://arxiv.org/html/2601.06874v1#S4.T2.2.2.7.5.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[15] S. He and H. Ding (2024) RefMask3D: language-guided transformer for 3d referring segmentation . In ACM MM , Cited by: Â§2.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[16] K. Huang, X. Li, L. Qi, S. Yan, and M. Yang (2025) Reason3d: searching and reasoning 3d segmentation via large language model . In International Conference on 3D Vision 2025 , Cited by: Table 2 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S4.T2.2.2.8.6.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[17] P. Huang, H. Lee, H. Chen, and T. Liu (2021) Text-guided graph neural networks for referring 3d instance segmentation . In AAAI , Cited by: Â§1 , Â§2.1 , Table 2 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S1.p1.1",
            "https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1",
            "https://arxiv.org/html/2601.06874v1#S4.T2.2.2.5.3.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[18] S. Huang, Y. Chen, J. Jia, and L. Wang (2022) Multi-view transformer for 3d visual grounding . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.Â 15524â€“15533 . Cited by: Â§2.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[19] J. Jain, J. Li, M. T. Chiu, A. Hassani, N. Orlov, and H. Shi (2023) Oneformer: one transformer to rule universal image segmentation . In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp.Â 2989â€“2998 . Cited by: Â§2.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[20] N. Keetha, N. MÃ¼ller, J. SchÃ¶nberger, L. Porzi, Y. Zhang, T. Fischer, A. Knapitsch, D. Zauss, E. Weber, N. Antunes, et al. (2025) MapAnything: universal feed-forward metric 3d reconstruction . arXiv preprint arXiv:2509.13414 . Cited by: Â§2.2 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS2.p2.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[21] Y. Kim, C. Chu, and S. Kurohashi (2022) Flexible visual grounding . In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop , pp.Â 285â€“299 . Cited by: Â§2.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[22] V. Leroy, Y. Cabon, and J. Revaud (2024) Grounding image matching in 3d with mast3r . In European Conference on Computer Vision , pp.Â 71â€“91 . Cited by: Â§2.2 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[23] H. Li, J. Qu, and L. Zhang (2025) OVSeg3R: learn open-vocabulary instance segmentation from 2d via 3d reconstruction . arXiv preprint arXiv:2509.23541 . Cited by: Â§2.2 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS2.p2.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[24] Y. Li, X. Wang, J. Xiao, W. Ji, and T. Chua (2023) Transformer-empowered invariant grounding for video question answering . IEEE Transactions on Pattern Analysis and Machine Intelligence . Cited by: Â§2.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[25] T. Liang, K. Lin, C. Tan, J. Zhang, W. Zheng, and J. Hu (2025) Referdino: referring video object segmentation with visual grounding foundations . arXiv preprint arXiv:2501.14607 . Cited by: Â§5.2 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S5.SS2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[26] H. Lin, Y. Luo, X. Zheng, L. Li, F. Chao, T. Jin, D. Luo, Y. Wang, L. Cao, and R. Ji (2023) A unified framework for 3d point cloud visual grounding . arXiv:2308.11887 . Cited by: Â§2.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[27] X. Liu, X. Xu, J. Li, Q. Zhang, X. Wang, N. Sebe, and L. Ma (2024) LESS: label-efficient and single-stage referring 3d segmentation . arXiv:2410.13294 . Cited by: Â§2.1 , Table 2 , Â§5.2 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1",
            "https://arxiv.org/html/2601.06874v1#S4.T2.2.2.10.8.1",
            "https://arxiv.org/html/2601.06874v1#S5.SS2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[28] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov (2019) Roberta: a robustly optimized bert pretraining approach . arXiv preprint arXiv:1907.11692 . Cited by: Â§5.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S5.SS1.SSS0.Px1.p1.4"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[29] I. Loshchilov and F. Hutter (2017) Decoupled weight decay regularization . arXiv preprint arXiv:1711.05101 . Cited by: Â§5.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S5.SS1.SSS0.Px1.p1.4"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[30] J. Luo, J. Fu, X. Kong, C. Gao, H. Ren, H. Shen, H. Xia, and S. Liu (2022) 3d-sps: single-stage 3d visual grounding via referred point progressive selection . In CVPR , Cited by: Â§2.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[31] R. Murai, E. Dexheimer, and A. J. Davison (2025) MASt3R-slam: real-time dense slam with 3d reconstruction priors . In Proceedings of the Computer Vision and Pattern Recognition Conference , pp.Â 16695â€“16705 . Cited by: Â§2.2 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[32] Z. Pataki, P. Sarlin, J. L. SchÃ¶nberger, and M. Pollefeys (2025) MP-sfm: monocular surface priors for robust structure-from-motion . In Proceedings of the Computer Vision and Pattern Recognition Conference , pp.Â 21891â€“21901 . Cited by: Â§2.2 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[33] C. R. Qi, O. Litany, K. He, and L. J. Guibas (2019) Deep hough voting for 3d object detection in point clouds . In proceedings of the IEEE/CVF International Conference on Computer Vision , pp.Â 9277â€“9286 . Cited by: Â§2.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[34] Z. Qian, Y. Ma, J. Ji, and X. Sun (2024) X-refseg3d: enhancing referring 3d instance segmentation via structured cross-modal graph neural networks . In AAAI , Cited by: Â§2.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[35] Y. Shen, Z. Zhang, Y. Qu, and L. Cao (2025) Fastvggt: training-free acceleration of visual geometry transformer . arXiv preprint arXiv:2509.02560 . Cited by: Â§2.2 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS2.p2.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[36] S. Subramanian, W. Merrill, T. Darrell, M. Gardner, S. Singh, and A. Rohrbach (2022) Reclip: a strong zero-shot baseline for referring expression comprehension . arXiv preprint arXiv:2204.05991 . Cited by: Â§2.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[37] G. Transformer IGGT: instance-grounded geometry trans-former for semantic 3d reconstruction . Cited by: Â§2.2 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS2.p2.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[38] C. B. Wang, C. Schmidt, J. Piekenbrinck, and B. Leibe (2025) Faster vggt with block-sparse global attention . arXiv preprint arXiv:2509.07120 . Cited by: Â§2.2 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS2.p2.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[39] H. Wang and L. Agapito (2024) 3d reconstruction with spatial memory . arXiv preprint arXiv:2408.16061 . Cited by: Â§2.2 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[40] J. Wang, M. Chen, N. Karaev, A. Vedaldi, C. Rupprecht, and D. Novotny (2025) Vggt: visual geometry grounded transformer . In Proceedings of the Computer Vision and Pattern Recognition Conference , pp.Â 5294â€“5306 . Cited by: Â§2.2 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS2.p2.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[41] Q. Wang, Y. Zhang, A. Holynski, A. A. Efros, and A. Kanazawa (2025) Continuous 3d perception model with persistent state . In Proceedings of the Computer Vision and Pattern Recognition Conference , pp.Â 10510â€“10522 . Cited by: Â§2.2 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[42] S. Wang, V. Leroy, Y. Cabon, B. Chidlovskii, and J. Revaud (2024) Dust3r: geometric 3d vision made easy . In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.Â 20697â€“20709 . Cited by: Â§2.2 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[43] Y. R. Wang, Y. Zhao, H. Xu, S. Eppel, A. Aspuru-Guzik, F. Shkurti, and A. Garg (2023) Mvtrans: multi-view perception of transparent objects . arXiv preprint arXiv:2302.11683 . Cited by: Â§2.2 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS2.p2.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[44] Y. Wang, J. Zhou, H. Zhu, W. Chang, Y. Zhou, Z. Li, J. Chen, J. Pang, C. Shen, and T. He (2025) Pi3: permutation-equivariant visual geometry learning . arXiv preprint arXiv:2507.13347 . Cited by: Â§2.2 , Â§5.1 , Â§5.2 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS2.p2.1",
            "https://arxiv.org/html/2601.06874v1#S5.SS1.SSS0.Px1.p1.4",
            "https://arxiv.org/html/2601.06874v1#S5.SS2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[45] W. WinT3R WINT3R: window-based streaming recon-struction with camera token pool . Cited by: Â§2.2 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[46] C. Wu, J. Ji, H. Wang, Y. Ma, Y. Huang, G. Luo, H. Fei, X. Sun, R. Ji, et al. (2024) Rg-san: rule-guided spatial awareness network for end-to-end 3d referring expression segmentation . Advances in Neural Information Processing Systems 37 , pp.Â 110972â€“110999 . Cited by: Â§1 , Â§2.1 , Table 2 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S1.p1.1",
            "https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1",
            "https://arxiv.org/html/2601.06874v1#S4.T2.2.2.9.7.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[47] C. Wu, Y. Liu, J. Ji, Y. Ma, H. Wang, G. Luo, H. Ding, X. Sun, and R. Ji (2024) 3d-gres: generalized 3d referring expression segmentation . In ACM MM , Cited by: Â§1 , Â§2.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S1.p1.1",
            "https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[48] C. Wu, Y. Ma, Q. Chen, H. Wang, G. Luo, J. Ji, and X. Sun (2024) 3d-stmn: dependency-driven superpoint-text matching network for end-to-end 3d referring expression segmentation . In AAAI , Cited by: Â§1 , Â§2.1 , Table 2 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S1.p1.1",
            "https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1",
            "https://arxiv.org/html/2601.06874v1#S4.T2.2.2.6.4.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[49] S. Wu, H. Fei, L. Qu, W. Ji, and T. Chua (2024) Next-gpt: any-to-any multimodal llm . In Forty-first International Conference on Machine Learning , Cited by: Â§2.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[50] Q. Xu, D. Wei, L. Zhao, W. Li, Z. Huang, S. Ji, and P. Liu (2025) SIU3R: simultaneous scene understanding and 3d reconstruction beyond feature alignment . arXiv preprint arXiv:2507.02705 . Cited by: Â§2.2 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS2.p2.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[51] Z. Yuan, X. Yan, Y. Liao, R. Zhang, S. Wang, Z. Li, and S. Cui (2021) Instancerefer: cooperative holistic understanding for visual grounding on point clouds through instance multi-level contextual referring . In ICCV , Cited by: Â§2.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[52] L. Zhang, A. Rao, and M. Agrawala (2023) Adding conditional control to text-to-image diffusion models . In Proceedings of the IEEE/CVF international conference on computer vision , pp.Â 3836â€“3847 . Cited by: Â§4.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S4.SS1.SSS0.Px3.p3.3"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[53] Y. Zhang, Z. Gong, and A. X. Chang (2023) Multi3drefer: grounding text description to multiple 3d objects . In ICCV , Cited by: Â§1 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[54] Z. Zhang, H. Yannakoudakis, X. Zhen, and E. Shutova (2023) CK-transformer: commonsense knowledge enhanced transformers for referring expression comprehension . arXiv preprint arXiv:2302.09027 . Cited by: Â§2.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S2.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "[55] L. Zhao, D. Cai, L. Sheng, and D. Xu (2021) 3dvg-transformer: relation modeling for visual grounding on point clouds . In ICCV , Cited by: Â§1 .",
          "urls": [
            "https://arxiv.org/html/2601.06874v1#S1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        }
      ],
      "references_dois": [],
      "fallback_urls": [],
      "errors": [],
      "missing_fields": [
        "doi",
        "versions",
        "last_updated_raw",
        "published_date",
        "references_dois"
      ],
      "url_hint_if_missing": "Champs manquants: doi, versions, last_updated_raw, published_date, references_dois. Tu peux vÃ©rifier ici: abs=https://arxiv.org/abs/2601.06874 | html=https://arxiv.org/html/2601.06874v1 | pdf=https://arxiv.org/pdf/2601.06874"
    },
    {
      "arxiv_id": "2601.06847",
      "title": "MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data",
      "authors": [
        "Mengmeng Zhang",
        "Xiaoping Wu",
        "Hao Luo",
        "Fan Wang",
        "Yisheng Lv"
      ],
      "abstract": "Vision-Language Models (VLMs) can generate convincing clinical narratives, yet frequently struggle to visually ground their statements. We posit this limitation arises from the scarcity of high-quality, large-scale clinical referring-localization pairs. To address this, we introduce MedGround, an automated pipeline that transforms segmentation resources into high-quality medical referring grounding data. Leveraging expert masks as spatial anchors, MedGround precisely derives localization targets, extracts shape and spatial cues, and guides VLMs to synthesize natural, clinically grounded queries that reflect morphology and location. To ensure data rigor, a multi-stage verification system integrates strict formatting checks, geometry- and medical-prior rules, and image-based visual judging to filter out ambiguous or visually unsupported samples. Finally, we present MedGround-35K, a novel multimodal medical dataset. Extensive experiments demonstrate that VLMs trained with MedGround-35K consistently achieve improved referring grounding performance, enhance multi-object semantic disambiguation, and exhibit strong generalization to unseen grounding settings. This work highlights MedGround as a scalable, data-driven approach to anchor medical language to verifiable visual evidence. Dataset and code will be released publicly upon acceptance.",
      "submitted_date": "11 January, 2026",
      "abs_url": "https://arxiv.org/abs/2601.06847",
      "pdf_url": "https://arxiv.org/pdf/2601.06847",
      "doi": "",
      "versions": [],
      "last_updated_raw": "",
      "html_url": "https://arxiv.org/html/2601.06847v1",
      "published_date": "",
      "content_text": "MedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data\nMengmeng Zhang\n1,2,3\n,\nXiaoping Wu\n3\n,\nHao Luo\n3,4\nâ€ \n,\nFan Wang\n3\n,\nYisheng Lv\n1,2\nâ€ \n1\nInstitute of Automation, Chinese Academy of Sciences,\n2\nSchool of Artificial Intelligence, University of Chinese Academy of Sciences,\n3\nDAMO Academy, Alibaba Group,\n4\nHupan Lab, Zhejiang Province\nCorrespondence:\nmichuan.lh@alibaba-inc.com\n,\nyisheng.lv@ia.ac.cn\nAbstract\nV\nision-\nL\nanguage\nM\nodels (VLMs) can generate convincing clinical narratives, yet frequently struggle to visually ground their statements. We posit this limitation arises from the scarcity of high-quality, large-scale clinical referring-localization pairs. To address this, we introduce MedGround, an automated pipeline that transforms segmentation resources into high-quality medical referring grounding data. Leveraging expert masks as spatial anchors, MedGround precisely derives localization targets, extracts shape and spatial cues, and guides VLMs to synthesize natural, clinically grounded queries that reflect morphology and location. To ensure data rigor, a multi-stage verification system integrates strict formatting checks, geometry- and medical-prior rules, and image-based visual judging to filter out ambiguous or visually unsupported samples. Finally, we present MedGround-35K, a novel multimodal medical dataset. Extensive experiments demonstrate that VLMs trained with MedGround-35K consistently achieve improved referring grounding performance, enhance multi-object semantic disambiguation, and exhibit strong generalization to unseen grounding settings. This work highlights MedGround as a scalable, data-driven approach to anchor medical language to verifiable visual evidence.\n2\n2\n2\nDataset and code will be released publicly upon acceptance.\nMedGround: Bridging the Evidence Gap in Medical Vision-Language Models with Verified Grounding Data\nMengmeng Zhang\n1,2,3\n,\nXiaoping Wu\n3\n,\nHao Luo\n3,4\nâ€ \n,\nFan Wang\n3\n,\nYisheng Lv\n1,2\nâ€ \n1\nInstitute of Automation, Chinese Academy of Sciences,\n2\nSchool of Artificial Intelligence, University of Chinese Academy of Sciences,\n3\nDAMO Academy, Alibaba Group,\n4\nHupan Lab, Zhejiang Province\nCorrespondence:\nmichuan.lh@alibaba-inc.com\n,\nyisheng.lv@ia.ac.cn\n1\n1\n1\nâ€ \nCorresponding authors.\n1\nIntroduction\nRecent VLMs have shown impressive capability in medical image understanding tasks such as report generation and clinical question answering\nLi\net al.\n(\n2023\n); Manzari\net al.\n(\n2023\n); Nath\net al.\n(\n2025\n); Sellergren\net al.\n(\n2025\n); Liu\net al.\n(\n2025\n); Yang\net al.\n(\n2024\n); Tarhini\net al.\n(\n2025\n); Wu\net al.\n(\n2025\n)\n. However, their linguistic fluency often outpaces fine-grained visual localization: a model may describe plausible findings while failing to identify where those findings appear in the image. Such visually unfaithful outputs undermine interpretability and can lead to â€œright-for-the-wrong-reasonâ€ predictions\nKim\net al.\n(\n2025\n); Xie\net al.\n(\n2023\n); Mahmood\net al.\n(\n2025\n); Ostmeier\net al.\n(\n2024\n); Pal\net al.\n(\n2023\n); Huang\net al.\n(\n2025\n); Xu (\n2025\n)\n.\nWe characterize this mismatch as a cognitiveâ€“perceptual gap, where a modelâ€™s cognitive competence is not mirrored by its perceptual grounding.\nFigure 1:\nMotivation of MedGround.\n(a) Models trained on image-text pairs fail to\n\"speak with substance\"\ndue to lack of grounding. (b) Segmentation-only training fails to achieve semantic understanding. (c)\nMedGround\n(Image-text-box triplets) activates the full potential of medical VLMs by bridging semantics and localization.\nWe argue that a key driver of this gap is the current data scarce. Medical data is plentiful in two largely disconnected forms: (1) imageâ€“text pairs from radiology reports, providing rich global semantics but weak spatial supervision; and (2) imageâ€“mask segmentation datasets, providing precise region annotations but impoverished language, often limited to a coarse category label. In contrast, large-scale datasets that pair natural referring expressions with explicit localization targets (boxes or masks) are scarce\nBannur\net al.\n(\n2024\n); Li\net al.\n(\n2024\n); Chen\net al.\n(\n2025\n); Deng\net al.\n(\n2025\n)\n.This scarcity limits the ability of VLMs to learn the alignment between clinically meaningful phrases and localized evidence.\nHowever, it is challenging to construct medical referring grounding data on a scale. One seemingly naive approach is to ground existing reports by asking VLMs to localize findings. Yet current VLMs are often vision-weak at fine-grained grounding, so this direction risks inheriting and amplifying their spatial biases\nHuang\net al.\n(\n2024\n); Ge\net al.\n(\n2025\n); Strudel\net al.\n(\n2022\n); Bai\net al.\n(\n2024\n); Zhang\net al.\n(\n2025b\n)\n. Instead, we reverse the process: starting from precise expert annotations in segmentation datasets, we use them as deterministic spatial anchors to synthesize referring expressions. This leverages the asymmetry of current modelsâ€”strong language generation but weaker groundingâ€”while maintaining spatial faithfulness through verification.\nWe introduce MedGround, a mask-guided semantic synthesis and verification pipeline that converts segmentation annotations into high-quality imageâ€“textâ€“box triplets(MedGround-35K dataset) for medical referring expression grounding. MedGround effectively generates these triplets by leveraging expert masks to derive precise bounding boxes and extract geometric and spatial attributes. It then prompts a VLM to synthesize medically meaningful referring queries, followed by a multi-stage filtering process, including image-based VLM judging, to ensure data quality. Extensive experiments across multiple settings show that training with our constructed MedGround-35K dataset effectively improves medical referring grounding, and that the introduced clinically grounded semantics help VLMs better follow morphology- and location-aware descriptions for more reliable target disambiguation. These findings support MedGround as an effective and scalable supervision pipline for bridging the cognitiveâ€“perceptual gap.\nIn summary, our key contributions are:\nâ€¢\nWe propose MedGround, a scalable pipeline for synthesizing and verifying medically grounded referring queries anchored to expert annotations.\nâ€¢\nWe release MedGround-35K, covering eight datasets and multiple modalities, and demonstrate improvements in referring grounding, semantic disambiguation, and zero-shot transfer.\nâ€¢\nWe extensively tested various models using the MedGround-35K dataset and discovered that existing VLMs commonly struggle with fine-grained medical referring grounding tasks. However, training with our data significantly mitigates these issues.\nTable 1:\nComparison of medical datasets.\nâœ“\n: provided;\nâœ—\n: not provided;\nâ–³\n\\triangle\n: limited/weak or implicit/derived. Auto Anno. means the samples are collected autonomously.\nDataset\nModality\nText Type\nLoc. Type\nGranularity\nAuto Anno.\n#Anno.\nTask\nMS-CXR\nBoecking\net al.\n(\n2022\n)\nCXR\nâœ“\nreport phrases\nâœ“\nbox\nfinding\nâœ—\n1,162\nphrase grounding\nChest ImaGenome\nWu\net al.\n(\n2021\n)\nCXR\nâœ“\nstructured phrases\nâ–³\n\\triangle\nregion-aligned\nregion/finding\nâœ—\n1,256\nregion-text alignment\nChestX-ray8\nWang\net al.\n(\n2017\n)\nCXR\nâ–³\n\\triangle\nlabels\nâœ“\nbox\ndisease\nâœ—\n888\ndisease localization\nDeepLesion\nYan\net al.\n(\n2018\n)\nCT\nâœ—\nmetadata\nâœ“\nbox\nlesion\nâœ—\n32,120\nlesion detection\nLIDC-IDRI\nArmato\net al.\n(\n2011\n)\nCT\nâœ—\nattributes\nâœ“\nmask/contour\nlesion\nâœ—\n875\nnodule analysis\nMedGround-35K (ours)\nmulti\nâœ“\nreferring queries\nâœ“\nbox\nlesion/structure\nâœ“\n35,324\nreferring grounding\nTable 2:\nThe statistics of the MedGround-35K.\nThe Anno. Tokens and Avg. Words columns show the total number of tokens and the average number of words for the medical grounding annotations regardless of task templates. The Modalities/Sources column shows the number of unique medical imaging sources/modalities involved in each split.\nSplit\n#Images\nAnno. Tokens\nAvg. Words\nModality Ratio\nTrain\n25.4k\n2773.5k\n12.0\nBacteria: 1.3%, CT: 8.2%, Dermoscopy: 9.9%, Nuclei: 20.5%, Ultrasound: 60.2%\nTest\n10.1k\n1115.9k\n12.7\nBacteria: 1.2%, CT: 18.3%, Dermoscopy: 11.8%, Nuclei: 25.3%, Ultrasound: 43.4%\nFigure 2:\nMedGround pipeline.\n(A) Convert segmentation masks into normalized ground-truth bounding box lists. (B) Use dataset-aware, mask-guided prompts to synthesize medically meaningful referring queries and select target box(es) as answers. (C) Perform multi-stage verification and cleaning (format/schema, geometryâ€“location rules, and VLM-based grounding). (D) Conduct manual review for final quality control. (E) Cases falling verification.\n2\nRelated Work\nOur work relates to medical VLMs, referring grounding, and VLM-based synthesis and verifocation, so we review them below.\n2.1\nMedical Visionâ€“Language Models\nMedical VLMs have rapidly advanced with large-scale pretraining on radiology report corpora and biomedical imageâ€“text resources, followed by instruction tuning for clinical question answering and report generation\nLi\net al.\n(\n2023\n); Moor\net al.\n(\n2023\n); Zhang\net al.\n(\n2023\n)\n. These models can produce fluent, clinically plausible narratives, but their outputs are not always evidence-aligned: models may describe findings without reliably localizing the corresponding visual regions, which limits interpretability and can lead to visually unfaithful reasoning\nPellegrini\net al.\n(\n2023\n); Moll\net al.\n(\n2025\n); Liu\net al.\n(\n2023\n); Gundersen\net al.\n(\n2025\n)\n. This has motivated growing interest in grounding-aware evaluation and training signals that connect medical language to spatial evidence. In this work, we target this limitation by providing explicit referring-style localization supervision derived from expert segmentation annotations, enabling medical VLMs to better align morphology- and location-bearing phrases with concrete visual evidence.\n2.2\nReferring Expression Grounding\nReferring expression grounding localizes an entity specified by a natural-language expression, emphasizing attribute understanding and spatial disambiguation. In natural images, large-scale referring and phrase grounding benchmarks have driven progress in models that bind text tokens to localized regions and handle fine-grained relational language\nLiu\net al.\n(\n2024\n); Li\net al.\n(\n2022\n); Kamath\net al.\n(\n2021\n)\n. In medical imaging, however, referring expression grounding datasets remain scarce. Available supervision is typically split between imageâ€“text pairs (rich clinical narratives but weak spatial alignment) and segmentation masks (precise localization but little to no language beyond class labels). This gap prevents medical VLMs from learning clinically meaningful referring cues such as morphology, laterality, anatomical sub-location, and multi-finding disambiguation\nBai\net al.\n(\n2024\n); Deng\net al.\n(\n2025\n); Liu\net al.\n(\n2023\n); Zhang\net al.\n(\n2025a\n); Yang\net al.\n(\n2025\n)\n. MedGround bridges this gap by converting segmentation annotations into scalable imageâ€“textâ€“box supervision, explicitly training the model to follow clinically grounded referring descriptions.\n2.3\nVLMs-based Synthesis and Verification\nVLMs-driven dataset construction has become a practical route to scale instruction and annotation resources, often paired with automated filtering, self-checking, or model-based judging to control noise. In medical settings, such synthesis is particularly fragile: hallucinated attributes, incorrect spatial relations, or visually unsupported statements can easily slip into training data and degrade evidence faithfulness\nPal\net al.\n(\n2023\n); Yu\net al.\n(\n2023\n); Khanna\net al.\n(\n2023\n); Dong\net al.\n(\n2023\n); Croxford\net al.\n(\n2025\n); Yue\net al.\n(\n2025\n)\n. MedGround adopts a conservative synthesis principle: rather than asking a model to discover or localize findings from free-form text, we start from expert masks as deterministic spatial anchors and only synthesize referring queries conditioned on mask-derived geometry and spatial cues. We then apply multi-stage verificationâ€”format constraints, rule-based geometric/medical priors, and image-based judgingâ€”to filter ambiguous or visually unsupported samples. This design explicitly reduces the risk of inheriting vision-side localization bias while retaining the scalability benefits of VLMs-based generation.\n3\nMedGround\nThis section introduces\nMedGround\n, a mask-guided synthesis-and-verification pipeline (Fig.\n2\n) that automatically converts segmentation annotations into high-quality\nmedical referring box grounding\ndatasets.\nStarting from expert masks, MedGround derives candidate ground-truth boxes, prompts a VLM to generate clinically grounded referring queries for randomly selected targets, and applies multi-stage verification to filter ambiguous or visually unsupported samples.\n3.1\nDataset Definition\nWe produce MedGround-35K with MedGround pipline. Each example of MedGround-35K is a triplet\n(\nI\n,\nQ\n,\nB\n)\n(I,Q,B)\n, where\nI\nI\nis a medical image,\nQ\nQ\nis a referring query that describes one or multiple target regions, and\nB\nB\nis the corresponding ground-truth set of 2D bounding boxes. Each box is represented as\n[\nx\nmin\n,\ny\nmin\n,\nx\nmax\n,\ny\nmax\n]\n[x_{\\min},y_{\\min},x_{\\max},y_{\\max}]\nand normalized to a\n1000\nÃ—\n1000\n1000\\times 1000\ncoordinate grid for consistent formatting across datasets and resolutions.\n3.2\nFrom Masks to Ground-Truth Box Lists\nMedGround-35K is built from eight public segmentation datasets(Tab.\n5\n), spanning dermatology, microscopy, and radiological imaging. Each dataset provides expert pixel masks\nM\nM\n. For each connected component in\nM\nM\n, we deterministically derive a tight bounding box\nb\n=\nbbox\nâ€‹\n(\nM\n)\nb=\\mathrm{bbox}(M)\n, and collect all boxes within the same image into a candidate list\nâ„¬\n=\n{\nb\n1\n,\nâ€¦\n,\nb\nn\n}\n\\mathcal{B}=\\{b_{1},\\ldots,b_{n}\\}\n.\nThis box list serves two roles: it provides the query generator (VLMs) with a\ncandidate target pool\n, i.e., explicit regions to refer to when composing queries, from which one or multiple boxes are randomly selected as the intended target(s); and it serves as the\nground-truth localization anchor\nduring verification, supplying the reference boxes needed to judge whether the generated text is visually faithful and unambiguous with respect to the image.\n3.3\nMask-Guided Query Synthesis\nGiven an image\nI\nI\nand its box list\nâ„¬\n\\mathcal{B}\n, we first compute mask-derived attributes to constrain generation:\n(1) geometry\n: area ratio, width/height, aspect ratio, elongation/compactness proxies,\n(2) spatial cues\n: centroid, coarse bins such as left/right and upper/lower, optionally with dataset-specific anatomical conventions,\nand\n(3) metadata\n: imaging modality/domain and coarse category labels when available.\nWe then construct dataset-aware prompts and query VLMs to synthesize referring queries. The prompts explicitly condition on the image modality, and instruct the VLMs to produce questions that:\n(1) reflect\nvisible\nproperties (shape/texture/boundary appearance),\n(2) incorporate location cues when needed for disambiguation, and\n(3) use appropriate medical terminology while avoiding claims that cannot be justified from the image (e.g., etiology, pathology stage, or non-visible symptoms).\nTo encourage diversity and avoid trivial templates, we ask the VLMs to\nrandomly select\none or multiple target boxes from\nâ„¬\n\\mathcal{B}\nand generate a corresponding query. Each generated sample is required to follow a fixed JSON schema containing the query text and the selected target box coordinates, enabling downstream automatic parsing.\nFigure 3:\nDiversity and Linguistic Complexity of the MedGround dataset.\nUp: The word cloud illustrates the distribution of medical terminology, anatomical landmarks, and clinical descriptors within the grounding annotations. Down: Comparative analysis of annotation richness across five distinct modalities based on three key metrics: (a) Clinical Entity Density, (b) Morphology Term Coverage, and (c) Spatial Relation Complexity.\n3.4\nMulti-Stage Data Verification and Cleaning\nSince free-form generation may introduce ambiguity or visually unsupported descriptions, MedGround applies a multi-stage verifier to filter noisy samples.\nStage I: format and schema checks.\nWe reject generations that do not conform to the required JSON format, contain missing fields, or output invalid box indices/coordinates.\nStage II: rule-based validation with geometry and location constraints.\nWe enforce consistency between textual descriptors and mask-derived attributes. For example, size adjectives must match area buckets, spatial phrases must match centroid bins, and prompts are constrained by domain-specific keyword allow/deny lists to prevent cross-domain leakage (e.g., thoracic anatomy terms in dermoscopy). Samples violating these rules are discarded.\nStage III: VLM-based consistency filtering.\nWe performs image-based data cleaning using a VLMs as a semantic verifier. Recall that during synthesis, the generator first selects one or multiple target box(es) from the candidate pool\nâ„¬\n\\mathcal{B}\nand then writes a question to refer to the selected region(s); therefore, the â€œanswerâ€\nA\nA\nof each QA pair is exactly the selected ground-truth box(es).\nWe feed the verifier the image together with the selected answer box(es) as an explicit localization anchor, and ask whether the visual content inside the box supports the question description. Concretely, the verifier is instructed to restate the observable attributes of the highlighted region and output a binary decision on whether the question is\ncorrectly grounded\nand\nunambiguous\n. If the verifier cannot recover the described cues from the given box, we treat the sample as hallucinated or ambiguous and discard it.\n3.5\nMedGround-35K\nMedGround-35K comprises\n35\n,\n480\n35,480\nimageâ€“textâ€“box triplets, with\n25\n,\n420\n25,420\nfor training and\n10\n,\n060\n10,060\nfor testing.\nTo characterize the semantic richness of the synthesized queries, we quantify three linguistically grounded properties: (1)\nClinical entity density:\nMeasured by extracting unique UMLS\nBodenreider (\n2004\n)\nconcepts via SciSpacy and normalizing by query length. (2)\nMorphology coverage:\nAssessed using a modality-aware lexicon of appearance descriptors across CT, dermoscopy, microscopy, and ultrasound. (3)\nSpatial complexity:\nCalculated as the average frequency of spatial prepositions and relational phrases per query. Details semantic index of MedGround-35K are shown in Fig.\n3\n. In addition, we log the outcome of each verification stage and report the pass rate per stage in appendix(Tab.\n6\n).\n3.6\nHuman Audit\nTo estimate the faithfulness of retained triplets and quantify residual noise, we conduct a full human audit on the entire MedGround-35K test set. Each triplet is independently reviewed by\nthree trained professional medical annotators\nand marked as\ngood\nif the referring query is clinically sensible and the target localization is visually consistent. We use\nmajority vote\nto define high-confidence samples: a triplet is considered accepted if it receives at least two\ngood\nvotes (\nâ‰¥\n\\geq\n2/3).\nOverall, we audit 10,060 test triplets, and the overall majority-vote accept rate is\n78%\n, indicating that most synthesized queries are faithful and visually grounded. We provide the per-dataset audit breakdown in Appendix Tab.\n7\n.\nDuring human auditing, most rejected samples were due to ambiguity. We treat such ambiguity as a form of\nnatural noise\nthat is difficult to eliminate in real-world medical grounding. Moreover, these lower-agreement subsets constitute only a small fraction of MedGround. Therefore, we keep them in the dataset to preserve coverage and realism, while the appendix transparently documents their audit outcomes and typical error patterns.\nNote:\nIn the following experiments, we train models on the\noriginal training split\nto reflect realistic data conditions and avoid introducing human-selection bias. For evaluation, to ensure reliable and accurate measurement, we report results on the\nhuman-verified test split\n, where only triplets that pass the audit are retained.\n4\nExperiments\nIn this section, we evaluate the effectiveness of\nMedGround-35K\nin enhancing the fine-grained grounding capabilities of VLMs. We focus on whether the synthesized imageâ€“textâ€“box triplets can empower models with evidence-grounded reasoning and improve medical semantic alignment beyond generic label-based supervision.\n4.1\nExperiment Settings\nWe evaluate models on three benchmarks that cover complementary aspects of medical grounding and generalization:\nMedical Referring Grounding\nWe use the full test split of MedGround-35K to assess medical referring grounding performance. Each sample consists of an image, a referring query, and the corresponding target box, requiring the model to localize the region described by the query.\nSemantic Alignment\nTo probe finer-grained semantic alignment, we construct a semantic test set from two datasets with multiple targets per image, namely MosMedPlus\nMorozov\net al.\n(\n2020\n)\nand FHPsAOP\nLu\net al.\n(\n2022\n); Jieyun and ZhanHong (\n2024\n)\n. We select multi-target images and collect question pairs that refer to different targets within the same image. This setting explicitly tests whether a model can distinguish subtle semantic cues and ground them to the correct instance among several plausible regions.\nZero-shot Generalization\nWe further evaluate zero-shot medical generalization on the QaTa-COV19\nDegerli\net al.\n(\n2021\n)\ndataset, where models are tested without task-specific tuning on it. This benchmark reflects out-of-distribution transfer and measures whether improvements from MedGround training generalize beyond the constructed data distribution.\nEvaluation Metrics\nWe utilize IoU to measure the precision of referring expression grounding.\nTraining Details\nWe select MedGemma-27B, MedGemma-4B\nSellergren\net al.\n(\n2025\n)\n, Qwen2.5-VL-7B\nBai\net al.\n(\n2025b\n)\n, and Qwen3-VL-8B\nBai\net al.\n(\n2025a\n)\nas the base models and fine-tune them on the MedGround-35K training data to investigate how fine-grained clinical semantics enhance their medical referring grounding capabilities. To rigorously isolate the impact of linguistic granularity, we also fine-tune these models on a label-based baseline where detailed referring expressions are replaced by coarse category names. All models are fine-tuned using LoRA\nHu\net al.\n(\n2022\n)\non 4 H20 GPUs for three epochs. (Further implementation details and hyper-parameters are provided in Appendix Tab.\n9\nâˆ¼\n\\sim\n12\n).\nVLMs Evaluated\nWe compare with both general purpose VLMs and medical expert VLMs. During the evaluation, we manually craft grounding prompts suitable for these VLMs.\n4.2\nResults and Analysis\nTable 3:\nMedical referring grounding performance across benchmarks.\nWe compare base models and their MedGround-finetuned counterparts. Colored deltas indicate changes over the corresponding base model (gain:\npurple\n, drop:\npink\n).\nType\nModel\nSize\nISIC2016\nBBBC010\nBriFiSeg\nCellNuclei\nDeepBACS\nFHPsAOP\nMoNuSAC\nMosMedPlus\nGeneral\nQwen2.5-VL-7B\n7B\n9.9\n0.3\n1.1\n0.5\n0.0\n0.1\n2.3\n1.3\nQwen3-VL-8B\n8B\n81.7\n44.7\n13.2\n15.9\n9.0\n21.0\n13.6\n11.1\nMedical\nVLMs\nLingshu-7B\n7B\n54.8\n6.1\n5.0\n3.2\n2.7\n15.4\n6.0\n6.6\nMedGemma-4B\n4B\n50.4\n4.0\n3.0\n2.3\n0.6\n17.1\n3.5\n3.5\nMedGemma-27B\n27B\n54.5\n8.6\n2.9\n3.2\n0.9\n22.0\n4.4\n3.0\nFinetuned\nQwen2.5-VL-7B\n7B\n83.0\n(+73.1)\n31.0\n(+30.7)\n20.2\n(+19.1)\n10.3\n(+9.8)\n7.1\n(+7.1)\n77.1\n(+77.0)\n10.9\n(+8.6)\n30.1\n(+28.8)\nQwen3-VL-8B\n8B\n86.4\n(+4.7)\n46.5\n(+1.8)\n24.6\n(+11.4)\n34.7\n(+18.8)\n23.0\n(+14.0)\n81.0\n(+60.0)\n13.5\n(-0.1)\n30.1\n(+19.0)\nLingshu-7B\n7B\n84.2\n(+29.4)\n44.1\n(+38.0)\n19.6\n(+14.6)\n9.7\n(+6.5)\n8.1\n(+5.4)\n79.0\n(+63.6)\n10.9\n(+4.9)\n36.3\n(+29.7)\nMedGemma-4B\n4B\n71.2\n(+20.8)\n16.3\n(+12.3)\n6.2\n(+3.2)\n4.7\n(+2.4)\n0.0\n(-0.6)\n72.8\n(+55.7)\n5.3\n(+1.8)\n33.1\n(+29.6)\nMedGemma-27B\n27B\n81.2\n(+26.7)\n26.8\n(+18.2)\n11.6\n(+8.7)\n11.2\n(+8.0)\n0.4\n(-0.5)\n80.6\n(+58.6)\n9.0\n(+4.6)\n39.3\n(+36.3)\nMedGround effectively enhances VLMsâ€™ medical referring grounding capability.\nWe evaluate the performance of VLM models trained on MedGround-35K. As shown in Tab.\n3\n, models fine-tuned with our knowledge-aware triplets demonstrate significant performance gains.\nFigure 4:\nLabel-based Dataset vs. MedGround-35K.\n(A) Label-based Dataset: conventional datasets typically group all ground-truth boxes under a single generic category label (e.g., \"lesion\") for the entire image. (B) MedGround-35K: provides distinct, fine-grained descriptive expressions for each localized box, capturing specific clinical nuances for individual regions.\nFigure 5:\nExamples of the Semantic Sensitivity Testing Dataset\nThe experimental results demonstrate that fine-tuning on the MedGround-35K serves as a transformative catalyst for the medical referring grounding capabilities of both general and medical-specific VLMs. By providing high-quality spatial-instructional alignment, MedGround enables models to overcome the limitations of zero-shot reasoning.\nThis consistent improvement across diverse modalities and model scales underscores MedGroundâ€™s efficacy in distilling specialized medical spatial knowledge into large-scale models, effectively bridging the gap between general visual perception and precise clinical localization.\nWhile MedGround-35K significantly improved overall performance, the MedGemma series showed minor regressions on DeepBACS\nSpahn\net al.\n(\n2022\n); Spahn and Heilemann (\n2021\n)\n(e.g., -0.5 for MedGemma-27B). This is likely due to the domain shift between specialized fluorescence microscopy and the macro-level clinical imagery in MedGround. Fine-tuning may have caused a slight \"feature forgetting\" of niche microscopic traits while prioritizing general anatomical logicâ€”a minor trade-off compared to the substantial gains achieved across the broader clinical spectrum.\nMedGround injects fine-grained medical semantic knowledge into VLMs.\nWe evaluate whether MedGround injects finer-grained clinical semantics than coarse label-level supervision using a Semantic Alignment setting. Concretely, we fine-tune the same backbone with LoRA on (i) MedGround-35K (clinically detailed referring expressions) and (ii) a label-based baseline built by pairing each annotated region with its coarse category name (Fig.\n4\n). We then compare the two models on the Semantic Alignment Evaluation benchmark.\nWe propose\nSemantic Sensitivity\nto measure whether a model can follow subtle semantic differences in multi-target images. Each test case contains one image with multiple targets and two queries referring to different objects (Fig.\n5\n). For each query, we compute the IoU between the predicted box and its corresponding ground-truth box; a target is counted as correct if IoU exceeds a threshold\nÏ„\n\\tau\n. The test case is scored as 1 only if\nboth\nqueries are correctly localized (otherwise 0). SS is the average score over all test cases.\nTable 4:\nComparison of the SS metric.\nGrounding performance is reported in Semantic Sensitivity (%). Comparison across zero-shot baselines, label-based SFT, and MedGround SFT.\nBase Model\nTraining Data\nFHPsAOP\nMosMedPlus\nQwen3VL-8B\nw/o SFT\n21.5\n5.9\nLabel-based SFT\n45.3\n5.8\nMedGround-35K SFT\n53.5\n18.0\nMedGemma-4B\nw/o SFT\n2.3\n1.4\nLabel-based SFT\n21.3\n4.4\nMedGround-35K SFT\n55.9\n13.1\nMedGemma-27B\nw/o SFT\n12.7\n0.0\nLabel-based SFT\n27.0\n2.3\nMedGround-35K SFT\n74.3\n22.5\nAs shown in Tab.\n4\n, models without medical referring grounding training (w/o SFT) exhibit very low Semantic Sensitivity, indicating limited ability to follow query-specific semantics and localize the correct targets in multi-object clinical images. After fine-tuning, MedGround-35K SFT consistently outperforms the label-based SFT baseline on both FHPsAOP and MosMedPlus. This advantage stems from MedGroundâ€™s fine-grained clinical semantics, whereas label-based supervision relies on coarse category names that are often insufficient to distinguish co-existing targets. These findings confirm that fine-grained clinical semantics are essential to bridge the \"cognitiveâ€“perceptual gap,\" successfully equipping VLMs with the ability to associate specific morphology- and location-aware descriptions with their corresponding spatial anchors in multi-target environments.\nMedGround Enables Zero-shot Generalization of VLMs.\nWe further evaluate whether MedGround improves cross-dataset transfer by testing on an external unseen dataset, QaTa-COV19. We construct a fine-grained referring grounding test set from its referring statements and ground-truth boxes, and compare the performance of the MedGround-fine-tuned model against the base model on this benchmark.\nFigure 6:\nZero-shot Performance.\nWe evaluate different models on QaTa-COV19 dataset.\nThe zero-shot evaluation on the external QaTa-COV19 dataset reveals that fine-tuning on MedGround-35K yields substantial and consistent improvements in cross-dataset transferability across all tested architectures. As illustrated in the Fig.\n6\n, the MedGround-35K enhanced models achieve remarkable performance gains compared to others. This significant performance leap on an entirely unseen datasetâ€”achieved without any task-specific trainingâ€”strongly demonstrates that MedGround does not merely facilitate dataset-specific memorization but rather imparts a robust, generalized medical spatial reasoning logic. Such results solidify MedGroundâ€™s role as a critical foundational resource for enabling VLMs to generalize their referring grounding capabilities to novel clinical scenarios and emerging disease distributions.\n5\nConclusion\nWe introduce\nMedGround\n, a mask-guided synthesis and verification pipeline that constructs medical referring grounding data as imageâ€“queryâ€“box triplets. By generating fine-grained referring expressions and filtering ambiguous or visually unsupported samples, MedGround provides semantically rich, visually verified supervision. Fine-tuning on MedGround-35K consistently improves grounding performance and transfer to unseen benchmarks, helping narrow the\ncognitiveâ€“perceptual gap\nby anchoring medical language to localized visual evidence. Since MedGround-35K is synthesized and verified from existing segmentation resources, it is highly scalable, enabling rapid extension to new modalities and anatomical systems. Beyond grounding, MedGround encourages models to justify clinical semantics with explicit spatial evidence, reducing reliance on linguistic priors and improving faithfulness.\n6\nAcknowledgment\nThis work was supported by Damo Academy through Damo Academy Research Intern Program.\nLimitations\nFirst, the VLM judge (Gemini-2.5-Pro) may introduce model-specific biases. Although the upstream stages of our pipeline are largely deterministic and rule-based, the final acceptance decision is made by a learned verifier and can reflect its preferences, failure modes, or sensitivity to prompt phrasing. Using alternative judges or ensembling multiple verifiers could further reduce this dependency, but is beyond the scope of this work. Second, our primary supervision is box-level rather than pixel-level. While bounding boxes provide scalable and broadly applicable grounding signals, they may not capture fine-grained boundaries or subtle morphology required by certain clinical applications (e.g., precise lesion extent or margin characterization). Third, synthesized queries may inherit LLM stylistic artifacts (e.g., phrasing patterns, verbosity, or implicit assumptions). We mitigate this with constrained prompting and verification, yet some residual biases may remain and could affect downstream generalization. Finally, MedGround is constructed from public segmentation datasets and therefore inherits their label spaces, imaging modalities, and population coverage; as a result, some anatomical regions, pathologies, and acquisition settings are underrepresented, and performance on out-of-distribution clinical data may be limited.\nReferences\nS. G. Armato, G. McLennan, L. Bidaut, M. F. McNitt-Gray, C. R. Meyer, A. P. Reeves,\net al.\n(2011)\nThe lung image database consortium (lidc) and image database resource initiative (idri): a completed reference database of lung nodules on ct scans\n.\nMedical Physics\n38\n(\n2\n),\npp.Â 915â€“931\n.\nExternal Links:\nDocument\nCited by:\nTable 1\n.\nF. Bai, Y. Du, T. Huang, M. Q. Meng, and B. Zhao (2024)\nM3d: advancing 3d medical image analysis with multi-modal large language models\n.\narXiv preprint arXiv:2404.00578\n.\nCited by:\nÂ§1\n,\nÂ§2.2\n.\nS. Bai, Y. Cai, R. Chen, K. Chen, X. Chen, Z. Cheng, L. Deng, W. Ding, C. Gao, C. Ge, W. Ge, Z. Guo, Q. Huang, J. Huang, F. Huang, B. Hui, S. Jiang, Z. Li, M. Li, M. Li, K. Li, Z. Lin, J. Lin, X. Liu, J. Liu, C. Liu, Y. Liu, D. Liu, S. Liu, D. Lu, R. Luo, C. Lv, R. Men, L. Meng, X. Ren, X. Ren, S. Song, Y. Sun, J. Tang, J. Tu, J. Wan, P. Wang, P. Wang, Q. Wang, Y. Wang, T. Xie, Y. Xu, H. Xu, J. Xu, Z. Yang, M. Yang, J. Yang, A. Yang, B. Yu, F. Zhang, H. Zhang, X. Zhang, B. Zheng, H. Zhong, J. Zhou, F. Zhou, J. Zhou, Y. Zhu, and K. Zhu (2025a)\nQwen3-vl technical report\n.\nExternal Links:\n2511.21631\n,\nLink\nCited by:\nÂ§B.1\n,\nÂ§4.1\n.\nS. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang,\net al.\n(2025b)\nQwen2. 5-vl technical report\n.\narXiv preprint arXiv:2502.13923\n.\nCited by:\nÂ§B.1\n,\nÂ§4.1\n.\nS. Bannur, K. Bouzid, D. C. Castro, A. Schwaighofer, A. Thieme, S. Bond-Taylor, M. Ilse, F. PÃ©rez-GarcÃ­a, V. Salvatelli, H. Sharma,\net al.\n(2024)\nMaira-2: grounded radiology report generation\n.\narXiv preprint arXiv:2406.04449\n.\nCited by:\nÂ§1\n.\nO. Bodenreider (2004)\nThe unified medical language system (umls): integrating biomedical terminology\n.\nNucleic acids research\n32\n(\nsuppl_1\n),\npp.Â D267â€“D270\n.\nCited by:\nÂ§3.5\n.\nB. Boecking, N. Usuyama, S. Bannur, D. C. de Castro, A. Schwaighofer, S. Hyland, M. T. Wetscherek, T. Naumann, A. Nori, J. A. Valle,\net al.\n(2022)\nMs-cxr: making the most of text semantics to improve biomedical vision-language processing\n.\nURL: http://dx. doi. org/10.13026/b90j-vb87\n.\nCited by:\nTable 1\n.\nJ. C. Caicedo, A. Goodman, K. W. Karhohs, B. A. Cimini, J. Ackerman, M. Haghighi, C. Heng, T. Becker, M. Doan, C. McQuin,\net al.\n(2019)\nNucleus segmentation across imaging experiments: the 2018 data science bowl\n.\nNature methods\n16\n(\n12\n),\npp.Â 1247â€“1253\n.\nCited by:\nÂ§A.1\n.\nY. Chen, D. Xu, Y. Huang, S. Zhan, H. Wang, D. Chen, X. Wang, M. Qiu, and H. Li (2025)\nMIMO: a medical vision language model with visual referring multimodal input and pixel grounding multimodal output\n.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n,\npp.Â 24732â€“24741\n.\nCited by:\nÂ§1\n.\nN. C. Codella, D. Gutman, M. E. Celebi, B. Helba, M. A. Marchetti, S. W. Dusza, A. Kalloo, K. Liopyris, N. Mishra, H. Kittler,\net al.\n(2018)\nSkin lesion analysis toward melanoma detection: a challenge at the 2017 international symposium on biomedical imaging (isbi), hosted by the international skin imaging collaboration (isic)\n.\nIn\n2018 IEEE 15th international symposium on biomedical imaging (ISBI 2018)\n,\npp.Â 168â€“172\n.\nCited by:\nÂ§A.1\n.\nE. Croxford, Y. Gao, E. First, N. Pellegrino, M. Schnier, J. Caskey, M. Oguss, G. Wills, G. Chen, D. Dligach,\net al.\n(2025)\nAutomating evaluation of ai text generation in healthcare with a large language model (llm)-as-a-judge\n.\nmedRxiv\n,\npp.Â 2025â€“04\n.\nCited by:\nÂ§2.3\n.\nA. Degerli, M. Ahishali, M. Yamac, S. Kiranyaz, M. E. Chowdhury, K. Hameed, T. Hamid, R. Mazhar, and M. Gabbouj (2021)\nCOVID-19 infection map generation and detection from chest x-ray images\n.\nHealth information science and systems\n9\n(\n1\n),\npp.Â 15\n.\nCited by:\nÂ§4.1\n.\nZ. Deng, R. He, J. Liu, Y. Wang, Z. Meng, S. Jiang, Y. Xie, and Z. Liu (2025)\nMed-glip: advancing medical language-image pre-training with large-scale grounded dataset\n.\narXiv preprint arXiv:2508.10528\n.\nCited by:\nÂ§1\n,\nÂ§2.2\n.\nH. Dong, W. Xiong, D. Goyal, Y. Zhang, W. Chow, R. Pan, S. Diao, J. Zhang, K. Shum, and T. Zhang (2023)\nRaft: reward ranked finetuning for generative foundation model alignment\n.\narXiv preprint arXiv:2304.06767\n.\nCited by:\nÂ§2.3\n.\nH. Ge, L. Hao, Z. Xu, Z. Lin, B. Li, S. Zhou, H. Zhao, and Y. Liu (2025)\nClinKD: cross-modal clinical knowledge distiller for multi-task medical images\n.\narXiv preprint arXiv:2502.05928\n.\nCited by:\nÂ§1\n.\nB. Gundersen, N. Deperrois, S. Ruiperez-Campillo, T. M. Sutter, J. E. Vogt, M. Moor, F. Nooralahzadeh, and M. Krauthammer (2025)\nEnhancing radiology report generation and visual grounding using reinforcement learning\n.\narXiv preprint arXiv:2512.10691\n.\nCited by:\nÂ§2.1\n.\nE. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen,\net al.\n(2022)\nLora: low-rank adaptation of large language models.\n.\nICLR\n1\n(\n2\n),\npp.Â 3\n.\nCited by:\nÂ§4.1\n.\nX. Huang, H. Huang, L. Shen, Y. Yang, F. Shang, J. Liu, and J. Liu (2024)\nA refer-and-ground multimodal large language model for biomedicine\n.\nIn\nInternational Conference on Medical Image Computing and Computer-Assisted Intervention\n,\npp.Â 399â€“409\n.\nCited by:\nÂ§1\n.\nX. Huang, L. Shen, J. Liu, F. Shang, H. Li, H. Huang, and Y. Yang (2025)\nTowards a multimodal large language model with pixel-level insight for biomedicine\n.\nIn\nProceedings of the AAAI Conference on Artificial Intelligence\n,\nVol.\n39\n,\npp.Â 3779â€“3787\n.\nCited by:\nÂ§1\n.\nB. Jieyun and O. ZhanHong (2024)\nPubic symphysis-fetal head segmentation and angle of progression\n.\nCited by:\nÂ§4.1\n.\nA. Kamath, M. Singh, Y. LeCun, G. Synnaeve, I. Misra, and N. Carion (2021)\nMdetr-modulated detection for end-to-end multi-modal understanding\n.\nIn\nProceedings of the IEEE/CVF international conference on computer vision\n,\npp.Â 1780â€“1790\n.\nCited by:\nÂ§2.2\n.\nS. Khanna, A. Dejl, K. Yoon, S. Q. Truong, H. Duong, A. Saenz, and P. Rajpurkar (2023)\nRadgraph2: modeling disease progression in radiology reports via hierarchical information extraction\n.\nIn\nMachine learning for healthcare conference\n,\npp.Â 381â€“402\n.\nCited by:\nÂ§2.3\n.\nS. Y. Kim, S. Cho, V. Yun, and G. Hwang (2025)\nMedCLM: learning to localize and reason via a cot-curriculum in medical vision-language models\n.\narXiv preprint arXiv:2510.04477\n.\nCited by:\nÂ§1\n.\nC. Li, C. Wong, S. Zhang, N. Usuyama, H. Liu, J. Yang, T. Naumann, H. Poon, and J. Gao (2023)\nLlava-med: training a large language-and-vision assistant for biomedicine in one day\n.\nAdvances in Neural Information Processing Systems\n36\n,\npp.Â 28541â€“28564\n.\nCited by:\nÂ§1\n,\nÂ§2.1\n.\nL. H. Li, P. Zhang, H. Zhang, J. Yang, C. Li, Y. Zhong, L. Wang, L. Yuan, L. Zhang, J. Hwang,\net al.\n(2022)\nGrounded language-image pre-training\n.\nIn\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition\n,\npp.Â 10965â€“10975\n.\nCited by:\nÂ§2.2\n.\nQ. Li, X. Yan, J. Xu, R. Yuan, Y. Zhang, R. Feng, Q. Shen, X. Zhang, and S. Wang (2024)\nAnatomical structure-guided medical vision-language pre-training\n.\nIn\nInternational Conference on Medical Image Computing and Computer-Assisted Intervention\n,\npp.Â 80â€“90\n.\nCited by:\nÂ§1\n.\nF. Liu, H. Zhou, B. Gu, X. Zou, J. Huang, J. Wu, Y. Li, S. S. Chen, Y. Hua, P. Zhou,\net al.\n(2025)\nApplication of large language models in medicine\n.\nNature Reviews Bioengineering\n,\npp.Â 1â€“20\n.\nCited by:\nÂ§1\n.\nJ. Liu, Z. Wang, Q. Ye, D. Chong, P. Zhou, and Y. Hua (2023)\nQilin-med-vl: towards chinese large vision-language model for general healthcare\n.\narXiv preprint arXiv:2310.17956\n.\nCited by:\nÂ§2.1\n,\nÂ§2.2\n.\nS. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, Q. Jiang, C. Li, J. Yang, H. Su,\net al.\n(2024)\nGrounding dino: marrying dino with grounded pre-training for open-set object detection\n.\nIn\nEuropean conference on computer vision\n,\npp.Â 38â€“55\n.\nCited by:\nÂ§2.2\n.\nV. Ljosa, K. L. Sokolnicki, and A. E. Carpenter (2012)\nAnnotated high-throughput microscopy image sets for validation\n.\nNature methods\n9\n(\n7\n),\npp.Â 637\n.\nCited by:\nÂ§A.1\n.\nY. Lu, M. Zhou, D. Zhi, M. Zhou, X. Jiang, R. Qiu, Z. Ou, H. Wang, D. Qiu, M. Zhong,\net al.\n(2022)\nThe jnu-ifm dataset for segmenting pubic symphysis-fetal head\n.\nData in brief\n41\n,\npp.Â 107904\n.\nCited by:\nÂ§A.1\n,\nÂ§4.1\n.\nR. Mahmood, P. Yan, D. M. Reyes, G. Wang, M. K. Kalra, P. Kaviani, J. T. Wu, and T. Syeda-Mahmood (2025)\nEvaluating automated radiology report quality through fine-grained phrasal grounding of clinical findings\n.\nIn\n2025 IEEE 22nd International Symposium on Biomedical Imaging (ISBI)\n,\npp.Â 1â€“5\n.\nCited by:\nÂ§1\n.\nO. N. Manzari, H. Ahmadabadi, H. Kashiani, S. B. Shokouhi, and A. Ayatollahi (2023)\nMedViT: a robust vision transformer for generalized medical image classification\n.\nComputers in biology and medicine\n157\n,\npp.Â 106791\n.\nCited by:\nÂ§1\n.\nG. Mathieu, E. D. Bachir,\net al.\n(2022)\nBrifiseg: a deep learning-based method for semantic and instance segmentation of nuclei in brightfield images\n.\narXiv preprint arXiv:2211.03072\n.\nCited by:\nÂ§A.1\n.\nJ. Moll, M. Graf, T. Lemke, N. Lenhart, D. Truhn, J. Delbrouck, J. Pan, D. Rueckert, L. C. Adams, and K. K. Bressem (2025)\nEvaluating reasoning faithfulness in medical vision-language models using multimodal perturbations\n.\narXiv preprint arXiv:2510.11196\n.\nCited by:\nÂ§2.1\n.\nM. Moor, Q. Huang, S. Wu, M. Yasunaga, C. Zakka, Y. Dalmia, E. P. Reis, P. Rajpurkar, and J. Leskovec (2023)\nMed-flamingo: a multimodal medical few-shot learner (2023)\n.\nURL: https://arxiv. org/abs/2307.15189\n.\nCited by:\nÂ§2.1\n.\nS. P. Morozov, A. E. Andreychenko, N. A. Pavlov, A. Vladzymyrskyy, N. V. Ledikhova, V. A. Gombolevskiy, I. A. Blokhin, P. B. Gelezhe, A. Gonchar, and V. Y. Chernina (2020)\nMosmeddata: chest ct scans with covid-19 related findings dataset\n.\narXiv preprint arXiv:2005.06465\n.\nCited by:\nÂ§A.1\n,\nÂ§4.1\n.\nV. Nath, W. Li, D. Yang, A. Myronenko, M. Zheng, Y. Lu, Z. Liu, H. Yin, Y. M. Law, Y. Tang,\net al.\n(2025)\nVila-m3: enhancing vision-language models with medical expert knowledge\n.\nIn\nProceedings of the Computer Vision and Pattern Recognition Conference\n,\npp.Â 14788â€“14798\n.\nCited by:\nÂ§1\n.\nS. Ostmeier, J. Xu, Z. Chen, M. Varma, L. Blankemeier, C. Bluethgen, A. E. M. Md, M. Moseley, C. Langlotz, A. S. Chaudhari,\net al.\n(2024)\nGreen: generative radiology report evaluation and error notation\n.\nIn\nFindings of the association for computational linguistics: EMNLP 2024\n,\npp.Â 374â€“390\n.\nCited by:\nÂ§1\n.\nA. Pal, L. K. Umapathi, and M. Sankarasubbu (2023)\nMed-HALT: medical domain hallucination test for large language models\n.\nIn\nProceedings of the 27th Conference on Computational Natural Language Learning (CoNLL)\n,\nJ. Jiang, D. Reitter, and S. Deng (Eds.)\n,\nSingapore\n,\npp.Â 314â€“334\n.\nExternal Links:\nDocument\nCited by:\nÂ§1\n,\nÂ§2.3\n.\nC. Pellegrini, E. Ã–zsoy, B. Busam, N. Navab, and M. Keicher (2023)\nRadialog: a large vision-language model for radiology report generation and conversational assistance\n.\narXiv preprint arXiv:2311.18681\n.\nCited by:\nÂ§2.1\n.\nA. Sellergren, S. Kazemzadeh, T. Jaroensri, A. Kiraly, M. Traverse, T. Kohlberger, S. Xu, F. Jamil, C. Hughes, C. Lau,\net al.\n(2025)\nMedgemma technical report\n.\narXiv preprint arXiv:2507.05201\n.\nCited by:\nÂ§B.1\n,\nÂ§1\n,\nÂ§4.1\n.\nC. Spahn, E. GÃ³mez-de-Mariscal, R. F. Laine, P. M. Pereira, L. von Chamier, M. Conduit, M. G. Pinho, G. Jacquemet, S. Holden, M. Heilemann,\net al.\n(2022)\nDeepBacs for multi-task bacterial image analysis using open-source deep learning approaches\n.\nCommunications Biology\n5\n(\n1\n),\npp.Â 688\n.\nCited by:\nÂ§A.1\n,\nÂ§4.2\n.\nC. Spahn and M. Heilemann (2021)\nDeepbacsâ€“escherichia coli bright field segmentation dataset\n.\nOctober\n.\nCited by:\nÂ§4.2\n.\nR. Strudel, I. Laptev, and C. Schmid (2022)\nWeakly-supervised segmentation of referring expressions\n.\narXiv preprint arXiv:2205.04725\n.\nCited by:\nÂ§1\n.\nA. A. Tarhini, P. Dave, and I. El Naqa (2025)\nGeneral artificial intelligence for the diagnosis and treatment of cancer: the rise of foundation models\n.\nBJR| Artificial Intelligence\n2\n(\n1\n),\npp.Â ubaf015\n.\nCited by:\nÂ§1\n.\nR. Verma, N. Kumar, A. Patil, N. C. Kurian, S. Rane, S. Graham, Q. D. Vu, M. Zwager, S. E. A. Raza, N. Rajpoot,\net al.\n(2021)\nMoNuSAC2020: a multi-organ nuclei segmentation and classification challenge\n.\nIEEE Transactions on Medical Imaging\n40\n(\n12\n),\npp.Â 3413â€“3423\n.\nCited by:\nÂ§A.1\n.\nX. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, and R. M. Summers (2017)\nChestX-ray8: hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases\n.\nIn\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n,\nCited by:\nTable 1\n.\nC. Wu, X. Zhang, Y. Zhang, H. Hui, Y. Wang, and W. Xie (2025)\nTowards generalist foundation model for radiology by leveraging web-scale 2d&3d medical data\n.\nNature Communications\n16\n(\n1\n),\npp.Â 7866\n.\nCited by:\nÂ§1\n.\nJ. T. Wu, N. N. Agu, I. Lourentzou, A. Sharma, J. A. Paguio, J. S. Yao, E. C. Dee, W. Mitchell, S. Kashyap, A. Giovannini,\net al.\n(2021)\nChest imagenome dataset for clinical reasoning\n.\narXiv preprint arXiv:2108.00316\n.\nCited by:\nTable 1\n.\nQ. Xie, E. J. Schenck, H. S. Yang, Y. Chen, Y. Peng, and F. Wang (2023)\nFaithful ai in medicine: a systematic review with large language models and beyond\n.\nMedRxiv\n.\nCited by:\nÂ§1\n.\nJ. Xu (2025)\nUncertainty estimation in large vision language models for automated radiology report generation\n.\nIn\nProceedings of the 4th Machine Learning for Health Symposium. PMLR\n,\npp.Â 1039â€“1052\n.\nCited by:\nÂ§1\n.\nW. Xu, H. P. Chan, L. Li, M. Aljunied, R. Yuan, J. Wang, C. Xiao, G. Chen, C. Liu, Z. Li,\net al.\n(2025)\nLingshu: a generalist foundation model for unified multimodal medical understanding and reasoning\n.\narXiv preprint arXiv:2506.07044\n.\nCited by:\nÂ§B.1\n.\nK. Yan, X. Wang, L. Lu, and R. M. Summers (2018)\nDeepLesion: automated mining of large-scale lesion annotations and universal lesion detection with deep learning\n.\nIn\nMedical Image Computing and Computer Assisted Intervention (MICCAI)\n,\nCited by:\nTable 1\n.\nL. Yang, S. Xu, A. Sellergren, T. Kohlberger, Y. Zhou, I. Ktena, A. Kiraly, F. Ahmed, F. Hormozdiari, T. Jaroensri,\net al.\n(2024)\nAdvancing multimodal medical capabilities of gemini\n.\narXiv preprint arXiv:2405.03162\n.\nCited by:\nÂ§1\n.\nX. Yang, J. Liu, P. Wang, G. Wang, Y. Yang, and H. T. Shen (2025)\nNew dataset and methods for fine-grained compositional referring expression comprehension via specialist-mllm collaboration\n.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\n.\nCited by:\nÂ§2.2\n.\nF. Yu, M. Endo, R. Krishnan, I. Pan, A. Tsai, E. P. Reis, E. K. U. N. Fonseca, H. M. H. Lee, Z. S. H. Abad, A. Y. Ng,\net al.\n(2023)\nEvaluating progress in automatic chest x-ray radiology report generation\n.\nPatterns\n4\n(\n9\n).\nCited by:\nÂ§2.3\n.\nJ. Yue, S. Zhang, Z. Jia, H. Xu, Z. Han, X. Liu, and G. Wang (2025)\nMedSG-bench: a benchmark for medical image sequences grounding\n.\narXiv preprint arXiv:2505.11852\n.\nCited by:\nÂ§2.3\n.\nW. Zhang, S. S. Chandra, and A. Nicolson (2025a)\nAnatomical grounding pre-training for medical phrase grounding\n.\nIn\n2025 IEEE 22nd International Symposium on Biomedical Imaging (ISBI)\n,\npp.Â 1â€“5\n.\nCited by:\nÂ§2.2\n.\nW. Zhang, S. S. Chandra, and A. Nicolson (2025b)\nGeneralized medical phrase grounding\n.\narXiv preprint arXiv:2512.01085\n.\nCited by:\nÂ§1\n.\nX. Zhang, C. Wu, Z. Zhao, W. Lin, Y. Zhang, Y. Wang, and W. Xie (2023)\nPmc-vqa: visual instruction tuning for medical visual question answering\n.\narXiv preprint arXiv:2305.10415\n.\nCited by:\nÂ§2.1\n.\nAppendix A\nDatasets\nA.1\nData source\nMedGround-35K is constructed by repurposing annotations from eight publicly available medical segmentation datasets, covering diverse imaging modalities and clinical scenarios, including BBBC010\nLjosa\net al.\n(\n2012\n)\n, BriFiSeg\nMathieu\net al.\n(\n2022\n)\n, CellNuclei\nCaicedo\net al.\n(\n2019\n)\n, DeepBacs\nSpahn\net al.\n(\n2022\n)\n, FHPsAOP\nLu\net al.\n(\n2022\n)\n, ISIC2016\nCodella\net al.\n(\n2018\n)\n, MoNuSAC\nVerma\net al.\n(\n2021\n)\n, and MosMedPlus\nMorozov\net al.\n(\n2020\n)\n. For each source dataset, we use its original train/val/test split as the raw data pool before applying our MedGround construction and verification pipeline. Detailed split statistics of these sources are summarized in Tab.\n5\n.\nTable 5:\nRaw split sizes of segmentation data sources before MedGround construction.\nDataset\nTrain\nVal\nTest\nTotal\nBBBC010\n70\n10\n20\n100\nBrifiseg\n38,463\n4,244\n576\n43,283\nCellNuclei\n469\n67\n134\n670\nDeepBacs\n17\n2\n15\n34\nDynamicNuclear\n4,950\n1,417\n717\n7,084\nFHPsAOP\n5,600\n800\n1,600\n8,000\nISIC2016\n810\n90\n379\n1,279\nMoNuSAC\n359\n35\n249\n643\nMosMedPlus\n1,910\n271\n547\n2,457\nTotal\n52,648\n6,936\n4,237\n63,550\nA.2\nPass rate of multi-stage verifaction pipline\nWe analyze rejection reasons across stages (I: format, II: rule-based checks, III: VLM judging) in Tab.\n6\n. Stages I and II retain almost all samples across datasets (\nâˆ¼\n\\sim\n92â€“100%), indicating that invalid formatting and obvious rule violations account for only a small fraction of failures. In contrast, Stage III is substantially more selective and becomes the main source of filtering: retention drops to 48â€“67% for several microscopy datasets (e.g., BBBC010, CellNuclei, DeepBACS, BriFiSeg), while remaining high for more visually distinctive settings such as FHPSAOP and ISIC2016 (\nâˆ¼\n\\sim\n88â€“90%). Overall, the final retention is about 80% on both splits (80.9% train, 80.3% test), suggesting that the VLM judge primarily removes samples that are\nambiguous\n(multiple plausible referents) or\nvisually unsupported\n(the description does not clearly match the highlighted target). This aligns with our human audit, where most rejected cases are due to ambiguity, and supports that MedGroundâ€™s verification prioritizes unambiguous, evidence-grounded referring expressions rather than merely enforcing formatting constraints.\nTable 6:\nPass rates of each verification stage across datasets.\nWe report the retention percentage and remaining sample count after Stages A/B/C for both the training and test splits.\nStage\nBBBC010\nBriFiSeg\nCellNuclei\nDeepBACS\nFHPSAOP\nISIC2016\nMoNuSAC\nMosMedPlus\nTotal\nTrain\nTest\nTrain\nTest\nTrain\nTest\nTrain\nTest\nTrain\nTest\nTrain\nTest\nTrain\nTest\nTrain\nTest\nTrain\nTest\nStage I\n97.2%\n97.0%\n98.3%\n98.4%\n98.1%\n98.4%\n98.2%\n97.2%\n100.0%\n100.0%\n92.1%\n92.4%\n96.2%\n96.7%\n96.4%\n94.8%\n98.3%\n97.6%\nStage II\n96.8%\n97.0%\n98.1%\n98.0%\n97.9%\n98.3%\n98.2%\n97.2%\n99.4%\n99.2%\n92.0%\n92.3%\n95.4%\n95.8%\n95.6%\n94.5%\n97.8%\n97.1%\nStage III\n56.9%\n49.3%\n67.1%\n65.9%\n57.1%\n56.0%\n48.2%\n47.2%\n89.7%\n89.3%\n88.2%\n88.5%\n63.8%\n60.6%\n78.4%\n94.5%\n80.9%\n80.3%\n#Remaining\n265\n66\n2369\n1212\n1651\n479\n54\n51\n15291\n4363\n2523\n1191\n1193\n855\n2074\n1843\n25420\n10060\nA.3\nHuman Audit Results and Failure Analysis\nFrom the overall statistics, the lower pass rates are mainly observed in the\nnuclei segmentation\ndatasets. By analyzing the auditorsâ€™ rejection reasons, we find that most failures stem from \"ambiguous matching\"â€”the referring description does not admit a unique target in the image, leading auditors to judge the imageâ€“text pair as not well aligned. This is consistent with the inherent properties of nuclei images, where scenes contain numerous visually similar instances and the language often relies on low-dimensional attributes that are insufficient for unique identification.\nTable 7:\nHuman audit on the MedGround test split.\nEach sample is reviewed by three auditors. We report the distribution of\ngood\nvotes and the majority-pass rate (Good Ratio,\nâ‰¥\n\\geq\n2/3\ngood\n).\nDataset\nTotal\n3 good\n2 good\n1 good\n0 good\nGood Ratio\nISIC2016_512_test\n1,141\n938 (82.2%)\n142 (12.4%)\n34 (3.0%)\n27 (2.4%)\n94.65%\nmosmedplus_512_test\n1,843\n1,531 (83.1%)\n57 (3.1%)\n49 (2.7%)\n206 (11.2%)\n86.16%\nfhpsaop_256_test\n4,363\n3,167 (72.6%)\n479 (11.0%)\n233 (5.3%)\n484 (11.1%)\n83.61%\nbrifiseg_512_test\n1,212\n334 (27.6%)\n500 (41.3%)\n283 (23.3%)\n95 (7.8%)\n68.81%\nbbbc010_512_test\n66\n34 (51.5%)\n10 (15.2%)\n13 (19.7%)\n9 (13.6%)\n66.67%\ncellnuclei_256_test\n479\n88 (18.4%)\n147 (30.7%)\n158 (33.0%)\n86 (18.0%)\n49.06%\nmonusac_512_test\n855\n129 (15.1%)\n267 (31.2%)\n316 (37.0%)\n143 (16.7%)\n46.32%\ndeepbacs_512_test\n51\n8 (15.7%)\n14 (27.5%)\n20 (39.2%)\n9 (17.6%)\n43.14%\nA.4\nTrainâ€“Test Distribution Analysis in Three Semantic Dimensions\nWe quantify linguistic properties relevant to grounding(Fig.\n7\n):\nClinical entity density using medical entity recognition/linking tools (e.g., UMLS-based pipelines).\nMorphology term coverage using a curated lexicon of appearance descriptors.\nSpatial relation complexity via counts of spatial prepositions and relational phrases.\nThese analyses show that MedGround queries contain substantially richer morphology and spatial language than category-only prompts, providing more informative supervision for grounding.\nFirst, clinical entity density remains stable between splits for most datasets, indicating similar average numbers of clinically relevant referents per sample. This stability is important for evaluating grounding in multi-target scenes, since the number of co-existing entities directly affects ambiguity and search difficulty.\nSecond, morphology term coverage shows noticeable cross-dataset variation and a mild trainâ€“test gap for some sources. In particular, datasets such as MosMedPlus exhibit higher coverage in the test split, implying that the audited test set contains richer morphological descriptions and thus places greater emphasis on fine-grained visual-semantic alignment. Conversely, nuclei-related datasets (e.g., BBBC010, DeepBACS, MoNuSAC) generally present lower morphology coverage, reflecting the inherent limitation that many instances are visually similar and hard to differentiate with morphology alone.\nThird, spatial relation complexity is consistently highest for MosMedPlus in both splits, reflecting that CT findings often require more location-aware language (e.g., lobes, peripheral vs. central, bilateral distribution) to uniquely specify targets. In contrast, pathology and dermoscopy datasets tend to have lower spatial complexity, consistent with their relatively localized lesions or simpler spatial context. This three-dimensional analysis highlights that different modalities stress different aspects of grounding: microscopy emphasizes dense-instance disambiguation, while CT emphasizes spatial reasoning, and the test split preserves (or slightly strengthens) these challenges.\nFigure 7:\nThree-dimensional analysis comparing the training split (top row, aâ€“c) and test split (bottom row, dâ€“f) across eight medical imaging datasets. Colors: blue=CT, dark purple=ultrasound, light purple=dermoscopy, green=nuclei, orange=bacteria.\nAppendix B\nImplementation Details\nB.1\nFine-Tuning Details\nWe select five open-source VLM backbones as our base models: Qwen2.5-VL\nBai\net al.\n(\n2025b\n)\n, Qwen3-VL\nBai\net al.\n(\n2025a\n)\n, MedGemma-4B, MedGemma-27B\nSellergren\net al.\n(\n2025\n)\n, and Lingshu-7B\nXu\net al.\n(\n2025\n)\n. We report the training hyperparameters for each model in Tab.\n8\nâˆ¼\n\\sim\nTab.\n12\n.\nOur training data are constructed from multiple segmentation datasets spanning different medical domains and imaging modalities(Tab.\n5\n). As a result, the collected training corpus exhibits notable cross-domain imbalance (e.g., microscopy data are substantially larger than some radiology or pathology sources). In our experiments, we do not apply explicit re-balancing strategies (e.g., re-sampling or per-domain weighting); instead, we directly fine-tune models on the naturally imbalanced mixture to reflect realistic data availability.\nAll training samples are obtained solely through our automatic verification pipeline, without any additional human filtering. In contrast, for evaluation we use a test set where all samples are manually audited to ensure correctness and unambiguity. This protocol isolates the effect of MedGround data quality and avoids overestimating performance due to noisy automatic annotations.\nAppendix C\nSamples\nC.1\nMedGround-35K Examples\nFig.\n8\npresents representative examples from MedGround-35K across imaging modalities. Each example consists of a medical image, a fine-grained referring expression, and the corresponding target annotation, illustrating the diversity of morphology- and location-aware clinical semantics in our dataset.\nFigure 8:\nExamples of MedGround-35K.\nFigure 9:\nExamples of MedGround-35K.\nTable 8:\nTraining hyper-parameters used for fine-tuning Lingshu-7B in our experiments.\nHyper-Parameter\nValue\nBackbone\nLingshu-7B\nTraining stage\nSFT\nFine-tuning method\nLoRA\nLoRA Rank\n8\nLoRA Alpha\n32\nLoRA Target\nall\nEpoch\n3\n#GPUs\n4\nPer-device batch size\n4\nGradient accumulation\n4\nGlobal batch size (effective)\n64\nLearning rate\n2\nÃ—\n10\nâˆ’\n4\n2\\times 10^{-4}\nLR scheduler\nCosine\nWarm-up ratio\n0.1\nModel max length\n2048\nPrecision\nBF16\nGradient checkpointing\nEnabled\nRandom seed\n42\nTable 9:\nTraining hyper-parameters used for fine-tuning Qwen2.5-VL-7B in our experiments.\nHyper-Parameter\nValue\nBackbone\nQwen2.5-VL-7B-Instruct\nTraining stage\nSFT\nFine-tuning method\nLoRA\nLoRA Rank\n8\nLoRA Alpha\n32\nLoRA Target\nall\nEpoch\n3\n#GPUs\n4\nPer-device batch size\n4\nGradient accumulation\n4\nGlobal batch size (effective)\n64\nLearning rate\n2\nÃ—\n10\nâˆ’\n4\n2\\times 10^{-4}\nLR scheduler\nCosine\nWarm-up ratio\n0.1\nModel max length\n2048\nPrecision\nBF16\nGradient checkpointing\nEnabled\nRandom seed\n42\nTable 10:\nTraining hyper-parameters used for fine-tuning MedGemma-4B in our experiments.\nHyper-Parameter\nValue\nBackbone\nMedGemma-4B-IT\nTraining stage\nSFT\nFine-tuning method\nLoRA\nLoRA Rank\n8\nLoRA Alpha\n32\nLoRA Target\nall\nEpoch\n3\n#GPUs\n8\nPer-device batch size\n16\nGradient accumulation\n1\nGlobal batch size (effective)\n128\nLearning rate\n2\nÃ—\n10\nâˆ’\n4\n2\\times 10^{-4}\nLR scheduler\nCosine\nWarm-up ratio\n0.1\nModel max length\n2048\nPrecision\nBF16\nGradient checkpointing\nEnabled\nRandom seed\n42\nTable 11:\nTraining hyper-parameters used for fine-tuning MedGemma-27B in our experiments.\nHyper-Parameter\nValue\nBackbone\nMedGemma-27B-IT\nTraining stage\nSFT\nFine-tuning method\nLoRA\nLoRA Rank\n8\nLoRA Alpha\n32\nLoRA Target\nall\nEpoch\n3\n#GPUs\n4\nPer-device batch size\n8\nGradient accumulation\n2\nGlobal batch size (effective)\n64\nLearning rate\n2\nÃ—\n10\nâˆ’\n4\n2\\times 10^{-4}\nLR scheduler\nCosine\nWarm-up ratio\n0.1\nModel max length\n2048\nPrecision\nBF16\nGradient checkpointing\nEnabled\nRandom seed\n42\nTable 12:\nTraining hyper-parameters used for fine-tuning Qwen3-VL-8B in our experiments.\nHyper-Parameter\nValue\nBackbone\nQwen3-VL-8B-Instruct\nTraining stage\nSFT\nFine-tuning method\nLoRA\nLoRA Rank\n8\nLoRA Alpha\n32\nLoRA Target\nall\nEpoch\n3\n#GPUs\n8\nPer-device batch size\n32\nGradient accumulation\n1\nGlobal batch size (effective)\n256\nLearning rate\n2\nÃ—\n10\nâˆ’\n4\n2\\times 10^{-4}\nLR scheduler\nCosine\nWarm-up ratio\n0.1\nModel max length\n2048\nPrecision\nBF16\nGradient checkpointing\nEnabled\nRandom seed\n42\nC.2\nFailure Cases in Manual Screening\nThe Fig.\n9\npresents failure cases identified during manual screening/curation. These examples were flagged as incorrect due to issues such as questionâ€“image mismatch, mislocalized or improperly sized bounding boxes, incorrect object counting (marking multiple regions when a single target is required, or vice versa), and misinterpretation of domain-specific findings (e.g., confusing blur, noise, or debris with true anatomical/pathological structures). We include these samples to illustrate common error patterns and to motivate stricter quality control and refinement of our annotation guidelines.\nAppendix D\nDetails of General Generate Prompt\nThe Fig.\n10\nshows the general system prompt we used to generate our synthetic data. This prompt serves as a unified instruction template that standardizes the modelâ€™s role, output format, and task-specific constraints, ensuring consistency and controllability across different synthesis scenarios.\nFigure 10:\nDetail of general generate system prompt.\nAppendix E\nPotential Use of MedGround\nWe further argue that referring grounding is intrinsically more effective at eliciting visual reasoning than conventional descriptive supervision. In descriptive tasks, models can often produce plausible responses by leaning on linguistic priors or memorized report patterns. In contrast, referring grounding requires the model to\ncommit\nto a specific region, forcing clinical claims to be supported by explicit pixel-level evidence. This evidence-binding property makes grounding not only a practical capability for downstream localization, but also a training signal that can stimulate broader diagnostic reasoning by aligning clinical semantics with visual anchors. We therefore view MedGround as a general-purpose resource that may benefit a wide range of medical VLM applications, including faithful visual question answering, evidence-based reporting, and interactive decision support.",
      "references": [
        {
          "raw_text": "S. G. Armato, G. McLennan, L. Bidaut, M. F. McNitt-Gray, C. R. Meyer, A. P. Reeves, et al. (2011) The lung image database consortium (lidc) and image database resource initiative (idri): a completed reference database of lung nodules on ct scans . Medical Physics 38 ( 2 ), pp.Â 915â€“931 . External Links: Document Cited by: Table 1 .",
          "urls": [
            "https://dx.doi.org/10.1118/1.3528204",
            "https://arxiv.org/html/2601.06847v1#S1.T1.4.2.2.2.2.2.2.6.3.1"
          ],
          "dois": [
            "https://dx.doi.org/10.1118/1.3528204"
          ],
          "pdf_links": []
        },
        {
          "raw_text": "F. Bai, Y. Du, T. Huang, M. Q. Meng, and B. Zhao (2024) M3d: advancing 3d medical image analysis with multi-modal large language models . arXiv preprint arXiv:2404.00578 . Cited by: Â§1 , Â§2.2 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S1.p3.1",
            "https://arxiv.org/html/2601.06847v1#S2.SS2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "S. Bai, Y. Cai, R. Chen, K. Chen, X. Chen, Z. Cheng, L. Deng, W. Ding, C. Gao, C. Ge, W. Ge, Z. Guo, Q. Huang, J. Huang, F. Huang, B. Hui, S. Jiang, Z. Li, M. Li, M. Li, K. Li, Z. Lin, J. Lin, X. Liu, J. Liu, C. Liu, Y. Liu, D. Liu, S. Liu, D. Lu, R. Luo, C. Lv, R. Men, L. Meng, X. Ren, X. Ren, S. Song, Y. Sun, J. Tang, J. Tu, J. Wan, P. Wang, P. Wang, Q. Wang, Y. Wang, T. Xie, Y. Xu, H. Xu, J. Xu, Z. Yang, M. Yang, J. Yang, A. Yang, B. Yu, F. Zhang, H. Zhang, X. Zhang, B. Zheng, H. Zhong, J. Zhou, F. Zhou, J. Zhou, Y. Zhu, and K. Zhu (2025a) Qwen3-vl technical report . External Links: 2511.21631 , Link Cited by: Â§B.1 , Â§4.1 .",
          "urls": [
            "https://arxiv.org/abs/2511.21631",
            "https://arxiv.org/html/2601.06847v1#A2.SS1.p1.1",
            "https://arxiv.org/html/2601.06847v1#S4.SS1.p6.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, et al. (2025b) Qwen2. 5-vl technical report . arXiv preprint arXiv:2502.13923 . Cited by: Â§B.1 , Â§4.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#A2.SS1.p1.1",
            "https://arxiv.org/html/2601.06847v1#S4.SS1.p6.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "S. Bannur, K. Bouzid, D. C. Castro, A. Schwaighofer, A. Thieme, S. Bond-Taylor, M. Ilse, F. PÃ©rez-GarcÃ­a, V. Salvatelli, H. Sharma, et al. (2024) Maira-2: grounded radiology report generation . arXiv preprint arXiv:2406.04449 . Cited by: Â§1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S1.p2.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "O. Bodenreider (2004) The unified medical language system (umls): integrating biomedical terminology . Nucleic acids research 32 ( suppl_1 ), pp.Â D267â€“D270 . Cited by: Â§3.5 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S3.SS5.p2.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "B. Boecking, N. Usuyama, S. Bannur, D. C. de Castro, A. Schwaighofer, S. Hyland, M. T. Wetscherek, T. Naumann, A. Nori, J. A. Valle, et al. (2022) Ms-cxr: making the most of text semantics to improve biomedical vision-language processing . URL: http://dx. doi. org/10.13026/b90j-vb87 . Cited by: Table 1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S1.T1.4.2.2.2.2.2.2.4.1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "J. C. Caicedo, A. Goodman, K. W. Karhohs, B. A. Cimini, J. Ackerman, M. Haghighi, C. Heng, T. Becker, M. Doan, C. McQuin, et al. (2019) Nucleus segmentation across imaging experiments: the 2018 data science bowl . Nature methods 16 ( 12 ), pp.Â 1247â€“1253 . Cited by: Â§A.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#A1.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "Y. Chen, D. Xu, Y. Huang, S. Zhan, H. Wang, D. Chen, X. Wang, M. Qiu, and H. Li (2025) MIMO: a medical vision language model with visual referring multimodal input and pixel grounding multimodal output . In Proceedings of the Computer Vision and Pattern Recognition Conference , pp.Â 24732â€“24741 . Cited by: Â§1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S1.p2.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "N. C. Codella, D. Gutman, M. E. Celebi, B. Helba, M. A. Marchetti, S. W. Dusza, A. Kalloo, K. Liopyris, N. Mishra, H. Kittler, et al. (2018) Skin lesion analysis toward melanoma detection: a challenge at the 2017 international symposium on biomedical imaging (isbi), hosted by the international skin imaging collaboration (isic) . In 2018 IEEE 15th international symposium on biomedical imaging (ISBI 2018) , pp.Â 168â€“172 . Cited by: Â§A.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#A1.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "E. Croxford, Y. Gao, E. First, N. Pellegrino, M. Schnier, J. Caskey, M. Oguss, G. Wills, G. Chen, D. Dligach, et al. (2025) Automating evaluation of ai text generation in healthcare with a large language model (llm)-as-a-judge . medRxiv , pp.Â 2025â€“04 . Cited by: Â§2.3 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S2.SS3.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "A. Degerli, M. Ahishali, M. Yamac, S. Kiranyaz, M. E. Chowdhury, K. Hameed, T. Hamid, R. Mazhar, and M. Gabbouj (2021) COVID-19 infection map generation and detection from chest x-ray images . Health information science and systems 9 ( 1 ), pp.Â 15 . Cited by: Â§4.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S4.SS1.p4.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "Z. Deng, R. He, J. Liu, Y. Wang, Z. Meng, S. Jiang, Y. Xie, and Z. Liu (2025) Med-glip: advancing medical language-image pre-training with large-scale grounded dataset . arXiv preprint arXiv:2508.10528 . Cited by: Â§1 , Â§2.2 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S1.p2.1",
            "https://arxiv.org/html/2601.06847v1#S2.SS2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "H. Dong, W. Xiong, D. Goyal, Y. Zhang, W. Chow, R. Pan, S. Diao, J. Zhang, K. Shum, and T. Zhang (2023) Raft: reward ranked finetuning for generative foundation model alignment . arXiv preprint arXiv:2304.06767 . Cited by: Â§2.3 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S2.SS3.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "H. Ge, L. Hao, Z. Xu, Z. Lin, B. Li, S. Zhou, H. Zhao, and Y. Liu (2025) ClinKD: cross-modal clinical knowledge distiller for multi-task medical images . arXiv preprint arXiv:2502.05928 . Cited by: Â§1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S1.p3.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "B. Gundersen, N. Deperrois, S. Ruiperez-Campillo, T. M. Sutter, J. E. Vogt, M. Moor, F. Nooralahzadeh, and M. Krauthammer (2025) Enhancing radiology report generation and visual grounding using reinforcement learning . arXiv preprint arXiv:2512.10691 . Cited by: Â§2.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S2.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, W. Chen, et al. (2022) Lora: low-rank adaptation of large language models. . ICLR 1 ( 2 ), pp.Â 3 . Cited by: Â§4.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S4.SS1.p6.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "X. Huang, H. Huang, L. Shen, Y. Yang, F. Shang, J. Liu, and J. Liu (2024) A refer-and-ground multimodal large language model for biomedicine . In International Conference on Medical Image Computing and Computer-Assisted Intervention , pp.Â 399â€“409 . Cited by: Â§1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S1.p3.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "X. Huang, L. Shen, J. Liu, F. Shang, H. Li, H. Huang, and Y. Yang (2025) Towards a multimodal large language model with pixel-level insight for biomedicine . In Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 39 , pp.Â 3779â€“3787 . Cited by: Â§1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "B. Jieyun and O. ZhanHong (2024) Pubic symphysis-fetal head segmentation and angle of progression . Cited by: Â§4.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S4.SS1.p3.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "A. Kamath, M. Singh, Y. LeCun, G. Synnaeve, I. Misra, and N. Carion (2021) Mdetr-modulated detection for end-to-end multi-modal understanding . In Proceedings of the IEEE/CVF international conference on computer vision , pp.Â 1780â€“1790 . Cited by: Â§2.2 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S2.SS2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "S. Khanna, A. Dejl, K. Yoon, S. Q. Truong, H. Duong, A. Saenz, and P. Rajpurkar (2023) Radgraph2: modeling disease progression in radiology reports via hierarchical information extraction . In Machine learning for healthcare conference , pp.Â 381â€“402 . Cited by: Â§2.3 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S2.SS3.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "S. Y. Kim, S. Cho, V. Yun, and G. Hwang (2025) MedCLM: learning to localize and reason via a cot-curriculum in medical vision-language models . arXiv preprint arXiv:2510.04477 . Cited by: Â§1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "C. Li, C. Wong, S. Zhang, N. Usuyama, H. Liu, J. Yang, T. Naumann, H. Poon, and J. Gao (2023) Llava-med: training a large language-and-vision assistant for biomedicine in one day . Advances in Neural Information Processing Systems 36 , pp.Â 28541â€“28564 . Cited by: Â§1 , Â§2.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S1.p1.1",
            "https://arxiv.org/html/2601.06847v1#S2.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "L. H. Li, P. Zhang, H. Zhang, J. Yang, C. Li, Y. Zhong, L. Wang, L. Yuan, L. Zhang, J. Hwang, et al. (2022) Grounded language-image pre-training . In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pp.Â 10965â€“10975 . Cited by: Â§2.2 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S2.SS2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "Q. Li, X. Yan, J. Xu, R. Yuan, Y. Zhang, R. Feng, Q. Shen, X. Zhang, and S. Wang (2024) Anatomical structure-guided medical vision-language pre-training . In International Conference on Medical Image Computing and Computer-Assisted Intervention , pp.Â 80â€“90 . Cited by: Â§1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S1.p2.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "F. Liu, H. Zhou, B. Gu, X. Zou, J. Huang, J. Wu, Y. Li, S. S. Chen, Y. Hua, P. Zhou, et al. (2025) Application of large language models in medicine . Nature Reviews Bioengineering , pp.Â 1â€“20 . Cited by: Â§1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "J. Liu, Z. Wang, Q. Ye, D. Chong, P. Zhou, and Y. Hua (2023) Qilin-med-vl: towards chinese large vision-language model for general healthcare . arXiv preprint arXiv:2310.17956 . Cited by: Â§2.1 , Â§2.2 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S2.SS1.p1.1",
            "https://arxiv.org/html/2601.06847v1#S2.SS2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "S. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, Q. Jiang, C. Li, J. Yang, H. Su, et al. (2024) Grounding dino: marrying dino with grounded pre-training for open-set object detection . In European conference on computer vision , pp.Â 38â€“55 . Cited by: Â§2.2 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S2.SS2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "V. Ljosa, K. L. Sokolnicki, and A. E. Carpenter (2012) Annotated high-throughput microscopy image sets for validation . Nature methods 9 ( 7 ), pp.Â 637 . Cited by: Â§A.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#A1.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "Y. Lu, M. Zhou, D. Zhi, M. Zhou, X. Jiang, R. Qiu, Z. Ou, H. Wang, D. Qiu, M. Zhong, et al. (2022) The jnu-ifm dataset for segmenting pubic symphysis-fetal head . Data in brief 41 , pp.Â 107904 . Cited by: Â§A.1 , Â§4.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#A1.SS1.p1.1",
            "https://arxiv.org/html/2601.06847v1#S4.SS1.p3.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "R. Mahmood, P. Yan, D. M. Reyes, G. Wang, M. K. Kalra, P. Kaviani, J. T. Wu, and T. Syeda-Mahmood (2025) Evaluating automated radiology report quality through fine-grained phrasal grounding of clinical findings . In 2025 IEEE 22nd International Symposium on Biomedical Imaging (ISBI) , pp.Â 1â€“5 . Cited by: Â§1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "O. N. Manzari, H. Ahmadabadi, H. Kashiani, S. B. Shokouhi, and A. Ayatollahi (2023) MedViT: a robust vision transformer for generalized medical image classification . Computers in biology and medicine 157 , pp.Â 106791 . Cited by: Â§1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "G. Mathieu, E. D. Bachir, et al. (2022) Brifiseg: a deep learning-based method for semantic and instance segmentation of nuclei in brightfield images . arXiv preprint arXiv:2211.03072 . Cited by: Â§A.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#A1.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "J. Moll, M. Graf, T. Lemke, N. Lenhart, D. Truhn, J. Delbrouck, J. Pan, D. Rueckert, L. C. Adams, and K. K. Bressem (2025) Evaluating reasoning faithfulness in medical vision-language models using multimodal perturbations . arXiv preprint arXiv:2510.11196 . Cited by: Â§2.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S2.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "M. Moor, Q. Huang, S. Wu, M. Yasunaga, C. Zakka, Y. Dalmia, E. P. Reis, P. Rajpurkar, and J. Leskovec (2023) Med-flamingo: a multimodal medical few-shot learner (2023) . URL: https://arxiv. org/abs/2307.15189 . Cited by: Â§2.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S2.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "S. P. Morozov, A. E. Andreychenko, N. A. Pavlov, A. Vladzymyrskyy, N. V. Ledikhova, V. A. Gombolevskiy, I. A. Blokhin, P. B. Gelezhe, A. Gonchar, and V. Y. Chernina (2020) Mosmeddata: chest ct scans with covid-19 related findings dataset . arXiv preprint arXiv:2005.06465 . Cited by: Â§A.1 , Â§4.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#A1.SS1.p1.1",
            "https://arxiv.org/html/2601.06847v1#S4.SS1.p3.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "V. Nath, W. Li, D. Yang, A. Myronenko, M. Zheng, Y. Lu, Z. Liu, H. Yin, Y. M. Law, Y. Tang, et al. (2025) Vila-m3: enhancing vision-language models with medical expert knowledge . In Proceedings of the Computer Vision and Pattern Recognition Conference , pp.Â 14788â€“14798 . Cited by: Â§1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "S. Ostmeier, J. Xu, Z. Chen, M. Varma, L. Blankemeier, C. Bluethgen, A. E. M. Md, M. Moseley, C. Langlotz, A. S. Chaudhari, et al. (2024) Green: generative radiology report evaluation and error notation . In Findings of the association for computational linguistics: EMNLP 2024 , pp.Â 374â€“390 . Cited by: Â§1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "A. Pal, L. K. Umapathi, and M. Sankarasubbu (2023) Med-HALT: medical domain hallucination test for large language models . In Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL) , J. Jiang, D. Reitter, and S. Deng (Eds.) , Singapore , pp.Â 314â€“334 . External Links: Document Cited by: Â§1 , Â§2.3 .",
          "urls": [
            "https://dx.doi.org/10.18653/v1/2023.conll-1.21",
            "https://arxiv.org/html/2601.06847v1#S1.p1.1",
            "https://arxiv.org/html/2601.06847v1#S2.SS3.p1.1"
          ],
          "dois": [
            "https://dx.doi.org/10.18653/v1/2023.conll-1.21"
          ],
          "pdf_links": []
        },
        {
          "raw_text": "C. Pellegrini, E. Ã–zsoy, B. Busam, N. Navab, and M. Keicher (2023) Radialog: a large vision-language model for radiology report generation and conversational assistance . arXiv preprint arXiv:2311.18681 . Cited by: Â§2.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S2.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "A. Sellergren, S. Kazemzadeh, T. Jaroensri, A. Kiraly, M. Traverse, T. Kohlberger, S. Xu, F. Jamil, C. Hughes, C. Lau, et al. (2025) Medgemma technical report . arXiv preprint arXiv:2507.05201 . Cited by: Â§B.1 , Â§1 , Â§4.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#A2.SS1.p1.1",
            "https://arxiv.org/html/2601.06847v1#S1.p1.1",
            "https://arxiv.org/html/2601.06847v1#S4.SS1.p6.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "C. Spahn, E. GÃ³mez-de-Mariscal, R. F. Laine, P. M. Pereira, L. von Chamier, M. Conduit, M. G. Pinho, G. Jacquemet, S. Holden, M. Heilemann, et al. (2022) DeepBacs for multi-task bacterial image analysis using open-source deep learning approaches . Communications Biology 5 ( 1 ), pp.Â 688 . Cited by: Â§A.1 , Â§4.2 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#A1.SS1.p1.1",
            "https://arxiv.org/html/2601.06847v1#S4.SS2.SSS0.Px1.p3.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "C. Spahn and M. Heilemann (2021) Deepbacsâ€“escherichia coli bright field segmentation dataset . October . Cited by: Â§4.2 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S4.SS2.SSS0.Px1.p3.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "R. Strudel, I. Laptev, and C. Schmid (2022) Weakly-supervised segmentation of referring expressions . arXiv preprint arXiv:2205.04725 . Cited by: Â§1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S1.p3.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "A. A. Tarhini, P. Dave, and I. El Naqa (2025) General artificial intelligence for the diagnosis and treatment of cancer: the rise of foundation models . BJR| Artificial Intelligence 2 ( 1 ), pp.Â ubaf015 . Cited by: Â§1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "R. Verma, N. Kumar, A. Patil, N. C. Kurian, S. Rane, S. Graham, Q. D. Vu, M. Zwager, S. E. A. Raza, N. Rajpoot, et al. (2021) MoNuSAC2020: a multi-organ nuclei segmentation and classification challenge . IEEE Transactions on Medical Imaging 40 ( 12 ), pp.Â 3413â€“3423 . Cited by: Â§A.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#A1.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri, and R. M. Summers (2017) ChestX-ray8: hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases . In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , Cited by: Table 1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S1.T1.4.2.2.2.2.2.2.2.2"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "C. Wu, X. Zhang, Y. Zhang, H. Hui, Y. Wang, and W. Xie (2025) Towards generalist foundation model for radiology by leveraging web-scale 2d&3d medical data . Nature Communications 16 ( 1 ), pp.Â 7866 . Cited by: Â§1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "J. T. Wu, N. N. Agu, I. Lourentzou, A. Sharma, J. A. Paguio, J. S. Yao, E. C. Dee, W. Mitchell, S. Kashyap, A. Giovannini, et al. (2021) Chest imagenome dataset for clinical reasoning . arXiv preprint arXiv:2108.00316 . Cited by: Table 1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S1.T1.3.1.1.1.1.1.1.1.2"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "Q. Xie, E. J. Schenck, H. S. Yang, Y. Chen, Y. Peng, and F. Wang (2023) Faithful ai in medicine: a systematic review with large language models and beyond . MedRxiv . Cited by: Â§1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "J. Xu (2025) Uncertainty estimation in large vision language models for automated radiology report generation . In Proceedings of the 4th Machine Learning for Health Symposium. PMLR , pp.Â 1039â€“1052 . Cited by: Â§1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "W. Xu, H. P. Chan, L. Li, M. Aljunied, R. Yuan, J. Wang, C. Xiao, G. Chen, C. Liu, Z. Li, et al. (2025) Lingshu: a generalist foundation model for unified multimodal medical understanding and reasoning . arXiv preprint arXiv:2506.07044 . Cited by: Â§B.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#A2.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "K. Yan, X. Wang, L. Lu, and R. M. Summers (2018) DeepLesion: automated mining of large-scale lesion annotations and universal lesion detection with deep learning . In Medical Image Computing and Computer Assisted Intervention (MICCAI) , Cited by: Table 1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S1.T1.4.2.2.2.2.2.2.5.2.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "L. Yang, S. Xu, A. Sellergren, T. Kohlberger, Y. Zhou, I. Ktena, A. Kiraly, F. Ahmed, F. Hormozdiari, T. Jaroensri, et al. (2024) Advancing multimodal medical capabilities of gemini . arXiv preprint arXiv:2405.03162 . Cited by: Â§1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "X. Yang, J. Liu, P. Wang, G. Wang, Y. Yang, and H. T. Shen (2025) New dataset and methods for fine-grained compositional referring expression comprehension via specialist-mllm collaboration . IEEE Transactions on Pattern Analysis and Machine Intelligence . Cited by: Â§2.2 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S2.SS2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "F. Yu, M. Endo, R. Krishnan, I. Pan, A. Tsai, E. P. Reis, E. K. U. N. Fonseca, H. M. H. Lee, Z. S. H. Abad, A. Y. Ng, et al. (2023) Evaluating progress in automatic chest x-ray radiology report generation . Patterns 4 ( 9 ). Cited by: Â§2.3 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S2.SS3.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "J. Yue, S. Zhang, Z. Jia, H. Xu, Z. Han, X. Liu, and G. Wang (2025) MedSG-bench: a benchmark for medical image sequences grounding . arXiv preprint arXiv:2505.11852 . Cited by: Â§2.3 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S2.SS3.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "W. Zhang, S. S. Chandra, and A. Nicolson (2025a) Anatomical grounding pre-training for medical phrase grounding . In 2025 IEEE 22nd International Symposium on Biomedical Imaging (ISBI) , pp.Â 1â€“5 . Cited by: Â§2.2 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S2.SS2.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "W. Zhang, S. S. Chandra, and A. Nicolson (2025b) Generalized medical phrase grounding . arXiv preprint arXiv:2512.01085 . Cited by: Â§1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S1.p3.1"
          ],
          "dois": [],
          "pdf_links": []
        },
        {
          "raw_text": "X. Zhang, C. Wu, Z. Zhao, W. Lin, Y. Zhang, Y. Wang, and W. Xie (2023) Pmc-vqa: visual instruction tuning for medical visual question answering . arXiv preprint arXiv:2305.10415 . Cited by: Â§2.1 .",
          "urls": [
            "https://arxiv.org/html/2601.06847v1#S2.SS1.p1.1"
          ],
          "dois": [],
          "pdf_links": []
        }
      ],
      "references_dois": [
        "https://dx.doi.org/10.1118/1.3528204",
        "https://dx.doi.org/10.18653/v1/2023.conll-1.21"
      ],
      "fallback_urls": [],
      "errors": [],
      "missing_fields": [
        "doi",
        "versions",
        "last_updated_raw",
        "published_date"
      ],
      "url_hint_if_missing": "Champs manquants: doi, versions, last_updated_raw, published_date. Tu peux vÃ©rifier ici: abs=https://arxiv.org/abs/2601.06847 | html=https://arxiv.org/html/2601.06847v1 | pdf=https://arxiv.org/pdf/2601.06847"
    }
  ],
  "bundle_html_file": "data_lake\\raw\\arxiv_bundle_20260113_164610.html",
  "supported_fields": [
    "arxiv_id",
    "title",
    "authors",
    "abstract",
    "submitted_date",
    "abs_url",
    "pdf_url",
    "doi",
    "versions",
    "last_updated_raw",
    "html_url",
    "published_date",
    "content_text",
    "references",
    "references_dois"
  ]
}