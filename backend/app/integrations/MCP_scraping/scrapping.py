# ============================================================  # # ğŸ“Œ DÃ©but du script
# ğŸ•·ï¸ arXiv Scraper (search/cs) -> JSON + sauvegarde data_lake/raw  # # ğŸ¯ Objectif du script
# ============================================================  # # ğŸ“Œ SÃ©parateur visuel

import os  # # ğŸ“ GÃ©rer les chemins et dossiers
import re  # # ğŸ” Extraire des infos via regex (ID, dates, versions)
import json  # # ğŸ§¾ Exporter en JSON
import time  # # â±ï¸ Pause polie entre requÃªtes
import random  # # ğŸ² Jitter pour Ã©viter un rythme trop "robot"
import datetime  # # ğŸ•’ GÃ©nÃ©rer timestamps pour fichiers
from typing import Dict, Any, List, Optional  # # ğŸ§© Typage pour clartÃ©
import requests  # # ğŸŒ Faire des requÃªtes HTTP GET
from bs4 import BeautifulSoup  # # ğŸ² Parser HTML et sÃ©lectionner des balises

ARXIV_BASE = "https://arxiv.org"  # # ğŸŒ Domaine arXiv
ARXIV_SEARCH_CS = "https://arxiv.org/search/cs"  # # ğŸ” Endpoint recherche Computer Science

DEFAULT_RAW_DIR = os.path.join("data_lake", "raw")  # # ğŸ“¦ Dossier de stockage raw
DEFAULT_META_DIR = os.path.join("data_lake", "metadata")  # # ğŸ§¾ Dossier metadata sources

MAX_RESULTS_HARD_LIMIT = 100  # # ğŸš§ Limite globale demandÃ©e (max 100)
PAGE_SIZE = 50  # # ğŸ“„ arXiv permet size=50 en gÃ©nÃ©ral (pratique pour paginer)


# ============================================================
# ğŸŒ B) GET â€” Politesse + GET robuste + stockage HTML brut
# ============================================================

def ensure_dir(path: str) -> None:  # # ğŸ“ Assurer que le dossier existe
    os.makedirs(path, exist_ok=True)  # # âœ… CrÃ©e le dossier si absent

def sleep_polite(min_s: float = 1.5, max_s: float = 2.0) -> None:  # # ğŸ˜‡ Pause polie entre requÃªtes
    time.sleep(random.uniform(min_s, max_s))  # # â³ Attendre un temps alÃ©atoire

def http_get(session: requests.Session, url: str, timeout_s: int = 30) -> str:  # # ğŸŒ GET = tÃ©lÃ©charger le HTML
    headers = {"User-Agent": "Mozilla/5.0 DIXITBOT-arXivScraper/1.0"}  # # ğŸªª User-Agent simple
    resp = session.get(url, headers=headers, timeout=timeout_s)  # # ğŸš€ RequÃªte GET
    resp.raise_for_status()  # # âŒ Erreur si 4xx/5xx
    return resp.text  # # ğŸ“„ Retourner le HTML brut

def save_text_file(folder: str, filename: str, content: str) -> str:  # # ğŸ’¾ Sauver du texte (HTML/JSON) dans un fichier
    ensure_dir(folder)  # # ğŸ“ CrÃ©er le dossier si besoin
    path = os.path.join(folder, filename)  # # ğŸ§© Construire le chemin complet
    with open(path, "w", encoding="utf-8") as f:  # # âœï¸ Ouvrir en Ã©criture UTF-8
        f.write(content)  # # ğŸ§¾ Ã‰crire le contenu
    return path  # # ğŸ“Œ Retourner le chemin du fichier

def now_iso_for_filename() -> str:  # # ğŸ•’ Timestamp format fichier
    return datetime.datetime.now().strftime("%Y%m%d_%H%M%S")  # # ğŸ§¾ Exemple: 20260113_104500

# ============================================================
# ğŸ” C) 
# ============================================================

def build_search_url(query: str, start: int, size: int = PAGE_SIZE) -> str:  # # ğŸ”— Construire lâ€™URL arXiv search
    q = requests.utils.quote(query)  # # ğŸ” Encoder la requÃªte (espaces, caractÃ¨res spÃ©ciaux)
    return f"{ARXIV_SEARCH_CS}?query={q}&searchtype=all&abstracts=show&order=-announced_date_first&size={size}&start={start}"  # # ğŸŒ URL paginÃ©e


# ============================================================
# ğŸ§ª 4
# ============================================================

def parse_search_page(html: str) -> List[Dict[str, Any]]:  # # ğŸ§© HTML -> liste dâ€™articles (donnÃ©es structurÃ©es)
    soup = BeautifulSoup(html, "lxml")  # # ğŸ² Parser le HTML
    items: List[Dict[str, Any]] = []  # # ğŸ“¦ Liste de rÃ©sultats

    for li in soup.select("ol.breathe-horizontal li.arxiv-result"):  # # ğŸ“š Chaque rÃ©sultat arXiv
        title_el = li.select_one("p.title")  # # ğŸ·ï¸ Balise titre
        authors_el = li.select_one("p.authors")  # # ğŸ‘¥ Balise auteurs
        abstract_el = li.select_one("span.abstract-full")  # # ğŸ§¾ Balise abstract (full)
        abs_link_el = li.select_one('p.list-title a[href^="/abs/"]')  # # ğŸ”— Lien vers page dÃ©tail /abs/
        pdf_link_el = li.select_one('a[href^="/pdf/"]')  # # ğŸ“„ Lien PDF

        title = title_el.get_text(" ", strip=True) if title_el else ""  # # ğŸ·ï¸ Texte du titre
        authors_txt = authors_el.get_text(" ", strip=True) if authors_el else ""  # # ğŸ‘¥ Texte auteurs brut
        authors = [a.strip() for a in authors_txt.replace("Authors:", "").split(",") if a.strip()]  # # ğŸ‘¥ Liste auteurs

        abstract = abstract_el.get_text(" ", strip=True) if abstract_el else ""  # # ğŸ§¾ Texte abstract
        abstract = abstract.replace("â–³ Less", "").strip()  # # ğŸ§¹ Nettoyage

        abs_url = (ARXIV_BASE + abs_link_el.get("href", "")) if abs_link_el else ""  # # ğŸ”— URL page /abs/
        pdf_url = (ARXIV_BASE + pdf_link_el.get("href", "")) if pdf_link_el else ""  # # ğŸ“„ URL PDF

        arxiv_id = ""  # # ğŸ†” ID arXiv (ex: 2401.12345)
        m = re.search(r"/abs/([^/]+)$", abs_url) if abs_url else None  # # ğŸ” Extraire lâ€™ID depuis lâ€™URL
        if m:  # # âœ… Si trouvÃ©
            arxiv_id = m.group(1)  # # ğŸ†” Stocker ID

        items.append({  # # ğŸ“¦ Ajouter un rÃ©sultat structurÃ©
            "arxiv_id": arxiv_id,  # # ğŸ†” Identifiant
            "title": title,  # # ğŸ·ï¸ Titre
            "authors": authors,  # # ğŸ‘¥ Auteurs
            "abstract": abstract,  # # ğŸ§¾ Abstract (oui, rÃ©cupÃ©rÃ©)
            "abs_url": abs_url,  # # ğŸ”— Lien dÃ©tail
            "pdf_url": pdf_url,  # # ğŸ“„ Lien PDF direct
            "source": "arxiv_search_cs",  # # ğŸ§¾ Source
        })  # # âœ… Fin item

    return items  # # ğŸ“¤ Retourner la liste dâ€™articles

# SECTION 5 â€” Fonction principale scrape_arxiv_cs (multi-pages + limite 100 + JSON)

def scrape_arxiv_cs(  # # ğŸš€ Fonction principale appelÃ©e par test local ou backend
    query: str,  # # ğŸ” Mots-clÃ©s
    max_results: int = 20,  # # ğŸ¯ Nombre max dâ€™articles Ã  rÃ©cupÃ©rer
    polite_min_s: float = 1.5,  # # ğŸ˜‡ Pause min
    polite_max_s: float = 2.0,  # # ğŸ˜‡ Pause max
    data_lake_raw_dir: str = DEFAULT_RAW_DIR,  # # ğŸ’¾ Dossier raw
) -> Dict[str, Any]:  # # ğŸ§¾ Retour JSON (dict)

    max_results = int(max_results)  # # ğŸ”¢ SÃ©curiser type
    if max_results < 1:  # # ğŸš« Cas invalide
        max_results = 1  # # âœ… Minimum 1
    if max_results > MAX_RESULTS_HARD_LIMIT:  # # ğŸš§ Appliquer limite 100
        max_results = MAX_RESULTS_HARD_LIMIT  # # âœ… Forcer 100

    session = requests.Session()  # # ğŸ”Œ Session HTTP rÃ©utilisable
    collected: List[Dict[str, Any]] = []  # # ğŸ“¦ RÃ©sultats cumulÃ©s multi-pages
    raw_pages_saved: List[str] = []  # # ğŸ’¾ Liste des fichiers HTML sauvegardÃ©s
    start = 0  # # ğŸ“„ Offset pagination
    ts = now_iso_for_filename()  # # ğŸ•’ Timestamp pour nommer les fichiers

    while len(collected) < max_results:  # # ğŸ” Tant quâ€™on nâ€™a pas assez de rÃ©sultats
        url = build_search_url(query=query, start=start, size=PAGE_SIZE)  # # ğŸ”— Construire URL page X
        html = http_get(session=session, url=url)  # # ğŸŒ GET : tÃ©lÃ©charger HTML

        html_file = f"arxiv_search_{ts}_start_{start}.html"  # # ğŸ§¾ Nom fichier HTML brut
        raw_path = save_text_file(data_lake_raw_dir, html_file, html)  # # ğŸ’¾ Sauvegarder HTML brut
        raw_pages_saved.append(raw_path)  # # ğŸ“Œ MÃ©moriser oÃ¹ on a stockÃ© le GET

        page_items = parse_search_page(html)  # # ğŸ” SELECT : extraire balises depuis le HTML
        if not page_items:  # # ğŸ›‘ Si la page ne renvoie rien, on stoppe
            break  # # âœ… Sortie boucle

        collected.extend(page_items)  # # â• Ajouter rÃ©sultats de cette page
        start += PAGE_SIZE  # # â¡ï¸ Passer Ã  la page suivante
        sleep_polite(min_s=polite_min_s, max_s=polite_max_s)  # # ğŸ˜‡ Pause polie

        if start > 1000:  # # ğŸ›¡ï¸ SÃ©curitÃ© anti-boucle infinie
            break  # # âœ… Stop

    collected = collected[:max_results]  # # âœ‚ï¸ Couper Ã  max_results exact

    hit_limit_100 = (max_results == MAX_RESULTS_HARD_LIMIT)  # # ğŸš§ Indicateur limite
    message_if_limit = "Limite 100 atteinte (max_results)." if hit_limit_100 else ""  # # ğŸ§¾ Message limite

    result: Dict[str, Any] = {  # # ğŸ§¾ JSON final
        "ok": True,  # # âœ… SuccÃ¨s
        "query": query,  # # ğŸ” RequÃªte
        "count": len(collected),  # # ğŸ”¢ Nombre dâ€™items
        "max_results": max_results,  # # ğŸ¯ Limite demandÃ©e
        "hit_limit_100": hit_limit_100,  # # ğŸš§ Limite 100 atteinte ?
        "message_if_limit": message_if_limit,  # # ğŸ§¾ Message
        "items": collected,  # # ğŸ“š RÃ©sultats
        "raw_html_files": raw_pages_saved,  # # ğŸ’¾ OÃ¹ sont stockÃ©s tous les GET (HTML bruts)
        "source_url_example": build_search_url(query=query, start=0, size=PAGE_SIZE),  # # ğŸ”— Exemple URL
    }  # # âœ… Fin JSON

    out_json_name = f"arxiv_raw_{ts}.json"  # # ğŸ§¾ Nom JSON raw
    out_json_path = os.path.join(data_lake_raw_dir, out_json_name)  # # ğŸ“ Chemin JSON
    ensure_dir(data_lake_raw_dir)  # # ğŸ“ Assurer dossier
    with open(out_json_path, "w", encoding="utf-8") as f:  # # âœï¸ Ouvrir fichier JSON
        json.dump(result, f, ensure_ascii=False, indent=2)  # # ğŸ§¾ Ã‰crire le JSON

    result["saved_to"] = out_json_path  # # ğŸ“Œ Ajouter le chemin du JSON crÃ©Ã©
    return result  # # ğŸ“¤ Retourner au caller (test local / backend)

# SECTION 6 â€” Test local activable/dÃ©sactivable avec 1 ligne

# ============================================================  # # ğŸ“Œ SÃ©parateur
# ğŸ§ª TEST LOCAL (exÃ©cution directe) â€” une ligne Ã  activer/dÃ©sactiver  # # âœ… Objectif: tester sans backend
# ============================================================  # # ğŸ“Œ SÃ©parateur

RUN_LOCAL_TEST = True  # # âœ… True = test ON | False = test OFF (ou mets # devant la ligne)

if __name__ == "__main__" and RUN_LOCAL_TEST:  # # â–¶ï¸ Lancer uniquement si exÃ©cutÃ© directement
    print("ğŸš€ Lancement du scraping arXiv (test local)...")  # # ğŸ–¨ï¸ Log dÃ©but
    results = scrape_arxiv_cs(query="multimodal transformer", max_results=5)  # # ğŸ•·ï¸ Appel scraper
    print(f"âœ… OK: {results.get('count')} articles rÃ©cupÃ©rÃ©s")  # # ğŸ–¨ï¸ Afficher le nombre
    print(f"ğŸ’¾ JSON sauvegardÃ©: {results.get('saved_to')}")  # # ğŸ“Œ Afficher chemin du JSON
    items = results.get("items", [])  # # ğŸ“¦ RÃ©cupÃ©rer liste items
    if items:  # # âœ… Si liste non vide
        print("ğŸ§¾ AperÃ§u 1er article:")  # # ğŸ–¨ï¸ Log aperÃ§u
        print(json.dumps(items[0], indent=2, ensure_ascii=False))  # # ğŸ§¾ Afficher 1 item
