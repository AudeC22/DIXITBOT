# DÃ©but scraping main
# ============================================================  # #
# ðŸš€ FastAPI Orchestrateur (Scraping arXiv + Qwen3 via Ollama)   # #
# - Objectif : exposer /health, /scrape/arxiv, /ask              # #
# - Robustesse : imports stables mÃªme si uvicorn change le CWD   # #
# - âœ… CORRECTION: on NE passe PAS debug_max_chars au scraper     # #
#   car scrape_arxiv_cs_scoped(...) ne le supporte pas            # #
# ============================================================  # #

# ===============================  # #
# ðŸ“š Imports standard              # #
# ===============================  # #
import os  # # Gestion chemins # # Ã‰tape: NORMALISATION/DECOUPLAGE (chemins stables)
import sys  # # sys.path pour imports robustes # # Ã‰tape: NORMALISATION/DECOUPLAGE (Ã©vite crash import)
import re  # # Nettoyage texte # # Ã‰tape: NORMALISATION/DECOUPLAGE (prompt/context propre)
from pathlib import Path  # # Chemins robustes # # Ã‰tape: NORMALISATION/DECOUPLAGE (Windows-friendly)
from typing import Any, Dict, List, Optional  # # Typage # # Ã‰tape: NORMALISATION/DECOUPLAGE (contrat stable)

# ===============================  # #
# ðŸš€ FastAPI + Pydantic            # #
# ===============================  # #
from fastapi import FastAPI  # # Framework API # # Ã‰tape: ORCHESTRATION (endpoints)
from pydantic import BaseModel, Field  # # Validation payload # # Ã‰tape: ORCHESTRATION (contrat dâ€™entrÃ©e)

# ===============================  # #
# ðŸŒ HTTP (Ollama)                 # #
# ===============================  # #
import requests  # # Appels HTTP # # Ã‰tape: ORCHESTRATION (appel LLM local)

# ===============================  # #
# ðŸ§± Bootstrap chemins (IMPORTANT) # #
# ===============================  # #
_THIS_FILE = Path(__file__).resolve()  # # Chemin absolu du fichier # # Ã‰tape: NORMALISATION/DECOUPLAGE (Ã©vite CWD)
_THIS_DIR = _THIS_FILE.parent  # # Dossier du main # # Ã‰tape: NORMALISATION/DECOUPLAGE

# ðŸ‘‰ On suppose que le project root est soit le dossier courant, soit son parent (selon ton arborescence).
#    On force lâ€™ajout dans sys.path pour que `module_MCP_scraping...` soit importable mÃªme sous uvicorn.
_CANDIDATES = [  # # Liste candidats racine # # Ã‰tape: NORMALISATION/DECOUPLAGE
    _THIS_DIR,  # # Candidat 1: dossier main.py # # Ã‰tape: NORMALISATION/DECOUPLAGE
    _THIS_DIR.parent,  # # Candidat 2: parent # # Ã‰tape: NORMALISATION/DECOUPLAGE
]
PROJECT_ROOT = None  # # Init # # Ã‰tape: NORMALISATION/DECOUPLAGE
for c in _CANDIDATES:  # # Boucle candidats # # Ã‰tape: NORMALISATION/DECOUPLAGE
    if (c / "data_lake").exists() or (c / "module_MCP_scraping").exists():  # # Marqueurs projet # # Ã‰tape: NORMALISATION/DECOUPLAGE
        PROJECT_ROOT = c  # # Fixe root # # Ã‰tape: NORMALISATION/DECOUPLAGE
        break  # # Stop # # Ã‰tape: NORMALISATION/DECOUPLAGE

if PROJECT_ROOT is None:  # # Si non trouvÃ© # # Ã‰tape: NORMALISATION/DECOUPLAGE
    PROJECT_ROOT = _THIS_DIR  # # Fallback # # Ã‰tape: NORMALISATION/DECOUPLAGE

if str(PROJECT_ROOT) not in sys.path:  # # Si pas dÃ©jÃ  dans sys.path # # Ã‰tape: NORMALISATION/DECOUPLAGE
    sys.path.insert(0, str(PROJECT_ROOT))  # # Ajoute au dÃ©but # # Ã‰tape: NORMALISATION/DECOUPLAGE

# ===============================  # #
# ðŸ•·ï¸ Import du scraper (robuste)  # #
# ===============================  # #
# âš ï¸ Ici, on importe la fonction attendue par lâ€™API.
# Si ton module/nom diffÃ¨re, adapte UNIQUEMENT la ligne import ci-dessous.
try:  # # Essai import # # Ã‰tape: ROBUSTESSE (erreur claire)
    from module_MCP_scraping.scrapping import scrape_arxiv_cs_scoped  # # Import scraper # # Ã‰tape: TOOL (scraping)
except Exception as e:  # # Si import Ã©choue # # Ã‰tape: ROBUSTESSE
    scrape_arxiv_cs_scoped = None  # # Placeholder # # Ã‰tape: ROBUSTESSE
    _SCRAPER_IMPORT_ERROR = str(e)  # # Stocke erreur # # Ã‰tape: ROBUSTESSE

# ===============================  # #
# ðŸ§  FastAPI app                   # #
# ===============================  # #
app = FastAPI(  # # CrÃ©e lâ€™app # # Ã‰tape: ORCHESTRATION
    title="DIXITBOT API",  # # Titre swagger # # Ã‰tape: ORCHESTRATION
    version="1.0.0",  # # Version # # Ã‰tape: ORCHESTRATION
)

# ===============================  # #
# ðŸ§¹ Helpers                       # #
# ===============================  # #
def _clean(s: str) -> str:  # # Nettoyage texte # # Ã‰tape: NORMALISATION/DECOUPLAGE (input stable)
    s = (s or "").strip()  # # Trim # # Ã‰tape: NORMALISATION/DECOUPLAGE
    s = re.sub(r"\s+", " ", s)  # # Espaces multiples -> 1 # # Ã‰tape: NORMALISATION/DECOUPLAGE
    return s  # # Retour # # Ã‰tape: NORMALISATION/DECOUPLAGE

def _ollama_generate(prompt: str, model: str) -> str:  # # Appel Ollama # # Ã‰tape: ORCHESTRATION (LLM)
    url = "http://localhost:11434/api/generate"  # # Endpoint Ollama # # Ã‰tape: ORCHESTRATION
    payload = {  # # JSON body # # Ã‰tape: NORMALISATION/DECOUPLAGE (contrat stable)
        "model": model,  # # ModÃ¨le # # Ã‰tape: ORCHESTRATION
        "prompt": prompt,  # # Prompt # # Ã‰tape: ORCHESTRATION
        "stream": False,  # # Sans streaming # # Ã‰tape: ORCHESTRATION
    }
    r = requests.post(url, json=payload, timeout=300)  # # POST # # Ã‰tape: ROBUSTESSE (timeout)
    r.raise_for_status()  # # LÃ¨ve si erreur HTTP # # Ã‰tape: ROBUSTESSE
    data = r.json()  # # Parse JSON # # Ã‰tape: NORMALISATION/DECOUPLAGE
    return (data.get("response") or "").strip()  # # Retour texte # # Ã‰tape: NORMALISATION/DECOUPLAGE

def _build_context(items: List[Dict[str, Any]], max_chars: int = 14000) -> str:  # # Contexte compact # # Ã‰tape: STRUCTURATION (anti-hallucination)
    chunks: List[str] = []  # # Blocs # # Ã‰tape: STRUCTURATION
    total = 0  # # Compteur # # Ã‰tape: STRUCTURATION
    for i, it in enumerate(items, start=1):  # # Parcours # # Ã‰tape: STRUCTURATION
        block = (  # # Bloc papier # # Ã‰tape: STRUCTURATION
            f"[PAPER {i}]\n"
            f"arxiv_id: {_clean(it.get('arxiv_id',''))}\n"
            f"title: {_clean(it.get('title',''))}\n"
            f"submitted_date: {_clean(it.get('submitted_date',''))}\n"
            f"abs_url: {_clean(it.get('abs_url',''))}\n"
            f"pdf_url: {_clean(it.get('pdf_url',''))}\n"
            f"doi: {_clean(it.get('doi',''))}\n"
            f"abstract: {_clean(it.get('abstract',''))}\n"
        )
        if total + len(block) > max_chars:  # # Stop si trop long # # Ã‰tape: STRUCTURATION
            break  # # Sort # # Ã‰tape: STRUCTURATION
        chunks.append(block)  # # Ajout # # Ã‰tape: STRUCTURATION
        total += len(block)  # # Compte # # Ã‰tape: STRUCTURATION
    return "\n".join(chunks)  # # Retour # # Ã‰tape: STRUCTURATION

# ===============================  # #
# 0) Healthcheck                   # #
# ===============================  # #
@app.get("/health")  # # Endpoint health # # Ã‰tape: ORCHESTRATION
def health() -> Dict[str, Any]:  # # Handler # # Ã‰tape: ORCHESTRATION
    if scrape_arxiv_cs_scoped is None:  # # Si scraper non importÃ© # # Ã‰tape: ROBUSTESSE
        return {  # # Retour # # Ã‰tape: NORMALISATION/DECOUPLAGE
            "ok": False,  # # KO # # Ã‰tape: NORMALISATION/DECOUPLAGE
            "service": "api",  # # Service # # Ã‰tape: NORMALISATION/DECOUPLAGE
            "scraper_import_ok": False,  # # Flag # # Ã‰tape: NORMALISATION/DECOUPLAGE
            "scraper_error": _SCRAPER_IMPORT_ERROR,  # # DÃ©tail # # Ã‰tape: ROBUSTESSE
            "project_root": str(PROJECT_ROOT),  # # Debug # # Ã‰tape: NORMALISATION/DECOUPLAGE
            "cwd": os.getcwd(),  # # Debug # # Ã‰tape: NORMALISATION/DECOUPLAGE
        }
    return {  # # OK # # Ã‰tape: NORMALISATION/DECOUPLAGE
        "ok": True,  # # OK # # Ã‰tape: NORMALISATION/DECOUPLAGE
        "service": "api",  # # Service # # Ã‰tape: NORMALISATION/DECOUPLAGE
        "scraper_import_ok": True,  # # Flag # # Ã‰tape: NORMALISATION/DECOUPLAGE
        "project_root": str(PROJECT_ROOT),  # # Debug # # Ã‰tape: NORMALISATION/DECOUPLAGE
        "cwd": os.getcwd(),  # # Debug # # Ã‰tape: NORMALISATION/DECOUPLAGE
    }

# ===============================  # #
# 1) /scrape/arxiv                 # #
# ===============================  # #
class ArxivScrapeRequest(BaseModel):  # # Input scraping # # Ã‰tape: NORMALISATION/DECOUPLAGE (contrat dâ€™entrÃ©e)
    query: str = Field(..., description="Texte de recherche (keywords)")  # # Query # # Ã‰tape: NORMALISATION/DECOUPLAGE
    theme: Optional[str] = Field(default=None, description="ai_ml|algo_ds|net_sys|cyber_crypto|pl_se|hci_data")  # # Theme # # Ã‰tape: NORMALISATION/DECOUPLAGE
    max_results: int = Field(default=20, ge=1, le=100)  # # Limite # # Ã‰tape: NORMALISATION/DECOUPLAGE
    sort: str = Field(default="relevance", description="relevance|submitted_date")  # # Tri # # Ã‰tape: NORMALISATION/DECOUPLAGE
    # âœ… CORRECTION: on retire debug_max_chars ici car le scraper ne lâ€™accepte pas

@app.post("/scrape/arxiv")  # # Endpoint scrape # # Ã‰tape: ORCHESTRATION
def scrape_arxiv(req: ArxivScrapeRequest) -> Dict[str, Any]:  # # Handler # # Ã‰tape: ORCHESTRATION
    if scrape_arxiv_cs_scoped is None:  # # Si scraper indisponible # # Ã‰tape: ROBUSTESSE
        return {  # # Retour # # Ã‰tape: NORMALISATION/DECOUPLAGE
            "ok": False,  # # KO # # Ã‰tape: NORMALISATION/DECOUPLAGE
            "errors": [f"SCRAPER_IMPORT_ERROR: {_SCRAPER_IMPORT_ERROR}"],  # # Liste erreurs # # Ã‰tape: NORMALISATION/DECOUPLAGE (contrat stable)
            "items": [],  # # Items vide # # Ã‰tape: NORMALISATION/DECOUPLAGE
        }

    try:  # # Try # # Ã‰tape: ROBUSTESSE
        # âœ… data_lake_raw_dir relatif => ton scraper le rend ABSOLU (et Ã©crit dans le projet)
        result = scrape_arxiv_cs_scoped(  # # Appel tool # # Ã‰tape: TOOL (scraping)
            user_query=req.query,  # # Query # # Ã‰tape: TOOL
            theme=req.theme,  # # Theme # # Ã‰tape: TOOL
            max_results=req.max_results,  # # Limite # # Ã‰tape: TOOL
            sort=req.sort,  # # Tri # # Ã‰tape: TOOL
            data_lake_raw_dir="data_lake/raw/cache",  # # Cache raw # # Ã‰tape: NORMALISATION/DECOUPLAGE (sortie prÃ©visible)
            enrich_abs=True,  # # Enrich /abs # # Ã‰tape: TOOL
            enable_keyword_filter=True,  # # Filtre fallback # # Ã‰tape: TOOL
            # âœ… CORRECTION: debug_max_chars supprimÃ© (sinon "unexpected keyword argument")
        )
        return result  # # Retour direct (contrat tool) # # Ã‰tape: NORMALISATION/DECOUPLAGE
    except Exception as e:  # # Catch # # Ã‰tape: ROBUSTESSE
        return {  # # Retour erreur # # Ã‰tape: NORMALISATION/DECOUPLAGE
            "ok": False,  # # KO # # Ã‰tape: NORMALISATION/DECOUPLAGE
            "errors": [f"SCRAPE_EXCEPTION: {str(e)}"],  # # Erreurs globales # # Ã‰tape: NORMALISATION/DECOUPLAGE
            "items": [],  # # Items vide # # Ã‰tape: NORMALISATION/DECOUPLAGE
        }

# ===============================  # #
# 2) /ask (scrape -> context -> LLM)# #
# ===============================  # #
class AskRequest(BaseModel):  # # Input ask # # Ã‰tape: NORMALISATION/DECOUPLAGE
    question: str = Field(..., description="Question utilisateur")  # # Question # # Ã‰tape: NORMALISATION/DECOUPLAGE
    theme: Optional[str] = Field(default="ai_ml", description="ThÃ¨me arXiv CS")  # # Theme # # Ã‰tape: NORMALISATION/DECOUPLAGE
    max_results: int = Field(default=3, ge=1, le=10)  # # Papiers # # Ã‰tape: NORMALISATION/DECOUPLAGE
    sort: str = Field(default="relevance", description="relevance|submitted_date")  # # Tri # # Ã‰tape: NORMALISATION/DECOUPLAGE
    model: str = Field(default="qwen3:1.7b", description="ModÃ¨le Ollama")  # # Model # # Ã‰tape: NORMALISATION/DECOUPLAGE
    debug: bool = Field(default=False, description="Renvoie infos debug")  # # Debug # # Ã‰tape: NORMALISATION/DECOUPLAGE

@app.post("/ask")  # # Endpoint QA # # Ã‰tape: ORCHESTRATION
def ask(req: AskRequest) -> Dict[str, Any]:  # # Handler # # Ã‰tape: ORCHESTRATION
    if scrape_arxiv_cs_scoped is None:  # # Si scraper indispo # # Ã‰tape: ROBUSTESSE
        return {  # # Retour # # Ã‰tape: NORMALISATION/DECOUPLAGE
            "ok": False,  # # KO # # Ã‰tape: NORMALISATION/DECOUPLAGE
            "errors": [f"SCRAPER_IMPORT_ERROR: {_SCRAPER_IMPORT_ERROR}"],  # # Erreurs # # Ã‰tape: NORMALISATION/DECOUPLAGE
            "items": [],  # # Items # # Ã‰tape: NORMALISATION/DECOUPLAGE
        }

    question = _clean(req.question)  # # Nettoie # # Ã‰tape: NORMALISATION/DECOUPLAGE
    if not question:  # # Vide # # Ã‰tape: NORMALISATION/DECOUPLAGE
        return {"ok": False, "errors": ["EMPTY_QUESTION"], "items": []}  # # Contrat stable # # Ã‰tape: NORMALISATION/DECOUPLAGE

    # 1) Scraping
    try:  # # Try # # Ã‰tape: ROBUSTESSE
        results = scrape_arxiv_cs_scoped(  # # Tool # # Ã‰tape: TOOL
            user_query=question,  # # Query # # Ã‰tape: TOOL
            theme=req.theme,  # # Theme # # Ã‰tape: TOOL
            max_results=req.max_results,  # # Limite # # Ã‰tape: TOOL
            sort=req.sort,  # # Tri # # Ã‰tape: TOOL
            data_lake_raw_dir="data_lake/raw/cache",  # # Cache # # Ã‰tape: NORMALISATION/DECOUPLAGE
            enrich_abs=True,  # # Enrich # # Ã‰tape: TOOL
            enable_keyword_filter=True,  # # Filtre # # Ã‰tape: TOOL
            # âœ… CORRECTION: debug_max_chars supprimÃ©
        )
    except Exception as e:  # # Catch # # Ã‰tape: ROBUSTESSE
        return {"ok": False, "errors": [f"SCRAPE_EXCEPTION: {str(e)}"], "items": []}  # # Contrat stable # # Ã‰tape: NORMALISATION/DECOUPLAGE

    items = results.get("items") or []  # # Items # # Ã‰tape: NORMALISATION/DECOUPLAGE
    errors_global = results.get("errors") or []  # # Erreurs tool # # Ã‰tape: NORMALISATION/DECOUPLAGE

    if not items:  # # Si rien # # Ã‰tape: ROBUSTESSE
        out = {  # # Sortie stable # # Ã‰tape: NORMALISATION/DECOUPLAGE
            "ok": False if errors_global else True,  # # KO si erreurs tool # # Ã‰tape: NORMALISATION/DECOUPLAGE
            "question": question,  # # Echo # # Ã‰tape: NORMALISATION/DECOUPLAGE
            "answer": "Aucun papier trouvÃ© (ou parsing impossible). Regarde le bundle HTML.",  # # Message # # Ã‰tape: NORMALISATION/DECOUPLAGE
            "items": [],  # # Items vide # # Ã‰tape: NORMALISATION/DECOUPLAGE
            "errors": errors_global,  # # Erreurs globales # # Ã‰tape: NORMALISATION/DECOUPLAGE
        }
        if req.debug:  # # Debug # # Ã‰tape: NORMALISATION/DECOUPLAGE
            out["debug"] = {  # # Bloc debug # # Ã‰tape: NORMALISATION/DECOUPLAGE
                "saved_to": results.get("saved_to"),  # # JSON # # Ã‰tape: NORMALISATION/DECOUPLAGE
                "bundle_html_file": results.get("bundle_html_file"),  # # HTML # # Ã‰tape: NORMALISATION/DECOUPLAGE
                "last_search_http": results.get("last_search_http"),  # # HTTP # # Ã‰tape: NORMALISATION/DECOUPLAGE
                "last_search_url": results.get("last_search_url"),  # # URL # # Ã‰tape: NORMALISATION/DECOUPLAGE
                "raw_cache_dir": results.get("raw_cache_dir"),  # # Dir # # Ã‰tape: NORMALISATION/DECOUPLAGE
            }
        return out  # # Retour # # Ã‰tape: NORMALISATION/DECOUPLAGE

    # 2) Context compact (anti-hallucination)
    context = _build_context(items, max_chars=14000)  # # Build context # # Ã‰tape: STRUCTURATION

    # 3) Prompt strict
    prompt = (  # # Prompt # # Ã‰tape: STRUCTURATION (contraintes)
        "Tu es un assistant de recherche.\n"
        "Tu dois rÃ©pondre UNIQUEMENT Ã  partir du CONTEXTE fourni.\n"
        "Si une info n'est pas dans le contexte, dis: \"Je ne peux pas l'affirmer avec ce contexte\".\n"
        "\n"
        "Format demandÃ©:\n"
        "1) RÃ©ponse courte (3-6 lignes)\n"
        "2) Points clÃ©s (5 bullets)\n"
        "3) Papiers citÃ©s (liste: arxiv_id + title)\n"
        "\n"
        f"QUESTION:\n{question}\n\n"
        f"CONTEXTE:\n{context}\n"
    )

    # 4) LLM
    try:  # # Try # # Ã‰tape: ROBUSTESSE
        answer = _ollama_generate(prompt=prompt, model=req.model)  # # Ollama # # Ã‰tape: ORCHESTRATION
    except Exception as e:  # # Catch # # Ã‰tape: ROBUSTESSE
        return {  # # Retour # # Ã‰tape: NORMALISATION/DECOUPLAGE
            "ok": False,  # # KO # # Ã‰tape: NORMALISATION/DECOUPLAGE
            "question": question,  # # Echo # # Ã‰tape: NORMALISATION/DECOUPLAGE
            "items": [],  # # Items # # Ã‰tape: NORMALISATION/DECOUPLAGE
            "errors": errors_global + [f"OLLAMA_EXCEPTION: {str(e)}"],  # # Erreurs # # Ã‰tape: NORMALISATION/DECOUPLAGE
        }

    # 5) RÃ©ponse API
    items_min = [  # # SimplifiÃ© # # Ã‰tape: NORMALISATION/DECOUPLAGE
        {"arxiv_id": it.get("arxiv_id", ""), "title": it.get("title", ""), "abs_url": it.get("abs_url", "")}  # # Champs # # Ã‰tape: NORMALISATION/DECOUPLAGE
        for it in items  # # Loop # # Ã‰tape: NORMALISATION/DECOUPLAGE
    ]

    out = {  # # Sortie # # Ã‰tape: NORMALISATION/DECOUPLAGE
        "ok": True,  # # OK # # Ã‰tape: NORMALISATION/DECOUPLAGE
        "question": question,  # # Echo # # Ã‰tape: NORMALISATION/DECOUPLAGE
        "theme": req.theme,  # # Theme # # Ã‰tape: NORMALISATION/DECOUPLAGE
        "query_used": results.get("query_used") or results.get("user_query") or question,  # # Trace # # Ã‰tape: NORMALISATION/DECOUPLAGE
        "count": len(items_min),  # # Count # # Ã‰tape: NORMALISATION/DECOUPLAGE
        "answer": answer,  # # Answer # # Ã‰tape: NORMALISATION/DECOUPLAGE
        "items": items_min,  # # Items # # Ã‰tape: NORMALISATION/DECOUPLAGE
        "errors": errors_global,  # # Erreurs tool (si warnings) # # Ã‰tape: NORMALISATION/DECOUPLAGE
    }

    if req.debug:  # # Debug # # Ã‰tape: NORMALISATION/DECOUPLAGE
        out["debug"] = {  # # Bloc # # Ã‰tape: NORMALISATION/DECOUPLAGE
            "saved_to": results.get("saved_to"),  # # JSON # # Ã‰tape: NORMALISATION/DECOUPLAGE
            "bundle_html_file": results.get("bundle_html_file"),  # # HTML # # Ã‰tape: NORMALISATION/DECOUPLAGE
            "last_search_http": results.get("last_search_http"),  # # HTTP # # Ã‰tape: NORMALISATION/DECOUPLAGE
            "last_search_url": results.get("last_search_url"),  # # URL # # Ã‰tape: NORMALISATION/DECOUPLAGE
            "raw_cache_dir": results.get("raw_cache_dir"),  # # Dir # # Ã‰tape: NORMALISATION/DECOUPLAGE
            "project_root": str(PROJECT_ROOT),  # # Root # # Ã‰tape: NORMALISATION/DECOUPLAGE
            "cwd": os.getcwd(),  # # CWD # # Ã‰tape: NORMALISATION/DECOUPLAGE
        }

    return out  # # Retour # # Ã‰tape: NORMALISATION/DECOUPLAGE

# ==========================
# End scrapping main
# ==========================


# ============================================================  # #
# ðŸ“§ Email endpoint (local MailHog)                              # #
# - POST /send-email                                             # #
# ============================================================  # #

from typing import Any, Dict, List, Optional  # # Typage # #
from pydantic import BaseModel, Field  # # Validation payload # #

try:  # # Import robuste du module email # #
    from module_Email.email_tool import send_conversation_email  # # Envoi email (local) # #
except Exception as e:  # # Si import Ã©choue # #
    send_conversation_email = None  # # Placeholder # #
    _EMAIL_IMPORT_ERROR = str(e)  # # DÃ©tail erreur # #


class ConversationMessage(BaseModel):  # # Un message de conversation # #
    role: str = Field(..., description="user|assistant")  # # RÃ´le # #
    content: str = Field(..., description="Contenu du message")  # # Texte # #
    timestamp: Optional[str] = Field(default=None, description="ISO timestamp optionnel")  # # Date optionnelle # #


class SendEmailRequest(BaseModel):  # # Payload du endpoint # #
    recipient_email: str = Field(..., description="Email destinataire")  # # Email # #
    conversation_history: List[ConversationMessage] = Field(..., description="Historique conversation")  # # Liste messages # #
    subject: Optional[str] = Field(default="Conversation DIXITBOT", description="Sujet email")  # # Sujet # #


@app.post("/send-email")  # # Endpoint POST # #
def send_email(req: SendEmailRequest) -> Dict[str, Any]:  # # Handler # #
    if send_conversation_email is None:  # # Si module non importÃ© # #
        return {  # # Retour KO # #
            "ok": False,  # # KO # #
            "error": "EMAIL_MODULE_IMPORT_ERROR",  # # Code erreur # #
            "detail": _EMAIL_IMPORT_ERROR,  # # DÃ©tail # #
        }

    payload = [m.model_dump() for m in req.conversation_history]  # # Convertit Pydantic -> dict # #
    return send_conversation_email(  # # Appelle lâ€™outil email # #
        recipient_email=req.recipient_email,  # # Destinataire # #
        conversation_history=payload,  # # Messages # #
        subject=req.subject or "Conversation DIXITBOT",  # # Sujet # #
    )
 
#-----------END EMAIL MAIL----------------------------