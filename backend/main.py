# D√©but scraping main
# ============================================================  # #
# üöÄ FastAPI Orchestrateur (Scraping arXiv + Qwen3 via Ollama)   # #
# - Objectif : exposer /health, /scrape/arxiv, /ask              # #
# - Robustesse : imports stables m√™me si uvicorn change le CWD   # #
# - ‚úÖ CORRECTION: on NE passe PAS debug_max_chars au scraper     # #
#   car scrape_arxiv_cs_scoped(...) ne le supporte pas            # #
# ============================================================  # #

# ===============================  # #
# üìö Imports standard              # #
# ===============================  # #
import os  # # Gestion chemins # # √âtape: NORMALISATION/DECOUPLAGE (chemins stables)
import sys  # # sys.path pour imports robustes # # √âtape: NORMALISATION/DECOUPLAGE (√©vite crash import)
import re  # # Nettoyage texte # # √âtape: NORMALISATION/DECOUPLAGE (prompt/context propre)
from pathlib import Path  # # Chemins robustes # # √âtape: NORMALISATION/DECOUPLAGE (Windows-friendly)
from typing import Any, Dict, List, Optional  # # Typage # # √âtape: NORMALISATION/DECOUPLAGE (contrat stable)

# ===============================  # #
# üöÄ FastAPI + Pydantic            # #
# ===============================  # #
from fastapi import FastAPI  # # Framework API # # √âtape: ORCHESTRATION (endpoints)
from pydantic import BaseModel, Field  # # Validation payload # # √âtape: ORCHESTRATION (contrat d‚Äôentr√©e)

# ===============================  # #
# üåê HTTP (Ollama)                 # #
# ===============================  # #
import requests  # # Appels HTTP # # √âtape: ORCHESTRATION (appel LLM local)

# ===============================  # #
# üß± Bootstrap chemins (IMPORTANT) # #
# ===============================  # #
_THIS_FILE = Path(__file__).resolve()  # # Chemin absolu du fichier # # √âtape: NORMALISATION/DECOUPLAGE (√©vite CWD)
_THIS_DIR = _THIS_FILE.parent  # # Dossier du main # # √âtape: NORMALISATION/DECOUPLAGE

# üëâ On suppose que le project root est soit le dossier courant, soit son parent (selon ton arborescence).
#    On force l‚Äôajout dans sys.path pour que `module_MCP_scraping...` soit importable m√™me sous uvicorn.
_CANDIDATES = [  # # Liste candidats racine # # √âtape: NORMALISATION/DECOUPLAGE
    _THIS_DIR,  # # Candidat 1: dossier main.py # # √âtape: NORMALISATION/DECOUPLAGE
    _THIS_DIR.parent,  # # Candidat 2: parent # # √âtape: NORMALISATION/DECOUPLAGE
]
PROJECT_ROOT = None  # # Init # # √âtape: NORMALISATION/DECOUPLAGE
for c in _CANDIDATES:  # # Boucle candidats # # √âtape: NORMALISATION/DECOUPLAGE
    if (c / "data_lake").exists() or (c / "module_MCP_scraping").exists():  # # Marqueurs projet # # √âtape: NORMALISATION/DECOUPLAGE
        PROJECT_ROOT = c  # # Fixe root # # √âtape: NORMALISATION/DECOUPLAGE
        break  # # Stop # # √âtape: NORMALISATION/DECOUPLAGE

if PROJECT_ROOT is None:  # # Si non trouv√© # # √âtape: NORMALISATION/DECOUPLAGE
    PROJECT_ROOT = _THIS_DIR  # # Fallback # # √âtape: NORMALISATION/DECOUPLAGE

if str(PROJECT_ROOT) not in sys.path:  # # Si pas d√©j√† dans sys.path # # √âtape: NORMALISATION/DECOUPLAGE
    sys.path.insert(0, str(PROJECT_ROOT))  # # Ajoute au d√©but # # √âtape: NORMALISATION/DECOUPLAGE

# ===============================  # #
# üï∑Ô∏è Import du scraper (robuste)  # #
# ===============================  # #
# ‚ö†Ô∏è Ici, on importe la fonction attendue par l‚ÄôAPI.
# Si ton module/nom diff√®re, adapte UNIQUEMENT la ligne import ci-dessous.
try:  # # Essai import # # √âtape: ROBUSTESSE (erreur claire)
    from module_MCP_scraping.scrapping import scrape_arxiv_cs_scoped  # # Import scraper # # √âtape: TOOL (scraping)
except Exception as e:  # # Si import √©choue # # √âtape: ROBUSTESSE
    scrape_arxiv_cs_scoped = None  # # Placeholder # # √âtape: ROBUSTESSE
    _SCRAPER_IMPORT_ERROR = str(e)  # # Stocke erreur # # √âtape: ROBUSTESSE

# ===============================  # #
# üß† FastAPI app                   # #
# ===============================  # #
app = FastAPI(  # # Cr√©e l‚Äôapp # # √âtape: ORCHESTRATION
    title="DIXITBOT API",  # # Titre swagger # # √âtape: ORCHESTRATION
    version="1.0.0",  # # Version # # √âtape: ORCHESTRATION
)

# ===============================  # #
# üßπ Helpers                       # #
# ===============================  # #
def _clean(s: str) -> str:  # # Nettoyage texte # # √âtape: NORMALISATION/DECOUPLAGE (input stable)
    s = (s or "").strip()  # # Trim # # √âtape: NORMALISATION/DECOUPLAGE
    s = re.sub(r"\s+", " ", s)  # # Espaces multiples -> 1 # # √âtape: NORMALISATION/DECOUPLAGE
    return s  # # Retour # # √âtape: NORMALISATION/DECOUPLAGE

def _ollama_generate(prompt: str, model: str) -> str:  # # Appel Ollama # # √âtape: ORCHESTRATION (LLM)
    url = "http://localhost:11434/api/generate"  # # Endpoint Ollama # # √âtape: ORCHESTRATION
    payload = {  # # JSON body # # √âtape: NORMALISATION/DECOUPLAGE (contrat stable)
        "model": model,  # # Mod√®le # # √âtape: ORCHESTRATION
        "prompt": prompt,  # # Prompt # # √âtape: ORCHESTRATION
        "stream": False,  # # Sans streaming # # √âtape: ORCHESTRATION
    }
    r = requests.post(url, json=payload, timeout=300)  # # POST # # √âtape: ROBUSTESSE (timeout)
    r.raise_for_status()  # # L√®ve si erreur HTTP # # √âtape: ROBUSTESSE
    data = r.json()  # # Parse JSON # # √âtape: NORMALISATION/DECOUPLAGE
    return (data.get("response") or "").strip()  # # Retour texte # # √âtape: NORMALISATION/DECOUPLAGE

def _build_context(items: List[Dict[str, Any]], max_chars: int = 14000) -> str:  # # Contexte compact # # √âtape: STRUCTURATION (anti-hallucination)
    chunks: List[str] = []  # # Blocs # # √âtape: STRUCTURATION
    total = 0  # # Compteur # # √âtape: STRUCTURATION
    for i, it in enumerate(items, start=1):  # # Parcours # # √âtape: STRUCTURATION
        block = (  # # Bloc papier # # √âtape: STRUCTURATION
            f"[PAPER {i}]\n"
            f"arxiv_id: {_clean(it.get('arxiv_id',''))}\n"
            f"title: {_clean(it.get('title',''))}\n"
            f"submitted_date: {_clean(it.get('submitted_date',''))}\n"
            f"abs_url: {_clean(it.get('abs_url',''))}\n"
            f"pdf_url: {_clean(it.get('pdf_url',''))}\n"
            f"doi: {_clean(it.get('doi',''))}\n"
            f"abstract: {_clean(it.get('abstract',''))}\n"
        )
        if total + len(block) > max_chars:  # # Stop si trop long # # √âtape: STRUCTURATION
            break  # # Sort # # √âtape: STRUCTURATION
        chunks.append(block)  # # Ajout # # √âtape: STRUCTURATION
        total += len(block)  # # Compte # # √âtape: STRUCTURATION
    return "\n".join(chunks)  # # Retour # # √âtape: STRUCTURATION

# ===============================  # #
# 0) Healthcheck                   # #
# ===============================  # #
@app.get("/health")  # # Endpoint health # # √âtape: ORCHESTRATION
def health() -> Dict[str, Any]:  # # Handler # # √âtape: ORCHESTRATION
    if scrape_arxiv_cs_scoped is None:  # # Si scraper non import√© # # √âtape: ROBUSTESSE
        return {  # # Retour # # √âtape: NORMALISATION/DECOUPLAGE
            "ok": False,  # # KO # # √âtape: NORMALISATION/DECOUPLAGE
            "service": "api",  # # Service # # √âtape: NORMALISATION/DECOUPLAGE
            "scraper_import_ok": False,  # # Flag # # √âtape: NORMALISATION/DECOUPLAGE
            "scraper_error": _SCRAPER_IMPORT_ERROR,  # # D√©tail # # √âtape: ROBUSTESSE
            "project_root": str(PROJECT_ROOT),  # # Debug # # √âtape: NORMALISATION/DECOUPLAGE
            "cwd": os.getcwd(),  # # Debug # # √âtape: NORMALISATION/DECOUPLAGE
        }
    return {  # # OK # # √âtape: NORMALISATION/DECOUPLAGE
        "ok": True,  # # OK # # √âtape: NORMALISATION/DECOUPLAGE
        "service": "api",  # # Service # # √âtape: NORMALISATION/DECOUPLAGE
        "scraper_import_ok": True,  # # Flag # # √âtape: NORMALISATION/DECOUPLAGE
        "project_root": str(PROJECT_ROOT),  # # Debug # # √âtape: NORMALISATION/DECOUPLAGE
        "cwd": os.getcwd(),  # # Debug # # √âtape: NORMALISATION/DECOUPLAGE
    }

# ===============================  # #
# 1) /scrape/arxiv                 # #
# ===============================  # #
class ArxivScrapeRequest(BaseModel):  # # Input scraping # # √âtape: NORMALISATION/DECOUPLAGE (contrat d‚Äôentr√©e)
    query: str = Field(..., description="Texte de recherche (keywords)")  # # Query # # √âtape: NORMALISATION/DECOUPLAGE
    theme: Optional[str] = Field(default=None, description="ai_ml|algo_ds|net_sys|cyber_crypto|pl_se|hci_data")  # # Theme # # √âtape: NORMALISATION/DECOUPLAGE
    max_results: int = Field(default=20, ge=1, le=100)  # # Limite # # √âtape: NORMALISATION/DECOUPLAGE
    sort: str = Field(default="relevance", description="relevance|submitted_date")  # # Tri # # √âtape: NORMALISATION/DECOUPLAGE
    # ‚úÖ CORRECTION: on retire debug_max_chars ici car le scraper ne l‚Äôaccepte pas

@app.post("/scrape/arxiv")  # # Endpoint scrape # # √âtape: ORCHESTRATION
def scrape_arxiv(req: ArxivScrapeRequest) -> Dict[str, Any]:  # # Handler # # √âtape: ORCHESTRATION
    if scrape_arxiv_cs_scoped is None:  # # Si scraper indisponible # # √âtape: ROBUSTESSE
        return {  # # Retour # # √âtape: NORMALISATION/DECOUPLAGE
            "ok": False,  # # KO # # √âtape: NORMALISATION/DECOUPLAGE
            "errors": [f"SCRAPER_IMPORT_ERROR: {_SCRAPER_IMPORT_ERROR}"],  # # Liste erreurs # # √âtape: NORMALISATION/DECOUPLAGE (contrat stable)
            "items": [],  # # Items vide # # √âtape: NORMALISATION/DECOUPLAGE
        }

    try:  # # Try # # √âtape: ROBUSTESSE
        # ‚úÖ data_lake_raw_dir relatif => ton scraper le rend ABSOLU (et √©crit dans le projet)
        result = scrape_arxiv_cs_scoped(  # # Appel tool # # √âtape: TOOL (scraping)
            user_query=req.query,  # # Query # # √âtape: TOOL
            theme=req.theme,  # # Theme # # √âtape: TOOL
            max_results=req.max_results,  # # Limite # # √âtape: TOOL
            sort=req.sort,  # # Tri # # √âtape: TOOL
            data_lake_raw_dir="data_lake/raw/cache",  # # Cache raw # # √âtape: NORMALISATION/DECOUPLAGE (sortie pr√©visible)
            enrich_abs=True,  # # Enrich /abs # # √âtape: TOOL
            enable_keyword_filter=True,  # # Filtre fallback # # √âtape: TOOL
            # ‚úÖ CORRECTION: debug_max_chars supprim√© (sinon "unexpected keyword argument")
        )
        return result  # # Retour direct (contrat tool) # # √âtape: NORMALISATION/DECOUPLAGE
    except Exception as e:  # # Catch # # √âtape: ROBUSTESSE
        return {  # # Retour erreur # # √âtape: NORMALISATION/DECOUPLAGE
            "ok": False,  # # KO # # √âtape: NORMALISATION/DECOUPLAGE
            "errors": [f"SCRAPE_EXCEPTION: {str(e)}"],  # # Erreurs globales # # √âtape: NORMALISATION/DECOUPLAGE
            "items": [],  # # Items vide # # √âtape: NORMALISATION/DECOUPLAGE
        }

# ===============================  # #
# 2) /ask (scrape -> context -> LLM)# #
# ===============================  # #
class AskRequest(BaseModel):  # # Input ask # # √âtape: NORMALISATION/DECOUPLAGE
    question: str = Field(..., description="Question utilisateur")  # # Question # # √âtape: NORMALISATION/DECOUPLAGE
    theme: Optional[str] = Field(default="ai_ml", description="Th√®me arXiv CS")  # # Theme # # √âtape: NORMALISATION/DECOUPLAGE
    max_results: int = Field(default=3, ge=1, le=10)  # # Papiers # # √âtape: NORMALISATION/DECOUPLAGE
    sort: str = Field(default="relevance", description="relevance|submitted_date")  # # Tri # # √âtape: NORMALISATION/DECOUPLAGE
    model: str = Field(default="qwen3:1.7b", description="Mod√®le Ollama")  # # Model # # √âtape: NORMALISATION/DECOUPLAGE
    debug: bool = Field(default=False, description="Renvoie infos debug")  # # Debug # # √âtape: NORMALISATION/DECOUPLAGE

@app.post("/ask")  # # Endpoint QA # # √âtape: ORCHESTRATION
def ask(req: AskRequest) -> Dict[str, Any]:  # # Handler # # √âtape: ORCHESTRATION
    if scrape_arxiv_cs_scoped is None:  # # Si scraper indispo # # √âtape: ROBUSTESSE
        return {  # # Retour # # √âtape: NORMALISATION/DECOUPLAGE
            "ok": False,  # # KO # # √âtape: NORMALISATION/DECOUPLAGE
            "errors": [f"SCRAPER_IMPORT_ERROR: {_SCRAPER_IMPORT_ERROR}"],  # # Erreurs # # √âtape: NORMALISATION/DECOUPLAGE
            "items": [],  # # Items # # √âtape: NORMALISATION/DECOUPLAGE
        }

    question = _clean(req.question)  # # Nettoie # # √âtape: NORMALISATION/DECOUPLAGE
    if not question:  # # Vide # # √âtape: NORMALISATION/DECOUPLAGE
        return {"ok": False, "errors": ["EMPTY_QUESTION"], "items": []}  # # Contrat stable # # √âtape: NORMALISATION/DECOUPLAGE

    # 1) Scraping
    try:  # # Try # # √âtape: ROBUSTESSE
        results = scrape_arxiv_cs_scoped(  # # Tool # # √âtape: TOOL
            user_query=question,  # # Query # # √âtape: TOOL
            theme=req.theme,  # # Theme # # √âtape: TOOL
            max_results=req.max_results,  # # Limite # # √âtape: TOOL
            sort=req.sort,  # # Tri # # √âtape: TOOL
            data_lake_raw_dir="data_lake/raw/cache",  # # Cache # # √âtape: NORMALISATION/DECOUPLAGE
            enrich_abs=True,  # # Enrich # # √âtape: TOOL
            enable_keyword_filter=True,  # # Filtre # # √âtape: TOOL
            # ‚úÖ CORRECTION: debug_max_chars supprim√©
        )
    except Exception as e:  # # Catch # # √âtape: ROBUSTESSE
        return {"ok": False, "errors": [f"SCRAPE_EXCEPTION: {str(e)}"], "items": []}  # # Contrat stable # # √âtape: NORMALISATION/DECOUPLAGE

    items = results.get("items") or []  # # Items # # √âtape: NORMALISATION/DECOUPLAGE
    errors_global = results.get("errors") or []  # # Erreurs tool # # √âtape: NORMALISATION/DECOUPLAGE

    if not items:  # # Si rien # # √âtape: ROBUSTESSE
        out = {  # # Sortie stable # # √âtape: NORMALISATION/DECOUPLAGE
            "ok": False if errors_global else True,  # # KO si erreurs tool # # √âtape: NORMALISATION/DECOUPLAGE
            "question": question,  # # Echo # # √âtape: NORMALISATION/DECOUPLAGE
            "answer": "Aucun papier trouv√© (ou parsing impossible). Regarde le bundle HTML.",  # # Message # # √âtape: NORMALISATION/DECOUPLAGE
            "items": [],  # # Items vide # # √âtape: NORMALISATION/DECOUPLAGE
            "errors": errors_global,  # # Erreurs globales # # √âtape: NORMALISATION/DECOUPLAGE
        }
        if req.debug:  # # Debug # # √âtape: NORMALISATION/DECOUPLAGE
            out["debug"] = {  # # Bloc debug # # √âtape: NORMALISATION/DECOUPLAGE
                "saved_to": results.get("saved_to"),  # # JSON # # √âtape: NORMALISATION/DECOUPLAGE
                "bundle_html_file": results.get("bundle_html_file"),  # # HTML # # √âtape: NORMALISATION/DECOUPLAGE
                "last_search_http": results.get("last_search_http"),  # # HTTP # # √âtape: NORMALISATION/DECOUPLAGE
                "last_search_url": results.get("last_search_url"),  # # URL # # √âtape: NORMALISATION/DECOUPLAGE
                "raw_cache_dir": results.get("raw_cache_dir"),  # # Dir # # √âtape: NORMALISATION/DECOUPLAGE
            }
        return out  # # Retour # # √âtape: NORMALISATION/DECOUPLAGE

    # 2) Context compact (anti-hallucination)
    context = _build_context(items, max_chars=14000)  # # Build context # # √âtape: STRUCTURATION

    # 3) Prompt strict
    prompt = (  # # Prompt # # √âtape: STRUCTURATION (contraintes)
        "Tu es un assistant de recherche.\n"
        "Tu dois r√©pondre UNIQUEMENT √† partir du CONTEXTE fourni.\n"
        "Si une info n'est pas dans le contexte, dis: \"Je ne peux pas l'affirmer avec ce contexte\".\n"
        "\n"
        "Format demand√©:\n"
        "1) R√©ponse courte (3-6 lignes)\n"
        "2) Points cl√©s (5 bullets)\n"
        "3) Papiers cit√©s (liste: arxiv_id + title)\n"
        "\n"
        f"QUESTION:\n{question}\n\n"
        f"CONTEXTE:\n{context}\n"
    )

    # 4) LLM
    try:  # # Try # # √âtape: ROBUSTESSE
        answer = _ollama_generate(prompt=prompt, model=req.model)  # # Ollama # # √âtape: ORCHESTRATION
    except Exception as e:  # # Catch # # √âtape: ROBUSTESSE
        return {  # # Retour # # √âtape: NORMALISATION/DECOUPLAGE
            "ok": False,  # # KO # # √âtape: NORMALISATION/DECOUPLAGE
            "question": question,  # # Echo # # √âtape: NORMALISATION/DECOUPLAGE
            "items": [],  # # Items # # √âtape: NORMALISATION/DECOUPLAGE
            "errors": errors_global + [f"OLLAMA_EXCEPTION: {str(e)}"],  # # Erreurs # # √âtape: NORMALISATION/DECOUPLAGE
        }

    # 5) R√©ponse API
    items_min = [  # # Simplifi√© # # √âtape: NORMALISATION/DECOUPLAGE
        {"arxiv_id": it.get("arxiv_id", ""), "title": it.get("title", ""), "abs_url": it.get("abs_url", "")}  # # Champs # # √âtape: NORMALISATION/DECOUPLAGE
        for it in items  # # Loop # # √âtape: NORMALISATION/DECOUPLAGE
    ]

    out = {  # # Sortie # # √âtape: NORMALISATION/DECOUPLAGE
        "ok": True,  # # OK # # √âtape: NORMALISATION/DECOUPLAGE
        "question": question,  # # Echo # # √âtape: NORMALISATION/DECOUPLAGE
        "theme": req.theme,  # # Theme # # √âtape: NORMALISATION/DECOUPLAGE
        "query_used": results.get("query_used") or results.get("user_query") or question,  # # Trace # # √âtape: NORMALISATION/DECOUPLAGE
        "count": len(items_min),  # # Count # # √âtape: NORMALISATION/DECOUPLAGE
        "answer": answer,  # # Answer # # √âtape: NORMALISATION/DECOUPLAGE
        "items": items_min,  # # Items # # √âtape: NORMALISATION/DECOUPLAGE
        "errors": errors_global,  # # Erreurs tool (si warnings) # # √âtape: NORMALISATION/DECOUPLAGE
    }

    if req.debug:  # # Debug # # √âtape: NORMALISATION/DECOUPLAGE
        out["debug"] = {  # # Bloc # # √âtape: NORMALISATION/DECOUPLAGE
            "saved_to": results.get("saved_to"),  # # JSON # # √âtape: NORMALISATION/DECOUPLAGE
            "bundle_html_file": results.get("bundle_html_file"),  # # HTML # # √âtape: NORMALISATION/DECOUPLAGE
            "last_search_http": results.get("last_search_http"),  # # HTTP # # √âtape: NORMALISATION/DECOUPLAGE
            "last_search_url": results.get("last_search_url"),  # # URL # # √âtape: NORMALISATION/DECOUPLAGE
            "raw_cache_dir": results.get("raw_cache_dir"),  # # Dir # # √âtape: NORMALISATION/DECOUPLAGE
            "project_root": str(PROJECT_ROOT),  # # Root # # √âtape: NORMALISATION/DECOUPLAGE
            "cwd": os.getcwd(),  # # CWD # # √âtape: NORMALISATION/DECOUPLAGE
        }

    return out  # # Retour # # √âtape: NORMALISATION/DECOUPLAGE

# ==========================
# End scrapping main
# ==========================
