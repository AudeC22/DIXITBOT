# ============================================================  # # ğŸ“Œ SÃ©parateur visuel (lisibilitÃ©)
# ğŸ“š Documentation Navigator (RAG simplifiÃ©)                     # # ğŸ¯ Objectif : chercher un extrait de doc (local) par "sens"
# âœ… TF-IDF + cosine similarity (scikit-learn)                   # # ğŸ§  Recherche sÃ©mantique simplifiÃ©e, sans base vectorielle
# ============================================================  # # ğŸ“Œ SÃ©parateur visuel (lisibilitÃ©)

# ==============================  # # ğŸ“Œ DÃ©but imports
# ğŸ“š Importation des bibliothÃ¨ques  # # ğŸ§  Modules nÃ©cessaires au moteur de recherche
# ==============================  # # ğŸ“Œ SÃ©parateur

from typing import List, Dict, Any, Optional, Tuple  # # ğŸ§© Typage : rend le code plus clair et robuste
import re  # # ğŸ” Regex : pour nettoyer la requÃªte (optionnel mais utile)

from sklearn.feature_extraction.text import TfidfVectorizer  # # ğŸ§  TF-IDF : transforme texte -> vecteurs
from sklearn.metrics.pairwise import cosine_similarity  # # ğŸ“ SimilaritÃ© cosinus : mesure proximitÃ© entre vecteurs

# ==============================  # # ğŸ“Œ SÃ©parateur
# ğŸ§± Base documentaire locale (mini dataset)  # # âœ… Simule une doc type PyTorch / Scikit-learn
# ==============================  # # ğŸ“Œ SÃ©parateur

DOCS_DB: List[Dict[str, Any]] = [  # # ğŸ“¦ Liste de dictionnaires : â€œbase de donnÃ©esâ€ simple
    {  # # ğŸ§¾ EntrÃ©e 1 (PyTorch)
        "library": "pytorch",  # # ğŸ·ï¸ BibliothÃ¨que : sert Ã  filtrer (option library)
        "name": "torch.nn.Linear",  # # ğŸ·ï¸ Nom de la fonction / classe
        "signature": "torch.nn.Linear(in_features, out_features, bias=True)",  # # ğŸ§¾ Signature pour contexte
        "description": (  # # ğŸ§¾ Description textuelle (servira au TF-IDF)
            "Couche linÃ©aire (affine) : y = xA^T + b. "
            "UtilisÃ©e dans les rÃ©seaux fully-connected. "
            "ParamÃ¨tres : in_features, out_features, bias."
        ),
    },
    {  # # ğŸ§¾ EntrÃ©e 2 (PyTorch)
        "library": "pytorch",  # # ğŸ·ï¸ BibliothÃ¨que
        "name": "torch.nn.CrossEntropyLoss",  # # ğŸ·ï¸ Nom
        "signature": "torch.nn.CrossEntropyLoss(weight=None, ignore_index=-100, reduction='mean')",  # # ğŸ§¾ Signature
        "description": (  # # ğŸ§¾ Description
            "Fonction de perte pour classification multi-classes. "
            "Combine log-softmax et negative log-likelihood. "
            "Attendu : logits (non normalisÃ©s) et labels entiers."
        ),
    },
    {  # # ğŸ§¾ EntrÃ©e 3 (Scikit-learn)
        "library": "scikit-learn",  # # ğŸ·ï¸ BibliothÃ¨que
        "name": "sklearn.model_selection.train_test_split",  # # ğŸ·ï¸ Nom
        "signature": "train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True)",  # # ğŸ§¾ Signature
        "description": (  # # ğŸ§¾ Description
            "SÃ©pare des tableaux en ensembles d'entraÃ®nement et de test. "
            "ParamÃ¨tres importants : test_size, random_state, shuffle. "
            "Renvoie X_train, X_test, y_train, y_test, etc."
        ),
    },
    {  # # ğŸ§¾ EntrÃ©e 4 (Scikit-learn)
        "library": "scikit-learn",  # # ğŸ·ï¸ BibliothÃ¨que
        "name": "sklearn.feature_extraction.text.TfidfVectorizer",  # # ğŸ·ï¸ Nom
        "signature": "TfidfVectorizer(analyzer='word', ngram_range=(1,1), stop_words=None)",  # # ğŸ§¾ Signature
        "description": (  # # ğŸ§¾ Description
            "Convertit une collection de documents texte en matrice TF-IDF. "
            "Utile pour la recherche d'information et le text mining. "
            "Options : stop_words, ngram_range, min_df, max_df."
        ),
    },
]  # # âœ… Fin dataset

# ==============================  # # ğŸ“Œ SÃ©parateur
# ğŸ§¼ Utilitaires (nettoyage + normalisation)  # # ğŸ¯ AmÃ©liore un peu la robustesse
# ==============================  # # ğŸ“Œ SÃ©parateur

def _normalize_text(text: str) -> str:  # # ğŸ§¼ Nettoie une chaÃ®ne pour stabiliser la recherche
    text = text or ""  # # âœ… Ã‰vite None : si text est None -> ""
    text = text.strip()  # # ğŸ§¹ EnlÃ¨ve espaces dÃ©but/fin
    text = re.sub(r"\s+", " ", text)  # # ğŸ§½ Remplace multi-espaces / retours lignes -> 1 espace
    return text  # # ğŸ“¤ Renvoie texte normalisÃ©

def _build_corpus(docs: List[Dict[str, Any]]) -> List[str]:  # # ğŸ§  Construit le texte qui sera vectorisÃ©
    corpus: List[str] = []  # # ğŸ“¦ Liste des documents (texte)
    for d in docs:  # # ğŸ” Parcourt chaque entrÃ©e doc
        lib = _normalize_text(str(d.get("library", "")))  # # ğŸ·ï¸ RÃ©cupÃ¨re bibliothÃ¨que
        name = _normalize_text(str(d.get("name", "")))  # # ğŸ·ï¸ RÃ©cupÃ¨re nom
        sig = _normalize_text(str(d.get("signature", "")))  # # ğŸ§¾ RÃ©cupÃ¨re signature
        desc = _normalize_text(str(d.get("description", "")))  # # ğŸ§¾ RÃ©cupÃ¨re description
        merged = f"{lib} {name} {sig} {desc}"  # # ğŸ”— Fusion : on met tout dans un seul â€œdocumentâ€ texte
        corpus.append(merged)  # # â• Ajoute au corpus
    return corpus  # # ğŸ“¤ Renvoie corpus texte

# ==============================  # # ğŸ“Œ SÃ©parateur
# ğŸ§  Moteur â€œRAG simplifiÃ©â€ : TF-IDF + cosine similarity  # # ğŸ¯ Trouver l'extrait le plus pertinent
# ==============================  # # ğŸ“Œ SÃ©parateur

def lookup_docs(query: str, library: Optional[str] = None) -> Dict[str, Any]:  # # ğŸš€ Fonction principale demandÃ©e
    query_norm = _normalize_text(query)  # # ğŸ§¼ Normalise la requÃªte utilisateur
    if query_norm == "":  # # ğŸš« Si requÃªte vide
        return {  # # ğŸ“¤ Retourne une rÃ©ponse propre (pas d'exception)
            "ok": False,  # # âŒ Indique Ã©chec (requÃªte vide)
            "error": "query_empty",  # # ğŸ§¾ Code erreur
            "message": "La requÃªte est vide. Donne une question ou un mot-clÃ©.",  # # ğŸ—£ï¸ Message clair
            "match": None,  # # ğŸ§¾ Pas de match
            "confidence": 0.0,  # # ğŸ“‰ Score nul
        }  # # âœ… Fin retour

    # --- Filtrage par bibliothÃ¨que (optionnel) ---  # # ğŸ§  Respecte le paramÃ¨tre library si donnÃ©
    docs = DOCS_DB  # # ğŸ“¦ Par dÃ©faut : on cherche dans toute la base
    if library is not None and _normalize_text(library) != "":  # # âœ… Si un filtre library est demandÃ©
        lib_norm = _normalize_text(library).lower()  # # ğŸ§¼ Normalise la bibliothÃ¨que (minuscule)
        docs = [d for d in DOCS_DB if _normalize_text(str(d.get("library", ""))).lower() == lib_norm]  # # ğŸ” Filtre strict

    if len(docs) == 0:  # # ğŸš« Si aucune doc ne correspond Ã  la bibliothÃ¨que
        return {  # # ğŸ“¤ RÃ©ponse propre
            "ok": False,  # # âŒ
            "error": "library_not_found",  # # ğŸ§¾
            "message": f"Aucune documentation trouvÃ©e pour library='{library}'.",  # # ğŸ—£ï¸
            "match": None,  # # ğŸ§¾
            "confidence": 0.0,  # # ğŸ“‰
        }  # # âœ…

    # --- Vectorisation TF-IDF ---  # # ğŸ§  Transforme texte -> vecteurs pour comparer â€œpar sensâ€
    corpus = _build_corpus(docs)  # # ğŸ“š Construit corpus texte depuis docs filtrÃ©es
    vectorizer = TfidfVectorizer(stop_words=None)  # # ğŸ§  Vectorizer (simple, dÃ©butant-friendly)
    X = vectorizer.fit_transform(corpus)  # # âœ… Matrice TF-IDF (docs)
    q_vec = vectorizer.transform([query_norm])  # # âœ… Vecteur TF-IDF de la requÃªte

    # --- SimilaritÃ© cosinus ---  # # ğŸ“ Compare la requÃªte Ã  chaque doc
    sims = cosine_similarity(q_vec, X)[0]  # # ğŸ“Š sims = tableau 1D (score par doc)
    best_idx = int(sims.argmax())  # # ğŸ¥‡ Index du meilleur score
    best_score = float(sims[best_idx])  # # ğŸ¯ Score du meilleur match (0..1 approximatif)
    best_doc = docs[best_idx]  # # ğŸ“Œ Doc correspondante

    # --- Score de confiance (simple) ---  # # ğŸ§  InterprÃ©tation pÃ©dagogique
    # âš ï¸ TF-IDF n'est pas une â€œvraieâ€ sÃ©mantique profonde : le score reste indicatif.  # # ğŸ“ Note
    confidence = best_score  # # âœ… On expose directement le cosine score comme â€œconfidenceâ€

    # --- Retour structurÃ© ---  # # ğŸ“¦ Format clair pour FastAPI / MCP / LLM
    return {  # # ğŸ“¤ RÃ©sultat final
        "ok": True,  # # âœ… SuccÃ¨s
        "query": query_norm,  # # ğŸ” RequÃªte normalisÃ©e
        "library_filter": _normalize_text(library) if library is not None else None,  # # ğŸ·ï¸ Filtre appliquÃ© (ou None)
        "match": {  # # ğŸ§¾ Extrait trouvÃ©
            "library": best_doc.get("library", ""),  # # ğŸ·ï¸ BibliothÃ¨que
            "name": best_doc.get("name", ""),  # # ğŸ·ï¸ Nom
            "signature": best_doc.get("signature", ""),  # # ğŸ§¾ Signature
            "description": best_doc.get("description", ""),  # # ğŸ§¾ Description
        },
        "confidence": round(confidence, 4),  # # ğŸ“ˆ Score arrondi pour lisibilitÃ©
    }  # # âœ… Fin retour

# ==============================  # # ğŸ“Œ SÃ©parateur
# ğŸ§ª Test local (dÃ©sactivable en 1 ligne)  # # âœ… Permet de tester immÃ©diatement
# ==============================  # # ğŸ“Œ SÃ©parateur

RUN_LOCAL_TEST = True  # # âœ… Mets True pour tester | mets False pour couper (attention: False avec F majuscule)

if __name__ == "__main__" and RUN_LOCAL_TEST:  # # â–¶ï¸ ExÃ©cute uniquement si lancÃ© en script direct
    print("ğŸš€ Test Documentation Navigator (RAG simplifiÃ©)")  # # ğŸ–¨ï¸ Log
    r1 = lookup_docs("comment sÃ©parer mon dataset en train et test ?", library="scikit-learn")  # # ğŸ” Test 1
    print(r1)  # # ğŸ–¨ï¸ Affiche rÃ©sultat
    r2 = lookup_docs("quelle loss pour classification multi classe avec logits ?", library="pytorch")  # # ğŸ” Test 2
    print(r2)  # # ğŸ–¨ï¸ Affiche rÃ©sultat
    r3 = lookup_docs("vectorisation tf idf pour texte")  # # ğŸ” Test 3 (sans filtre library)
    print(r3)  # # ğŸ–¨ï¸ Affiche rÃ©sultat
